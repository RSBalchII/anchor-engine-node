# ece/agents/tier1/orchestrator/orchestrator_agent.py
"""
Orchestrator Agent for the External Context Engine (ECE) v2.0 (Async).
This version has a temporarily simplified router for the bootstrapping process.
"""
import os
import httpx
import asyncio
import yaml
import traceback
from typing import List, Dict, Any
from xml.etree.ElementTree import ElementTree as ET  # <-- THIS LINE IS NOW CORRECT

# Import other agents and modules from the ECE structure
from ece.agents.tier2.conversational_agent import ConversationalAgent
from ece.agents.tier2.explorer_agent import ExplorerAgent
from ece.agents.tier2.critique_agent import CritiqueAgent
from ece.agents.tier2.web_search_agent import WebSearchAgent
from ece.common.sandbox import run_code_in_sandbox

# --- Thinker Classes ---
class BaseThinker:
    """A base class for a specialized reasoning agent."""
    def __init__(self, name: str = "Default", model: str = None):
        self.name = name
        self.model = model
        self.ollama_endpoint = os.getenv("OLLAMA_API_BASE_URL", "http://host.docker.internal:11434/api/chat")
        self.system_prompt = f"You are a helpful AI assistant acting as the '{self.name}' Thinker. Provide a concise analysis from this specific perspective."

    async def think(self, prompt: str) -> str:
        """Generates a thought or analysis based on the prompt."""
        print(f"  -> {self.name} Thinker processing with model {self.model}...")
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(self.ollama_endpoint, json=payload)
                response.raise_for_status()
                data = response.json()
                content = data.get('message', {}).get('content', '')
                return f"<poml><perspective thinker='{self.name}'><analysis>{content}</analysis></perspective>"
        except httpx.RequestError as e:
            error_message = f"HTTP request failed: {e.__class__.__name__} - {e}"
            print(f"Error in {self.name} Thinker: {error_message}")
            return f"<poml><perspective thinker='{self.name}'><analysis>Error: {error_message}</analysis></perspective>"
        except Exception as e:
            error_message = f"An unexpected error occurred: {e}"
            print(f"Error in {self.name} Thinker: {error_message}")
            return f"<poml><perspective thinker='{self.name}'><analysis>Error: {error_message}</analysis></perspective>"

class SynthesisThinker(BaseThinker):
    """A specialized thinker for synthesizing multiple perspectives."""
    def __init__(self, name: str = "Synthesis", model: str = None):
        super().__init__(name, model)
        self.system_prompt = "You are a master synthesizer. Your job is to take multiple, diverse perspectives on a topic and combine them into a single, coherent, and easy-to-read final analysis."

    async def think(self, prompt: str) -> str:
        """Generates a synthesized analysis."""
        print(f"  -> {self.name} Thinker processing with model {self.model}...")
        # In a real implementation, this would have logic to process multiple inputs.
        # For now, it will behave like a standard BaseThinker but return raw text.
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(self.ollama_endpoint, json=payload)
                response.raise_for_status()
                data = response.json()
                return data.get('message', {}).get('content', '')
        except Exception as e:
            print(f"Error in {self.name} Thinker: {e}")
            return f"Error during synthesis: {e}"

def get_all_thinkers(config: Dict[str, Any]) -> List[BaseThinker]:
    """Factory function to create all configured Thinker agents."""
    model = config.get('agents', {}).get('ThinkerAgent', {}).get('default_model')
    thinker_names = ["Optimist", "Pessimist", "Creative", "Analytical", "Pragmatic"]
    return [BaseThinker(name, model=model) for name in thinker_names]

class OrchestratorAgent:
    """The central coordinating agent for the ECE."""
    def __init__(self):
        with open('config.yaml', 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.client = httpx.AsyncClient(timeout=60.0)
        self.thinkers = get_all_thinkers(self.config)
        synthesis_model = self.config['agents']['SynthesisThinker']['default_model']
        self.synthesis_thinker = SynthesisThinker(model=synthesis_model)

        # Initialize all agents based on configuration
        self.conversational_agent = ConversationalAgent(model=self.config['agents']['ThinkerAgent']['default_model'])
        self.explorer_agent = ExplorerAgent(model=self.config['agents']['ExplorerAgent']['default_model'])
        self.critique_agent = CritiqueAgent(model=self.config['agents']['CritiqueAgent']['default_model'], success_threshold=self.config['agents']['CritiqueAgent']['success_threshold'])
        self.web_search_agent = WebSearchAgent()
        self.distiller_agent = ConversationalAgent(model=self.config['agents']['DistillerAgent']['default_model'])

    def _route_prompt(self, prompt: str) -> str:
        """
        TEMPORARY BOOTSTRAP ROUTER: This logic is simplified for the one-time corpus ingestion.
        It ONLY looks for the 'BOOTSTRAP_DISTILL:' command.
        """
        prompt_lower = prompt.lower().strip()
        
        if prompt_lower.startswith("bootstrap_distill:"):
            print(f"Routing based on explicit 'BOOTSTRAP_DISTILL' command -> DistillerAgent")
            return "DistillerAgent"
            
        print(f"Defaulting to ConversationalAgent")
        return "ConversationalAgent"

    async def process_prompt(self, prompt: str) -> str:
        """
        Processes the prompt with comprehensive error logging.
        """
        print(f"Orchestrator processing prompt: '{prompt[:100]}...'")
        
        try:
            target_agent_name = self._route_prompt(prompt)
            print(f"Routing to: {target_agent_name}")

            if target_agent_name == "DistillerAgent":
                return await self.distiller_agent.respond(prompt)
            else:
                return await self.conversational_agent.respond(prompt)

        except Exception as e:
            print(f"n--- [!!!] ECE INTERNAL ERROR ---")
            traceback.print_exc()
            print(f"--- [!!!] END OF ERROR --- \n")
            raise e

    # --- Placeholder methods for other workflows ---
    
    async def _execute_parallel_thinking(self, prompt: str) -> List[str]:
        # This is a placeholder for the parallel thinking workflow logic
        pass

    async def _synthesize_parallel_response(self, prompt: str, parallel_results: List[str]) -> str:
        # This is a placeholder for the synthesis logic
        pass

    async def _execute_exploratory_problem_solving(self, prompt: str, max_iterations: int = 5) -> str:
        # This is a placeholder for the exploratory problem-solving loop
        pass

    def _extract_code_from_explorer_response(self, response: str) -> str:
        # This is a placeholder for code extraction logic
        pass

    def _extract_score_from_critique(self, critique_response: str) -> float:
        # This is a placeholder for score extraction logic
        pass