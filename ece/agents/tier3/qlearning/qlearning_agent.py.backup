import asyncio
import logging
import random
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GraphState:
    """Represents a state in the knowledge graph"""
    node_id: str
    node_name: str
    node_type: str = "Concept"
    features: Dict[str, Any] = None


@dataclass
class Action:
    """Represents an action (edge traversal) in the graph"""
    from_node: str
    to_node: str
    relationship_type: str
    weight: float = 1.0

class MemoryPath(BaseModel):
    """Represents a path through the knowledge graph"""
    nodes: List[str] = Field(default_factory=list, description="Node names in the path")
    relationships: List[Dict[str, Any]] = Field(default_factory=list, description="Relationships in the path")
    score: float = Field(default=0.0, description="Path relevance score")
    length: int = Field(default=0, description="Path length (number of hops)")
    context_summary: str = Field(default="", description="Summary of context along this path")
    token_count: int = Field(default=0, description="Estimated token count for this path")


class QLearningGraphAgent:
    """
    Q-Learning agent for intelligent navigation of the Neo4j knowledge graph.
    
    This agent is used by the Archivist to find optimal paths between concepts
    using a Reinforcement Learning approach.
    """
    
    def __init__(self, graph_manager=None, config=None):
        """
        Initialize the QLearningGraphAgent.
        
        Args:
            graph_manager: Manager for interacting with the Neo4j database
            config: Configuration parameters for the agent
        """
        self.graph_manager = graph_manager
        self.config = config or {}
        self.q_table = {}  # In-memory Q-table for state-action values
        self.learning_rate = self.config.get('learning_rate', 0.1)
        self.discount_factor = self.config.get('discount_factor', 0.9)
        self.epsilon = self.config.get('epsilon', 0.1)  # For epsilon-greedy exploration
        self.is_training = False
        self.training_task = None  # Background training task
        self.training_interval = self.config.get('training_interval', 300)  # 5 minutes default
        
        # Connect to the graph manager if provided
        if self.graph_manager:
            try:
                self.graph_manager.connect()
            except Exception as e:
                logger.error(f"Failed to connect to graph manager: {e}")
                
    async def find_optimal_path(self, keywords: List[str], max_tokens: int = 1000000) -> List[MemoryPath]:
        """
        Find optimal paths related to a list of keywords, respecting token limits.

        Args:
            keywords: A list of keywords to search for.
            max_tokens: Maximum number of tokens to retrieve (default 1M)
            
        Returns:
            List of MemoryPath objects ranked by Q-values
        """
        logger.info(f"Finding optimal paths for keywords: {keywords}")
        logger.info(f"Max tokens: {max_tokens}")
        
        if not self.graph_manager:
            logger.warning("No graph manager available")
            return []

        # Find nodes related to the keywords
        nodes = await self.graph_manager.find_nodes_by_keywords(keywords)
        
        if not nodes:
            logger.info("No nodes found for the given keywords.")
            return []

        # For simplicity, find paths between all pairs of found nodes
        paths = []
        total_tokens = 0
        
        for i in range(len(nodes)):
            if total_tokens >= max_tokens:
                logger.info(f"Reached token limit with {len(paths)} paths")
                break
                
            for j in range(i + 1, len(nodes)):
                if total_tokens >= max_tokens:
                    break
                    
                start_node = nodes[i]['id']
                end_node = nodes[j]['id']
                path = await self._q_learning_pathfinding(start_node, end_node)
                if path:
                    # Estimate token count for this path
                    path_token_estimate = len(" ".join(path.nodes)) * 1.3  # Rough estimate
                    
                    # Check if we can add this path without exceeding token limits
                    if total_tokens + path_token_estimate <= max_tokens:
                        # Add context summary to the path
                        path.context_summary = await self._get_path_context_summary(path)
                        path.token_count = int(path_token_estimate)
                        paths.append(path)
                        total_tokens += path_token_estimate
                        logger.debug(f"Added path with {path_token_estimate} estimated tokens")
                    else:
                        # Add partial context if near limit
                        remaining_tokens = max_tokens - total_tokens
                        if remaining_tokens > 100:  # Only add if meaningful
                            # Truncate path or add partial information
                            path.context_summary = f"[Truncated] Path from {start_node} to {end_node}"
                            path.token_count = remaining_tokens
                            paths.append(path)
                            total_tokens += remaining_tokens
                            logger.debug(f"Added truncated path with {remaining_tokens} tokens")
                        break
                        
        logger.info(f"Returning {len(paths)} paths with approximately {total_tokens} tokens")
        return paths
        
    async def _get_path_context_summary(self, path: MemoryPath) -> str:
        """
        Get a summary of context along a path.
        
        Args:
            path: The path to summarize
            
        Returns:
            Summary of context along the path
        """
        try:
            if not self.graph_manager:
                return "No context available (no graph manager)"
                
            # Get detailed information about nodes and relationships in the path
            context_parts = []
            
            # Add node information
            if path.nodes:
                node_info = f"Path nodes: {', '.join(path.nodes[:5])}"  # Limit to first 5 nodes
                context_parts.append(node_info)
                
            # Add relationship information
            if path.relationships:
                rel_types = list(set([rel.get('type', 'RELATED_TO') for rel in path.relationships[:3]]))
                rel_info = f"Relationship types: {', '.join(rel_types)}"
                context_parts.append(rel_info)
                
            # Add path metrics
            metrics_info = f"Path length: {path.length} hops, Score: {path.score:.2f}"
            context_parts.append(metrics_info)
            
            # In a real implementation, this would:
            # 1. Query the Neo4j database for detailed node properties
            # 2. Retrieve text content associated with nodes
            # 3. Extract key information from relationships
            # 4. Create a coherent summary of the path context
            
            return "; ".join(context_parts)
            
        except Exception as e:
            logger.error(f"Error getting path context summary: {str(e)}")
            return f"Error retrieving context: {str(e)}"
            
    async def update_q_values(self, path: MemoryPath, reward: float) -> None:
        """
        Update Q-values based on the path taken and reward received.
        
        Args:
            path: The path that was taken
            reward: The reward received for taking this path
        """
        logger.info(f"Updating Q-values for path with reward {reward}")
        
        if len(path.nodes) < 2:
            logger.warning("Path too short to update Q-values")
            return
            
        # Update Q-values for each state-action pair in the path
        for i in range(len(path.nodes) - 1):
            state = path.nodes[i]
            next_state = path.nodes[i + 1]
            
            # Find the relationship to get the action
            action = None
            for rel in path.relationships:
                if rel.get('start_node') == state and rel.get('end_node') == next_state:
                    action = f"{next_state}:{rel.get('type', 'RELATED_TO')}"
                    break
                    
            if action:
                # Get current Q-value
                current_q = self._get_q_value(state, action)
                
                # Calculate max Q-value for next state
                max_next_q = self._get_max_q_value(next_state)
                
                # Apply Q-learning update rule
                new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
                
                # Update Q-table
                self._set_q_value(state, action, new_q)
                
                logger.debug(f"Updated Q-value for {state}->{action}: {current_q} -> {new_q}")
                
    async def train(self, training_data: List[Tuple[str, str, float]]) -> None:
        """
        Train the agent with historical path data.
        
        Args:
            training_data: List of (start_node, end_node, reward) tuples
        """
        logger.info(f"Training with {len(training_data)} data points")
        
        self.is_training = True
        try:
            for start_node, end_node, reward in training_data:
                # Find path using current policy
                path = await self._q_learning_pathfinding(start_node, end_node)
                if path:
                    # Update Q-values based on reward
                    await self.update_q_values(path, reward)
        finally:
            self.is_training = False
            
    async def start_continuous_training(self) -> None:
        """
        Start the continuous training loop as a background task.
        """
        if self.training_task and not self.training_task.done():
            logger.warning("Training task is already running")
            return
            
        self.training_task = asyncio.create_task(self._continuous_training_loop())
        logger.info("Started continuous training loop")
        
    async def stop_continuous_training(self) -> None:
        """
        Stop the continuous training loop.
        """
        if self.training_task and not self.training_task.done():
            self.training_task.cancel()
            try:
                await self.training_task
            except asyncio.CancelledError:
                pass
            logger.info("Stopped continuous training loop")
            
    async def _continuous_training_loop(self) -> None:
        """
        Background loop for continuous training.
        """
        logger.info("Continuous training loop started")
        
        while True:
            try:
                # Perform continuous training
                await self._perform_continuous_training()
                
                # Wait for the next training interval
                await asyncio.sleep(self.training_interval)
            except asyncio.CancelledError:
                logger.info("Continuous training loop cancelled")
                break
            except Exception as e:
                logger.error(f"Error in continuous training loop: {e}")
                # Wait before retrying
                await asyncio.sleep(60)
                
    async def _perform_continuous_training(self) -> None:
        """
        Perform one iteration of continuous training by exploring the graph.
        """
        logger.info("Performing continuous training iteration")
        
        if not self.graph_manager:
            logger.warning("No graph manager available for continuous training")
            return
            
        # Get a random node to start exploration
        start_node = await self._get_random_node()
        if not start_node:
            logger.warning("No nodes available for continuous training")
            return
            
        # Explore from this node to learn more about the graph
        await self._explore_from_node(start_node)
        
        # Synchronize Q-values to the graph
        await self.sync_q_values_to_graph()
        
    async def _get_random_node(self) -> Optional[str]:
        """
        Get a random node ID from the graph for exploration.
        
        Returns:
            A random node ID or None if no nodes found
        """
        # This is a simplified implementation
        # In a real implementation, you might query the database for a random node
        if self.q_table:
            # Return a random node from our Q-table
            return random.choice(list(self.q_table.keys()))
        return None
        
    async def _explore_from_node(self, start_node: str, max_steps: int = 10) -> None:
        """
        Explore the graph from a starting node to learn Q-values.
        
        Args:
            start_node: The node ID to start exploration from
            max_steps: Maximum number of steps to take during exploration
        """
        logger.debug(f"Exploring from node {start_node}")
        
        current_node = start_node
        path = MemoryPath(nodes=[start_node])
        
        for step in range(max_steps):
            # Get possible actions from current node
            actions = await self._query_graph_structure(current_node)
            
            if not actions:
                logger.debug(f"No actions available from node {current_node}")
                break
                
            # Choose a random action for exploration
            action = random.choice(actions)
            
            # Add action to path
            path.nodes.append(action.to_node)
            path.relationships.append({
                "start_node": action.from_node,
                "end_node": action.to_node,
                "type": action.relationship_type
            })
            path.length += 1
            
            # Move to next node
            current_node = action.to_node
            
        # Update Q-values based on exploration (using a default reward)
        # In a real implementation, this reward would be based on some metric
        await self.update_q_values(path, reward=0.1)
        
    async def sync_q_values_to_graph(self) -> None:
        """
        Synchronize in-memory Q-values with Neo4j relationship properties.
        """
        logger.info("Synchronizing Q-values to graph")
        
        if self.graph_manager:
            try:
                self.graph_manager.sync_q_values_to_graph(self.q_table)
                logger.info("Successfully synchronized Q-values to graph")
            except Exception as e:
                logger.error(f"Failed to synchronize Q-values to graph: {e}")
        else:
            logger.warning("No graph manager available for synchronization")
            
    def get_convergence_metrics(self) -> Dict[str, Any]:
        """
        Get convergence metrics for monitoring the Q-Learning process.
        
        Returns:
            Dictionary containing convergence metrics
        """
        if not self.q_table:
            return {
                "q_table_size": 0,
                "total_q_values": 0,
                "average_q_value": 0.0,
                "max_q_value": 0.0,
                "min_q_value": 0.0
            }
            
        total_q_values = 0
        sum_q_values = 0.0
        max_q_value = float('-inf')
        min_q_value = float('inf')
        
        for state_actions in self.q_table.values():
            for q_value in state_actions.values():
                total_q_values += 1
                sum_q_values += q_value
                max_q_value = max(max_q_value, q_value)
                min_q_value = min(min_q_value, q_value)
                
        return {
            "q_table_size": len(self.q_table),
            "total_q_values": total_q_values,
            "average_q_value": sum_q_values / total_q_values if total_q_values > 0 else 0.0,
            "max_q_value": max_q_value if max_q_value != float('-inf') else 0.0,
            "min_q_value": min_q_value if min_q_value != float('inf') else 0.0
        }
        
    async def _query_graph_structure(self, node_id: str) -> List[Action]:
        """
        Query the graph to get possible actions from a given node.
        
        Args:
            node_id: The node ID to query
            
        Returns:
            List of possible actions from this node
        """
        logger.debug(f"Querying graph structure for node {node_id}")
        
        if not self.graph_manager:
            logger.warning("No graph manager available")
            return []
            
        try:
            neighbors = self.graph_manager.get_neighbors(node_id)
            actions = []
            
            for neighbor in neighbors:
                action = Action(
                    from_node=node_id,
                    to_node=neighbor["to_node"],
                    relationship_type=neighbor["relationship_type"],
                    weight=1.0  # Could be enhanced with relationship properties
                )
                actions.append(action)
                
            return actions
        except Exception as e:
            logger.error(f"Failed to query graph structure: {e}")
            return []
            
    async def _q_learning_pathfinding(self, start_node: str, end_node: str, max_steps: int = 20) -> MemoryPath:
        """
        Use Q-learning to find a path from start to end node.
        
        Args:
            start_node: The starting node ID
            end_node: The target node ID
            max_steps: Maximum number of steps to take
            
        Returns:
            MemoryPath representing the found path
        """
        current_node = start_node
        path = MemoryPath(nodes=[start_node])
        visited = {start_node}
        
        for step in range(max_steps):
            # Check if we've reached the target
            if current_node == end_node:
                path.score = 1.0  # Perfect score for reaching target
                return path
                
            # Get possible actions from current node
            actions = await self._query_graph_structure(current_node)
            
            if not actions:
                logger.debug(f"No actions available from node {current_node}")
                break
                
            # Choose action using epsilon-greedy policy
            if random.random() < self.epsilon and not self.is_training:
                # Explore: choose random action
                action = random.choice(actions)
                logger.debug(f"Exploring: randomly chose action {action.to_node}")
            else:
                # Exploit: choose best action based on Q-values
                best_action = None
                best_q_value = float('-inf')
                
                for action in actions:
                    action_key = f"{action.to_node}:{action.relationship_type}"
                    q_value = self._get_q_value(current_node, action_key)
                    
                    if q_value > best_q_value:
                        best_q_value = q_value
                        best_action = action
                        
                action = best_action
                logger.debug(f"Exploiting: chose best action {action.to_node} with Q-value {best_q_value}")
                
            # Add action to path
            path.nodes.append(action.to_node)
            path.relationships.append({
                "start_node": action.from_node,
                "end_node": action.to_node,
                "type": action.relationship_type
            })
            path.length += 1
            
            # Move to next node
            current_node = action.to_node
            
            # Check for cycles
            if current_node in visited:
                logger.debug(f"Detected cycle at node {current_node}")
                break
            visited.add(current_node)
            
        # If we didn't reach the target, calculate partial score
        path.score = self._calculate_path_score(path, end_node)
        return path
        
    def _get_q_value(self, state: str, action: str) -> float:
        """
        Get Q-value for a state-action pair.
        
        Args:
            state: The state (node ID)
            action: The action (to_node:relationship_type)
            
        Returns:
            Q-value for the state-action pair
        """
        if state in self.q_table and action in self.q_table[state]:
            return self.q_table[state][action]
        return 0.0  # Default Q-value
        
    def _set_q_value(self, state: str, action: str, value: float) -> None:
        """
        Set Q-value for a state-action pair.
        
        Args:
            state: The state (node ID)
            action: The action (to_node:relationship_type)
            value: The Q-value to set
        """
        if state not in self.q_table:
            self.q_table[state] = {}
        self.q_table[state][action] = value
        
    def _get_max_q_value(self, state: str) -> float:
        """
        Get maximum Q-value for all actions from a state.
        
        Args:
            state: The state (node ID)
            
        Returns:
            Maximum Q-value for all actions from this state
        """
        if state in self.q_table:
            if self.q_table[state]:
                return max(self.q_table[state].values())
        return 0.0  # Default when no actions or no Q-values
        
    def _calculate_path_score(self, path: MemoryPath, target_node: str) -> float:
        """
        Calculate a score for a path based on how close it gets to the target.
        
        Args:
            path: The path to score
            target_node: The target node ID
            
        Returns:
            Score for the path (0.0 to 1.0)
        """
        if not path.nodes:
            return 0.0
            
        # Perfect score for reaching target
        if path.nodes[-1] == target_node:
            return 1.0
            
        # Partial score based on path length (shorter is better)
        # This is a simple heuristic - could be enhanced with graph distance metrics
        max_reasonable_length = 10  # Assumption about reasonable path length
        length_score = max(0.0, 1.0 - (path.length / max_reasonable_length))
        
        return length_score * 0.5  # Max 0.5 for non-target paths

    async def refine_relationships(self, path: MemoryPath, reward: float):
        """Refine relationships in the graph based on a path and a reward."""
        logger.info(f"Refining relationships for path with reward {reward}")
        await self.update_q_values(path, reward)
