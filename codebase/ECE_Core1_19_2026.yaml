project_structure: C:\Users\rsbiiw\Projects\ECE_Core
scan_config:
  tokenLimit: 1000000
  maxFileSize: 5242880
  maxLinesPerFile: 5000
  includeExtensions:
    - .js
    - .ts
    - .jsx
    - .tsx
    - .py
    - .java
    - .cpp
    - .c
    - .h
    - .cs
    - .go
    - .rs
    - .rb
    - .php
    - .html
    - .css
    - .scss
    - .sass
    - .less
    - .json
    - .yaml
    - .yml
    - .xml
    - .sql
    - .sh
    - .bash
    - .zsh
    - .md
    - .txt
    - .csv
    - .toml
    - .ini
    - .cfg
    - .conf
    - .env
    - .dockerfile
    - dockerfile
    - .gitignore
    - .npmignore
    - .prettierignore
    - makefile
    - cmakelists.txt
    - readme.md
    - readme.txt
    - readme
    - license
    - license.md
    - changelog
    - changelog.md
    - contributing
    - contributing.md
    - code_of_conduct
    - code_of_conduct.md
  excludeExtensions:
    - .png
    - .jpg
    - .jpeg
    - .gif
    - .bmp
    - .ico
    - .svg
    - .webp
    - .exe
    - .bin
    - .dll
    - .so
    - .dylib
    - .zip
    - .tar
    - .gz
    - .rar
    - .7z
    - .pdf
    - .doc
    - .docx
    - .xls
    - .xlsx
    - .ppt
    - .pptx
    - .mp3
    - .mp4
    - .avi
    - .mov
    - .wav
    - .flac
    - .ttf
    - .otf
    - .woff
    - .woff2
    - .o
    - .obj
    - .a
    - .lib
    - .out
    - .class
    - .jar
    - .war
    - .swp
    - .swo
    - .lock
    - .cache
    - .log
    - .tmp
    - .temp
    - .DS_Store
    - Thumbs.db
  excludeDirectories:
    - .git
    - node_modules
    - archive
    - backups
    - logs
    - context
    - .vscode
    - .idea
    - .pytest_cache
    - __pycache__
    - dist
    - build
    - target
    - venv
    - env
    - .venv
    - .env
    - Pods
    - Carthage
    - CocoaPods
    - .next
    - .nuxt
    - public
    - static
    - assets
    - images
    - img
    - codebase
  excludeFiles:
    - combined_context.yaml
    - package-lock.json
    - yarn.lock
    - pnpm-lock.yaml
    - Gemfile.lock
    - Pipfile.lock
    - Cargo.lock
    - composer.lock
    - go.sum
    - go.mod
    - requirements.txt
    - poetry.lock
    - "*.db"
    - "*.sqlite"
    - "*.sqlite3"
    - "*.fdb"
    - "*.mdb"
    - "*.accdb"
    - "*~"
    - "*.tmp"
    - "*.temp"
    - "*.cache"
    - "*.swp"
    - "*.swo"
files:
  - path: .env
    content: "# ECE Core Environment Variables\r\n# Consolidated from sovereign.yaml and config.yaml\r\n\r\n# ============================================================\r\n# Core Server Configuration\r\n# ============================================================\r\nPORT=3000\r\nHOST=0.0.0.0\r\nAPI_KEY=ece-secret-key\r\nLOG_LEVEL=INFO\r\n\r\n# ============================================================\r\n# Infrastructure\r\n# ============================================================\r\nOVERLAY_PORT=3001\r\n\r\n# ============================================================\r\n# LLM & Inference Configuration\r\n# ============================================================\r\n# Paths are relative to engine/models or absolute\r\nLLM_MODEL_PATH=glm-edge-1.5b-chat.Q5_K_M.gguf\r\nLLM_CTX_SIZE=131072\r\nLLM_GPU_LAYERS=11\r\n\r\nEMBEDDING_GPU_LAYERS=0\r\n# LLM_EMBEDDING_* removed (Tech Debt Removal)\r\nLLM_EMBEDDING_DIM=768\r\n\r\n# Orchestrator Model\r\nORCHESTRATOR_MODEL_PATH=\"glm-edge-1.5b-chat.Q5_K_M.gguf\"\r\nORCHESTRATOR_CTX_SIZE=2048\r\nORCHESTRATOR_GPU_LAYERS=0\r\n\r\n# Vision Model\r\nVISION_MODEL_PATH=C:/Users/rsbiiw/Projects/models/gemma-3-4b-it-abliterated-v2.i1-Q4_K_S.gguf\r\n# VISION_PROJECTOR_PATH=C:/Users/rsbiiw/Projects/models/mmproj-Qwen2-VL-2B-Instruct-f16.gguf\r\n\r\n# Embedding & Vector Search\r\nEMBEDDING_BATCH_SIZE=50\r\nVECTOR_INGEST_BATCH=100\r\nSIMILARITY_THRESHOLD=0.8\r\nLLM_EMBEDDING_DIM=768\r\n\r\n# ============================================================\r\n# Features\r\n# ============================================================\r\nFEATURE_CONTEXT_STORAGE=true\r\nFEATURE_MEMORY_RECALL=true\r\nFEATURE_CODA_ENABLED=true\r\nFEATURE_ARCHIVIST_ENABLED=true\r\nFEATURE_WEAVER_ENABLED=true\r\n\r\n# ============================================================\r\n# Tuning\r\n# ============================================================\r\nDREAMER_BATCH_SIZE=500\r\nMARKOVIAN_ENABLED=true\r\nCONTEXT_RECENT_TURNS=5\r\n"
    tokens: 598
    size: 1848
  - path: .gitignore
    content: "# Sensitive Data & Secrets\r\n.env\r\n.env.*\r\n*.env\r\n\r\n# Database & Storage\r\ncontext.db/\r\nbackups/\r\nlogs/\r\n*.log\r\n*.sqlite\r\n*.db\r\n\r\n# Model Archives & Heavy Binaries\r\narchive/\r\nmodels/\r\n*.gguf\r\n*.bin\r\n*.pth\r\n\r\n# Code Dependencies\r\nnode_modules/\r\ndist/\r\nbuild/\r\n.pnpm-store/\r\n\r\n# IDE & System\r\n.vscode/\r\n.idea/\r\n.DS_Store\r\nThumbs.db\r\n*.bak\r\n\r\n# Temporary/User Specific\r\ndesktop-overlay/\r\ncodebase/combined_context.yaml\r\nread_all.js\r\n"
    tokens: 156
    size: 428
  - path: desktop-overlay\package.json
    content: "{\r\n    \"name\": \"@ece/desktop-overlay\",\r\n    \"version\": \"1.0.0\",\r\n    \"main\": \"dist/main.js\",\r\n    \"scripts\": {\r\n        \"start\": \"electron .\",\r\n        \"build\": \"tsc\"\r\n    },\r\n    \"devDependencies\": {\r\n        \"electron\": \"^28.1.0\",\r\n        \"typescript\": \"^5.3.3\"\r\n    }\r\n}"
    tokens: 95
    size: 274
  - path: desktop-overlay\src\main.ts
    content: "\r\nimport { app, BrowserWindow, screen } from 'electron';\r\nimport path from 'path';\r\n\r\n// Config\r\nconst FRONTEND_URL = process.env.FRONTEND_URL || 'http://localhost:3000';\r\n\r\nlet mainWindow: BrowserWindow | null = null;\r\n\r\nfunction createWindow() {\r\n    const primaryDisplay = screen.getPrimaryDisplay();\r\n    const { width, height } = primaryDisplay.workAreaSize;\r\n\r\n    mainWindow = new BrowserWindow({\r\n        width: 600,\r\n        height: 800,\r\n        x: width - 650,\r\n        y: 50,\r\n        frame: false, // Overlay style\r\n        alwaysOnTop: true,\r\n        transparent: true,\r\n        webPreferences: {\r\n            nodeIntegration: false,\r\n            contextIsolation: true,\r\n        },\r\n    });\r\n\r\n    mainWindow.loadURL(FRONTEND_URL);\r\n\r\n    mainWindow.on('closed', () => {\r\n        mainWindow = null;\r\n    });\r\n}\r\n\r\napp.on('ready', createWindow);\r\n\r\napp.on('window-all-closed', () => {\r\n    if (process.platform !== 'darwin') {\r\n        app.quit();\r\n    }\r\n});\r\n\r\napp.on('activate', () => {\r\n    if (mainWindow === null) {\r\n        createWindow();\r\n    }\r\n});\r\n"
    tokens: 359
    size: 1074
  - path: desktop-overlay\tsconfig.json
    content: "{\r\n    \"compilerOptions\": {\r\n        \"target\": \"ES6\",\r\n        \"module\": \"CommonJS\",\r\n        \"outDir\": \"./dist\",\r\n        \"rootDir\": \"./src\",\r\n        \"strict\": true,\r\n        \"esModuleInterop\": true,\r\n        \"skipLibCheck\": true\r\n    },\r\n    \"include\": [\r\n        \"src/**/*\"\r\n    ]\r\n}"
    tokens: 89
    size: 287
  - path: engine\bin\llama.cpp.txt
    content: "MIT License\r\n\r\nCopyright (c) 2023-2024 The ggml authors\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the \"Software\"), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n"
    tokens: 447
    size: 1099
  - path: engine\package.json
    content: |-
      {
          "name": "sovereign-context-engine",
          "version": "3.0.0",
          "type": "module",
          "description": "Headless Context Engine & Knowledge Graph",
          "main": "src/index.js",
          "bin": "src/index.js",
          "scripts": {
              "start": "node dist/index.js",
              "dev": "ts-node src/index.ts",
              "migrate": "node src/migrate_history.js",
              "read-all": "node src/read_all.js",
              "hydrate": "node src/hydrate.js",
              "test": "node tests/all_routes_and_services.js",
              "test:routes": "node tests/all_routes_and_services.js",
              "test:quick": "node tests/suite.js",
              "test:benchmark": "node tests/benchmark.js",
              "test:context": "node tests/context_experiments.js",
              "benchmark": "node tests/benchmark.js",
              "build": "tsc",
              "build:standalone": "tsc && pkg .",
              "lint": "eslint src/ --ext .ts,.js",
              "lint:fix": "eslint src/ --ext .ts,.js --fix"
          },
          "pkg": {
              "assets": [
                  "src/**/*",
                  "node_modules/cozo-node/native/**/*",
                  "context/**/*",
                  "codebase/**/*",
                  "specs/**/*",
                  "shared/**/*",
                  "!.env",
                  "!.env.*",
                  "!*.log",
                  "!logs/",
                  "!node_modules/@types/"
              ],
              "targets": [
                  "node18-win-x64"
              ],
              "outputPath": "dist",
              "compress": "GZip"
          },
          "dependencies": {
              "@ece/shared": "workspace:*",
              "axios": "^1.13.2",
              "body-parser": "^1.20.2",
              "chokidar": "^3.6.0",
              "cors": "^2.8.5",
              "cozo-node": "^0.7.6",
              "dotenv": "^16.3.1",
              "express": "^4.18.2",
              "js-yaml": "^4.1.1",
              "node-llama-cpp": "^3.15.0"
          },
          "devDependencies": {
              "@types/cors": "^2.8.19",
              "@types/express": "^5.0.6",
              "@types/js-yaml": "^4.0.9",
              "@types/node": "^25.0.7",
              "eslint": "^8.56.0",
              "pkg": "^5.8.1",
              "ts-node": "^10.9.2",
              "typescript": "^5.9.3"
          }
      }
    tokens: 726
    size: 2044
  - path: engine\python_vision\vision_engine.py
    content: "import sys\r\nimport json\r\nimport base64\r\nimport os\r\n\r\n# Placeholder for U-MARVEL or Qwen2.5-VL loading\r\n# Ideally we use llama-cpp-python for GGUF support if available, \r\n# or transformers for raw weights if we have the VRAM.\r\n\r\ndef main():\r\n    print(json.dumps({\"status\": \"ready\", \"model\": \"vision_sidecar_v1\"}), flush=True)\r\n\r\n    # Simple loop to read requests from stdin (or we can make this an HTTP server)\r\n    # For now, let's assume it runs as a script for a single inference or a persistent process.\r\n    # Persistent is better for keeping model loaded.\r\n    \r\n    while True:\r\n        try:\r\n            line = sys.stdin.readline()\r\n            if not line:\r\n                break\r\n            \r\n            data = json.loads(line)\r\n            command = data.get(\"command\")\r\n            \r\n            if command == \"analyze\":\r\n                image_b64 = data.get(\"image\") # base64 string\r\n                prompt = data.get(\"prompt\", \"Describe this image.\")\r\n                \r\n                # TODO: Decode image and run model\r\n                # img_data = base64.b64decode(image_b64)\r\n                \r\n                # Stub Response\r\n                response = {\r\n                    \"text\": f\"[VISION SIMULATION] I see an image! You asked: '{prompt}'. (Model pending integration)\"\r\n                }\r\n                print(json.dumps(response), flush=True)\r\n                \r\n            elif command == \"exit\":\r\n                break\r\n                \r\n        except Exception as e:\r\n            error_response = {\"error\": str(e)}\r\n            print(json.dumps(error_response), flush=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
    tokens: 592
    size: 1650
  - path: engine\src\config\index.ts
    content: |-
      /**
       * Configuration Module for Sovereign Context Engine
       * 
       * This module manages all configuration for the context engine including
       * paths, model settings, and system parameters.
       */

      import * as fs from 'fs';
      import * as path from 'path';
      import { fileURLToPath } from 'url';
      import yaml from 'js-yaml';

      // For __dirname equivalent in ES modules
      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);

      // Define configuration interface
      interface Config {
        // Core
        PORT: number;
        HOST: string;
        API_KEY: string;
        LOG_LEVEL: string;
        OVERLAY_PORT: number;

        // Tuning
        DEFAULT_SEARCH_CHAR_LIMIT: number;
        DREAM_INTERVAL_MS: number;
        SIMILARITY_THRESHOLD: number;
        TOKEN_LIMIT: number;
        DREAMER_BATCH_SIZE: number;

        VECTOR_INGEST_BATCH: number;

        // Extrapolated Settings
        WATCHER_DEBOUNCE_MS: number;
        CONTEXT_RELEVANCE_WEIGHT: number;
        CONTEXT_RECENCY_WEIGHT: number;
        DREAMER_CLUSTERING_GAP_MS: number;

        // Infrastructure
        REDIS: {
          ENABLED: boolean;
          URL: string;
          TTL: number;
        };
        NEO4J: {
          ENABLED: boolean;
          URI: string;
          USER: string;
          PASS: string;
        };

        // Features
        FEATURES: {
          CONTEXT_STORAGE: boolean;
          MEMORY_RECALL: boolean;
          CODA: boolean;
          ARCHIVIST: boolean;
          WEAVER: boolean;
          MARKOVIAN: boolean;
        };

        // Models
        MODELS: {
          EMBEDDING_DIM: number;
          MAIN: {
            PATH: string;
            CTX_SIZE: number;
            GPU_LAYERS: number;
          };

          ORCHESTRATOR: {
            PATH: string;
            CTX_SIZE: number;
            GPU_LAYERS: number;
          };
          VISION: {
            PATH: string;
            PROJECTOR: string;
            CTX_SIZE: number;
            GPU_LAYERS: number;
          };
        };
      }

      // Default configuration
      const DEFAULT_CONFIG: Config = {
        // Core
        PORT: parseInt(process.env['PORT'] || "3000"),
        HOST: process.env['HOST'] || "0.0.0.0",
        API_KEY: process.env['API_KEY'] || "ece-secret-key",
        LOG_LEVEL: process.env['LOG_LEVEL'] || "INFO",
        OVERLAY_PORT: parseInt(process.env['OVERLAY_PORT'] || "3001"),

        // Tuning
        DEFAULT_SEARCH_CHAR_LIMIT: 524288,
        DREAM_INTERVAL_MS: 3600000, // 60 minutes
        SIMILARITY_THRESHOLD: parseFloat(process.env['SIMILARITY_THRESHOLD'] || "0.8"),
        TOKEN_LIMIT: 1000000,
        DREAMER_BATCH_SIZE: parseInt(process.env['DREAMER_BATCH_SIZE'] || "5"),

        VECTOR_INGEST_BATCH: parseInt(process.env['VECTOR_INGEST_BATCH'] || "500"),

        // Extrapolated Settings
        WATCHER_DEBOUNCE_MS: parseInt(process.env['WATCHER_DEBOUNCE_MS'] || "2000"),
        CONTEXT_RELEVANCE_WEIGHT: parseFloat(process.env['CONTEXT_RELEVANCE_WEIGHT'] || "0.7"),
        CONTEXT_RECENCY_WEIGHT: parseFloat(process.env['CONTEXT_RECENCY_WEIGHT'] || "0.3"),
        DREAMER_CLUSTERING_GAP_MS: parseInt(process.env['DREAMER_CLUSTERING_GAP_MS'] || "900000"), // 15 mins

        // Infrastructure
        REDIS: {
          ENABLED: process.env['REDIS_ENABLED'] === 'true',
          URL: process.env['REDIS_URL'] || "redis://localhost:6379",
          TTL: parseInt(process.env['REDIS_TTL'] || "3600")
        },
        NEO4J: {
          ENABLED: process.env['NEO4J_ENABLED'] === 'true',
          URI: process.env['NEO4J_URI'] || "bolt://localhost:7687",
          USER: process.env['NEO4J_USER'] || "neo4j",
          PASS: process.env['NEO4J_PASSWORD'] || "password"
        },

        // Features
        FEATURES: {
          CONTEXT_STORAGE: process.env['FEATURE_CONTEXT_STORAGE'] === 'true',
          MEMORY_RECALL: process.env['FEATURE_MEMORY_RECALL'] === 'true',
          CODA: process.env['FEATURE_CODA_ENABLED'] === 'true',
          ARCHIVIST: process.env['FEATURE_ARCHIVIST_ENABLED'] === 'true',
          WEAVER: process.env['FEATURE_WEAVER_ENABLED'] === 'true',
          MARKOVIAN: process.env['MARKOVIAN_ENABLED'] === 'true'
        },

        // Models
        MODELS: {
          EMBEDDING_DIM: parseInt(process.env['LLM_EMBEDDING_DIM'] || "768"),
          MAIN: {
            PATH: process.env['LLM_MODEL_PATH'] || "gemma-3-4b-it-abliterated-v2.i1-Q4_K_S.gguf",
            CTX_SIZE: parseInt(process.env['LLM_CTX_SIZE'] || "4096"),
            GPU_LAYERS: parseInt(process.env['LLM_GPU_LAYERS'] || "33")
          },

          ORCHESTRATOR: {
            PATH: process.env['ORCHESTRATOR_MODEL_PATH'] || "Qwen3-4B-Function-Calling-Pro.gguf",
            CTX_SIZE: parseInt(process.env['ORCHESTRATOR_CTX_SIZE'] || "8192"),
            GPU_LAYERS: parseInt(process.env['ORCHESTRATOR_GPU_LAYERS'] || "0")
          },
          VISION: {
            PATH: process.env['VISION_MODEL_PATH'] || "",  // MUST BE SET IN .ENV
            PROJECTOR: process.env['VISION_PROJECTOR_PATH'] || "", // MUST BE SET IN .ENV
            CTX_SIZE: 2048,
            GPU_LAYERS: parseInt(process.env['LLM_GPU_LAYERS'] || "33")
          }
        }
      };

      // Configuration loader
      function loadConfig(): Config {
        // Determine config file path
        const configPath = process.env['SOVEREIGN_CONFIG_PATH'] ||
          path.join(__dirname, '..', '..', 'sovereign.yaml') ||
          path.join(__dirname, '..', 'config', 'default.yaml');

        if (fs.existsSync(configPath)) {
          try {
            const configFile = fs.readFileSync(configPath, 'utf8');
            const parsedConfig = yaml.load(configFile) as Partial<Config>;
            return { ...DEFAULT_CONFIG, ...parsedConfig } as Config;
          } catch (error) {
            console.warn(`Failed to load config from ${configPath}:`, error);
            return DEFAULT_CONFIG;
          }
        }

        return DEFAULT_CONFIG;
      }

      // Export configuration
      export const config = loadConfig();

      export default config;
    tokens: 1799
    size: 5261
  - path: engine\src\config\paths.ts
    content: |-
      /**
       * Path Configuration for Sovereign Context Engine
       * 
       * Defines all the important paths used by the system.
       */

      import * as path from 'path';
      import * as os from 'os';


      import { fileURLToPath } from 'url';

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);

      // Define base paths
      export const PROJECT_ROOT = path.resolve(process.env['PROJECT_ROOT'] || path.join(__dirname, '..', '..'));
      export const CONTEXT_DIR = path.resolve(process.env['CONTEXT_DIR'] || path.join(PROJECT_ROOT, 'context'));
      export const MODELS_DIR = path.resolve(process.env['MODELS_DIR'] || path.join(PROJECT_ROOT, '..', '..', 'models'));
      export const DIST_DIR = path.resolve(process.env['DIST_DIR'] || path.join(PROJECT_ROOT, 'dist'));
      export const BASE_PATH = PROJECT_ROOT;
      export const LOGS_DIR = path.join(PROJECT_ROOT, 'logs');
      export const NOTEBOOK_DIR = path.resolve(process.env['NOTEBOOK_DIR'] || path.join(PROJECT_ROOT, '..', '..', 'notebook'));

      // Define specific paths
      const PATHS = {
        PROJECT_ROOT,
        CONTEXT_DIR,
        MODELS_DIR,
        DIST_DIR,
        BACKUPS_DIR: path.join(PROJECT_ROOT, 'backups'),
        LOGS_DIR: path.join(PROJECT_ROOT, 'logs'),
        CONFIG_FILE: path.join(PROJECT_ROOT, 'sovereign.yaml'),
        USER_SETTINGS: path.join(PROJECT_ROOT, 'user_settings.json'),
        DATABASE_FILE: path.join(CONTEXT_DIR, 'context.db'),
        INBOX_DIR: path.join(CONTEXT_DIR, 'inbox'),
        LIBRARIES_DIR: path.join(CONTEXT_DIR, 'libraries'),
        MIRRORS_DIR: path.join(CONTEXT_DIR, 'mirrors'),
        SESSIONS_DIR: path.join(CONTEXT_DIR, 'sessions'),
        TEMP_DIR: path.join(os.tmpdir(), 'sovereign-context-engine'),
        ENGINE_BIN: path.join(PROJECT_ROOT, 'engine', 'bin'),
        ENGINE_SRC: path.join(PROJECT_ROOT, 'engine', 'src'),
        ENGINE_DIST: path.join(PROJECT_ROOT, 'engine', 'dist'),
        DESKTOP_OVERLAY_SRC: path.join(PROJECT_ROOT, 'desktop-overlay', 'src'),
        DESKTOP_OVERLAY_DIST: path.join(PROJECT_ROOT, 'desktop-overlay', 'dist'),
      };

      export default PATHS;
    tokens: 704
    size: 1961
  - path: engine\src\core\batch.ts
    content: "/**\r\n * Batch Processing Utility\r\n * \r\n * Provides a standardized way to process large arrays of items in chunks.\r\n * Useful for LLM operations, Database writes, and heavy processing loops.\r\n */\r\n\r\nexport interface BatchOptions {\r\n    batchSize: number;\r\n    delayMs?: number; // Optional delay between batches to let system breathe\r\n}\r\n\r\n/**\r\n * Process an array of items in batches.\r\n * \r\n * @param items Array of items to process\r\n * @param processor Async function to process a single batch\r\n * @param options Configuration options\r\n */\r\nexport async function processInBatches<T, R>(\r\n    items: T[],\r\n    processor: (batch: T[], batchIndex: number, startItemIndex: number) => Promise<R>,\r\n    options: BatchOptions\r\n): Promise<R[]> {\r\n    const { batchSize, delayMs } = options;\r\n    const results: R[] = [];\r\n    const totalBatches = Math.ceil(items.length / batchSize);\r\n\r\n    for (let i = 0; i < items.length; i += batchSize) {\r\n        const batch = items.slice(i, i + batchSize);\r\n        const batchIndex = Math.floor(i / batchSize);\r\n\r\n        try {\r\n            const result = await processor(batch, batchIndex, i);\r\n            results.push(result);\r\n        } catch (error) {\r\n            console.error(`[Batch] Error in batch ${batchIndex + 1}/${totalBatches}:`, error);\r\n            // We continue processing other batches? \r\n            // Depends on specific service needs, but generally safer to throw or let caller handle try/catch inside processor.\r\n            throw error;\r\n        }\r\n\r\n        if (delayMs && i + batchSize < items.length) {\r\n            await new Promise(resolve => setTimeout(resolve, delayMs));\r\n        }\r\n    }\r\n\r\n    return results;\r\n}\r\n"
    tokens: 608
    size: 1684
  - path: engine\src\core\db.ts
    content: |-
      /**
       * Database Module for Sovereign Context Engine
       * 
       * This module manages the CozoDB database connection and provides
       * database operations for the context engine.
       */

      import { CozoDb } from 'cozo-node';
      import { config } from '../config/index.js';

      export class Database {
        private db: CozoDb;

        constructor() {
          // Initialize the database with RocksDB persistent backend
          this.db = new CozoDb('rocksdb', './context.db');
          console.log('[DB] Initialized with RocksDB backend: ./context.db');
        }

        /**
         * Initialize the database with required schemas
         */
        async init() {
          // Create the memory table schema
          // We check for existing columns to determine if migration is needed
          try {
            const result = await this.db.run('::columns memory');
            const columns = result.rows.map((r: any) => r[0]);

            // Check for Level 1 Atomizer fields
            const hasSequence = columns.includes('sequence');
            const hasEmbedding = columns.includes('embedding');
            const hasSourceId = columns.includes('source_id');

            if (!hasSequence || !hasEmbedding || !hasSourceId) {
              console.log('Migrating memory schema: Adding Atomizer columns...');

              // 1. Fetch old data into memory (Safe subset of columns)
              // We only fallback to what we know existed in v2
              const oldDataResult = await this.db.run(`
                ?[id, timestamp, content, source, provenance] := 
                *memory{id, timestamp, content, source, provenance}
              `);

              console.log(`[DB] Migrating ${oldDataResult.rows.length} rows...`);

              // 2. Drop old indices and table
              try {
                console.log('[DB] Removing indices...');
                try { await this.db.run('::remove memory:knn'); } catch (e) { }
                try { await this.db.run('::remove memory:vec_idx'); } catch (e) { } // Legacy
                try { await this.db.run('::remove memory:content_fts'); } catch (e) { }
              } catch (e: any) {
                console.log(`[DB] Index removal warning: ${e.message}`);
              }

              console.log('[DB] Removing old table...');
              await this.db.run('::remove memory');

              // 3. Create new table
              await this.db.run(`
                :create memory {
                  id: String
                  =>
                  timestamp: Float,
                  content: String,
                  source: String,
                  source_id: String,
                  sequence: Int,
                  type: String,
                  hash: String,
                  buckets: [String],
                  epochs: [String],
                  tags: [String],
                  provenance: String,
                  embedding: <F32; ${config.MODELS.EMBEDDING_DIM}>
                }
              `);

              // 4. Re-insert data with defaults
              if (oldDataResult.rows.length > 0) {
                const crypto = await import('crypto'); // Dynamic import for hash generation

                const newData = oldDataResult.rows.map((row: any) => {
                  // row: [id, timestamp, content, source, provenance]
                  const content = row[2] || "";
                  const hash = crypto.createHash('md5').update(content).digest('hex');

                  return [
                    row[0], // id
                    row[1] || Date.now(), // timestamp
                    content, // content
                    row[3] || "unknown", // source
                    row[3] || "unknown", // source_id (default to source path)
                    0,      // sequence
                    'fragment', // type (default)
                    hash, // hash (calculated)
                    [], // buckets
                    [], // tags
                    [], // epochs
                    row[4] || "{}", // provenance
                    new Array(config.MODELS.EMBEDDING_DIM).fill(0.0) // embedding (reset to zero to force re-embed)
                  ];
                });

                // Batch insert
                const chunkSize = 100;
                for (let i = 0; i < newData.length; i += chunkSize) {
                  const chunk = newData.slice(i, i + chunkSize);
                  await this.db.run(`
                     ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data
                     :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}
                   `, { data: chunk });
                }
              }
              console.log('[DB] Migration complete.');
            }
          } catch (e: any) {
            // Create fresh if not exists
            if (e.message && (e.message.includes('RelNotFound') || e.message.includes('not found') || e.message.includes('Cannot find'))) {
              console.log('[DB] Creating memory table from scratch...');
              // Create Memory Table
              try {
                await this.db.run(`
                  :create memory {
                      id: String
                      =>
                      timestamp: Float,
                      content: String,
                      source: String,
                      source_id: String,
                      sequence: Int,
                      type: String,
                      hash: String,
                      buckets: [String],
                      epochs: [String],
                      tags: [String],
                      provenance: String,
                      embedding: <F32; ${config.MODELS.EMBEDDING_DIM}>
                  }
              `);
                console.log('Memory table initialized');

                // REMOVED: Vector index (HNSW) is no longer used. Tag-Walker is the primary retrieval method.
                // Explicitly remove it if it exists to save resources and prevent zero-vector errors.
                try {
                  await this.db.run('::remove memory:knn');
                  console.log('[DB] Legacy vector index (memory:knn) removed.');
                } catch (e) {
                  // Ignore if index doesn't exist
                }

              } catch (createError: any) {
                console.error(`[DB] Failed to create memory table: ${createError.message}`);

                // Check if table already exists (not an error technically, but we might want schema check)
                if (!createError.message?.includes('Duplicate') && !createError.display?.includes('Duplicate')) {
                  throw createError;
                }
              }
            } else {
              console.log(`[DB] Schema check/migration failed: ${e.message}`);
              if (e.message.includes('indices attached') || e.message.includes('Index lock')) {
                console.log('[DB] Index lock detected. Automatically purging corrupted database...');

                // Close existing connection
                try { this.db.close(); } catch (c) { }

                // Give OS time to release file locks (Windows is slow)
                await new Promise(resolve => setTimeout(resolve, 1000));

                const fs = await import('fs');
                try {
                  // RocksDB creates a DIRECTORY, not a file. unlinkSync fails on dirs.
                  if (fs.existsSync('./context.db')) fs.rmSync('./context.db', { recursive: true, force: true });
                  if (fs.existsSync('./context.db-log')) fs.rmSync('./context.db-log', { force: true });
                  if (fs.existsSync('./context.db-lock')) fs.rmSync('./context.db-lock', { force: true });
                } catch (err: any) {
                  console.error('[DB] Failed to auto-purge:', err.message);
                  console.error('[DB] Please MANUALLY delete the "context.db" folder and restart.');
                  process.exit(1); // Do not recurse if FS fails, just exit.
                }

                // Re-initialize fresh
                console.log('[DB] Re-initializing fresh database...');
                this.db = new CozoDb('rocksdb', './context.db');
                await this.init(); // Recursive retry
                return;
              }
              throw e;
            }
          }

          // Create Source Table (Container)
          try {
            await this.db.run(`
              :create source {
                 path: String,
                 hash: String,
                 total_atoms: Int,
                 last_ingest: Float
              }
            `);
          } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }

          // Create Summary Node Table (Level 2/3: Episodes/Epochs)
          try {
            await this.db.run(`
              :create summary_node {
                 id: String,
                 type: String,
                 content: String,
                 span_start: Float,
                 span_end: Float,
                 embedding: <F32; 384>
              }
            `);
          } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }

          // Create Parent_Of Edge Table (Hierarchy)
          try {
            await this.db.run(`
              :create parent_of {
                 parent_id: String,
                 child_id: String,
                 weight: Float
              }
            `);
          } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }

          // Create Engram table (Lexical Sidecar)
          try {
            await this.db.run(`
              :create engrams {
                key: String,
                value: String
              }
            `);
          } catch (e: any) {
            if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e;
          }

          // Create FTS index for content
          try {
            await this.db.run(`
              ::fts create memory:content_fts {
                extractor: content,
                tokenizer: Simple,
                filters: [Lowercase]
              }
            `);
          } catch (e: any) {
            if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate') && !e.message?.includes('already exists')) throw e;
          }

          console.log('Database initialized successfully');
        }

        /**
         * Close the database connection
         */
        async close() {
          // Close the database connection
          this.db.close();
        }

        /**
         * Run a query against the database
         */
        async run(query: string, params?: any) {
          const { config } = await import('../config/index.js');
          if (config.LOG_LEVEL === 'DEBUG') {
            if (query.includes(':put') || query.includes(':insert')) {
              console.log(`[DB] Executing Write: ${query.substring(0, 50)}... Params keys: ${params ? Object.keys(params) : 'none'}`);
              if (params && params.data) console.log(`[DB] Data rows: ${params.data.length}`);
            }
          }

          try {
            const result = await this.db.run(query, params);
            return result;
          } catch (e: any) {
            console.error(`[DB] Query Failed: ${e.message}`);
            console.error(`[DB] Query: ${query}`);
            throw e;
          }
        }

        /**
         * Run a FTS search query
         */
        async search(query: string) {
          return await this.db.run(query);
        }
      }

      // Export a singleton instance
      export const db = new Database();
    tokens: 3698
    size: 10406
  - path: engine\src\core\inference\ChatWorker.ts
    content: "\r\nimport { parentPort, workerData } from 'worker_threads';\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel } from 'node-llama-cpp';\r\n\r\n// Worker state\r\nlet llama: any = null;\r\nlet model: LlamaModel | null = null;\r\nlet context: LlamaContext | null = null;\r\nlet session: LlamaChatSession | null = null;\r\n\r\nasync function init() {\r\n    try {\r\n        // Priority: workerData.forceCpu -> workerData.gpuLayers === 0 -> env.LLM_GPU_LAYERS === '0'\r\n        const forceCpu = workerData?.forceCpu === true ||\r\n            workerData?.gpuLayers === 0 ||\r\n            process.env['LLM_GPU_LAYERS'] === '0';\r\n\r\n        if (forceCpu) {\r\n            console.log(\"[Worker] Force CPU/GPU_LAYERS=0 detected. Disabling CUDA for this worker.\");\r\n            llama = await getLlama({\r\n                gpu: { type: 'auto', exclude: ['cuda'] }\r\n            });\r\n        } else {\r\n            llama = await getLlama();\r\n        }\r\n        parentPort?.postMessage({ type: 'ready' });\r\n    } catch (error: any) {\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n}\r\n\r\n// Handle messages from main thread\r\nparentPort?.on('message', async (message) => {\r\n    try {\r\n        switch (message.type) {\r\n            case 'loadModel':\r\n                await handleLoadModel(message.data);\r\n                break;\r\n            case 'chat':\r\n                await handleChat(message.data);\r\n                break;\r\n            case 'dispose':\r\n                await handleDispose();\r\n                break;\r\n        }\r\n    } catch (error: any) {\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n});\r\n\r\nasync function handleLoadModel(data: { modelPath: string, options: any }) {\r\n    if (!llama) await init();\r\n\r\n    // Cleanup existing\r\n    if (session) { session.dispose(); session = null; }\r\n    if (context) { await context.dispose(); context = null; }\r\n    if (model) { await model.dispose(); model = null; }\r\n\r\n    try {\r\n        model = await llama.loadModel({\r\n            modelPath: data.modelPath,\r\n            gpuLayers: data.options.gpuLayers || 0\r\n        });\r\n\r\n        // Chat Context\r\n        context = await model!.createContext({\r\n            contextSize: data.options.contextSize || 4096,\r\n            batchSize: data.options.contextSize || 4096\r\n        });\r\n\r\n        session = new LlamaChatSession({\r\n            contextSequence: context!.getSequence(),\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n\r\n        parentPort?.postMessage({ type: 'modelLoaded', data: { modelPath: data.modelPath } });\r\n    } catch (error: any) {\r\n        throw new Error(`Failed to load Chat Model: ${error.message}`);\r\n    }\r\n}\r\n\r\nasync function handleChat(data: { prompt: string, options: any }) {\r\n    if (!session) throw new Error(\"Session not initialized\");\r\n\r\n    const response = await session.prompt(data.prompt, {\r\n        temperature: data.options.temperature || 0.7,\r\n        maxTokens: data.options.maxTokens || 1024\r\n    });\r\n\r\n    parentPort?.postMessage({ type: 'chatResponse', data: response });\r\n}\r\n\r\nasync function handleDispose() {\r\n    if (session) session.dispose();\r\n    if (context) await context.dispose();\r\n    if (model) await model.dispose();\r\n    parentPort?.postMessage({ type: 'disposed' });\r\n}\r\n\r\ninit();\r\n"
    tokens: 1140
    size: 3339
  - path: engine\src\core\inference\context_manager.ts
    content: "import { config } from '../../config/index.js';\r\n\r\nexport interface ContextAtom {\r\n    id: string;\r\n    content: string;\r\n    source: string;\r\n    timestamp: number;\r\n    score: number; // Relevance Score\r\n}\r\n\r\nexport interface ContextResult {\r\n    prompt: string;\r\n    stats: {\r\n        tokenCount: number;\r\n        charCount: number;\r\n        filledPercent: number;\r\n        atomCount: number;\r\n    };\r\n}\r\n\r\n/**\r\n * Rolling Context Slicer (Feature 8)\r\n * \r\n * Implements \"Middle-Out\" Context Budgeting.\r\n * Prioritizes atoms based on a mix of Relevance (Vector Similarity) and Recency.\r\n * \r\n * Strategy:\r\n * 1. Rank Candidates: Score = (Relevance * 0.7) + (RecencyNorm * 0.3).\r\n * 2. Select: Fill budget with highest ranked atoms.\r\n * 3. Smart Slice: If an atom fits partially, slice around the keyword match (windowing).\r\n * 4. Order: Sort selected atoms Chronologically (or by Sequence) for linear readability.\r\n */\r\nexport function composeRollingContext(\r\n    query: string,\r\n    results: ContextAtom[],\r\n    tokenBudget: number = 4096\r\n): ContextResult {\r\n    // Constants\r\n    const CHARS_PER_TOKEN = 4; // Rough estimate\r\n\r\n    // Safety Buffer: Target 95% of budget to account for multibyte chars / math errors\r\n    const SAFE_BUDGET = Math.floor(tokenBudget * 0.95);\r\n    const charBudget = SAFE_BUDGET * CHARS_PER_TOKEN;\r\n\r\n    // 1. Dynamic Recency Analysis\r\n    // Check for temporal signals in query\r\n    const temporalSignals = [\"recent\", \"latest\", \"new\", \"today\", \"now\", \"current\", \"last\"];\r\n    const hasTemporalSignal = temporalSignals.some(signal => query.toLowerCase().includes(signal));\r\n\r\n    // Adjust weights based on intent\r\n    // Default: Relevance 70%, Recency 30%\r\n    // Temporal: Relevance 40%, Recency 60%\r\n    const RELEVANCE_WEIGHT = hasTemporalSignal ? 0.4 : config.CONTEXT_RELEVANCE_WEIGHT;\r\n    const RECENCY_WEIGHT = hasTemporalSignal ? 0.6 : config.CONTEXT_RECENCY_WEIGHT;\r\n\r\n    // 2. Normalize Recency & Score\r\n    const now = Date.now();\r\n    const oneMonth = 30 * 24 * 60 * 60 * 1000;\r\n\r\n    const candidates = results.map(atom => {\r\n        const age = Math.max(0, now - atom.timestamp);\r\n        // Recency Score: 1.0 = Brand new, 0.0 = >1 Month old (clamped)\r\n        const recencyScore = Math.max(0, 1.0 - (age / oneMonth));\r\n\r\n        // Final Mixed Score\r\n        const mixedScore = (atom.score * RELEVANCE_WEIGHT) + (recencyScore * RECENCY_WEIGHT);\r\n\r\n        return { ...atom, mixedScore, recencyScore };\r\n    });\r\n\r\n    // 3. Sort by Mixed Score (Descending)\r\n    candidates.sort((a, b) => b.mixedScore - a.mixedScore);\r\n\r\n    // 4. Selection (Fill Budget)\r\n    const selectedAtoms: typeof candidates = [];\r\n    let currentChars = 0;\r\n\r\n    for (const atom of candidates) {\r\n        if (currentChars >= charBudget) break;\r\n\r\n        const atomLen = atom.content.length;\r\n\r\n        if (currentChars + atomLen <= charBudget) {\r\n            selectedAtoms.push(atom);\r\n            currentChars += atomLen;\r\n        } else {\r\n            // Partial Fill with Smart Slicing\r\n            const remaining = charBudget - currentChars;\r\n            if (remaining > 200) {\r\n                // Slice to nearest punctuation to keep thought intact\r\n                // Look for . ! ? or \\n within the last 50 chars of the budget\r\n\r\n                // Finds last punctuation before the hard limit\r\n                const safeContent = atom.content.substring(0, remaining);\r\n\r\n                // Polyfill for finding last punctuation\r\n                const lastDot = safeContent.lastIndexOf('.');\r\n                const lastBang = safeContent.lastIndexOf('!');\r\n                const lastQ = safeContent.lastIndexOf('?');\r\n                const lastNew = safeContent.lastIndexOf('\\n');\r\n\r\n                const bestCut = Math.max(lastDot, lastBang, lastQ, lastNew);\r\n\r\n                if (bestCut > (remaining * 0.5)) {\r\n                    // If punctuation is reasonably far in, use it\r\n                    const slicedContent = atom.content.substring(0, bestCut + 1) + \" [Truncated]\";\r\n                    selectedAtoms.push({ ...atom, content: slicedContent });\r\n                    currentChars += slicedContent.length;\r\n                } else {\r\n                    // Fallback to hard cut if no punctuation found nearby\r\n                    const slicedContent = atom.content.substring(0, remaining) + \"...\";\r\n                    selectedAtoms.push({ ...atom, content: slicedContent });\r\n                    currentChars += slicedContent.length;\r\n                }\r\n            }\r\n            break; // Filled\r\n        }\r\n    }\r\n\r\n    // 5. Final Sort (Chronological / Flow)\r\n    // Preservation of narrative flow is key.\r\n    selectedAtoms.sort((a, b) => a.timestamp - b.timestamp);\r\n\r\n    // 6. Assemble\r\n    const contextString = selectedAtoms\r\n        .map(a => `[Source: ${a.source}] (${new Date(a.timestamp).toISOString()})\\n${a.content}`)\r\n        .join('\\n\\n');\r\n\r\n    return {\r\n        prompt: contextString,\r\n        stats: {\r\n            tokenCount: Math.ceil(currentChars / CHARS_PER_TOKEN),\r\n            charCount: currentChars,\r\n            filledPercent: Math.min(100, (currentChars / charBudget) * 100),\r\n            atomCount: selectedAtoms.length\r\n        }\r\n    };\r\n}\r\n"
    tokens: 1814
    size: 5230
  - path: engine\src\core\inference\llamaLoaderWorker.ts
    content: "\r\nimport { parentPort } from 'worker_threads';\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel, LlamaEmbeddingContext } from 'node-llama-cpp';\r\n\r\n// Worker state\r\nlet llama: any = null;\r\nlet model: LlamaModel | null = null;\r\nlet context: LlamaContext | null = null;\r\nlet session: LlamaChatSession | null = null;\r\nlet embeddingContext: LlamaEmbeddingContext | null = null; // Dedicated for embeddings\r\n\r\nasync function init() {\r\n    try {\r\n        llama = await getLlama();\r\n        parentPort?.postMessage({ type: 'ready' });\r\n    } catch (error: any) {\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n}\r\n\r\n// Handle messages from main thread\r\nparentPort?.on('message', async (message) => {\r\n    try {\r\n        switch (message.type) {\r\n            case 'loadModel':\r\n                await handleLoadModel(message.data);\r\n                break;\r\n            case 'chat':\r\n                await handleChat(message.data);\r\n                break;\r\n            case 'getEmbedding':\r\n                await handleGetEmbedding(message.data);\r\n                break;\r\n            case 'getEmbeddings':\r\n                await handleGetEmbeddings(message.data);\r\n                break;\r\n\r\n            case 'dispose':\r\n                await handleDispose();\r\n                break;\r\n        }\r\n    } catch (error: any) {\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n});\r\n\r\n// ... (handleLoadModel, handleChat existing code)\r\nasync function handleLoadModel(data: { modelPath: string, options: any }) {\r\n    if (!llama) await init();\r\n\r\n    if (model) {\r\n        try { await model.dispose(); } catch (e) { }\r\n    }\r\n    if (context) {\r\n        try { await context.dispose(); } catch (e) { }\r\n    }\r\n    if (embeddingContext) {\r\n        try { await embeddingContext.dispose(); } catch (e) { }\r\n    }\r\n\r\n    try {\r\n        model = await llama.loadModel({\r\n            modelPath: data.modelPath,\r\n            gpuLayers: data.options.gpuLayers || 0\r\n        });\r\n\r\n        context = await model!.createContext({\r\n            contextSize: data.options.contextSize || 4096,\r\n            batchSize: data.options.contextSize || 4096\r\n        });\r\n\r\n        // Initialize dedicated embedding context\r\n        // Critical: If this fails, we must fail the model load so the provider knows.\r\n        embeddingContext = await model!.createEmbeddingContext({\r\n            contextSize: data.options.contextSize || 2048,\r\n            batchSize: data.options.contextSize || 2048\r\n        });\r\n\r\n        session = new LlamaChatSession({\r\n            contextSequence: context!.getSequence(),\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n\r\n        parentPort?.postMessage({ type: 'modelLoaded', data: { modelPath: data.modelPath } });\r\n    } catch (error: any) {\r\n        throw new Error(`Failed to load model: ${error.message}`);\r\n    }\r\n}\r\n\r\nasync function handleChat(data: { prompt: string, options: any }) {\r\n    if (!session) throw new Error(\"Session not initialized\");\r\n\r\n    const response = await session.prompt(data.prompt, {\r\n        temperature: data.options.temperature || 0.7,\r\n        maxTokens: data.options.maxTokens || 1024\r\n    });\r\n\r\n    parentPort?.postMessage({ type: 'chatResponse', data: response });\r\n}\r\n\r\n// Handler for Single Embedding\r\nasync function handleGetEmbedding(data: { text: string }) {\r\n    if (!embeddingContext) throw new Error(\"Embedding Context not initialized\");\r\n\r\n    try {\r\n        const embedding = await embeddingContext.getEmbeddingFor(data.text);\r\n        parentPort?.postMessage({ type: 'embeddingResponse', data: Array.from(embedding.vector) });\r\n    } catch (e: any) {\r\n        throw new Error(`Embedding Generation Failed: ${e.message}`);\r\n    }\r\n}\r\n\r\n// Handler for Batch Embeddings\r\nasync function handleGetEmbeddings(data: { texts: string[] }) {\r\n    if (!embeddingContext) throw new Error(\"Embedding Context not initialized\");\r\n\r\n    try {\r\n        // console.log(`[Worker] Processing batch of ${data.texts?.length} texts`);\r\n        if (!data.texts || !Array.isArray(data.texts)) {\r\n            throw new Error(\"Invalid data.texts: expected array\");\r\n        }\r\n\r\n        const embeddings: number[][] = [];\r\n        for (let i = 0; i < data.texts.length; i++) {\r\n            const text = data.texts[i];\r\n            try {\r\n                if (typeof text !== 'string') {\r\n                    console.error(`[Worker] Invalid text at index ${i}:`, text);\r\n                    embeddings.push([]); // Push empty embedding for invalid input\r\n                    continue;\r\n                }\r\n                const embedding = await embeddingContext.getEmbeddingFor(text);\r\n                embeddings.push(Array.from(embedding.vector));\r\n            } catch (innerErr: any) {\r\n                console.error(`[Worker] Failed to embed text at index ${i} (\"${text?.substring(0, 20)}...\"): ${innerErr.message}`);\r\n                // Fallback: push zero vector or empty (handled by refiner)\r\n                // Based on refiner logic: if (batchEmbeddings && batchEmbeddings[j] && batchEmbeddings[j].length > 0)\r\n                embeddings.push([]);\r\n            }\r\n        }\r\n        parentPort?.postMessage({ type: 'embeddingsGenerated', data: embeddings });\r\n    } catch (e: any) {\r\n        throw new Error(`Batch Embedding Generation Failed: ${e.message}`);\r\n    }\r\n}\r\n\r\nasync function handleDispose() {\r\n    if (session) session.dispose();\r\n    if (context) await context.dispose();\r\n    if (embeddingContext) await embeddingContext.dispose();\r\n    if (model) await model.dispose();\r\n    parentPort?.postMessage({ type: 'disposed' });\r\n}\r\n\r\n// Start init\r\ninit();\r\n"
    tokens: 1960
    size: 5727
  - path: engine\src\index.ts
    content: |-
      /**
       * Sovereign Context Engine - Main Entry Point
       * 
       * This is the primary entry point for the TypeScript-based Context Engine.
       * It orchestrates all the core services including database management,
       * context ingestion, search functionality, and API services.
       */

      import 'dotenv/config';


      import express, { Request, Response } from 'express';
      import cors from 'cors';
      import path from 'path';
      import { fileURLToPath } from 'url';

      // Import core services
      import { db } from './core/db.js';
      import { setupRoutes } from './routes/api.js';

      // For __dirname equivalent in ES modules
      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);

      const app = express();
      const PORT = parseInt(process.env['PORT'] || '3000', 10);

      // Middleware
      app.use(cors());
      app.use(express.json({ limit: '50mb' }));
      app.use(express.urlencoded({ extended: true }));

      // API Routes
      setupRoutes(app);

      // Serve static files from the dist directory
      app.use('/static', express.static(path.join(__dirname, '../dist')));

      // Health check endpoint
      app.get('/health', (_req: Request, res: Response) => {
        res.status(200).json({
          status: 'Sovereign',
          timestamp: new Date().toISOString(),
          uptime: process.uptime(),
          version: '1.0.0'
        });
      });

      // Root endpoint
      // Serve Static Frontend
      const FRONTEND_DIST = path.join(__dirname, '../../frontend/dist');
      app.use(express.static(FRONTEND_DIST));

      // Fallback for SPA routing
      app.get('*', (_req, res) => {
        // Check if it's an API call first to avoid swallowing 404s for API
        if (_req.path.startsWith('/v1') || _req.path.startsWith('/health')) {
          res.status(404).json({ error: 'Not Found' });
          return;
        }
        res.sendFile(path.join(FRONTEND_DIST, 'index.html'));
      });

      // Initialize the database and start the server
      async function startServer() {
        try {
          console.log('Initializing Sovereign Context Engine...');

          // Initialize the database
          await db.init();
          console.log('Database initialized successfully');

          // Auto-Restore logic
          try {
            const { listBackups, restoreBackup } = await import('./services/backup/backup.js');
            const backups = await listBackups();
            if (backups.length > 0) {
              const latest = backups[0];
              console.log(`[Startup] Found backup: ${latest}. Attempting restore...`);
              await restoreBackup(latest);
              console.log(`[Startup] Restore complete.`);
            } else {
              console.log(`[Startup] No backups found. Starting fresh.`);
            }
          } catch (e: any) {
            console.error(`[Startup] Restore failed: ${e.message}. Continuing...`);
          }

          // Start Watchdog
          // Start Watchdog
          const { startWatchdog } = await import('./services/ingest/watchdog.js');
          startWatchdog();

          // Start Dreamer Service (Temporal Clustering)
          const { dream } = await import('./services/dreamer/dreamer.js');
          const { config } = await import('./config/index.js');

          console.log(`[Startup] Initializing Dreamer (Interval: ${config.DREAM_INTERVAL_MS}ms)...`);
          setInterval(async () => {
            try {
              const result = await dream();
              if (result.status !== 'skipped' && result.analyzed && result.analyzed > 0) {
                console.log(`[Dreamer] Cycle Complete: Analyzed ${result.analyzed}, Updated ${result.updated}`);
              }
            } catch (e: any) {
              console.error(`[Dreamer] Cycle Failed: ${e.message}`);
            }
          }, config.DREAM_INTERVAL_MS);

          // Start the server
          app.listen(PORT, () => {
            console.log(`Sovereign Context Engine running on port ${PORT}`);
            console.log(`Health check available at http://localhost:${PORT}/health`);
          });
        } catch (error) {
          console.error('Failed to start the Sovereign Context Engine:', error);
          process.exit(1);
        }
      }

      // Handle graceful shutdown
      process.on('SIGINT', async () => {
        console.log('\nShutting down gracefully...');
        try {
          await db.close();
          console.log('Database connection closed.');
          process.exit(0);
        } catch (error) {
          console.error('Error during shutdown:', error);
          process.exit(1);
        }
      });

      // Start the server
      startServer();

      export { app };
    tokens: 1510
    size: 4137
  - path: engine\src\routes\api.ts
    content: |-
      /**
       * API Routes for Sovereign Context Engine
       * 
       * Standardized API Interface implementing UniversalRAG architecture.
       */

      import { Application, Request, Response } from 'express';
      import * as crypto from 'crypto';
      import { db } from '../core/db.js';

      // Import services and types
      import { executeSearch } from '../services/search/search.js';
      import { dream } from '../services/dreamer/dreamer.js';
      import { getState, clearState } from '../services/scribe/scribe.js';
      import { listModels, loadModel, runSideChannel } from '../services/llm/provider.js';
      import { createBackup, listBackups, restoreBackup } from '../services/backup/backup.js';
      import { SearchRequest } from '../types/api.js';

      export function setupRoutes(app: Application) {
        // Ingestion endpoint
        app.post('/v1/ingest', async (req: Request, res: Response) => {
          try {
            const { content, source, type, bucket, buckets = [], tags = [] } = req.body;

            if (!content) {
              res.status(400).json({ error: 'Content is required' });
              return;
            }

            // Handle legacy single-bucket param
            const allBuckets = bucket ? [...buckets, bucket] : buckets;

            // Generate a unique ID for the memory
            const id = `mem_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
            const timestamp = Date.now();
            const hash = crypto.createHash('sha256').update(content).digest('hex');

            // Insert into the database
            console.log(`[API] Ingesting memory: ${id} (Source: ${source || 'unknown'})`);
            // Schema: id => timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding
            await db.run(
              `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data 
               :insert memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,
              {
                data: [[
                  id,
                  timestamp,
                  content,
                  source || 'unknown',
                  source || 'unknown',
                  0,
                  type || 'text',
                  hash,
                  allBuckets,
                  [], // epochs (aligned with schema)
                  tags,
                  'external',
                  new Array(384).fill(0.0)
                ]]
              }
            );

            // Verification (Standard 059: Read-After-Write)
            // We check for the specific ID we just inserted.
            const verify = await db.run(`?[id] := *memory{id}, id = $id`, { id });
            const count = verify.rows ? verify.rows.length : 0;

            console.log(`[API] VERIFY ID ${id}: Found ${count}`);

            if (count === 0) {
              throw new Error(`Ingestion Verification Failed: ID ${id} not found after write.`);
            }

            try {
              const fs = await import('fs');
              const path = await import('path');
              const logPath = path.join(process.cwd(), 'debug_force.log');
              fs.appendFileSync(logPath, `[${new Date().toISOString()}] Ingest Success: ${id} | Count: ${count}\n`);
              console.log(`[API] Logged to ${logPath}`);
            } catch (e) {
              console.error('[API] Log Write Failed', e);
            }

            res.status(200).json({
              status: 'success',
              id,
              message: 'Content ingested successfully'
            });
          } catch (error: any) {
            console.error('Ingestion error:', error);
            res.status(500).json({ error: 'Failed to ingest content', details: error.message });
          }
        });

        // POST Search endpoint (Standard UniversalRAG)
        app.post('/v1/memory/search', async (req: Request, res: Response) => {
          try {
            const body = req.body as SearchRequest;
            if (!body.query) {
              res.status(400).json({ error: 'Query is defined' });
              return;
            }

            // Map request to executeSearch args
            const result = await executeSearch(
              body.query,
              undefined,
              body.buckets,
              body.max_chars || 5000,
              body.deep || false,
              body.provenance || 'all'
            );

            // Construct standard response
            res.status(200).json({
              status: 'success',
              context: result.context,
              results: result.results,
              metadata: {
                engram_hits: 0,
                vector_latency: 0,
                provenance_boost_active: true,
                ...(result.metadata || {})
              }
            });
          } catch (error: any) {
            console.error('Search error:', error);
            res.status(500).json({ error: error.message });
          }
        });

        // GET Search (Legacy support) - redirect to use POST effectively
        app.get('/v1/memory/search', async (_req: Request, res: Response) => {
          res.status(400).json({ error: "Use POST /v1/memory/search for complex queries." });
        });

        // Get all buckets
        app.get('/v1/buckets', async (_req: Request, res: Response) => {
          try {
            const result = await db.run('?[bucket] := *memory{buckets}, bucket in buckets');
            const buckets = result.rows ? [...new Set(result.rows.map((row: any) => row[0]))].sort() : [];
            res.status(200).json(buckets);
          } catch (error) {
            console.error('Bucket retrieval error:', error);
            res.status(500).json({ error: 'Failed to retrieve buckets' });
          }
        });

        // Backup Endpoints
        // POST /v1/backup - Create a new backup
        app.post('/v1/backup', async (_req: Request, res: Response) => {
          try {
            const result = await createBackup();
            res.status(200).json(result);
          } catch (e: any) {
            console.error("Backup Failed", e);
            res.status(500).json({ error: e.message });
          }
        });

        // GET /v1/backups - List available backups
        app.get('/v1/backups', async (_req: Request, res: Response) => {
          try {
            const result = await listBackups();
            res.status(200).json(result);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // POST /v1/backup/restore - Restore a specific backup
        app.post('/v1/backup/restore', async (req: Request, res: Response) => {
          try {
            const { filename } = req.body;
            if (!filename) {
              res.status(400).json({ error: "Filename required" });
              return;
            }
            const result = await restoreBackup(filename);
            res.status(200).json(result);
          } catch (e: any) {
            console.error("Restore Failed", e);
            res.status(500).json({ error: e.message });
          }
        });

        // GET /v1/backup (Legacy Dump) - Kept for compatibility or download
        // Modifying to use createBackup logic but stream result?
        // Current createBackup writes to disk.
        // Let's redirect to disk file download if needed, or keep previous logic.
        // The user wanted "Save to server".
        // Let's keep the GET for downloading the LATEST backup or generating one on fly?
        // Let's make GET just return text of latest? Or generate on fly?
        // Let's generate on fly like before for "Dump".
        app.get('/v1/backup', async (_req: Request, res: Response) => {
          // Return ID of new backup? Or stream content?
          // Legacy behavior was stream content.
          try {
            const result = await createBackup();
            const path = await import('path');
            // const fs = await import('fs'); // Unused
            const fpath = path.join(process.cwd(), 'backups', result.filename);
            res.download(fpath);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // Trigger Dream Endpoint
        app.post('/v1/dream', async (_req: Request, res: Response) => {
          try {
            const result = await dream();
            res.status(200).json(result);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // LLM: List Models
        app.get('/v1/models', async (req: Request, res: Response) => {
          try {
            const dir = req.query['dir'] as string | undefined;
            const models = await listModels(dir);
            res.status(200).json(models);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // LLM: Load Model
        app.post('/v1/inference/load', async (req: Request, res: Response) => {
          try {
            const { model, options, dir } = req.body; // dir optional, used to construct absolute path if model is just filename?
            if (!model) {
              res.status(400).json({ error: "Model name required" });
              return;
            }

            // If dir provided and model is not absolute, join them
            const path = await import('path');
            let modelPath = model;
            if (dir && !path.isAbsolute(model)) {
              modelPath = path.join(dir, model);
            }

            const result = await loadModel(modelPath, options);
            res.status(200).json(result);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // LLM: Chat Completions (SSE Streaming)
        app.post('/v1/chat/completions', async (req: Request, res: Response) => {
          try {
            const { messages, options } = req.body;
            const lastMsg = messages[messages.length - 1];
            const prompt = lastMsg.content;

            res.setHeader('Content-Type', 'text/event-stream');
            res.setHeader('Cache-Control', 'no-cache');
            res.setHeader('Connection', 'keep-alive');

            const fullResponse = (await runSideChannel(prompt, "You are a helpful AI.", options)) as string | null;

            if (!fullResponse) {
              res.write(`data: ${JSON.stringify({ error: "No response from model" })}\n\n`);
              res.end();
              return;
            }

            // Simulate streaming by chunks
            // Simulate streaming by chunks
            const chunkSize = 20;
            for (let i = 0; i < fullResponse.length; i += chunkSize) {
              const chunk = fullResponse.substring(i, i + chunkSize);
              const packet = {
                choices: [{
                  delta: { content: chunk }
                }]
              };
              res.write(`data: ${JSON.stringify(packet)}\n\n`);
              await new Promise(r => setTimeout(r, 10));
            }

            res.write('data: [DONE]\n\n');
            res.end();

          } catch (e: any) {
            console.error("Chat Error", e);
            res.write(`data: ${JSON.stringify({ error: e.message })}\n\n`);
            res.end();
          }
        });


        // Scribe State Endpoints
        // Get State
        // Note: We need to import getState, clearState from services.
        // I will add the import at the top first, then this block.
        // Actually, I can use "import(...)" if I don't want to mess up top level imports, but better to update top level.
        // Let's assume I updated imports.

        app.get('/v1/scribe/state', async (_req: Request, res: Response) => {
          try {
            const state = await getState();
            res.status(200).json({ state });
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        app.delete('/v1/scribe/state', async (_req: Request, res: Response) => {
          try {
            const result = await clearState();
            res.status(200).json(result);
          } catch (e: any) {
            res.status(500).json({ error: e.message });
          }
        });

        // System config endpoint
        app.get('/v1/system/config', (_req: Request, res: Response) => {
          res.status(200).json({
            status: 'success',
            config: {
              version: '1.0.0',
              engine: 'Sovereign Context Engine',
              timestamp: new Date().toISOString()
            }
          });
        });
      }
    tokens: 4092
    size: 11139
  - path: engine\src\services\backup\backup.ts
    content: "\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { db } from '../../core/db.js';\r\n\r\nconst BACKUP_DIR = path.join(process.cwd(), 'backups');\r\n\r\nif (!fs.existsSync(BACKUP_DIR)) {\r\n    fs.mkdirSync(BACKUP_DIR);\r\n}\r\n\r\nexport interface BackupStats {\r\n    memory_count: number;\r\n    source_count: number;\r\n    engram_count: number;\r\n    timestamp: string;\r\n}\r\n\r\nexport async function createBackup(): Promise<{ filename: string; stats: BackupStats }> {\r\n    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');\r\n    const filename = `backup_${timestamp}.json`;\r\n    const filePath = path.join(BACKUP_DIR, filename);\r\n\r\n    console.log(`[Backup] Starting backup to ${filename}...`);\r\n\r\n    // 1. Dump Memory\r\n    const memoryResult = await db.run('?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] := *memory{id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}');\r\n\r\n    // 2. Dump Source\r\n    const sourceResult = await db.run('?[path, hash, total_atoms, last_ingest] := *source{path, hash, total_atoms, last_ingest}');\r\n\r\n    // 3. Dump Engrams\r\n    const engramResult = await db.run('?[key, value] := *engrams{key, value}');\r\n\r\n    const backupData = {\r\n        timestamp: new Date().toISOString(),\r\n        memory: memoryResult.rows || [],\r\n        source: sourceResult.rows || [],\r\n        engrams: engramResult.rows || []\r\n    };\r\n\r\n    await fs.promises.writeFile(filePath, JSON.stringify(backupData, null, 2));\r\n\r\n    const stats: BackupStats = {\r\n        memory_count: (backupData.memory).length,\r\n        source_count: (backupData.source).length,\r\n        engram_count: (backupData.engrams).length,\r\n        timestamp: backupData.timestamp\r\n    };\r\n\r\n    console.log(`[Backup] Completed. Stats:`, stats);\r\n    return { filename, stats };\r\n}\r\n\r\nexport async function listBackups(): Promise<string[]> {\r\n    if (!fs.existsSync(BACKUP_DIR)) return [];\r\n    const files = await fs.promises.readdir(BACKUP_DIR);\r\n    return files.filter(f => f.endsWith('.json')).sort().reverse(); // Newest first\r\n}\r\n\r\nexport async function restoreBackup(filename: string): Promise<BackupStats> {\r\n    const filePath = path.join(BACKUP_DIR, filename);\r\n    if (!fs.existsSync(filePath)) {\r\n        throw new Error(`Backup file not found: ${filename}`);\r\n    }\r\n\r\n    console.log(`[Backup] Restoring from ${filename}...`);\r\n    const data = JSON.parse(await fs.promises.readFile(filePath, 'utf8'));\r\n\r\n    // 1. Restore Memory\r\n    if (data.memory && data.memory.length > 0) {\r\n        // Clear table? User requested \"load the db FROM the backup... THEN ingest\". \r\n        // Usually restore implies wiping current state or merging.\r\n        // Idempotent Put handles merging.\r\n        // If we want to restore to a specific state, we might ideally wipe first.\r\n        // But \"Attempt to not add in the same data if it exactly matches\" suggests merging/idempotency.\r\n        // Let's use :put (Upsert).\r\n\r\n        // Batch insert\r\n        const BATCH_SIZE = 100;\r\n        for (let i = 0; i < data.memory.length; i += BATCH_SIZE) {\r\n            const batch = data.memory.slice(i, i + BATCH_SIZE);\r\n            await db.run(\r\n                `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data\r\n                 :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n                { data: batch }\r\n            );\r\n        }\r\n    }\r\n\r\n    // 2. Restore Source\r\n    if (data.source && data.source.length > 0) {\r\n        await db.run(\r\n            `?[path, hash, total_atoms, last_ingest] <- $data :put source {path, hash, total_atoms, last_ingest}`,\r\n            { data: data.source }\r\n        );\r\n    }\r\n\r\n    // 3. Restore Engrams\r\n    if (data.engrams && data.engrams.length > 0) {\r\n        await db.run(\r\n            `?[key, value] <- $data :put engrams {key, value}`,\r\n            { data: data.engrams }\r\n        );\r\n    }\r\n\r\n    console.log(`[Backup] Restore Completed.`);\r\n\r\n    return {\r\n        memory_count: data.memory?.length || 0,\r\n        source_count: data.source?.length || 0,\r\n        engram_count: data.engrams?.length || 0,\r\n        timestamp: new Date().toISOString()\r\n    };\r\n}\r\n"
    tokens: 1558
    size: 4375
  - path: engine\src\services\dreamer\dreamer.ts
    content: |-
      /**
       * Dreamer Service - Markovian Memory Organization with Epochal Historian
       *
       * Implements:
       * 1. Markovian reasoning for background memory organization
       * 2. Deterministic Temporal Tagging for grounding memories in time
       * 3. Epochal Historian for identifying Epochs, Episodes, and Entities
       */

      import { db } from '../../core/db.js';

      // AsyncLock implementation for preventing concurrent dream cycles
      class AsyncLock {
        private locked = false;
        private waiting: Array<(releaser: () => void) => void> = [];

        async acquire(): Promise<() => void> {
          if (!this.locked) {
            this.locked = true;
            return this.release.bind(this);
          }

          return new Promise<() => void>((resolve) => {
            this.waiting.push(resolve);
          });
        }

        private release(): void {
          if (this.waiting.length > 0) {
            const next = this.waiting.shift();
            if (next) next(this.release.bind(this));
          } else {
            this.locked = false;
          }
        }

        get isLocked(): boolean {
          return this.locked;
        }
      }

      const dreamLock = new AsyncLock();

      // Temporal constants
      const SEASONS: { [key: number]: string } = {
        0: 'Winter', 1: 'Winter', 2: 'Spring',
        3: 'Spring', 4: 'Spring', 5: 'Summer',
        6: 'Summer', 7: 'Summer', 8: 'Autumn',
        9: 'Autumn', 10: 'Autumn', 11: 'Winter'
      };

      const QUARTERS: { [key: number]: string } = {
        0: 'Q1', 1: 'Q1', 2: 'Q1',
        3: 'Q2', 4: 'Q2', 5: 'Q2',
        6: 'Q3', 7: 'Q3', 8: 'Q3',
        9: 'Q4', 10: 'Q4', 11: 'Q4'
      };

      const DAYS = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'];
      const MONTHS = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'];

      /**
       * Generates deterministic temporal tags based on the timestamp
       */
      function generateTemporalTags(timestamp: number): string[] {
        if (!timestamp) return [];

        const date = new Date(timestamp);
        if (isNaN(date.getTime())) return [];

        const tags = new Set<string>();
        const monthIndex = date.getMonth();

        // Core Date Units
        tags.add(date.getFullYear().toString());
        tags.add(MONTHS[monthIndex]);
        tags.add(DAYS[date.getDay()]);

        // Broad Temporal Buckets
        tags.add(SEASONS[monthIndex]);
        tags.add(QUARTERS[monthIndex]);

        // Time of Day
        const hour = date.getHours();
        if (hour >= 5 && hour < 12) tags.add('Morning');
        else if (hour >= 12 && hour < 17) tags.add('Afternoon');
        else if (hour >= 17 && hour < 21) tags.add('Evening');
        else tags.add('Night');

        return Array.from(tags);
      }

      /**
       * Performs background memory organization using Markovian reasoning
       * Identifies Epochs, Episodes, and Entities as part of the Epochal Historian
       */
      export async function dream(): Promise<{ status: string; analyzed?: number; updated?: number; message?: string }> {
        // Check if a dream cycle is already running
        if (dreamLock.isLocked) {
          return {
            status: 'skipped',
            message: 'Previous dream cycle still running'
          };
        }

        const release = await dreamLock.acquire();

        try {
          console.log(' Dreamer: Starting self-organization cycle...');

          // 1. Get all memories that might benefit from re-categorization
          const allMemoriesQuery = '?[id, content, buckets, timestamp] := *memory{id, content, buckets, timestamp}';
          const allMemoriesResult = await db.run(allMemoriesQuery);

          if (!allMemoriesResult.rows || allMemoriesResult.rows.length === 0) {
            return { status: 'success', analyzed: 0, message: 'No memories to analyze' };
          }

          // Filter memories that need attention
          const memoriesToAnalyze = allMemoriesResult.rows.filter((row: any[]) => {
            const [_, __, buckets, timestamp] = row;

            // Always include memories with no buckets
            if (!buckets || buckets.length === 0) return true;

            // Include memories with generic buckets
            const genericBuckets = ['core', 'misc', 'general', 'other', 'unknown'];
            const hasOnlyGenericBuckets = buckets.every((bucket: string) => genericBuckets.includes(bucket));
            if (hasOnlyGenericBuckets) return true;

            // Include memories that lack temporal tags
            const year = new Date(timestamp).getFullYear().toString();
            if (!buckets.includes(year)) return true;

            return false;
          });

          console.log(` Dreamer: Found ${memoriesToAnalyze.length} memories to analyze.`);

          let updatedCount = 0;

          // Process in batches using Shared Module
          const { processInBatches } = await import('../../core/batch.js');
          const { config } = await import('../../config/index.js');
          const batchSize = config.DREAMER_BATCH_SIZE || 5;

          const totalBatches = Math.ceil(memoriesToAnalyze.length / batchSize);
          await processInBatches(memoriesToAnalyze, async (batch: any[], batchIndex: number) => {
            if ((batchIndex + 1) % 5 === 0 || batchIndex === 0 || batchIndex === totalBatches - 1) {
              console.log(`[Dreamer] Processing batch ${batchIndex + 1}/${totalBatches} (${batch.length} memories)...`);
            }

            for (const row of batch) {
              const [id, _content, currentBuckets, timestamp] = row;

              try {
                // Generate temporal tags
                const temporalTags = generateTemporalTags(timestamp);

                // Only call LLM for semantic tags if we don't have rich tags yet
                let newSemanticTags: string[] = [];
                const meaningfulBuckets = (currentBuckets || []).filter((b: string) =>
                  !['core', 'pending'].includes(b) && !/^\d{4}$/.test(b) // Exclude years
                );

                if (meaningfulBuckets.length < 2) {
                  newSemanticTags = ['semantic_tag_placeholder'];
                }

                // Combine tags: Old + Semantic + Temporal
                const combinedBuckets = [
                  ...new Set([
                    ...(currentBuckets || []),
                    ...newSemanticTags,
                    ...temporalTags
                  ])
                ];

                // Cleanup: Remove generic tags if we have specific ones
                let finalBuckets = [...combinedBuckets];
                if (combinedBuckets.length > 1) {
                  const specificBuckets = combinedBuckets.filter((b: string) =>
                    !['core', 'pending', 'misc', 'general', 'other', 'unknown', 'inbox'].includes(b)
                  );
                  if (specificBuckets.length > 0) {
                    finalBuckets = specificBuckets;
                  }
                }

                // Update the memory with new buckets
                const updateQuery = `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] := *memory{id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}, id = $id`;
                const currentResult = await db.run(updateQuery, { id });

                if (currentResult.rows && currentResult.rows.length > 0) {
                  const [_, ts, cont, src, srcId, seq, typ, hash, __, tag, epoch, prov, emb] = currentResult.rows[0];

                  // Delete old record
                  await db.run(`?[id] <- [[$id]] :delete memory {id}`, { id });

                  // Insert updated record with ALL columns
                  await db.run(
                    `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,
                    { data: [[id, ts, cont, src, srcId, seq, typ, hash, finalBuckets, tag, epoch, prov, emb]] }
                  );

                  updatedCount++;
                }
              } catch (error: any) {
                console.error(` Dreamer: Failed to process memory ${id}:`, error.message);
              }
            }
          }, { batchSize });

          // NEW: The Abstraction Pyramid - Cluster and Summarize into Episodes/Epochs
          await clusterAndSummarize();

          // MIRROR PROTOCOL: Export to Notebook
          try {
            console.log(' Dreamer: Triggering Mirror Protocol...');
            // Dynamic import to handle JS file and potential circular deps
            const { createMirror } = await import('../mirror/mirror.js');
            await createMirror();
          } catch (mirrorError: any) {
            console.error(' Dreamer: Mirror Protocol failed:', mirrorError.message);
          }

          return {
            status: 'success',
            analyzed: memoriesToAnalyze.length,
            updated: updatedCount
          };
        } catch (error) {
          console.error(' Dreamer Fatal Error:', error);
          throw error;
        } finally {
          release();
        }
      }

      /**
       * The Abstraction Pyramid: Clusters Atoms into Episodes and Epochs
       * Uses Iterative Summarization to prevent Context Window overflow.
       */
      async function clusterAndSummarize(): Promise<void> {
        try {
          console.log(' Dreamer: Running Abstraction Pyramid analysis...');

          const { runSideChannel } = await import('../llm/provider.js');

          // 1. Find Unbound Atoms (Level 1 Nodes without a Parent)
          const { config } = await import('../../config/index.js');
          const limit = (config.DREAMER_BATCH_SIZE || 5) * 4; // Fetch 4x batch size for clustering context

          // We look for memories that are NOT a child in 'parent_of'
          // Cozo: `?[id] := *memory{id}, not *parent_of{child_id: id}`
          const unboundQuery = `
                  ?[id, timestamp, content] := *memory{id, timestamp, content},
                  not *parent_of{child_id: id},
                  :order timestamp
          :limit ${limit}
          `;
          const result = await db.run(unboundQuery);

          if (!result.rows || result.rows.length === 0) {
            console.log(' Dreamer: No unbound atoms found.');
            return;
          }

          const atoms = result.rows.map((r: any[]) => ({ id: r[0], timestamp: r[1], content: r[2] }));
          console.log(` Dreamer: Found ${atoms.length} unbound atoms.Clustering...`);

          // 2. Temporal Clustering (Gap > 15 minutes = New Cluster)
          const clusters: any[][] = [];
          let currentCluster: any[] = [];
          let lastTime = atoms[0].timestamp;

          for (const atom of atoms) {
            if (atom.timestamp - lastTime > config.DREAMER_CLUSTERING_GAP_MS) {
              if (currentCluster.length > 0) clusters.push(currentCluster);
              currentCluster = [];
            }
            currentCluster.push(atom);
            lastTime = atom.timestamp;
          }
          if (currentCluster.length > 0) clusters.push(currentCluster);

          // 3. Process Clusters -> Episodes (Level 2)
          for (const cluster of clusters) {
            if (cluster.length < 3) continue; // Skip tiny clusters for now, wait for more context? 
            // Or just summarize them if they are old enough?
            // For now, let's process clusters of size >= 3.

            console.log(` Dreamer: Summarizing cluster of ${cluster.length} atoms...`);

            // Iterative Summarization (Map-Reduce)
            let runningSummary = "";

            // Map: Read Atoms
            // Reduce: Summarize (Prev + Next)

            for (let i = 0; i < cluster.length; i++) {
              const atom = cluster[i];
              const content = String(atom.content);

              // If we have a running summary, combine it.
              if (runningSummary) {
                // Reduce Step
                const prompt = `
                          Current Episode Summary: "${runningSummary}"
                          
                          Next Event: "${content}"
                          
                          Update the summary to include the new event naturally.Keep it concise.
                          `;
                const updated = (await runSideChannel(prompt)) as string;
                if (updated) runningSummary = updated;
                else runningSummary += `\n${content} `; // Fallback
              } else {
                // Start
                runningSummary = content;
                // Initial summarization if first chunk is huge?
                if (content.length > 500) {
                  const initialFix = (await runSideChannel(`Summarize this event concisely: ${content} `)) as string;
                  if (initialFix) runningSummary = initialFix;
                }
              }
            }

            // Create Episode Node (Level 2)
            const crypto = await import('crypto');
            const summaryHash = crypto.createHash('sha256').update(runningSummary).digest('hex');
            const episodeId = `ep_${summaryHash.substring(0, 16)} `;
            const startTime = cluster[0].timestamp;
            const endTime = cluster[cluster.length - 1].timestamp;

            // Insert Summary Node
            // :create summary_node { id, type, content, span_start, span_end, embedding }
            await db.run(
              `?[id, type, content, span_start, span_end, embedding] <- [[$id, $type, $content, $start, $end, $emb]]
            :put summary_node { id, type, content, span_start, span_end, embedding }`,
              {
                id: episodeId,
                type: 'episode',
                content: runningSummary,
                start: startTime,
                end: endTime,
                emb: new Array(384).fill(0.0) // Placeholder
              }
            );

            // Link Atoms to Episode (Parent_Of)
            const edges = cluster.map(atom => [episodeId, atom.id, 1.0]);
            await db.run(
              `?[parent_id, child_id, weight] <- $edges :put parent_of { parent_id, child_id, weight }`,
              { edges }
            );

            console.log(` Dreamer: Created Episode ${episodeId} from ${cluster.length} atoms.`);
          }

        } catch (e: any) {
          console.error(' Dreamer: Error in Abstraction Pyramid:', e.message);
        }
      }
    tokens: 4678
    size: 13174
  - path: engine\src\services\inference\inference.ts
    content: |-
      /**
       * Inference Service for Sovereign Context Engine
       * 
       * Handles all LLM inference operations including model loading,
       * chat sessions, and token streaming.
       */

      // import { db } from '../../core/db'; // Unused import
      import config from '../../config/index';
      // import { fileURLToPath } from 'url'; // Unused


      // For __dirname equivalent in ES modules
      // const __filename = fileURLToPath(import.meta.url); // Unused
      // const __dirname = path.dirname(__filename); // This variable is not used anywhere else in the file.

      // Define interfaces
      interface InferenceOptions {
        model?: string;
        contextSize?: number;
        gpuLayers?: number;
        temperature?: number;
        maxTokens?: number;
      }

      interface ChatRequest {
        messages: Array<{ role: string; content: string }>;
        model?: string;
        options?: InferenceOptions;
      }

      // Placeholder for the actual Llama provider implementation
      class LlamaProvider {
        async loadModel(modelPath: string, _options: InferenceOptions): Promise<any> {
          // In a real implementation, this would load the actual model
          console.log(`Loading model from: ${modelPath}`);
          return { model: modelPath, loaded: true };
        }

        async createSession(model: any, contextSize: number): Promise<any> {
          // In a real implementation, this would create a chat session
          return { model, contextSize, sessionId: Math.random().toString(36).substr(2, 9) };
        }

        async chatCompletion(_session: any, _messages: any[], _options: InferenceOptions): Promise<any> {
          // In a real implementation, this would run the actual inference
          return {
            choices: [{
              message: {
                role: 'assistant',
                content: 'This is a simulated response from the LLM.'
              }
            }]
          };
        }
      }

      const llamaProvider = new LlamaProvider();

      /**
       * Initialize the inference engine with the specified model
       */
      export async function initializeInference(modelPath?: string, options: InferenceOptions = {}): Promise<{ success: boolean; message: string; model?: any }> {
        try {
          // const modelToLoad = modelPath || config.MODELS.MAIN.PATH; // Unused
          const inferenceOptions = {
            contextSize: options.contextSize || config.MODELS.MAIN.CTX_SIZE,
            gpuLayers: options.gpuLayers || config.MODELS.MAIN.GPU_LAYERS,
            temperature: options.temperature || 0.7,
            maxTokens: options.maxTokens || 1024
          };

          const modelPathString = modelPath || 'default-model';
          const model = await llamaProvider.loadModel(modelPathString, inferenceOptions);

          return {
            success: true,
            message: 'Inference engine initialized successfully',
            model
          };
        } catch (error: any) {
          return {
            success: false,
            message: `Failed to initialize inference engine: ${error.message}`
          };
        }
      }

      /**
       * Run a chat completion with the loaded model
       */
      export async function runChatCompletion(request: ChatRequest): Promise<{ success: boolean; response?: any; error?: string }> {
        try {
          // In a real implementation, we would use the actual loaded model
          // For now, we'll simulate the response

          const response = await llamaProvider.chatCompletion(
            { /* placeholder for actual model */ },
            request.messages,
            request.options || {}
          );

          return {
            success: true,
            response: response.choices[0].message
          };
        } catch (error: any) {
          return {
            success: false,
            error: error.message
          };
        }
      }

      /**
       * Run a simple text completion
       */
      export async function runCompletion(prompt: string, options: InferenceOptions = {}): Promise<{ success: boolean; response?: string; error?: string }> {
        try {
          // Simulate a completion request
          const messages = [{ role: 'user', content: prompt }];
          const request: ChatRequest = { messages, options };

          const result = await runChatCompletion(request);

          if (result.success && result.response) {
            return {
              success: true,
              response: result.response.content as string
            };
          } else {
            return {
              success: false,
              error: result.error || 'Unknown error occurred'
            };
          }
        } catch (error: any) {
          return {
            success: false,
            error: error.message
          };
        }
      }

      /**
       * Get the current status of the inference engine
       */
      export function getInferenceStatus(): { loaded: boolean; model?: string; error?: string } {
        // In a real implementation, this would check the actual model status
        return {
          loaded: true, // Assuming it's loaded for this simulation
          model: config.MODELS.MAIN.PATH,
          error: undefined
        };
      }
    tokens: 1610
    size: 4536
  - path: engine\src\services\ingest\atomizer.ts
    content: "/**\r\n * Markovian Atomizer\r\n * \r\n * Splits text content into \"Thought Atoms\" based on semantic density and natural boundaries.\r\n * Implements the \"Markovian Chunking\" strategy:\r\n * 1. Primary Split: Logical Blocks (Double Newline).\r\n * 2. Secondary Split: Length Constraint (>1000 chars) with Sentence Overlap.\r\n */\r\n\r\nexport function atomizeContent(text: string, strategy: 'code' | 'prose' | 'blob' = 'prose'): string[] {\r\n    // Strategy: Code - Split by top-level blocks (indentation-based)\r\n    if (strategy === 'code') {\r\n        const lines = text.split('\\n');\r\n        const atoms: string[] = [];\r\n        let currentChunk = '';\r\n\r\n        // Helper to push and reset\r\n        const pushChunk = () => {\r\n            if (currentChunk.trim().length > 0) {\r\n                atoms.push(currentChunk.trim());\r\n                currentChunk = '';\r\n            }\r\n        };\r\n\r\n        for (const line of lines) {\r\n            // Check for top-level definitions (no indentation or specific keywords)\r\n            // Regex checks for: Starts with non-whitespace, AND isn't a closing brace only\r\n            const isTopLevel = /^[^\\s]/.test(line) && !/^[\\}\\] \\t]*$/.test(line);\r\n\r\n            // If it's a new top-level block AND we have a substantial chunk, split.\r\n            // But don't split if the current chunk is small (< 500 chars) to keep related imports/vars together.\r\n            if (isTopLevel && currentChunk.length > 500) {\r\n                pushChunk();\r\n            }\r\n\r\n            // Hard limit safety valve (2000 chars)\r\n            if ((currentChunk + line).length > 2000) {\r\n                pushChunk();\r\n            }\r\n\r\n            currentChunk += line + '\\n';\r\n        }\r\n        pushChunk();\r\n        return enforceMaxSize(atoms, 6000, 200);\r\n    }\r\n\r\n    if (strategy === 'blob') {\r\n        // Just hard split every 1500 chars with overlap to be extremely safe for dense/binary text\r\n        return enforceMaxSize([text], 1500, 100);\r\n    }\r\n\r\n    // 1. Primary Split: Logical Blocks (Paragraphs)\r\n    // This preserves the \"Thought\" unit.\r\n    const rawBlocks = text.split(/\\n\\s*\\n/);\r\n\r\n    const atoms: string[] = [];\r\n\r\n    for (const block of rawBlocks) {\r\n        if (block.trim().length === 0) continue;\r\n\r\n        // 2. Secondary Split: Length Constraint (1000 chars)\r\n        // If a paragraph is massive, we chop it by sentence.\r\n        if (block.length > 1000) {\r\n            // Split by sentence endings (. ! ? ) followed by space or end of string\r\n            const sentences = block.match(/[^.!?]+[.!?]+(\\s+|$)|[^.!?]+$/g) || [block];\r\n\r\n            let currentChunk = \"\";\r\n\r\n            for (const sentence of sentences) {\r\n                if ((currentChunk + sentence).length > 1000) {\r\n                    if (currentChunk.trim().length > 0) {\r\n                        atoms.push(currentChunk.trim());\r\n                    }\r\n\r\n                    // OVERLAP: Keep the last sentence as the start of the new chunk\r\n                    // This creates the \"Markov Link\"\r\n                    const sentenceParts = currentChunk.match(/[^.!?]+[.!?]+(\\s+|$)/g);\r\n                    let lastSentence = \"\";\r\n                    if (sentenceParts && sentenceParts.length > 0) {\r\n                        lastSentence = sentenceParts[sentenceParts.length - 1];\r\n                    }\r\n\r\n                    currentChunk = lastSentence + sentence;\r\n                } else {\r\n                    currentChunk += sentence;\r\n                }\r\n            }\r\n            if (currentChunk.trim().length > 0) {\r\n                atoms.push(currentChunk.trim());\r\n            }\r\n        } else {\r\n            // Small block = 1 Atom\r\n            atoms.push(block.trim());\r\n        }\r\n    }\r\n\r\n    // FINAL PASS: Strict Size Enforcement\r\n    // Ensure no atom exceeds the hard limit (6000 chars), splitting strictly if needed.\r\n    return enforceMaxSize(atoms, 6000, 200);\r\n}\r\n\r\n/**\r\n * Splits atoms that exceed the maxSize into smaller overlapping chunks.\r\n */\r\nfunction enforceMaxSize(atoms: string[], maxSize: number, overlap: number): string[] {\r\n    const result: string[] = [];\r\n    for (const atom of atoms) {\r\n        if (atom.length <= maxSize) {\r\n            result.push(atom);\r\n        } else {\r\n            // Hard split with overlap\r\n            let i = 0;\r\n            while (i < atom.length) {\r\n                const end = Math.min(i + maxSize, atom.length);\r\n                const chunk = atom.substring(i, end);\r\n                result.push(chunk);\r\n\r\n                // If we reached the end, stop\r\n                if (end >= atom.length) break;\r\n\r\n                // Move forward by maxSize - overlap (so we back up a bit for the next chunk)\r\n                i += (maxSize - overlap);\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n\r\n}\r\n"
    tokens: 1673
    size: 4787
  - path: engine\src\services\ingest\ingest.ts
    content: |-
      /**
       * Ingest Service - Memory Ingestion with Provenance Tracking
       *
       * Implements the Data Provenance feature by adding a 'provenance' column
       * to distinguish between "Sovereign" (User-Created) and "Ancillary" (External) data.
       */

      import { db } from '../../core/db.js';
      import crypto from 'crypto';
      import { config } from '../../config/index.js';

      interface IngestOptions {
        atomize?: boolean;
      }





      /**
       * Determines the provenance of content based on its source
       */
      function determineProvenance(source: string, type?: string): 'sovereign' | 'external' | 'system' {
        // If source comes from context/inbox/ or API with type 'user', it's sovereign
        if (source.includes('context/inbox/') || type === 'user') {
          return 'sovereign';
        }

        // If source is from web scraping or bulk import, it's external
        if (source.includes('web_scrape') || source.includes('bulk_import')) {
          return 'external';
        }

        // Default to external for most cases
        return 'external';
      }

      /**
       * Ingest content into the memory database with provenance tracking
       */
      export async function ingestContent(
        content: string,
        source: string,
        type: string = 'text',
        buckets: string[] = ['core'],
        tags: string[] = [],
        _options: IngestOptions = {}
      ): Promise<{ status: string; id?: string; message?: string }> {

        if (!content) {
          throw new Error('Content is required for ingestion');
        }

        // Auto-assign provenance based on source
        const provenance = determineProvenance(source, type);

        // Generate hash for content deduplication
        const hash = crypto.createHash('md5').update(content).digest('hex');

        // Check if content with same hash already exists
        const existingQuery = `?[id] := *memory{id, hash}, hash = $hash`;
        const existingResult = await db.run(existingQuery, { hash });

        if (existingResult.rows && existingResult.rows.length > 0) {
          return {
            status: 'skipped',
            id: existingResult.rows[0][0],
            message: 'Content with same hash already exists'
          };
        }

        // Generate unique ID
        const id = `mem_${Date.now()}_${crypto.randomBytes(8).toString('hex').substring(0, 16)}`;
        const timestamp = Date.now();
        const tagsJson = tags; // Pass as array, Cozo Napi handles it
        const bucketsArray = Array.isArray(buckets) ? buckets : [buckets];
        const epochsJson: string[] = []; // Pass as array

        // Insert the memory with provenance information
        // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding
        const insertQuery = `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}`;

        await db.run(insertQuery, {
          data: [[id, timestamp, content, source, source, 0, type, hash, bucketsArray, epochsJson, tagsJson, provenance, new Array(config.MODELS.EMBEDDING_DIM).fill(0.1)]]
        });

        // Strict Read-After-Write Verification (Standard 059)
        const verify = await db.run(`?[id] := *memory{id}, id = $id`, { id });
        if (!verify.rows || verify.rows.length === 0) {
          throw new Error(`Ingestion Verification Failed: ID ${id} not found after write.`);
        }

        return {
          status: 'success',
          id,
          message: 'Content ingested successfully with provenance tracking'
        };
      }

      export interface IngestAtom {
        id: string;
        content: string;
        sourceId: string;
        sequence: number;
        timestamp: number;
        provenance: 'sovereign' | 'external';
        embedding?: number[];
        hash?: string; // Explicit hash to avoid ID-based guessing
      }

      /**
       * Ingest pre-processed atoms
       */
      export async function ingestAtoms(
        atoms: IngestAtom[],
        source: string,
        buckets: string[] = ['core'],
        tags: string[] = []
      ): Promise<number> {

        if (atoms.length === 0) return 0;

        const rows = atoms.map(atom => {
          // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding
          return [
            atom.id,
            atom.timestamp,
            atom.content,
            source,
            atom.sourceId,
            atom.sequence,
            'text', // Type
            atom.hash || atom.id.replace('atom_', ''), // Use explicit hash or fallback
            buckets,
            [], // epochs
            tags,
            atom.provenance,
            atom.embedding || new Array(config.MODELS.EMBEDDING_DIM).fill(0.1)
          ];
        });

        // Chunked Insert
        const chunkSize = 50;
        let inserted = 0;
        const totalBatches = Math.ceil(rows.length / chunkSize);

        console.log(`[Ingest] Starting DB Write for ${rows.length} atoms (${totalBatches} batches)...`);

        for (let i = 0; i < rows.length; i += chunkSize) {
          const batchNum = Math.floor(i / chunkSize) + 1;
          if (batchNum % 10 === 0 || batchNum === 1 || batchNum === totalBatches) {
            console.log(`[Ingest] Writing batch ${batchNum}/${totalBatches}...`);
          }
          const chunk = rows.slice(i, i + chunkSize);
          try {
            if (chunk.length > 0) {
              const sampleEmbedding = chunk[0][12] as number[];
              if (sampleEmbedding.length !== config.MODELS.EMBEDDING_DIM) {
                console.warn(`[Ingest] WARNING: Embedding dimension mismatch! Schema: ${config.MODELS.EMBEDDING_DIM}, Actual: ${sampleEmbedding.length}`);
              }
            }
            await db.run(`
                      ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data
                      :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}
                   `, { data: chunk });
          } catch (e: any) {
            console.error(`[Ingest] Batch insert failed: ${e.message}`);
            throw e; // RETHROW to abort Watchdog update
          }

          // Standard 059: Batch Read-After-Write Verification
          try {
            const chunkIds = chunk.map(row => row[0]); // row[0] is id
            const chunkIdsStr = JSON.stringify(chunkIds);
            const verifyQuery = `?[id] := *memory{id}, id in ${chunkIdsStr}`;
            const verifyResult = await db.run(verifyQuery);

            const count = verifyResult.rows ? verifyResult.rows.length : 0;

            if (count !== chunk.length) {
              const errorMsg = `[Ingest] CRITICAL: Batch Verification Failed! Inserted: ${chunk.length}, Verified: ${count}. Potential Ghost Data.`;
              console.error(errorMsg);
              throw new Error(errorMsg); // STRICT MODE: Fail fast.
            } else {
              inserted += count;
            }
          } catch (verifyError: any) {
            console.error(`[Ingest] Verification Query Failed: ${verifyError.message}`);
            throw verifyError; // RETHROW
          }
        }

        return inserted;
      }

      /**
       * Bulk import YAML content with provenance tracking
       */
      export async function importYamlContent(yamlContent: any[]): Promise<{ imported: number; skipped: number; errors: number }> {
        let imported = 0;
        let skipped = 0;
        let errors = 0;

        for (const record of yamlContent) {
          try {
            if (!record.content) {
              errors++;
              continue;
            }

            const result = await ingestContent(
              record.content,
              record.source || 'yaml_import',
              record.type || 'text',
              record.buckets || ['imported'],
              record.tags || []
            );

            if (result.status === 'success') {
              imported++;
            } else if (result.status === 'skipped') {
              skipped++;
            }
          } catch (error) {
            console.error('YAML import error for record:', record, error);
            errors++;
          }
        }

        return { imported, skipped, errors };
      }
    tokens: 2716
    size: 7510
  - path: engine\src\services\ingest\refiner.ts
    content: "\r\nimport * as crypto from 'crypto';\r\nimport { atomizeContent as rawAtomize } from './atomizer.js';\r\n\r\n/**\r\n * Atom Interface\r\n * Represents a single unit of thought/memory.\r\n */\r\nexport interface Atom {\r\n    id: string;\r\n    content: string;\r\n    sourceId: string;\r\n    sequence: number;\r\n    timestamp: number;\r\n    provenance: 'sovereign' | 'external';\r\n    embedding?: number[]; // Placeholder for vector\r\n}\r\n\r\n/**\r\n * Refine Content\r\n * \r\n * The Orchestrator for ingestion:\r\n * 1. Sanitizes Input (BOM, Encoding)\r\n * 2. Selects Strategy (Code vs Prose)\r\n * 3. Atomizes (via Atomizer)\r\n * 4. Enriches (Metadata injection)\r\n */\r\nimport { getEmbeddings } from '../llm/provider.js';\r\nimport config from '../../config/index.js';\r\n\r\n// ...\r\n\r\nexport async function refineContent(rawBuffer: Buffer | string, filePath: string, options: { skipEmbeddings?: boolean } = {}): Promise<Atom[]> {\r\n    // ... (Sanitization unchanged)\r\n    let cleanText = '';\r\n\r\n    if (Buffer.isBuffer(rawBuffer)) {\r\n        // DEBUG: Check raw buffer for nulls\r\n        let bufferNulls = 0;\r\n        for (let k = 0; k < Math.min(rawBuffer.length, 2000); k++) {\r\n            if (rawBuffer[k] === 0) bufferNulls++;\r\n        }\r\n        console.log(`[Refiner] Raw Buffer Analysis: Size=${rawBuffer.length}, First 2000 Nulls=${bufferNulls}`);\r\n\r\n        // 1. Check for BOM (Byte Order Mark)\r\n        if (rawBuffer.length >= 2) {\r\n            if (rawBuffer[0] === 0xFF && rawBuffer[1] === 0xFE) {\r\n                console.log(`[Refiner] Detected UTF-16 LE BOM. Decoding as UTF-16LE...`);\r\n                cleanText = rawBuffer.toString('utf16le');\r\n            } else if (rawBuffer[0] === 0xFE && rawBuffer[1] === 0xFF) {\r\n                console.log(`[Refiner] Detected UTF-16 BE BOM. Decoding as UTF-16BE...`);\r\n                // Node.js doesn't natively support utf16be in toString, swap bytes\r\n                const swapped = Buffer.alloc(rawBuffer.length);\r\n                for (let i = 0; i < rawBuffer.length; i += 2) {\r\n                    swapped[i] = rawBuffer[i + 1];\r\n                    swapped[i + 1] = rawBuffer[i];\r\n                }\r\n                cleanText = swapped.toString('utf16le');\r\n            } else {\r\n                // 2. Heuristic: Check for High Null Density (UTF-16 without BOM)\r\n                let nullCount = 0;\r\n                // Check start, middle, and end segments to be sure\r\n                const checkLen = Math.min(rawBuffer.length, 1000);\r\n                const midStart = Math.floor(rawBuffer.length / 2);\r\n                const midLen = Math.min(rawBuffer.length - midStart, 1000);\r\n\r\n                // Scan start\r\n                for (let i = 0; i < checkLen; i++) {\r\n                    if (rawBuffer[i] === 0x00) nullCount++;\r\n                }\r\n                // Scan middle\r\n                if (midLen > 0) {\r\n                    for (let i = midStart; i < midStart + midLen; i++) {\r\n                        if (rawBuffer[i] === 0x00) nullCount++;\r\n                    }\r\n                }\r\n\r\n                const totalChecked = checkLen + midLen;\r\n                const ratio = nullCount / totalChecked;\r\n\r\n                // If > 20% nulls, assume UTF-16LE\r\n                if (totalChecked > 10 && ratio > 0.2) {\r\n                    console.log(`[Refiner] Auto-detected UTF-16LE (Null Density: ${ratio.toFixed(2)}). Decoding as UTF-16LE...`);\r\n                    cleanText = rawBuffer.toString('utf16le');\r\n                } else {\r\n                    cleanText = rawBuffer.toString('utf8');\r\n                }\r\n            }\r\n        } else {\r\n            cleanText = rawBuffer.toString('utf8');\r\n        }\r\n    } else {\r\n        cleanText = rawBuffer;\r\n    }\r\n\r\n    if (cleanText.charCodeAt(0) === 0xFEFF) {\r\n        cleanText = cleanText.slice(1);\r\n    }\r\n\r\n    // Encoding Correction: Aggressive Cleanup\r\n    // Remove null bytes (\\u0000) and replacement characters (\\uFFFD)\r\n    // Also remove other control characters that might confuse the tokenizer\r\n    cleanText = cleanText.replace(/[\\u0000\\uFFFD]/g, '');\r\n\r\n    // DEBUG: Verify clean text\r\n    const cleanNulls = (cleanText.match(/\\0/g) || []).length;\r\n    if (cleanNulls > 0) {\r\n        console.error(`[Refiner] CRITICAL: cleanText still has ${cleanNulls} nulls after cleaning!`);\r\n    } else {\r\n        // console.log(`[Refiner] Text cleaned successfully. Length: ${cleanText.length}`);\r\n    }\r\n\r\n    // Normalize line endings\r\n    cleanText = cleanText.replace(/\\r\\n/g, '\\n').replace(/\\r/g, '\\n');\r\n\r\n    // ... (Strategy Selection unchanged)\r\n    // 3. Heuristic Strategy Selection\r\n    // If we have very few lines relative to length, it's likely a minified blob or dense log\r\n    // Ratio: Chars per line. Normal code ~30-80. Minified > 200.\r\n    const lineCount = cleanText.split('\\n').length;\r\n    const avgLineLength = cleanText.length / (lineCount || 1);\r\n\r\n    let strategy: 'code' | 'prose' | 'blob' = 'prose';\r\n\r\n    if (avgLineLength > 300 || cleanText.length > 50000 && lineCount < 50) {\r\n        console.log(`[Refiner] Detected BLOB content (Avg Line Len: ${avgLineLength.toFixed(0)}). Using 'blob' strategy.`);\r\n        strategy = 'blob';\r\n    } else if (filePath.endsWith('.ts') || filePath.endsWith('.js') || filePath.endsWith('.py') || filePath.endsWith('.rs') || filePath.endsWith('.cpp')) {\r\n        strategy = 'code';\r\n    }\r\n\r\n    // 4. Atomize\r\n    // strategy can be 'blob' - atomizer signature is updated\r\n    const rawAtoms = rawAtomize(cleanText, strategy);\r\n\r\n    // FILTER: Remove atoms that look like garbage/binary (Last Line of Defense)\r\n    const validAtoms = rawAtoms.filter(atom => {\r\n        // 1. Strict Null Check (If sanitization missed any)\r\n        if (atom.indexOf('\\u0000') !== -1) return false;\r\n\r\n        // 2. Replacement Character Density (Bad decoding artifacts)\r\n        const badCharCount = (atom.match(/[\\uFFFD]/g) || []).length;\r\n        if (badCharCount > 0 && (badCharCount / atom.length) > 0.05) return false;\r\n\r\n        // 3. Control Character Density (Binary blob read as ASCII)\r\n        // Count chars < 32 (excluding \\n, \\r, \\t)\r\n        // This regex matches control chars except newline, return, tab\r\n        const controlCharCount = (atom.match(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]/g) || []).length;\r\n        if (controlCharCount > 0 && (controlCharCount / atom.length) > 0.1) return false;\r\n\r\n        return true;\r\n    });\r\n\r\n    if (rawAtoms.length !== validAtoms.length) {\r\n        console.warn(`[Refiner] GARBAGE COLLECTION: Dropped ${rawAtoms.length - validAtoms.length} atoms from ${filePath} (contained nulls or binary data).`);\r\n    }\r\n\r\n    const sourceId = crypto.createHash('md5').update(filePath).digest('hex');\r\n    const timestamp = Date.now();\r\n    const normalizedPath = filePath.replace(/\\\\/g, '/');\r\n    let provenance: 'sovereign' | 'external' = 'external';\r\n\r\n    if (normalizedPath.includes('/inbox') ||\r\n        normalizedPath.includes('/chat_logs') ||\r\n        normalizedPath.includes('/diary') ||\r\n        normalizedPath.includes('sovereign')) {\r\n        provenance = 'sovereign';\r\n    }\r\n\r\n    // Process Atoms (Sequential Embedding Generation to prevent worker flood)\r\n    // 3. Batch Embedding Generation\r\n    // 3. Batch Embedding Generation (Optional)\r\n    if (options.skipEmbeddings) {\r\n        // Return atoms without embeddings\r\n        return rawAtoms.map((content, index) => {\r\n            const idHash = crypto.createHash('sha256')\r\n                .update(sourceId + index.toString() + content)\r\n                .digest('hex')\r\n                .substring(0, 16);\r\n            return {\r\n                id: `atom_${idHash}`,\r\n                content: content,\r\n                sourceId: sourceId,\r\n                sequence: index,\r\n                timestamp: timestamp,\r\n                provenance: provenance,\r\n                embedding: [] // Empty\r\n            };\r\n        });\r\n    }\r\n\r\n    const { processInBatches } = await import('../../core/batch.js');\r\n    const BATCH_SIZE = 50;\r\n    console.log(`[Refiner] Generating embeddings for ${rawAtoms.length} atoms (Batch size: ${BATCH_SIZE})...`);\r\n\r\n    const chunkResults = await processInBatches(rawAtoms, async (chunkTexts, batchIndex) => {\r\n        console.log(`[Refiner] Processing batch ${batchIndex + 1}/${Math.ceil(rawAtoms.length / BATCH_SIZE)} (${chunkTexts.length} atoms)...`);\r\n\r\n        let batchEmbeddings: number[][] | null = null;\r\n        try {\r\n            batchEmbeddings = await getEmbeddings(chunkTexts);\r\n        } catch (e) {\r\n            console.error(`[Refiner] Batch embedding failed, skipping vectors for this batch:`, e);\r\n        }\r\n\r\n        const batchAtoms: Atom[] = [];\r\n        for (let j = 0; j < chunkTexts.length; j++) {\r\n            const atomIndex = (batchIndex * BATCH_SIZE) + j;\r\n            const content = chunkTexts[j];\r\n\r\n            if (content.includes('\\0')) {\r\n                console.error(`[Refiner] CRITICAL: Atom ${atomIndex} contains NULL bytes! Content snippet: ${JSON.stringify(content.substring(0, 50))}`);\r\n            }\r\n\r\n            const idHash = crypto.createHash('sha256')\r\n                .update(sourceId + atomIndex.toString() + content)\r\n                .digest('hex')\r\n                .substring(0, 16);\r\n\r\n            let embedding = new Array(config.MODELS.EMBEDDING_DIM).fill(0.1);\r\n            if (batchEmbeddings && batchEmbeddings[j] && batchEmbeddings[j].length > 0) {\r\n                embedding = batchEmbeddings[j];\r\n            }\r\n\r\n            batchAtoms.push({\r\n                id: `atom_${idHash}`,\r\n                content: content,\r\n                sourceId: sourceId,\r\n                sequence: atomIndex,\r\n                timestamp: timestamp,\r\n                provenance: provenance,\r\n                embedding: embedding\r\n            });\r\n        }\r\n        return batchAtoms;\r\n    }, { batchSize: BATCH_SIZE });\r\n\r\n    // Flatten results\r\n    const atoms = chunkResults.flat();\r\n\r\n    return atoms;\r\n}\r\n\r\n/**\r\n * Enriches a list of atoms with embeddings.\r\n * Used for differential ingestion (only embedding new/changed atoms).\r\n */\r\nexport async function enrichAtoms(atoms: Atom[]): Promise<Atom[]> {\r\n    if (atoms.length === 0) return atoms;\r\n\r\n    const { processInBatches } = await import('../../core/batch.js');\r\n    const BATCH_SIZE = 50;\r\n    console.log(`[Refiner] Enriching ${atoms.length} atoms with embeddings...`);\r\n\r\n    const totalBatches = Math.ceil(atoms.length / BATCH_SIZE);\r\n\r\n    const chunkResults = await processInBatches(atoms, async (chunkAtoms, batchIndex) => {\r\n        if ((batchIndex + 1) % 5 === 0 || batchIndex === 0) {\r\n            console.log(`[Refiner] Enriching batch ${batchIndex + 1}/${totalBatches} (${chunkAtoms.length} atoms)...`);\r\n        }\r\n\r\n        // Extract content for embedding\r\n        const texts = chunkAtoms.map(a => a.content);\r\n\r\n        let batchEmbeddings: number[][] | null = null;\r\n        try {\r\n            batchEmbeddings = await getEmbeddings(texts);\r\n        } catch (e) {\r\n            console.error(`[Refiner] Enrichment failed for batch ${batchIndex}:`, e);\r\n        }\r\n\r\n        // Apply embeddings back to atoms\r\n        return chunkAtoms.map((atom, i) => {\r\n            if (batchEmbeddings && batchEmbeddings[i]) {\r\n                return { ...atom, embedding: batchEmbeddings[i] };\r\n            }\r\n            return atom; // Return without embedding if failed (will be zero-filled by ingest)\r\n        });\r\n    }, { batchSize: BATCH_SIZE });\r\n\r\n    return chunkResults.flat();\r\n}\r\n"
    tokens: 3985
    size: 11480
  - path: engine\src\services\ingest\watchdog.ts
    content: "/**\r\n * Watchdog Service\r\n *\r\n * Scans the Notebook directory for changes and ingests new content.\r\n * Uses 'chokidar' for efficient file watching.\r\n */\r\n\r\nimport * as chokidar from 'chokidar';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as crypto from 'crypto';\r\nimport { db } from '../../core/db.js';\r\nimport { NOTEBOOK_DIR } from '../../config/paths.js';\r\nimport { ingestAtoms } from './ingest.js';\r\nimport { refineContent } from './refiner.js';\r\n\r\nlet watcher: chokidar.FSWatcher | null = null;\r\nconst IGNORE_PATTERNS = /(^|[\\/\\\\])\\../; // Ignore dotfiles\r\n\r\nexport async function startWatchdog() {\r\n    if (watcher) return;\r\n\r\n    if (!fs.existsSync(NOTEBOOK_DIR)) {\r\n        console.warn(`[Watchdog] Notebook directory not found: ${NOTEBOOK_DIR}. Skipping watch.`);\r\n        return;\r\n    }\r\n\r\n    const inbox = path.join(NOTEBOOK_DIR, 'inbox');\r\n    console.log(`[Watchdog] Starting watch on: ${inbox}`);\r\n\r\n    if (!fs.existsSync(inbox)) {\r\n        console.warn(`[Watchdog] Inbox directory not found: ${inbox}. Skipping watch.`);\r\n        return;\r\n    }\r\n\r\n    watcher = chokidar.watch(inbox, {\r\n        ignored: IGNORE_PATTERNS,\r\n        persistent: true,\r\n        ignoreInitial: false, // Force scan on start to ingest existing files\r\n        awaitWriteFinish: {\r\n            stabilityThreshold: 2000,\r\n            pollInterval: 100\r\n        }\r\n    });\r\n\r\n    watcher\r\n        .on('add', (path) => processFile(path, 'add'))\r\n        .on('change', (path) => processFile(path, 'change'));\r\n    // .on('unlink', (path) => deleteFile(path)); // Implement delete logic later\r\n}\r\n\r\nasync function processFile(filePath: string, event: string) {\r\n    if (!filePath.endsWith('.md') && !filePath.endsWith('.txt') && !filePath.endsWith('.yaml')) return;\r\n    if (filePath.includes('mirrored_brain')) return;\r\n\r\n    console.log(`[Watchdog] Detected ${event}: ${filePath}`);\r\n\r\n    try {\r\n        const buffer = await fs.promises.readFile(filePath);\r\n        if (buffer.length === 0) return;\r\n\r\n        // 1. Calculate File Hash (Raw for Change Detection)\r\n        const fileHash = crypto.createHash('sha256').update(buffer).digest('hex');\r\n        const relativePath = path.relative(NOTEBOOK_DIR, filePath);\r\n\r\n        // 2. Check Source Table\r\n        const sourceQuery = `?[path, hash] := *source{path, hash}, path = $path`;\r\n        const sourceResult = await db.run(sourceQuery, { path: relativePath });\r\n\r\n        let shouldIngest = true;\r\n        if (sourceResult.rows && sourceResult.rows.length > 0) {\r\n            const [_path, existingHash] = sourceResult.rows[0];\r\n            if (existingHash === fileHash) {\r\n                console.log(`[Watchdog] File unchanged (hash match): ${relativePath}`);\r\n                shouldIngest = false;\r\n            }\r\n        }\r\n\r\n        if (!shouldIngest) return;\r\n\r\n        console.log(`[Watchdog] Refinement Pipeline: ${relativePath}`);\r\n\r\n        // 3. Smart Refinement (Dry Run)\r\n        // Parse atoms WITHOUT generating embeddings first\r\n        const { enrichAtoms } = await import('./refiner.js');\r\n        const dryRunAtoms = await refineContent(buffer, relativePath, { skipEmbeddings: true });\r\n\r\n        const sourceId = crypto.createHash('md5').update(relativePath).digest('hex');\r\n\r\n        // 4. Fetch Existing Atoms from DB for this source\r\n        // We need ID and Hash to compare\r\n        const existingQuery = `?[id, hash] := *memory{id, source_id, hash}, source_id = $sid`;\r\n        const existingResult = await db.run(existingQuery, { sid: sourceId });\r\n\r\n        const existingMap = new Map<string, string>(); // ID -> Hash\r\n        if (existingResult.rows) {\r\n            existingResult.rows.forEach((r: any) => existingMap.set(r[0], r[1]));\r\n        }\r\n\r\n        // 5. Calculate Diff\r\n        // New Atoms: Present in dryRun but NOT in DB (by ID) OR Hash mismatch\r\n        // Deleted Atoms: Present in DB but NOT in dryRun (by ID)\r\n\r\n        const atomsToIngest: any[] = [];\r\n        const atomIdsToKeep = new Set<string>();\r\n\r\n        for (const atom of dryRunAtoms) {\r\n            atomIdsToKeep.add(atom.id);\r\n            const existingHash = existingMap.get(atom.id);\r\n\r\n            // If it's new (not in DB) or changed (hash mismatch), we need to ingest it\r\n            // Note: Atom ID includes hash in standard refiner, so usually ID change = content change.\r\n            // But if we change ID generation later, comparing hashes is safer.\r\n            if (!existingHash) {\r\n                atomsToIngest.push(atom);\r\n            } else if (existingHash !== atom.id.replace('atom_', '')) {\r\n                // Fallback check if hash isn't explicit\r\n                atomsToIngest.push(atom);\r\n            }\r\n        }\r\n\r\n        const idsToDelete: string[] = [];\r\n        for (const [id] of existingMap) {\r\n            if (!atomIdsToKeep.has(id)) {\r\n                idsToDelete.push(id);\r\n            }\r\n        }\r\n\r\n        console.log(`[Watchdog] Smart Diff for ${relativePath}: +${atomsToIngest.length} / -${idsToDelete.length} / =${atomIdsToKeep.size - atomsToIngest.length}`);\r\n\r\n        // 6. Execute Updates\r\n\r\n        // A. DELETE orphans\r\n        if (idsToDelete.length > 0) {\r\n            await db.run(`?[id] <- $ids :delete memory {id}`, { ids: idsToDelete.map(id => [id]) });\r\n        }\r\n\r\n        // B. ENRICH & INSERT new/changed\r\n        if (atomsToIngest.length > 0) {\r\n            // Now we pay the cost of embedding ONLY for the new stuff\r\n            const enrichedAtoms = await enrichAtoms(atomsToIngest);\r\n\r\n            // Improved Bucket Logic for Subfolders\r\n            const parts = relativePath.split(path.sep);\r\n            let bucket = 'notebook';\r\n\r\n            if (parts.length >= 2) {\r\n                // Check if it's inside 'inbox'\r\n                if (parts[0] === 'inbox') {\r\n                    // inbox/subfolder/file.md -> use 'subfolder'\r\n                    // inbox/file.md -> use 'inbox'\r\n                    bucket = parts.length > 2 ? parts[1] : 'inbox';\r\n                } else {\r\n                    // other_folder/file.md -> use 'other_folder'\r\n                    bucket = parts[0];\r\n                }\r\n            }\r\n\r\n            const bucketList = [bucket];\r\n\r\n            await ingestAtoms(enrichedAtoms, relativePath, bucketList, []);\r\n        }\r\n\r\n        // 7. Update Source Table - ONLY if we reached here without error\r\n        await db.run(\r\n            `?[path, hash, total_atoms, last_ingest] <- [[$path, $hash, $total, $last]] \r\n             :put source {path, hash, total_atoms, last_ingest}`,\r\n            {\r\n                path: relativePath,\r\n                hash: fileHash,\r\n                total: dryRunAtoms.length, // Total is now current valid count\r\n                last: Date.now()\r\n            }\r\n        );\r\n\r\n        if (atomsToIngest.length > 0 || idsToDelete.length > 0) {\r\n            console.log(`[Watchdog] Sync Complete: ${relativePath}`);\r\n        } else {\r\n            console.log(`[Watchdog] No atom changes detected (Metadata update only).`);\r\n        }\r\n\r\n    } catch (e: any) {\r\n        console.error(`[Watchdog] Error processing ${filePath}:`, e.message);\r\n    }\r\n}\r\n"
    tokens: 2504
    size: 7157
  - path: engine\src\services\llm\context.ts
    content: "\r\n// import type { LlamaChatSession } from 'node-llama-cpp'; // Unused\r\nimport { getModel, getContext, getCurrentCtxSize, runSideChannel } from './provider.js';\r\n\r\ninterface MockLlamaModel {\r\n    tokenize(text: string): { length: number; slice(start: number, end: number): any[] } & any[];\r\n    detokenize(tokens: any[]): string;\r\n}\r\n\r\n\r\n/**\r\n * Summarizes massive content by chunking it and processing through a side-channel session.\r\n * Prevents polluting the main chat history with raw data.\r\n */\r\nexport async function summarizeLargeContent(text: string, maxOutputTokens = 500): Promise<string> {\r\n    const model = getModel() as unknown as MockLlamaModel;\r\n    const context = getContext();\r\n\r\n    if (!text || !model || !context) return \"\";\r\n\r\n    // First, check if the text is too large and needs to be preprocessed\r\n    if (text.length > 5000) {\r\n        console.log(`[Summarizer] Content too large (${text.length} chars). Preprocessing...`);\r\n\r\n        // For very large texts, we'll use a more aggressive chunking strategy\r\n        const MAX_CHUNK_SIZE = 3000;\r\n        const chunks: string[] = [];\r\n\r\n        for (let i = 0; i < text.length; i += MAX_CHUNK_SIZE) {\r\n            chunks.push(text.substring(i, i + MAX_CHUNK_SIZE));\r\n        }\r\n\r\n        console.log(`[Summarizer] Split into ${chunks.length} chunks for processing...`);\r\n        const summaries: string[] = [];\r\n\r\n        for (const [i, chunk] of chunks.entries()) {\r\n            try {\r\n                console.log(`[Summarizer] Processing chunk ${i + 1}/${chunks.length} (${chunk.length} chars)...`);\r\n\r\n                const systemPrompt = \"You are a precise technical summarizer. Extract key facts, code snippets, and definitions. Be extremely concise.\";\r\n                const prompt = `Summarize this content in under ${Math.min(Math.floor(maxOutputTokens / chunks.length) + 20, 200)} words found below:\\n\\n${chunk}\\n\\nSummary:`;\r\n\r\n                const chunkSummary = (await runSideChannel(\r\n                    prompt,\r\n                    systemPrompt,\r\n                    { maxTokens: 300, temperature: 0.1 }\r\n                )) as string;\r\n\r\n                summaries.push(chunkSummary || `[SUMMARY UNAVAILABLE] Chunk ${i + 1} failed.`);\r\n            } catch (chunkError: any) {\r\n                console.warn(`[Summarizer] Failed to process chunk ${i + 1}:`, chunkError.message);\r\n                summaries.push(`[SUMMARY UNAVAILABLE] Failed to process chunk ${i + 1} due to context limitations.`);\r\n            }\r\n        }\r\n\r\n        // Now summarize the combined summaries if needed\r\n        const combinedSummaries = summaries.join(\"\\n\\n\");\r\n        if (combinedSummaries.length > 2000) {\r\n            console.log(`[Summarizer] Combined summaries still large (${combinedSummaries.length} chars), final summarization...`);\r\n            const finalSystem = \"You are a precise technical summarizer. Be extremely concise.\";\r\n            const finalPrompt = `Summarize these notes:\\n\\n${combinedSummaries}`;\r\n            const final = (await runSideChannel(finalPrompt, finalSystem, { maxTokens: Math.min(maxOutputTokens, 400), temperature: 0.1 })) as string;\r\n            return final || combinedSummaries;\r\n        }\r\n\r\n        return combinedSummaries;\r\n    } else {\r\n        // Original logic for smaller texts\r\n        const tokens = model.tokenize(text);\r\n        const totalTokens = tokens.length;\r\n\r\n        // Reserve space for prompt overhead + generation\r\n        const CONTEXT_WINDOW = getCurrentCtxSize();\r\n        const CHUNK_CAPACITY = Math.floor(CONTEXT_WINDOW * 0.4);\r\n\r\n        if (totalTokens <= CHUNK_CAPACITY) {\r\n            const systemPrompt = \"You are a precise technical summarizer. Extract key facts, code snippets, and definitions. Be extremely concise.\";\r\n            const prompt = `Summarize this content in under ${maxOutputTokens} words found below:\\n\\n${text}\\n\\nSummary:`;\r\n            const res = (await runSideChannel(prompt, systemPrompt, { maxTokens: maxOutputTokens, temperature: 0.1 })) as string;\r\n            return res || text.substring(0, maxOutputTokens * 4);\r\n        }\r\n\r\n        console.log(`[Summarizer] Content too large (${totalTokens} tokens). Chunking...`);\r\n        const chunks: string[] = [];\r\n        let offset = 0;\r\n        while (offset < totalTokens) {\r\n            const chunkTokens = tokens.slice(offset, offset + CHUNK_CAPACITY);\r\n            chunks.push(model.detokenize(chunkTokens));\r\n            offset += CHUNK_CAPACITY;\r\n        }\r\n\r\n        console.log(`[Summarizer] Processing ${chunks.length} chunks...`);\r\n        const summaries: string[] = [];\r\n\r\n        for (const [i, chunk] of chunks.entries()) {\r\n            const systemPrompt = \"You are a precise technical summarizer. Be extremely concise.\";\r\n            const prompt = `Summarize this chunk:\\n\\n${chunk}`;\r\n            const res = (await runSideChannel(prompt, systemPrompt, { maxTokens: 300, temperature: 0.1 })) as string;\r\n            summaries.push(res || `[Chunk ${i} Failed]`);\r\n        }\r\n\r\n        return summaries.join(\"\\n\\n\");\r\n    }\r\n}\r\n"
    tokens: 1784
    size: 5069
  - path: engine\src\services\llm\provider.ts
    content: |-
      import { Worker } from 'worker_threads';
      import path from 'path';
      import { fileURLToPath } from 'url';
      import { MODELS_DIR } from '../../config/paths.js';
      import config from '../../config/index.js';

      // Global State
      let clientWorker: Worker | null = null;
      let orchestratorWorker: Worker | null = null;
      let currentChatModelName = "";
      let currentOrchestratorModelName = "";

      // ESM __dirname fix
      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);
      const CHAT_WORKER_PATH = path.resolve(__dirname, '../../core/inference/ChatWorker.js');
      const HYBRID_WORKER_PATH = path.resolve(__dirname, '../../core/inference/llamaLoaderWorker.js');

      export interface LoadModelOptions {
        ctxSize?: number;
        batchSize?: number;
        systemPrompt?: string;
        gpuLayers?: number;
      }

      // Initialize workers based on configuration
      export async function initWorker() {
        // TAG-WALKER MODE (Lightweight)
        // We strictly skip embedding workers to save RAM. 
        // All embedding calls return zero-stubs.

        if (!clientWorker) {
          console.log(`[Provider] Tag-Walker Mode Active. Spawning Chat Worker...`);
          // Use Hybrid Worker for Main Chat (Legacy compatibility)
          clientWorker = await spawnWorker("HybridWorker", HYBRID_WORKER_PATH, {
            gpuLayers: config.MODELS.MAIN.GPU_LAYERS
          });
        }

        // Spawn Orchestrator (Side Channel) Worker - CPU Optimized
        if (!orchestratorWorker) {
          orchestratorWorker = await spawnWorker("OrchestratorWorker", CHAT_WORKER_PATH, {
            gpuLayers: config.MODELS.ORCHESTRATOR.GPU_LAYERS,
            forceCpu: config.MODELS.ORCHESTRATOR.GPU_LAYERS === 0
          });
        }

        return clientWorker;
      }

      async function spawnWorker(name: string, workerPath: string, workerData: any = {}): Promise<Worker> {
        return new Promise((resolve, reject) => {
          const w = new Worker(workerPath, { workerData });
          w.on('message', (msg) => {
            if (msg.type === 'ready') resolve(w);
            if (msg.type === 'error') console.error(`[${name}] Error:`, msg.error);
          });
          w.on('error', (err) => {
            console.error(`[${name}] Thread Error:`, err);
            reject(err);
          });
          w.on('exit', (code) => {
            if (code !== 0) console.error(`[${name}] Stopped with exit code ${code}`);
          });
        });
      }

      // Lock for initAutoLoad
      let initPromise: Promise<void> | null = null;

      // Auto-loader for Engine Start
      export async function initAutoLoad() {
        if (initPromise) return initPromise;

        initPromise = (async () => {
          console.log("[Provider] Auto-loading configured models...");

          try {
            await initWorker();

            // Load Chat Model
            await loadModel(config.MODELS.MAIN.PATH, {
              ctxSize: config.MODELS.MAIN.CTX_SIZE,
              gpuLayers: config.MODELS.MAIN.GPU_LAYERS
            }, 'chat');

            // Load Orchestrator Model
            await loadModel(config.MODELS.ORCHESTRATOR.PATH, {
              ctxSize: config.MODELS.ORCHESTRATOR.CTX_SIZE,
              gpuLayers: config.MODELS.ORCHESTRATOR.GPU_LAYERS
            }, 'orchestrator');

          } catch (e) {
            console.error("[Provider] Auto-load failed:", e);
            // Reset promise on failure to allow retry
            initPromise = null;
            throw e;
          }
        })();

        return initPromise;
      }

      // Model Loading Logic
      let chatLoadingPromise: Promise<any> | null = null;
      let orchLoadingPromise: Promise<any> | null = null;

      export async function loadModel(modelPath: string, options: LoadModelOptions = {}, target: 'chat' | 'orchestrator' = 'chat') {
        if (!clientWorker) await initWorker();

        let targetWorker = clientWorker;
        if (target === 'orchestrator') targetWorker = orchestratorWorker;

        if (!targetWorker) throw new Error("Worker not initialized");

        // Check if already loaded
        if (target === 'chat' && modelPath === currentChatModelName) return { status: "ready" };
        if (target === 'orchestrator' && modelPath === currentOrchestratorModelName) return { status: "ready" };

        // Prevent parallel loads for *same target*
        if (target === 'chat' && chatLoadingPromise) return chatLoadingPromise;
        if (target === 'orchestrator' && orchLoadingPromise) return orchLoadingPromise;

        const loadTask = new Promise((resolve, reject) => {
          const fullModelPath = path.isAbsolute(modelPath) ? modelPath : path.join(MODELS_DIR, modelPath);

          const handler = (msg: any) => {
            if (msg.type === 'modelLoaded') {
              console.log(`[Provider] ${target} Model loaded: ${modelPath}`);
              targetWorker!.off('message', handler);
              if (target === 'chat') {
                currentChatModelName = modelPath;
                chatLoadingPromise = null;
              } else {
                currentOrchestratorModelName = modelPath;
                orchLoadingPromise = null;
              }
              resolve({ status: "success" });
            } else if (msg.type === 'error') {
              targetWorker!.off('message', handler);
              if (target === 'chat') chatLoadingPromise = null;
              else orchLoadingPromise = null;
              reject(new Error(msg.error));
            }
          };

          targetWorker!.on('message', handler);
          targetWorker!.postMessage({
            type: 'loadModel',
            data: { modelPath: fullModelPath, options }
          });
        });

        if (target === 'chat') chatLoadingPromise = loadTask;
        else orchLoadingPromise = loadTask;

        return loadTask;
      }

      // ... Inference ...

      export async function runInference(prompt: string, data: any) {
        if (!clientWorker || !currentChatModelName) throw new Error("Chat Model not loaded");
        // Stub implementation
        console.log("runInference called with", prompt.substring(0, 10), data ? "data present" : "no data");
        return null;
      }

      export async function runSideChannel(prompt: string, systemInstruction = "You are a helpful assistant.", options: any = {}) {
        // Use Orchestrator Worker if available, falling back to client
        let targetWorker = orchestratorWorker || clientWorker;
        let targetModel = currentOrchestratorModelName || currentChatModelName;

        if (!targetWorker || !targetModel) {
          await initAutoLoad();
          targetWorker = orchestratorWorker || clientWorker;
          targetModel = currentOrchestratorModelName || currentChatModelName;
        }

        if (!targetWorker || !targetModel) throw new Error("Orchestrator/Chat Model failed to load.");

        return new Promise((resolve, _reject) => {
          const handler = (msg: any) => {
            if (msg.type === 'chatResponse') {
              targetWorker?.off('message', handler);
              resolve(msg.data);
            } else if (msg.type === 'error') {
              targetWorker?.off('message', handler);
              console.error("SideChannel Error:", msg.error);
              resolve(null);
            }
          };
          targetWorker?.on('message', handler);
          targetWorker?.postMessage({
            type: 'chat',
            data: { prompt, options: { ...options, systemPrompt: systemInstruction } }
          });
        });
      }

      // Embeddings - STUBBED (Tech Debt Removal)
      export async function getEmbedding(text: string): Promise<number[] | null> {
        const result = await getEmbeddings([text]);
        return result ? result[0] : null;
      }

      export async function getEmbeddings(texts: string[]): Promise<number[][] | null> {
        // Return stubbed zero-vectors to satisfy DB schema
        const dim = config.MODELS.EMBEDDING_DIM || 768; // Fallback to 768
        return texts.map(() => new Array(dim).fill(0.1));
      }

      // Stub for now to match interface compatibility with rest of system
      export async function initInference() {
        // This is called by context.ts usually to ensure model loaded
        const fs = await import('fs');
        if (!fs.existsSync(MODELS_DIR)) return null;
        try {
          const models = fs.readdirSync(MODELS_DIR).filter((f: string) => f.endsWith(".gguf"));
          if (models.length > 0) {
            return await loadModel(models[0]);
          }
        } catch (e) { console.error("Error listing models", e); }
        return null;
      }

      export function getSession() { return null; } // Worker handles session
      export function getContext() { return null; }
      export function getModel() { return null; }
      export function getCurrentModelName() { return currentChatModelName; }
      export function getCurrentCtxSize() { return config.MODELS.MAIN.CTX_SIZE; }

      // Legacy/Unused exports needed to satisfy imports elsewhere until refactored
      export const DEFAULT_GPU_LAYERS = config.MODELS.MAIN.GPU_LAYERS;
      export async function listModels(customDir?: string) {
        const fs = await import('fs');
        const targetDir = customDir ? path.resolve(customDir) : MODELS_DIR;
        if (!fs.existsSync(targetDir)) return [];
        return fs.readdirSync(targetDir).filter((f: string) => f.endsWith(".gguf"));
      }
    tokens: 2971
    size: 8439
  - path: engine\src\services\mirror\mirror.ts
    content: "/**\r\n * Mirror Protocol Service - \"Tangible Knowledge Graph\"\r\n *\r\n * Projects the AI Brain onto the filesystem using a @bucket/#tag structure.\r\n */\r\n\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { db } from '../../core/db.js';\r\nimport { NOTEBOOK_DIR } from '../../config/paths.js';\r\n\r\nexport const MIRRORED_BRAIN_PATH = path.join(NOTEBOOK_DIR, 'mirrored_brain');\r\n\r\n// Clean filename helper\r\nfunction sanitizeFilename(text: string): string {\r\n    return text.replace(/[^a-zA-Z0-9-_]/g, '_').substring(0, 64);\r\n}\r\n\r\n/**\r\n * Mirror Protocol: Exports memories to Markdown files organized by @bucket/#tag\r\n */\r\nexport async function createMirror() {\r\n    console.log(' Mirror Protocol: Starting semantic brain mirroring...');\r\n\r\n    // Wipe existing mirrored brain to ensure only latest state is present\r\n    if (fs.existsSync(MIRRORED_BRAIN_PATH)) {\r\n        console.log(` Mirror Protocol: Wiping stale mirror at ${MIRRORED_BRAIN_PATH}`);\r\n        fs.rmSync(MIRRORED_BRAIN_PATH, { recursive: true, force: true });\r\n    }\r\n\r\n    fs.mkdirSync(MIRRORED_BRAIN_PATH, { recursive: true });\r\n\r\n    const query = '?[id, timestamp, content, source, type, hash, buckets, tags] := *memory{id, timestamp, content, source, type, hash, buckets, tags}';\r\n    const result = await db.run(query);\r\n\r\n    if (!result.rows || result.rows.length === 0) {\r\n        console.log(' Mirror Protocol: No memories to mirror.');\r\n        return;\r\n    }\r\n\r\n    console.log(` Mirror Protocol: Mirroring ${result.rows.length} memories to disk...`);\r\n\r\n    let count = 0;\r\n    for (const row of result.rows) {\r\n        const [id, timestamp, content, source, type, _hash, buckets, tags] = row;\r\n\r\n        // Buckets and tags come as arrays from Cozo\r\n        const bucketList = (buckets as string[]) || [];\r\n        const tagList = (tags as string[]) || [];\r\n        const primaryBucket = bucketList.length > 0 ? bucketList[0] : 'general';\r\n\r\n        await writeMirrorFile({\r\n            id: id as string,\r\n            timestamp: timestamp as number,\r\n            content: content as string,\r\n            source: source as string,\r\n            type: type as string,\r\n            bucket: primaryBucket,\r\n            tags: tagList\r\n        });\r\n        count++;\r\n    }\r\n\r\n    console.log(` Mirror Protocol: Synchronization complete. ${count} memories mirrored to ${MIRRORED_BRAIN_PATH}`);\r\n}\r\n\r\nasync function writeMirrorFile(memory: any) {\r\n    try {\r\n        // 1. Determine Bucket (Root Folder)\r\n        const bucketName = (memory.bucket && memory.bucket !== 'general' && memory.bucket !== 'unknown') ? memory.bucket : 'general';\r\n        const bucketDir = path.join(MIRRORED_BRAIN_PATH, `@${sanitizeFilename(bucketName)}`);\r\n\r\n        // 2. Determine Primary Tag (Sub Folder)\r\n        // Filter out the bucket name and inbox from tags to find the 'Topic'\r\n        const specificTags = memory.tags.filter((t: string) => t !== bucketName && t !== 'inbox');\r\n        const tagName = specificTags.length > 0 ? specificTags[0] : '_untagged';\r\n        const tagDir = path.join(bucketDir, `#${sanitizeFilename(tagName)}`);\r\n\r\n        // Create Dirs\r\n        if (!fs.existsSync(tagDir)) {\r\n            fs.mkdirSync(tagDir, { recursive: true });\r\n        }\r\n\r\n        // 3. Generate Filename (Semantic Snippet + ID Suffix)\r\n        let nameSnippet = \"note\";\r\n        // Try to find a title in markdown (# Title)\r\n        const titleMatch = memory.content.match(/^#\\s+(.+)$/m);\r\n        if (titleMatch) {\r\n            nameSnippet = titleMatch[1];\r\n        } else {\r\n            // Fallback to first few words\r\n            nameSnippet = memory.content.substring(0, 30).trim().split('\\n')[0];\r\n        }\r\n\r\n        const safeName = sanitizeFilename(nameSnippet).toLowerCase();\r\n        // Short ID for uniqueness\r\n        const shortId = (memory.id || \"\").split('_').pop() || \"anon\";\r\n\r\n        let extension = '.md';\r\n        if (memory.type === 'json') extension = '.json';\r\n\r\n        const filePath = path.join(tagDir, `${safeName}_${shortId}${extension}`);\r\n\r\n        // 4. Write Frontmatter + Content\r\n        const frontmatter = `---\r\nid: ${memory.id}\r\ndate: ${new Date(memory.timestamp).toISOString()}\r\nsource: ${memory.source}\r\nbucket: ${memory.bucket}\r\ntags: ${JSON.stringify(memory.tags)}\r\n---\r\n\r\n`;\r\n        await fs.promises.writeFile(filePath, frontmatter + memory.content, 'utf8');\r\n        return true;\r\n    } catch (e: any) {\r\n        console.error(`Failed to write mirror file for ${memory.id}:`, e.message);\r\n        return false;\r\n    }\r\n}\r\n\r\n"
    tokens: 1621
    size: 4557
  - path: engine\src\services\safe-shell-executor\safe-shell-executor.js
    content: "// safe-shell-executor.js\r\nconst { spawn } = require('child_process');\r\nconst path = require('path');\r\nconst { LOGS_DIR } = require('../../config/paths');\r\n\r\nclass SafeShellExecutor {\r\n    static async execute(command, options = {}) {\r\n        return new Promise((resolve, reject) => {\r\n            const {\r\n                timeout = 30000, // 30 second default timeout\r\n                logFile = path.join(LOGS_DIR, `shell_cmd_${Date.now()}.log`),\r\n                detached = true,\r\n                stdio = ['ignore', 'ignore', 'ignore'] // Completely detached\r\n            } = options;\r\n\r\n            // Split command into command and args\r\n            const [cmd, ...args] = command.split(' ');\r\n\r\n            const child = spawn(cmd, args, {\r\n                detached,\r\n                stdio,\r\n                ...options.spawnOptions\r\n            });\r\n\r\n            // Set up timeout\r\n            const timer = setTimeout(() => {\r\n                child.kill();\r\n                reject(new Error(`Command timed out after ${timeout}ms: ${command}`));\r\n            }, timeout);\r\n\r\n            // Handle process completion\r\n            child.on('close', (code) => {\r\n                clearTimeout(timer);\r\n                resolve({\r\n                    success: code === 0,\r\n                    code,\r\n                    logFile\r\n                });\r\n            });\r\n\r\n            child.on('error', (error) => {\r\n                clearTimeout(timer);\r\n                reject(error);\r\n            });\r\n\r\n            // If detached, unref to not keep Node.js process alive\r\n            if (detached) {\r\n                child.unref();\r\n            }\r\n        });\r\n    }\r\n}\r\n\r\nmodule.exports = SafeShellExecutor;\r\n"
    tokens: 560
    size: 1710
  - path: engine\src\services\scribe\scribe.ts
    content: |-
      /**
       * Scribe Service - Markovian Rolling Context
       *
       * Maintains a "Session State" that summarizes the current conversation.
       * This enables the model to maintain coherence across long conversations
       * without requiring the full history in context.
       */

      import { db } from '../../core/db.js';

      // Lazy-load inference to avoid circular dependency
      let inferenceModule: any = null;
      function getInference() {
          if (!inferenceModule) {
              inferenceModule = require('../inference/inference');
          }
          return inferenceModule;
      }

      const SESSION_STATE_ID = 'session_state';
      const STATE_BUCKET = ['system', 'state'];

      interface HistoryItem {
          role: string;
          content: string;
      }

      interface UpdateStateResult {
          status: string;
          summary?: string;
          message?: string;
      }

      interface ClearStateResult {
          status: string;
          message?: string;
      }

      /**
       * Updates the rolling session state based on recent conversation history.
       * Uses the LLM to compress recent turns into a coherent state summary.
       *
       * @param {HistoryItem[]} history - Array of {role, content} message objects
       * @returns {Promise<UpdateStateResult>} - {status, summary} or {status, error}
       */
      export async function updateState(history: HistoryItem[]): Promise<UpdateStateResult> {
          console.log(' Scribe: Analyzing conversation state...');

          try {
              // 1. Flatten last 10 turns into readable text
              const recentTurns = history.slice(-10);
              const recentText = recentTurns
                  .map(m => `${m.role.toUpperCase()}: ${m.content}`)
                  .join('\n\n');

              if (!recentText.trim()) {
                  return { status: 'skipped', message: 'No conversation history to analyze' };
              }

              // 2. Construct the state extraction prompt
              const prompt = `Analyze this conversation segment and produce a concise "Session State" summary.

      Keep it under 200 words. Focus on:
      - Current Goal: What is the user trying to accomplish?
      - Key Decisions: What has been decided or agreed upon?
      - Active Tasks: What work is in progress or pending?
      - Important Context: What background information is critical to remember?

      Conversation:
      ${recentText}

      ---
      Session State Summary:`;

              // 3. Generate the state summary
              const inf = getInference();
              const summary = await inf.rawCompletion(prompt);

              if (!summary || summary.trim().length < 10) {
                  return { status: 'error', message: 'Failed to generate meaningful state' };
              }

              // 4. Persist to database with special ID
              const timestamp = Date.now();
              const query = `?[id, timestamp, content, source, type, hash, buckets, tags] <- $data :put memory {id, timestamp, content, source, type, hash, buckets, tags}`;

              await db.run(query, {
                  data: [[
                      SESSION_STATE_ID,
                      timestamp,
                      summary.trim(),
                      'Scribe',
                      'state',
                      `state_${timestamp}`,
                      STATE_BUCKET,
                      '[]'  // tags as JSON string
                  ]]
              });

              console.log(' Scribe: State updated successfully');
              return { status: 'updated', summary: summary.trim() };

          } catch (e: any) {
              console.error(' Scribe Error:', e.message);
              return { status: 'error', message: e.message };
          }
      }

      /**
       * Retrieves the current session state from the database.
       *
       * @returns {Promise<string | null>} - The state summary or null if not found
       */
      export async function getState(): Promise<string | null> {
          try {
              const query = '?[content] := *memory{id: mem_id, content}, mem_id == $id';
              const res = await db.run(query, { id: SESSION_STATE_ID });

              if (res.rows && res.rows.length > 0) {
                  return res.rows[0][0] as string;
              }
              return null;
          } catch (e: any) {
              console.error(' Scribe: Failed to retrieve state:', e.message);
              return null;
          }
      }

      /**
       * Clears the current session state.
       * Useful for starting a fresh conversation.
       *
       * @returns {Promise<ClearStateResult>} - {status}
       */
      export async function clearState(): Promise<ClearStateResult> {
          try {
              const query = `?[id] <- [[$id]] :delete memory {id}`;
              await db.run(query, { id: SESSION_STATE_ID });
              console.log(' Scribe: State cleared');
              return { status: 'cleared' };
          } catch (e: any) {
              console.error(' Scribe: Failed to clear state:', e.message);
              return { status: 'error', message: e.message };
          }
      }
    tokens: 1619
    size: 4556
  - path: engine\src\services\search\search.ts
    content: |-
      /**
       * Search Service with Engram Layer and Provenance Boosting
       *
       * Implements:
       * 1. Engram Layer (Fast Lookup) - O(1) lookup for known entities
       * 2. Provenance Boosting - Sovereign content gets boost
       * 3. Tag-Walker Protocol - Graph-based associative retrieval (Replacing Vector Search)
       */

      import { db } from '../../core/db.js';
      import { createHash } from 'crypto';
      import { composeRollingContext } from '../../core/inference/context_manager.js';

      interface SearchResult {
        id: string;
        content: string;
        source: string;
        timestamp: number;
        buckets: string[];
        tags: string;
        epochs: string;
        provenance: string;
        score: number;
      }

      /**
       * Helper to sanitize queries for CozoDB FTS engine
       */
      function sanitizeFtsQuery(query: string): string {
        return query
          .replace(/[^a-zA-Z0-9\s]/g, ' ')
          .replace(/\s+/g, ' ')
          .trim()
          .toLowerCase();
      }

      /**
       * Create or update an engram (lexical sidecar) for fast entity lookup
       */
      export async function createEngram(key: string, memoryIds: string[]): Promise<void> {
        const normalizedKey = key.toLowerCase().trim();
        const engramId = createHash('md5').update(normalizedKey).digest('hex');

        const insertQuery = `?[key, value] <- $data :put engrams {key, value}`;
        await db.run(insertQuery, {
          data: [[engramId, JSON.stringify(memoryIds)]]
        });
      }

      /**
       * Lookup memories by engram key (O(1) operation)
       */
      export async function lookupByEngram(key: string): Promise<string[]> {
        const normalizedKey = key.toLowerCase().trim();
        const engramId = createHash('md5').update(normalizedKey).digest('hex');

        const query = `?[value] := *engrams{key, value}, key = $engramId`;
        const result = await db.run(query, { engramId });

        if (result.rows && result.rows.length > 0) {
          return JSON.parse(result.rows[0][0] as string);
        }

        return [];
      }

      /**
       * Perform Graph-Based Associative "Neighbor Walk"
       * Phase 3 of Tag-Walker Algorithm
       */
      /**
       * Tag-Walker Associative Search (Replaces Vector Search)
       * Strategy:
       * 1. Anchor (70%): Find direct text matches (FTS).
       * 2. Walk (30%): Find neighbors that share specific tags with the Anchors.
       */
      async function tagWalkerSearch(
        query: string,
        buckets: string[] = [],
        _maxChars: number = 524288
      ): Promise<SearchResult[]> {
        try {
          const sanitizedQuery = sanitizeFtsQuery(query);
          if (!sanitizedQuery) return [];

          // 1. Direct Search (The Anchor)
          // We use FTS to find the "Entry Nodes" into the graph
          const anchorQuery = `
                  ?[id, content, source, timestamp, buckets, tags, epochs, provenance, score] := 
                  ~memory:content_fts{id | query: $query, k: 50, bind_score: fts_score},
                  *memory{id, content, source, timestamp, buckets, tags, epochs, provenance},
                  score = 100.0 * fts_score
                  ${buckets.length > 0 ? ', length(intersection(buckets, $buckets)) > 0' : ''}
                  :limit 20
              `;

          const anchorResult = await db.run(anchorQuery, { query: sanitizedQuery, buckets });
          if (!anchorResult.rows || anchorResult.rows.length === 0) return [];

          // Map Anchors
          const anchors = anchorResult.rows.map((row: any[]) => ({
            id: row[0],
            content: row[1],
            source: row[2],
            timestamp: row[3],
            buckets: row[4],
            tags: row[5],
            epochs: row[6],
            provenance: row[7],
            score: row[8]
          }));

          // 2. The Walk (Associative Discovery)
          const anchorIds = anchors.map((a: any) => a.id);

          // Cozo Query: Find nodes sharing tags with our anchors
          const walkQuery = `
                  ?[id, content, source, timestamp, buckets, tags, epochs, provenance, score] := 
                  *memory{id: anchor_id, tags: anchor_tags},
                  anchor_id in $anchorIds,
                  tag in anchor_tags,
                  *memory{id, content, source, timestamp, buckets, tags, epochs, provenance},
                  tag in tags,
                  id != anchor_id,
                  score = 50.0
                  :limit 10
              `;

          const walkResult = await db.run(walkQuery, { anchorIds });
          const neighbors = (walkResult.rows || []).map((row: any[]) => ({
            id: row[0],
            content: row[1],
            source: row[2],
            timestamp: row[3],
            buckets: row[4],
            tags: row[5],
            epochs: row[6],
            provenance: row[7],
            score: row[8]
          }));

          return [...anchors, ...neighbors];

        } catch (e) {
          console.error('[Search] Tag-Walker failed:', e);
          return [];
        }
      }


      /**
       * Execute search with Tag-Walker Protocol
       */
      export async function executeSearch(
        query: string,
        bucket?: string,
        buckets?: string[],
        maxChars: number = 524288,
        _deep: boolean = false,
        provenance: 'sovereign' | 'external' | 'all' = 'all'
      ): Promise<{ context: string; results: SearchResult[]; toAgentString: () => string; metadata?: any }> {
        console.log(`[Search] executeSearch (Tag-Walker) called with provenance: ${provenance}`);

        const targetBuckets = buckets || (bucket ? [bucket] : []);

        // 1. ENGRAM LOOKUP
        const engramResults = await lookupByEngram(query);
        let finalResults: SearchResult[] = [];
        const includedIds = new Set<string>();

        if (engramResults.length > 0) {
          console.log(`[Search] Found ${engramResults.length} via Engram: ${query}`);
          const engramContextQuery = `?[id, content, source, timestamp, buckets, tags, epochs, provenance] := *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}, id in $ids`;
          const engramContentResult = await db.run(engramContextQuery, { ids: engramResults });
          if (engramContentResult.rows) {
            engramContentResult.rows.forEach((row: any[]) => {
              if (!includedIds.has(row[0])) {
                finalResults.push({
                  id: row[0], content: row[1], source: row[2], timestamp: row[3], buckets: row[4], tags: row[5], epochs: row[6], provenance: row[7], score: 200
                });
                includedIds.add(row[0]);
              }
            });
          }
        }

        // 2. TAG-WALKER SEARCH (Hybrid FTS + Graph)
        const walkerResults = await tagWalkerSearch(query, targetBuckets, maxChars);

        // Merge and Apply Provenance Boosting
        walkerResults.forEach(r => {
          let score = r.score;

          // Apply Sovereign Bias
          if (provenance === 'sovereign') {
            if (r.provenance === 'sovereign') score *= 3.0;
            else score *= 0.5;
          } else if (provenance === 'external') {
            if (r.provenance !== 'sovereign') score *= 1.5;
          } else {
            if (r.provenance === 'sovereign') score *= 2.0;
          }

          if (!includedIds.has(r.id)) {
            finalResults.push({ ...r, score });
            includedIds.add(r.id);
          }
        });

        console.log(`[Search] Total Results: ${finalResults.length}`);

        // Final Sort by Score
        finalResults.sort((a, b) => b.score - a.score);

        return formatResults(finalResults, maxChars);
      }


      // Helper for FTS
      export async function runTraditionalSearch(query: string, buckets: string[]): Promise<SearchResult[]> {
        const sanitizedQuery = sanitizeFtsQuery(query);

        if (!sanitizedQuery) return [];

        let queryCozo = '';
        // Use single-line query format to avoid parser issues
        if (buckets.length > 0) {
          queryCozo = `?[id, score, content, source, timestamp, buckets, tags, epochs, provenance] := ~memory:content_fts{id | query: $q, k: 500, bind_score: score}, *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}, length(intersection(buckets, $buckets)) > 0`;
        } else {
          queryCozo = `?[id, score, content, source, timestamp, buckets, tags, epochs, provenance] := ~memory:content_fts{id | query: $q, k: 500, bind_score: score}, *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}`;
        }

        try {
          const result = await db.run(queryCozo, { q: sanitizedQuery, buckets });

          if (!result.rows) return [];

          return result.rows.map((row: any[]) => ({
            id: row[0],
            score: row[1],
            content: row[2],
            source: row[3],
            timestamp: row[4],
            buckets: row[5],
            tags: row[6],
            epochs: row[7],
            provenance: row[8]
          }));

        } catch (e) {
          console.error('[Search] FTS failed', e);
          return [];
        }
      }

      /**
       * Format search results within character budget
       */
      function formatResults(results: SearchResult[], maxChars: number): { context: string; results: SearchResult[]; toAgentString: () => string; metadata?: any } {
        // Convert SearchResult to ContextAtom
        const candidates = results.map(r => ({
          id: r.id,
          content: r.content,
          source: r.source,
          timestamp: r.timestamp,
          score: r.score
        }));

        const tokenBudget = Math.floor(maxChars / 4);
        const rollingContext = composeRollingContext("query_placeholder", candidates, tokenBudget);

        const sortedResults = results.sort((a, b) => b.score - a.score);

        return {
          context: rollingContext.prompt || 'No results found.',
          results: sortedResults,
          toAgentString: () => {
            // Safe substring in case content is missing (though our types enforce it)
            return sortedResults.map(r => `[${r.provenance}] ${r.source}: ${(r.content || "").substring(0, 200)}...`).join('\n');
          },
          metadata: rollingContext.stats
        };
      }

      export function parseQuery(query: string): { phrases: string[]; temporal: string[]; buckets: string[]; keywords: string[]; } {
        const result = { phrases: [] as string[], temporal: [] as string[], buckets: [] as string[], keywords: [] as string[] };
        const phraseRegex = /"([^"]+)"/g;
        let phraseMatch;
        while ((phraseMatch = phraseRegex.exec(query)) !== null) result.phrases.push(phraseMatch[1]);
        let remainingQuery = query.replace(/"[^"]+"/g, '');
        const temporalRegex = /@(\w+)/g;
        let temporalMatch;
        while ((temporalMatch = temporalRegex.exec(remainingQuery)) !== null) result.temporal.push(temporalMatch[1]);
        remainingQuery = remainingQuery.replace(/@\w+/g, '');
        const bucketRegex = /#(\w+)/g;
        let bucketMatch;
        while ((bucketMatch = bucketRegex.exec(remainingQuery)) !== null) result.buckets.push(bucketMatch[1]);
        remainingQuery = remainingQuery.replace(/#\w+/g, '');
        result.keywords = remainingQuery.split(/\s+/).filter(kw => kw.length > 0);
        return result;
      }
    tokens: 3631
    size: 10041
  - path: engine\src\services\vision\vision_service.js
    content: "const { spawn } = require('child_process');\r\nconst path = require('path');\r\nconst fs = require('fs');\r\nconst http = require('http');\r\nconst paths = require('../../config/paths');\r\nconst Config = require('../../config');\r\n\r\nlet serverProcess = null;\r\nlet lastVisionError = null;\r\nconst SERVER_PORT = 8081;\r\nconst BIN_PATH = path.join(paths.BASE_PATH, 'engine/bin/llama-server.exe');\r\nconst MODEL_DIR = path.join(paths.BASE_PATH, 'engine/models/vision');\r\nconst VISION_CONFIG = Config.MODELS.VISION;\r\n\r\n// Auto-detect model file\r\nconst getModelPath = () => {\r\n    try {\r\n        // Prioritize User's custom model from Config\r\n        if (VISION_CONFIG.PATH) {\r\n            // Check if absolute path\r\n            if (fs.existsSync(VISION_CONFIG.PATH)) {\r\n                console.log(`[Vision] Using configured path: ${VISION_CONFIG.PATH}`);\r\n                return VISION_CONFIG.PATH;\r\n            }\r\n            // Check if relative to MODEL_DIR\r\n            const relativePath = path.join(MODEL_DIR, VISION_CONFIG.PATH);\r\n            if (fs.existsSync(relativePath)) {\r\n                console.log(`[Vision] Using configured model (relative): ${relativePath}`);\r\n                return relativePath;\r\n            }\r\n        }\r\n\r\n        if (!fs.existsSync(MODEL_DIR)) {\r\n            console.log(`[Vision] MODEL_DIR not found: ${MODEL_DIR}`);\r\n            return null;\r\n        }\r\n        const files = fs.readdirSync(MODEL_DIR);\r\n        const gguf = files.find(f => f.endsWith('.gguf') && !f.includes('mmproj'));\r\n        return gguf ? path.join(MODEL_DIR, gguf) : null;\r\n    } catch (e) {\r\n        console.error(`[Vision] Error detecting models: ${e.message}`);\r\n        return null;\r\n    }\r\n};\r\n\r\n// Optional: detect separate projector if exists\r\nconst getMmprojPath = () => {\r\n    try {\r\n        // Check Config first\r\n        if (VISION_CONFIG.PROJECTOR) {\r\n            const configProjPath = path.isAbsolute(VISION_CONFIG.PROJECTOR)\r\n                ? VISION_CONFIG.PROJECTOR\r\n                : path.join(MODEL_DIR, VISION_CONFIG.PROJECTOR);\r\n\r\n            if (fs.existsSync(configProjPath)) return configProjPath;\r\n        }\r\n\r\n        if (!fs.existsSync(MODEL_DIR)) return null;\r\n        const files = fs.readdirSync(MODEL_DIR);\r\n        const proj = files.find(f => f.includes('mmproj'));\r\n        return proj ? path.join(MODEL_DIR, proj) : null;\r\n    } catch (e) { return null; }\r\n};\r\n\r\nasync function startVisionServer() {\r\n    if (serverProcess) {\r\n        // Double check if process is really alive, otherwise nullify\r\n        if (serverProcess.exitCode !== null) {\r\n            console.warn(\"[Vision] Process found but it has exited. Restarting...\");\r\n            serverProcess = null;\r\n        } else {\r\n            return;\r\n        }\r\n    }\r\n\r\n    const modelPath = getModelPath();\r\n    if (!modelPath) {\r\n        console.warn(\"[Vision] No GGUF model found. Vision features disabled.\");\r\n        return;\r\n    }\r\n\r\n    const args = [\r\n        '-m', modelPath,\r\n        '--port', SERVER_PORT.toString(),\r\n        '-c', VISION_CONFIG.CTX_SIZE.toString(),\r\n        '--n-gpu-layers', VISION_CONFIG.GPU_LAYERS.toString(),\r\n    ];\r\n\r\n    // Check if separate mmproj exists\r\n    const mmproj = getMmprojPath();\r\n    if (mmproj) {\r\n        args.push('--mmproj', mmproj);\r\n    }\r\n\r\n    console.log(`[Vision] Launching Binary Sidecar: llama-server.exe on port ${SERVER_PORT}`);\r\n    console.log(`[Vision] Model Path: ${modelPath}`);\r\n    if (mmproj) console.log(`[Vision] Projector Path: ${mmproj}`);\r\n\r\n    try {\r\n        serverProcess = spawn(BIN_PATH, args, {\r\n            stdio: ['ignore', 'pipe', 'pipe']\r\n        });\r\n\r\n        serverProcess.stdout.on('data', (data) => {\r\n            const msg = data.toString();\r\n            // console.log(`[Vision Binary] ${msg}`); \r\n        });\r\n\r\n        serverProcess.stderr.on('data', (data) => {\r\n            const msg = data.toString();\r\n            if (msg.includes('server is listening') || msg.includes('HTTP server listening')) {\r\n                console.log(`[Vision] Sidecar Ready.`);\r\n            }\r\n\r\n            // Detect specific architecture errors\r\n            if (msg.includes('unknown model architecture')) {\r\n                lastVisionError = \"Incompatible Binary: Your llama-server.exe does not support this model type (e.g. Qwen2-VL). Please update engine/bin or use a different model.\";\r\n                console.error(`[Vision Critical] ${lastVisionError}`);\r\n            }\r\n\r\n            // LOG ALL ERRORS\r\n            if (msg.includes('error') || msg.includes('Error') || msg.includes('failed')) {\r\n                console.error(`[Vision Binary Error] ${msg.trim()}`);\r\n            }\r\n        });\r\n\r\n        serverProcess.on('close', (code) => {\r\n            console.log(`[Vision] Sidecar exited with code ${code}`);\r\n            serverProcess = null;\r\n        });\r\n    } catch (e) {\r\n        console.error(`[Vision] Failed to spawn sidecar: ${e.message}`);\r\n    }\r\n}\r\n\r\nfunction stopVisionServer() {\r\n    if (serverProcess) {\r\n        serverProcess.kill();\r\n        serverProcess = null;\r\n    }\r\n}\r\n\r\nasync function analyzeImage(base64Image, prompt) {\r\n    if (!serverProcess) {\r\n        lastVisionError = null;\r\n        await startVisionServer();\r\n        if (!serverProcess) throw new Error(\"Vision server failed to start (Mock Mode or Missing Binary).\");\r\n        // Wait for boot\r\n        await new Promise(r => setTimeout(r, 4000));\r\n\r\n        if (!serverProcess) {\r\n            // Return the specific error if captured, otherwise generic\r\n            throw new Error(lastVisionError || \"Vision server crashed during startup.\");\r\n        }\r\n    }\r\n\r\n    return new Promise((resolve, reject) => {\r\n        // Standard ChatML format for Qwen2-VL\r\n        const payload = JSON.stringify({\r\n            prompt: `<|im_start|>system\\nYou are a helpful visual assistant. You can see the image provided. Describe it in detail.<|im_end|>\\n<|im_start|>user\\n<image>\\n${prompt}<|im_end|>\\n<|im_start|>assistant\\n`,\r\n            image_data: [{ data: base64Image, id: 12 }],\r\n            n_predict: 400,\r\n            temperature: 0.1,\r\n            cache_prompt: true\r\n        });\r\n\r\n        const options = {\r\n            hostname: 'localhost',\r\n            port: SERVER_PORT,\r\n            path: '/completion',\r\n            method: 'POST',\r\n            headers: {\r\n                'Content-Type': 'application/json',\r\n                'Content-Length': payload.length\r\n            }\r\n        };\r\n\r\n        const req = http.request(options, (res) => {\r\n            let data = '';\r\n            res.on('data', (chunk) => data += chunk);\r\n            res.on('end', () => {\r\n                if (!data || data.trim().length === 0) {\r\n                    return reject(new Error(\"Vision sidecar returned empty response. It may have crashed.\"));\r\n                }\r\n                try {\r\n                    const json = JSON.parse(data);\r\n                    // Standard llama-server completion response\r\n                    resolve(json.content || json.text || String(data));\r\n                } catch (e) {\r\n                    // If not JSON, it might be raw text error output\r\n                    if (data.includes('error') || data.includes('failed')) {\r\n                        reject(new Error(`Vision sidecar error: ${data.substring(0, 100)}`));\r\n                    } else {\r\n                        reject(new Error(`Failed to parse vision response: ${e.message}`));\r\n                    }\r\n                }\r\n            });\r\n        });\r\n\r\n        req.on('error', (e) => {\r\n            reject(new Error(`Vision Request Error: ${e.message}`));\r\n        });\r\n\r\n        req.write(payload);\r\n        req.end();\r\n    });\r\n}\r\n\r\nmodule.exports = { startVisionServer, stopVisionServer, analyzeImage };\r\n"
    tokens: 2671
    size: 7764
  - path: engine\src\types\api.ts
    content: "\r\nexport interface Menu {\r\n    id: string;\r\n    content: string;\r\n    source: string;\r\n    type: string;\r\n    timestamp: number;\r\n    buckets: string[];\r\n    tags: string;\r\n    epochs: string;\r\n    provenance: string;\r\n    score?: number;\r\n}\r\n\r\nexport interface SearchRequest {\r\n    query: string;           // The natural language query\r\n    limit?: number;          // Elastic Window (default 20)\r\n    max_chars?: number;      // Character budget\r\n    deep?: boolean;          // If true, trigger 'Epochal' search (Dreamer layers)\r\n\r\n    // The \"UniversalRAG\" Routing Layer\r\n    buckets?: string[];      // e.g., [\"@code\", \"@visual\", \"@memory\"]\r\n    provenance?: 'sovereign' | 'external' | 'all'; // Data Provenance filter\r\n}\r\n\r\nexport interface SearchResponse {\r\n    context: string;\r\n    results: Menu[];\r\n    metadata: {\r\n        engram_hits: number;   // Did we find exact entity matches?\r\n        vector_latency: number;\r\n        provenance_boost_active: boolean;\r\n    }\r\n}\r\n"
    tokens: 335
    size: 982
  - path: engine\src\utils\llamaLoader.ts
    content: "\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel } from 'node-llama-cpp';\r\n\r\nlet llama: any = null;\r\n\r\nexport async function getLlamaInstance() {\r\n    if (!llama) {\r\n        llama = await getLlama();\r\n    }\r\n    return llama;\r\n}\r\n\r\nexport async function getLlamaComponents() {\r\n    return {\r\n        LlamaChatSession,\r\n        LlamaContext,\r\n        LlamaModel\r\n    };\r\n}\r\n"
    tokens: 129
    size: 388
  - path: engine\tests\check_db_count.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function checkDb() {\r\n    await db.init();\r\n    const result = await db.run(\"?[count(id)] := *memory{id}\");\r\n    console.log(\"Memory count:\", result.rows);\r\n}\r\n\r\ncheckDb().catch(console.error);\r\n"
    tokens: 94
    size: 246
  - path: engine\tests\context_experiments.js
    content: "/**\r\n * Context Experiments - Verification Script\r\n * \r\n * Verifies the \"UniversalRAG\" pipeline:\r\n * 1. Vector Search (Semantic Retrieval)\r\n * 2. Context Assembly (Markovian + Graph-R1 simulation)\r\n * 3. Configuration Compliance\r\n */\r\n\r\nimport 'dotenv/config'; // Load .env first\r\nimport { db } from '../dist/core/db.js';\r\nimport { config } from '../dist/config/index.js';\r\n\r\nasync function runExperiments() {\r\n    console.log(' Starting Context Experiments...');\r\n\r\n    // 1. Verify Configuration\r\n    console.log(`\\n[Config Check] Embedding Dimension: ${config.MODELS.EMBEDDING.DIM}`);\r\n    if (!config.MODELS.EMBEDDING.DIM || config.MODELS.EMBEDDING.DIM === 0) {\r\n        console.error(' CRITICAL: LLM_EMBEDDING_DIM is 0 or undefined!');\r\n        process.exit(1);\r\n    } else {\r\n        console.log(' Config Loaded Successfully');\r\n    }\r\n\r\n    try {\r\n        await db.init();\r\n\r\n        // 2. Vector Search Test\r\n        const query = \"What is the capital of France?\"; // Simple query\r\n        console.log(`\\n[Search Test] Query: \"${query}\"`);\r\n\r\n        // Mock embedding generation (using random vector for connectivity test)\r\n        // In real usage, we'd call the LLM. Here we just test the DB path.\r\n        const mockEmbedding = new Array(config.MODELS.EMBEDDING.DIM).fill(0.01);\r\n\r\n        // Manual HNSW search query simulation\r\n        // (Note: HNSW index creation is disabled in db.ts, so this checks the linear scan fallback or basic query)\r\n        const vecQuery = `\r\n            ?[id, distance] := *memory{id, embedding}, \r\n            distance = cosine_dist(embedding, $queryVec),\r\n            distance < 0.2\r\n            :sort distance\r\n            :limit 5\r\n        `;\r\n\r\n        // Using explicit run to test syntax\r\n        // const results = await db.run(vecQuery, { queryVec: mockEmbedding });\r\n        // NOTE: CozoDB might fail on large vector literals in query string.\r\n        // We really want to verify that the table HAS data.\r\n\r\n        const countQuery = `?[id] := *memory{id}`;\r\n        const countResult = await db.run(countQuery);\r\n        console.log(`\\n[DB Status] Total Memories: ${countResult.rows ? countResult.rows.length : 0}`);\r\n\r\n        if ((countResult.rows ? countResult.rows.length : 0) === 0) {\r\n            console.warn('  Database is empty. Please add data to `notebook/inbox` to test retrieval.');\r\n        } else {\r\n            // 3. Retrieve some atoms to check structure\r\n            const sampleQuery = `\r\n                ?[id, content, source_id, embedding_len] := *memory{id, content, source_id, embedding},\r\n                embedding_len = length(embedding)\r\n                :limit 3\r\n             `;\r\n            const sample = await db.run(sampleQuery);\r\n            console.log('\\n[Sample Atoms]:');\r\n            sample.rows.forEach(row => {\r\n                console.log(`- ID: ${row[0]}`);\r\n                console.log(`  SourceID: ${row[2]}`);\r\n                console.log(`  Embedding Length: ${row[3]}`);\r\n                if (row[3] !== config.MODELS.EMBEDDING.DIM) {\r\n                    console.error(` DIMENSION MISMATCH! Expected ${config.MODELS.EMBEDDING.DIM}, Got ${row[3]}`);\r\n                } else {\r\n                    console.log(' Dimension OK');\r\n                }\r\n            });\r\n        }\r\n\r\n        // 4. Test Graph-R1 Flow (Simulation)\r\n        // Ideally we'd trace a relationship, e.g., Next/Prev\r\n        // For now, listing available sources is a good proxy for \"Graph Nodes\"\r\n        const sourceQuery = `?[path, total_atoms] := *source{path, total_atoms}`;\r\n        const sources = await db.run(sourceQuery);\r\n        console.log(`\\n[Graph Sources] Found ${sources.rows ? sources.rows.length : 0}:`);\r\n        if (sources.rows) {\r\n            sources.rows.forEach(r => console.log(`- ${r[0]} (${r[1]} atoms)`));\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(' Experiment Failed:', e);\r\n    } finally {\r\n        await db.close();\r\n    }\r\n}\r\n\r\nrunExperiments();\r\n"
    tokens: 1427
    size: 3991
  - path: engine\tests\dynamic_import_validation.test.js
    content: |-
      import fs from 'fs';
      import path from 'path';
      import { fileURLToPath } from 'url';

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);

      /**
       * Test to validate that all dynamic imports in the codebase use the correct .js extension
       * This prevents ESM/CJS interop issues when running the application
       */

      // Function to recursively find all .js, .ts, .mjs, and .cjs files in a directory
      function getAllSourceFiles(dir, fileList = []) {
          const files = fs.readdirSync(dir);
          
          for (const file of files) {
              const filePath = path.join(dir, file);
              const stat = fs.statSync(filePath);
              
              if (stat.isDirectory()) {
                  // Skip node_modules and dist directories to focus on source code
                  if (file !== 'node_modules' && file !== 'dist' && !file.startsWith('.')) {
                      getAllSourceFiles(filePath, fileList);
                  }
              } else if (/\.(js|ts|mjs|cjs)$/.test(path.extname(filePath))) {
                  fileList.push(filePath);
              }
          }
          
          return fileList;
      }

      // Function to find all dynamic import statements in a file
      function findDynamicImports(content, filePath) {
          // Regular expression to match dynamic import statements
          // Looks for await import(...) or import(...) patterns
          const dynamicImportRegex = /(await\s+)?import\s*\(\s*["'](.*?\.(js|ts))["']\s*\)/g;
          const matches = [];
          let match;
          
          while ((match = dynamicImportRegex.exec(content)) !== null) {
              matches.push({
                  fullMatch: match[0],
                  hasAwait: match[1] ? true : false,
                  importPath: match[2],
                  extension: match[3],
                  position: match.index
              });
          }
          
          return matches;
      }

      describe('Dynamic Import Validation', () => {
          it('should ensure all dynamic imports use .js extension for ESM compatibility', () => {
              // Get all source files from the src directory
              const srcDir = path.join(__dirname, '../src');
              const sourceFiles = getAllSourceFiles(srcDir);
              
              const errors = [];
              
              for (const filePath of sourceFiles) {
                  const content = fs.readFileSync(filePath, 'utf8');
                  const dynamicImports = findDynamicImports(content, filePath);
                  
                  for (const imp of dynamicImports) {
                      // Check if the import path ends with .js for ESM compatibility
                      if (!imp.importPath.endsWith('.js')) {
                          errors.push({
                              file: filePath,
                              importStatement: imp.fullMatch,
                              position: imp.position,
                              message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`
                          });
                      }
                  }
              }
              
              // Also check some key files in the root and other directories
              const additionalFiles = [
                  path.join(__dirname, '../server.js'),
                  path.join(__dirname, '../index.js'),
                  path.join(__dirname, '../src/index.ts'),
                  path.join(__dirname, '../src/index.js')
              ];
              
              for (const filePath of additionalFiles) {
                  if (fs.existsSync(filePath)) {
                      const content = fs.readFileSync(filePath, 'utf8');
                      const dynamicImports = findDynamicImports(content, filePath);
                      
                      for (const imp of dynamicImports) {
                          if (!imp.importPath.endsWith('.js')) {
                              errors.push({
                                  file: filePath,
                                  importStatement: imp.fullMatch,
                                  position: imp.position,
                                  message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`
                              });
                          }
                      }
                  }
              }
              
              // Report any errors found
              if (errors.length > 0) {
                  console.error('\n Dynamic Import Validation Failed!');
                  console.error('Found dynamic imports that do not use .js extension:');
                  
                  for (const error of errors) {
                      console.error(`\nFile: ${error.file}`);
                      console.error(`Line: ${getLineNumber(error.file, error.position)}`);
                      console.error(`Import: ${error.importStatement}`);
                      console.error(`Issue: ${error.message}`);
                  }
                  
                  throw new Error(`${errors.length} dynamic import(s) need to be updated to use .js extension`);
              }
              
              console.log(` All dynamic imports validated successfully! Checked ${sourceFiles.length} source files.`);
          });
      });

      // Helper function to get line number from position in file
      function getLineNumber(filePath, position) {
          const content = fs.readFileSync(filePath, 'utf8');
          const lines = content.substring(0, position).split('\n');
          return lines.length;
      }

      // Additional test to validate specific known problematic files
      describe('Specific Dynamic Import Checks', () => {
          it('should validate dynamic imports in key service files', () => {
              const keyFilesToCheck = [
                  path.join(__dirname, '../src/services/inference/inference.ts'),
                  path.join(__dirname, '../src/controllers/SearchController.js'),
                  path.join(__dirname, '../src/controllers/ChatController.js'),
                  path.join(__dirname, '../src/services/scribe/scribe.js'),
                  path.join(__dirname, '../src/services/dreamer/dreamer.js'),
                  path.join(__dirname, '../src/services/refiner/refiner.js')
              ];
              
              const errors = [];
              
              for (const filePath of keyFilesToCheck) {
                  if (fs.existsSync(filePath)) {
                      const content = fs.readFileSync(filePath, 'utf8');
                      const dynamicImports = findDynamicImports(content, filePath);
                      
                      for (const imp of dynamicImports) {
                          if (!imp.importPath.endsWith('.js')) {
                              errors.push({
                                  file: filePath,
                                  importStatement: imp.fullMatch,
                                  position: imp.position,
                                  message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`
                              });
                          }
                      }
                  }
              }
              
              if (errors.length > 0) {
                  console.error('\n Specific Dynamic Import Validation Failed!');
                  console.error('Found issues in key service files:');
                  
                  for (const error of errors) {
                      console.error(`\nFile: ${error.file}`);
                      console.error(`Line: ${getLineNumber(error.file, error.position)}`);
                      console.error(`Import: ${error.importStatement}`);
                      console.error(`Issue: ${error.message}`);
                  }
                  
                  throw new Error(`${errors.length} dynamic import(s) in key files need to be updated`);
              }
              
              console.log(` All key service files validated successfully!`);
          });
      });
    tokens: 2512
    size: 7292
  - path: engine\tests\suite.js
    content: "/**\r\n * ECE Test Suite\r\n * \r\n * Verifies core API functionality:\r\n * - Health endpoint\r\n * - Ingestion pipeline\r\n * - Search/Retrieval\r\n * - Scribe (Markovian State)\r\n * \r\n * Run: npm test (or node tests/suite.js)\r\n */\r\n\r\nconst BASE_URL = process.env.ECE_URL || 'http://localhost:3000';\r\n\r\n// Test results tracking\r\nlet passed = 0;\r\nlet failed = 0;\r\n\r\n/**\r\n * Test runner with pretty output\r\n */\r\nasync function test(name, fn) {\r\n    try {\r\n        process.stdout.write(`  ${name}... `);\r\n        await fn();\r\n        console.log(' PASS');\r\n        passed++;\r\n    } catch (e) {\r\n        console.log(' FAIL');\r\n        console.error(`      ${e.message}`);\r\n        failed++;\r\n    }\r\n}\r\n\r\n// Shim for ESM __dirname if needed\r\nimport { fileURLToPath } from 'url';\r\nimport { dirname } from 'path';\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = dirname(__filename);\r\n\r\n/**\r\n * Assert helper\r\n */\r\nfunction assert(condition, message) {\r\n    if (!condition) throw new Error(message || 'Assertion failed');\r\n}\r\n\r\n/**\r\n * Main test suite\r\n */\r\nasync function runSuite() {\r\n    console.log('\\n');\r\n    console.log('     ECE TEST SUITE                     ');\r\n    console.log('\\n');\r\n    console.log(`Target: ${BASE_URL}\\n`);\r\n\r\n    // \r\n    // SECTION 1: Core Health\r\n    // \r\n    console.log(' Core Health ');\r\n\r\n    await test('Health Endpoint', async () => {\r\n        const res = await fetch(`${BASE_URL}/health`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'Sovereign', `Unexpected status: ${json.status}`);\r\n    });\r\n\r\n    await test('Models List', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/models`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const models = await res.json();\r\n        assert(Array.isArray(models), 'Expected array of models');\r\n    });\r\n\r\n    // \r\n    // SECTION 2: Ingestion Pipeline\r\n    // \r\n    console.log('\\n Ingestion Pipeline ');\r\n\r\n    const testId = `test_${Date.now()}`;\r\n    const testContent = `ECE Test Memory: ${testId}. The secret code is ALPHA_BRAVO.`;\r\n\r\n    await test('Ingest Memory', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/ingest`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                content: testContent,\r\n                source: 'Test Suite',\r\n                type: 'test',\r\n                buckets: ['test', 'verification']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'success', `Ingest failed: ${JSON.stringify(json)}`);\r\n    });\r\n\r\n    // Brief pause for consistency (increased to 1500ms for FTS indexing/flush)\r\n    await new Promise(r => setTimeout(r, 1500));\r\n\r\n    // \r\n    // SECTION 3: Retrieval\r\n    // \r\n    console.log('\\n Retrieval ');\r\n\r\n    await test('Search by ID', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: testId,\r\n                buckets: ['test']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // Log response if failure suspected\r\n        if (!json.context || !json.context.includes(testId)) {\r\n            console.log('     [DEBUG] Search by ID Response:', JSON.stringify(json).substring(0, 200));\r\n        }\r\n        assert(json.context && json.context.includes(testId), 'Test memory not found in search results');\r\n    });\r\n\r\n    await test('Search by Content', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: 'ALPHA_BRAVO',\r\n                buckets: ['test']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.context && json.context.includes('ALPHA_BRAVO'), 'Secret code not found');\r\n    });\r\n\r\n    await test('Bucket Filtering', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: testId,\r\n                buckets: ['nonexistent_bucket']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // Should NOT find results in wrong bucket\r\n        const found = json.context && json.context.includes(testId);\r\n        assert(!found, 'Should not find test memory in wrong bucket');\r\n    });\r\n\r\n    // \r\n    // SECTION 4: Scribe (Markovian State)\r\n    // \r\n    console.log('\\n Scribe (Markovian State) ');\r\n\r\n    await test('Get State (Empty)', async () => {\r\n        // Clear first\r\n        await fetch(`${BASE_URL}/v1/scribe/state`, { method: 'DELETE' });\r\n\r\n        const res = await fetch(`${BASE_URL}/v1/scribe/state`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // State might be null or have previous data - just check structure\r\n        assert('state' in json, 'Missing state field');\r\n    });\r\n\r\n    await test('Clear State', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/scribe/state`, { method: 'DELETE' });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'cleared' || json.status === 'error', 'Unexpected response');\r\n    });\r\n\r\n    // \r\n    // SECTION 5: Buckets\r\n    // \r\n    console.log('\\n Buckets ');\r\n\r\n    await test('List Buckets', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/buckets`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const buckets = await res.json();\r\n        assert(Array.isArray(buckets), 'Expected array of buckets');\r\n        assert(buckets.includes('test'), 'Test bucket should exist');\r\n    });\r\n\r\n    // \r\n    // SECTION 6: Watchdog & Mirror Verification\r\n    // \r\n    console.log('\\n Watchdog & Mirror Verification ');\r\n\r\n    // NOTE: This test requires the engine to be running with access to NOTEBOOK_DIR\r\n    // We will attempt to write a file to the inbox and verify it appears in search\r\n    // and then after a dream, appears in the mirror.\r\n\r\n    await test('Watchdog Ingestion', async () => {\r\n        // 1. Create a dummy file in the inbox\r\n        // We need to know where the inbox is. \r\n        // We can't easily import 'path' or config here if we want to be a standalone test suite\r\n        // relying only on API. BUT, we are running in the same environment likely.\r\n        // Let's assume we can use 'fs' and 'path' if we import them.\r\n\r\n        // Dynamic import for fs/path to avoid top-level issues if running in browser-like environment (though this is node)\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n        const os = await import('os');\r\n\r\n        // Resolve Notebook Dir - this is tricky without config.\r\n        // We'll rely on the user's setup effectively matching what we expect.\r\n        // Test suite is running in engine/tests/\r\n        // __dirname is .../engine/tests\r\n        // .. -> engine\r\n        // .. -> ECE_Core\r\n        // .. -> Projects\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const INBOX_DIR = path.join(NOTEBOOK_DIR, 'inbox');\r\n\r\n        if (!fs.existsSync(INBOX_DIR)) {\r\n            // Create it if missing (recovery)\r\n            fs.mkdirSync(INBOX_DIR, { recursive: true });\r\n        }\r\n\r\n        const uniqueId = `watchdog_test_${Date.now()}`;\r\n        const filePath = path.join(INBOX_DIR, `${uniqueId}.txt`);\r\n        const fileContent = `This is a watchdog test file. ID: ${uniqueId}`;\r\n\r\n        await fs.promises.writeFile(filePath, fileContent);\r\n\r\n        // Wait for Watchdog to pick it up (debounce is small but depends on poll)\r\n        // Give it 2 seconds\r\n        await new Promise(r => setTimeout(r, 2000));\r\n\r\n        // Search for it\r\n        let found = false;\r\n        let attempts = 0;\r\n        while (!found && attempts < 3) {\r\n            const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n                method: 'POST',\r\n                headers: { 'Content-Type': 'application/json' },\r\n                body: JSON.stringify({\r\n                    query: uniqueId,\r\n                    buckets: ['inbox'] // It should be in 'inbox' bucket\r\n                })\r\n            });\r\n            const json = await res.json();\r\n            if (json.context && json.context.includes(uniqueId)) {\r\n                found = true;\r\n            } else {\r\n                await new Promise(r => setTimeout(r, 1000));\r\n                attempts++;\r\n            }\r\n        }\r\n\r\n        assert(found, `Watchdog failed to ingest file ${uniqueId}`);\r\n\r\n        // Cleanup input file\r\n        await fs.promises.unlink(filePath);\r\n    });\r\n\r\n    await test('Mirror Protocol', async () => {\r\n        // Trigger Dream\r\n        const res = await fetch(`${BASE_URL}/v1/dream`, { method: 'POST' });\r\n        assert(res.ok, `Dream request failed: ${res.status}`);\r\n\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const MIRROR_DIR = path.join(NOTEBOOK_DIR, 'mirrored_brain');\r\n        const inboxMirror = path.join(MIRROR_DIR, 'inbox'); // Bucket is likely 'inbox'\r\n        const year = new Date().getFullYear().toString();\r\n        const yearDir = path.join(inboxMirror, year);\r\n\r\n        // Verification might be flaky if dream queue is slow, but we awaited the response which awaits the dream\r\n        // Check for ANY file in recent mirror\r\n        if (fs.existsSync(yearDir)) {\r\n            const files = await fs.promises.readdir(yearDir);\r\n            assert(files.length >= 0, 'Directory exists');\r\n            if (files.length > 0) console.log(`      Verified ${files.length} mirrored memories.`);\r\n        } else {\r\n            console.log('      Mirror directory not yet created (acceptable if no new memories processed)');\r\n        }\r\n    });\r\n\r\n    // \r\n    // SECTION 7: Semantic Decompression (Atomizer)\r\n    // \r\n    console.log('\\n Semantic Decompression (Atomizer) ');\r\n\r\n    await test('Atomizer splitting', async () => {\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const INBOX_DIR = path.join(NOTEBOOK_DIR, 'inbox');\r\n\r\n        const atomId = `atom_test_${Date.now()}`;\r\n        const filePath = path.join(INBOX_DIR, `${atomId}.md`);\r\n        // Create 3 paragraphs -> Should be 3 atoms\r\n        const content = `Block 1: ${atomId}.\\n\\nBlock 2: ${atomId} continued.\\n\\nBlock 3: ${atomId} ending.`;\r\n\r\n        await fs.promises.writeFile(filePath, content);\r\n        await new Promise(r => setTimeout(r, 2000)); // Wait for Watchdog\r\n\r\n        // Search should return 3 results or we check context\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({ query: atomId, buckets: ['inbox'] })\r\n        });\r\n        const json = await res.json();\r\n\r\n        // This is a rough check. Ideally we'd inspect the DB structure directly or backup\r\n        // But if we find the content, ingestion worked.\r\n        assert(json.context && json.context.includes(atomId), 'Atom content not found');\r\n\r\n        // Cleanup\r\n        await fs.promises.unlink(filePath);\r\n    });\r\n\r\n    // \r\n    // SECTION 8: Abstraction Pyramid\r\n    // \r\n    console.log('\\n Abstraction Pyramid ');\r\n\r\n    await test('Dreamer / Abstraction', async () => {\r\n        // Trigger Dream again to process new atoms\r\n        const res = await fetch(`${BASE_URL}/v1/dream`, { method: 'POST' });\r\n        assert(res.ok, 'Dream failed');\r\n        const json = await res.json();\r\n\r\n        // Check \"updated\" or \"analyzed\" count\r\n        if (json.analyzed > 0) {\r\n            console.log(`      Analyzed ${json.analyzed} memories.`);\r\n        }\r\n        // Use Backup API to inspect for 'summary_node' if possible, or just trust the dream status.\r\n        // If we implement 'GET /v1/backup', we could check it.\r\n        // For now, status Verified.\r\n        assert(json.status === 'success', 'Dream status not success');\r\n    });\r\n\r\n    // \r\n    // RESULTS\r\n    // \r\n    console.log('\\n');\r\n    console.log(`  Results: ${passed} passed, ${failed} failed`.padEnd(41) + '');\r\n    console.log('\\n');\r\n\r\n    process.exit(failed > 0 ? 1 : 0);\r\n}\r\n\r\n// Run\r\nrunSuite().catch(e => {\r\n    console.error('Suite crashed:', e);\r\n    process.exit(1);\r\n});\r\n"
    tokens: 4951
    size: 16157
  - path: engine\tests\test_ingest_atom_debug.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\nimport config from '../src/config/index.js';\r\n\r\nasync function runTest() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    // Raw Failing Atom (Copied from Debug Log)\r\n    // \"atom_3449feb29c1ea488\", 1768844295178, \"Block 2...\", \"inbox\\\\atom_test...\", \"beacbd...\", 1, \"text\", \"3449fe...\", [\"inbox\"], [], [], \"external\", [0.1...]\r\n\r\n    // Construct exactly as ingestAtoms does\r\n    // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding\r\n\r\n    const atomData = [\r\n        \"atom_3449feb29c1ea488\",\r\n        1768844295178,\r\n        \"Block 2: atom_test_1768659429156 continued.\",\r\n        \"inbox\\\\atom_test_1768659429156.md\",\r\n        \"beacbd2a7598600c6acb4fe2e7c36323\",\r\n        1,\r\n        \"text\",\r\n        \"3449feb29c1ea488\",\r\n        [\"inbox\"],\r\n        [], // epochs\r\n        [], // tags\r\n        \"external\",\r\n        new Array(768).fill(0.1)\r\n    ];\r\n\r\n    const chunk = [atomData];\r\n\r\n    console.log(\"Attempting Insert...\");\r\n    try {\r\n        await db.run(`\r\n            ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data\r\n            :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}\r\n        `, { data: chunk });\r\n        console.log(\"SUCCESS: Insert worked!\");\r\n    } catch (e: any) {\r\n        console.error(\"FAILURE: Insert failed:\", e.message);\r\n    }\r\n}\r\n\r\nrunTest().catch(console.error);\r\n"
    tokens: 545
    size: 1578
  - path: engine\tests\test_mirror_trigger.ts
    content: "\r\nimport { createMirror } from '../src/services/mirror/mirror.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testMirror() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    console.log(\"Triggering Mirror 2.0...\");\r\n    await createMirror();\r\n    console.log(\"Mirroring complete.\");\r\n}\r\n\r\ntestMirror().catch(console.error);\r\n"
    tokens: 132
    size: 361
  - path: engine\tests\test_provenance_manual.ts
    content: "import { db } from '../src/core/db.js';\r\nimport { executeSearch, runTraditionalSearch } from '../src/services/search/search.js';\r\n\r\nasync function run() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    // Clean up stale data from previous failed runs\r\n    try {\r\n        await db.run(`?[id] := *memory{id}, starts_with(id, 'test_') :rm memory {id}`);\r\n        console.log(\"Cleaned up stale test data.\");\r\n    } catch (e) {\r\n        console.log(\"No stale data to clean or cleanup failed.\");\r\n    }\r\n\r\n    const idSovereign = `test_sov_${Date.now()}`;\r\n    const idExternal = `test_ext_${Date.now()}`;\r\n    const idNeighbor = `test_neigh_${Date.now()}`;\r\n\r\n    // Anchor content has keywords\r\n    const content = \"provenance test content unique phrase\";\r\n    // Neighbor content has NO keywords, but shares tags\r\n    const neighborContent = \"this is a hidden connection found via tags\";\r\n\r\n    console.log(\"Ingesting test data...\");\r\n\r\n    // Sovereign Item (Anchor)\r\n    await db.run(\r\n        `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data \r\n         :insert memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n        {\r\n            data: [[\r\n                idSovereign, Date.now(), content, 'Test', 'src_sov', 0, 'text', 'hash_sov', ['test'], ['#bridge_tag'], [], 'sovereign', new Array(768).fill(0.1)\r\n            ]]\r\n        }\r\n    );\r\n\r\n    // External Item (Anchor)\r\n    await db.run(\r\n        `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data \r\n         :insert memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n        {\r\n            data: [[\r\n                idExternal, Date.now(), content, 'Test', 'src_ext', 0, 'text', 'hash_ext', ['test'], ['#bridge_tag'], [], 'external', new Array(768).fill(0.1)\r\n            ]]\r\n        }\r\n    );\r\n\r\n    // Neighbor Item (Hidden)\r\n    await db.run(\r\n        `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data \r\n         :insert memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n        {\r\n            data: [[\r\n                idNeighbor, Date.now(), neighborContent, 'Test', 'src_neigh', 0, 'text', 'hash_neigh', ['test'], ['#bridge_tag'], [], 'sovereign', new Array(768).fill(0.1)\r\n            ]]\r\n        }\r\n    );\r\n\r\n    try {\r\n        console.log(\"\\n--- TEST CASE 1: Sovereign Bias (Frontend Toggle ON) ---\");\r\n        let resSov = await executeSearch(content, undefined, ['test'], 2000, false, 'sovereign');\r\n        console.log(`Results: ${resSov.results.length}`);\r\n        resSov.results.forEach(r => console.log(`[${r.id}] Score: ${r.score}`));\r\n\r\n        console.log(\"\\n--- TEST CASE 2: Neutral Bias (Frontend Toggle OFF) ---\");\r\n        let resAll = await executeSearch(content, undefined, ['test'], 2000, false, 'all');\r\n        console.log(`Results: ${resAll.results.length}`);\r\n        resAll.results.forEach(r => console.log(`[${r.id}] Score: ${r.score}`));\r\n\r\n    } catch (e) {\r\n        console.error(\"Test execution failed:\", e);\r\n    }\r\n\r\n    try {\r\n        console.log(\"Testing Provenance: ALL (Tag-Walker)\");\r\n        // We expect Anchors (Sovereign + External) via FTS\r\n        // AND Neighbor via Tag-Walk (Phase 3)\r\n        let res = await executeSearch(content, undefined, ['test'], 2000, false, 'all');\r\n\r\n        console.log(\"Results Found:\", res.results.length);\r\n        res.results.forEach(r => {\r\n            console.log(`[${r.id}] ${r.content.substring(0, 30)}... (Score: ${r.score})`);\r\n        });\r\n\r\n        const neighborFound = res.results.find(r => r.id === idNeighbor);\r\n        if (neighborFound) {\r\n            console.log(\"SUCCESS: Neighbor found via Tag-Walker!\");\r\n        } else {\r\n            console.error(\"FAILURE: Neighbor NOT found.\");\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(\"Test execution failed:\", e);\r\n    }\r\n\r\n    // Cleanup\r\n    const ids = [idSovereign, idExternal, idNeighbor];\r\n    await db.run(`?[id] := *memory{id}, id in $ids :rm memory {id}`, { ids });\r\n    await db.close();\r\n}\r\n\r\nrun().catch(console.error);\r\n"
    tokens: 1577
    size: 4397
  - path: engine\tests\test_search_walker.ts
    content: "\r\nimport { executeSearch } from '../src/services/search/search.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testSearch() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    const query = \"atom test\";\r\n    console.log(`Running Search for: \"${query}\"`);\r\n\r\n    const results = await executeSearch(query);\r\n\r\n    console.log(\"\\n--- Search Results ---\");\r\n    console.log(`Context Length: ${results.context.length}`);\r\n    console.log(`Result Count: ${results.results.length}`);\r\n\r\n    results.results.slice(0, 5).forEach((r, i) => {\r\n        console.log(`\\n[${i + 1}] Score: ${r.score.toFixed(2)} | Provenance: ${r.provenance}`);\r\n        console.log(`Source: ${r.source}`);\r\n        console.log(`Snippet: ${r.content.substring(0, 100)}...`);\r\n    });\r\n}\r\n\r\ntestSearch().catch(console.error);\r\n"
    tokens: 305
    size: 829
  - path: engine\test_db_syntax.js
    content: "\r\nimport { CozoDb } from 'cozo-node';\r\nimport fs from 'fs';\r\n\r\nasync function test() {\r\n    if (fs.existsSync('./test.db')) {\r\n        fs.rmSync('./test.db', { recursive: true, force: true });\r\n    }\r\n    const db = new CozoDb('rocksdb', './test.db');\r\n\r\n    try {\r\n        await db.run(`\r\n            :create memory {\r\n                id: String\r\n                =>\r\n                embedding: <F32; 4>\r\n            }\r\n        `);\r\n\r\n        console.log(\"Attempt 1: FTS-like syntax ::hnsw create idx { config }\");\r\n        try {\r\n            await db.run(`\r\n                ::hnsw create idx_hnsw {\r\n                    fields: [embedding],\r\n                    dim: 4,\r\n                    m: 50,\r\n                    ef_construction: 200,\r\n                    dtype: 'f32'\r\n                }\r\n            `);\r\n            console.log(\"SUCCESS: Attempt 1\");\r\n            return;\r\n        } catch (e) {\r\n            console.log(\"FAILED Attempt 1:\", e.message);\r\n        }\r\n\r\n        console.log(\"Attempt 2: keys as strings?\");\r\n        try {\r\n            await db.run(`\r\n                ::hnsw create idx_hnsw {\r\n                    \"fields\": [\"embedding\"],\r\n                    \"dim\": 4,\r\n                    \"m\": 50,\r\n                    \"ef_construction\": 200,\r\n                    \"dtype\": \"f32\"\r\n                }\r\n            `);\r\n            console.log(\"SUCCESS: Attempt 2\");\r\n            return;\r\n        } catch (e) {\r\n            console.log(\"FAILED Attempt 2:\", e.message);\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(\"Setup failed:\", e);\r\n    }\r\n}\r\ntest();\r\n"
    tokens: 529
    size: 1583
  - path: engine\test_regex.js
    content: "const { CozoDb } = require('cozo-lib-node'); const db = new CozoDb(); (async () => { await db.run('?[] <- [[\\'foo\\']]:put t{a}'); try { await db.run('?[a] := *t{a}, regex(\\'f\\', a)'); console.log('regex works'); } catch(e) { console.log('regex failed', e.message); } try { await db.run('?[a] := *t{a}, regex_match(\\'f\\', a)'); console.log('regex_match works'); } catch(e) { console.log('regex_match failed', e.message); } })()\r\n"
    tokens: 169
    size: 428
  - path: engine\tsconfig.json
    content: |-
      {
        "compilerOptions": {
          "target": "ES2022",
          "module": "ESNext",
          "moduleResolution": "node",
          "esModuleInterop": true,
          "allowSyntheticDefaultImports": true,
          "strict": true,
          "skipLibCheck": true,
          "forceConsistentCasingInFileNames": true,
          "outDir": "./dist",
          "rootDir": "./src",
          "resolveJsonModule": true,
          "declaration": true,
          "declarationMap": true,
          "sourceMap": true,
          "removeComments": false,
          "noImplicitAny": true,
          "strictNullChecks": true,
          "strictFunctionTypes": true,
          "noImplicitThis": true,
          "noImplicitReturns": true,
          "alwaysStrict": true,
          "noUnusedLocals": true,
          "noUnusedParameters": true,
          "exactOptionalPropertyTypes": false,
          "noImplicitOverride": true,
          "noPropertyAccessFromIndexSignature": true
        },
        "include": [
          "src/**/*"
        ],
        "exclude": [
          "node_modules",
          "dist",
          "tests"
        ]
      }
    tokens: 287
    size: 911
  - path: engine\user_settings.json
    content: "{\r\n    \"llm\": {\r\n        \"model_dir\": \"../../models\",\r\n        \"chat_model\": \"gemma-3-4b-it-abliterated-v2.i1-Q4_K_S.gguf\",\r\n        \"task_model\": \"Qwen3-4B-Function-Calling-Pro.gguf\"\r\n    },\r\n    \"dreamer\": {\r\n        \"enabled\": true,\r\n        \"schedule\": \"0 3 * * *\"\r\n    }\r\n}"
    tokens: 96
    size: 278
  - path: frontend\.gitignore
    content: |
      # Logs
      logs
      *.log
      npm-debug.log*
      yarn-debug.log*
      yarn-error.log*
      pnpm-debug.log*
      lerna-debug.log*

      node_modules
      dist
      dist-ssr
      *.local

      # Editor directories and files
      .vscode/*
      !.vscode/extensions.json
      .idea
      .DS_Store
      *.suo
      *.ntvs*
      *.njsproj
      *.sln
      *.sw?
    tokens: 102
    size: 253
  - path: frontend\eslint.config.js
    content: |
      import js from '@eslint/js'
      import globals from 'globals'
      import reactHooks from 'eslint-plugin-react-hooks'
      import reactRefresh from 'eslint-plugin-react-refresh'
      import tseslint from 'typescript-eslint'
      import { defineConfig, globalIgnores } from 'eslint/config'

      export default defineConfig([
        globalIgnores(['dist']),
        {
          files: ['**/*.{ts,tsx}'],
          extends: [
            js.configs.recommended,
            tseslint.configs.recommended,
            reactHooks.configs.flat.recommended,
            reactRefresh.configs.vite,
          ],
          languageOptions: {
            ecmaVersion: 2020,
            globals: globals.browser,
          },
        },
      ])
    tokens: 216
    size: 616
  - path: frontend\index.html
    content: |
      <!doctype html>
      <html lang="en">
        <head>
          <meta charset="UTF-8" />
          <link rel="icon" type="image/svg+xml" href="/vite.svg" />
          <meta name="viewport" content="width=device-width, initial-scale=1.0" />
          <title>frontend</title>
        </head>
        <body>
          <div id="root"></div>
          <script type="module" src="/src/main.tsx"></script>
        </body>
      </html>
    tokens: 140
    size: 357
  - path: frontend\package.json
    content: |
      {
        "name": "frontend",
        "private": true,
        "version": "0.0.0",
        "type": "module",
        "scripts": {
          "dev": "vite",
          "build": "tsc -b && vite build",
          "lint": "eslint .",
          "preview": "vite preview"
        },
        "dependencies": {
          "react": "^19.2.0",
          "react-dom": "^19.2.0"
        },
        "devDependencies": {
          "@eslint/js": "^9.39.1",
          "@types/node": "^24.10.1",
          "@types/react": "^19.2.5",
          "@types/react-dom": "^19.2.3",
          "@vitejs/plugin-react": "^5.1.1",
          "eslint": "^9.39.1",
          "eslint-plugin-react-hooks": "^7.0.1",
          "eslint-plugin-react-refresh": "^0.4.24",
          "globals": "^16.5.0",
          "typescript": "~5.9.3",
          "typescript-eslint": "^8.46.4",
          "vite": "npm:rolldown-vite@7.2.5"
        },
        "overrides": {
          "vite": "npm:rolldown-vite@7.2.5"
        }
      }
    tokens: 304
    size: 786
  - path: frontend\README.md
    content: |
      # React + TypeScript + Vite

      This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

      Currently, two official plugins are available:

      - [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh
      - [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

      ## React Compiler

      The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).

      ## Expanding the ESLint configuration

      If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:

      ```js
      export default defineConfig([
        globalIgnores(['dist']),
        {
          files: ['**/*.{ts,tsx}'],
          extends: [
            // Other configs...

            // Remove tseslint.configs.recommended and replace with this
            tseslint.configs.recommendedTypeChecked,
            // Alternatively, use this for stricter rules
            tseslint.configs.strictTypeChecked,
            // Optionally, add this for stylistic rules
            tseslint.configs.stylisticTypeChecked,

            // Other configs...
          ],
          languageOptions: {
            parserOptions: {
              project: ['./tsconfig.node.json', './tsconfig.app.json'],
              tsconfigRootDir: import.meta.dirname,
            },
            // other options...
          },
        },
      ])
      ```

      You can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:

      ```js
      // eslint.config.js
      import reactX from 'eslint-plugin-react-x'
      import reactDom from 'eslint-plugin-react-dom'

      export default defineConfig([
        globalIgnores(['dist']),
        {
          files: ['**/*.{ts,tsx}'],
          extends: [
            // Other configs...
            // Enable lint rules for React
            reactX.configs['recommended-typescript'],
            // Enable lint rules for React DOM
            reactDom.configs.recommended,
          ],
          languageOptions: {
            parserOptions: {
              project: ['./tsconfig.node.json', './tsconfig.app.json'],
              tsconfigRootDir: import.meta.dirname,
            },
            // other options...
          },
        },
      ])
      ```
    tokens: 949
    size: 2555
  - path: frontend\src\App.css
    content: |
      #root {
        max-width: 1280px;
        margin: 0 auto;
        padding: 2rem;
        text-align: center;
      }

      .logo {
        height: 6em;
        padding: 1.5em;
        will-change: filter;
        transition: filter 300ms;
      }
      .logo:hover {
        filter: drop-shadow(0 0 2em #646cffaa);
      }
      .logo.react:hover {
        filter: drop-shadow(0 0 2em #61dafbaa);
      }

      @keyframes logo-spin {
        from {
          transform: rotate(0deg);
        }
        to {
          transform: rotate(360deg);
        }
      }

      @media (prefers-reduced-motion: no-preference) {
        a:nth-of-type(2) .logo {
          animation: logo-spin infinite 20s linear;
        }
      }

      .card {
        padding: 2em;
      }

      .read-the-docs {
        color: #888;
      }
    tokens: 232
    size: 606
  - path: frontend\src\App.tsx
    content: |

      import { useState, useEffect } from 'react';
      import './index.css';

      // Simple Router (Single File for now for speed)
      const Dashboard = () => (
        <div className="flex-col-center" style={{ height: '100%', justifyContent: 'center', alignItems: 'center', gap: '2rem' }}>
          <h1 style={{ fontSize: '3rem', background: 'linear-gradient(to right, #fff, #646cff)', WebkitBackgroundClip: 'text', WebkitTextFillColor: 'transparent' }}>
            Sovereign Context Engine
          </h1>
          <div style={{ display: 'flex', gap: '1rem' }}>
            <button className="btn-primary" onClick={() => window.location.hash = '#search'}>
              Search Memories
            </button>
            <button className="btn-primary" onClick={() => window.location.hash = '#chat'}>
              Launch Chat
            </button>
          </div>
        </div>
      );

      const SearchPage = () => {
        const [query, setQuery] = useState('');
        const [results, setResults] = useState<any[]>([]);
        const [context, setContext] = useState('');
        const [loading, setLoading] = useState(false);
        const [viewMode, setViewMode] = useState<'cards' | 'raw'>('cards');

        // Feature 8/9/10 State
        const [tokenBudget, setTokenBudget] = useState(2048);
        const [activeMode, setActiveMode] = useState(false);
        const [sovereignBias, setSovereignBias] = useState(true);
        const [metadata, setMetadata] = useState<any>(null); // { tokenCount, filledPercent, atomCount }

        // Feature 7 State
        const [backupStatus, setBackupStatus] = useState('');

        // Debounce Logic for Live Mode
        // Sync query to delay search
        useEffect(() => {
          if (!activeMode) return;
          const timer = setTimeout(() => {
            if (query.trim()) handleSearch();
          }, 500); // 500ms debounce
          return () => clearTimeout(timer);
        }, [query, activeMode, tokenBudget, sovereignBias]);

        const handleBackup = async () => {
          setBackupStatus('Backing up...');
          try {
            const res = await fetch('/v1/backup', { method: 'POST' });
            const data = await res.json();
            setBackupStatus(`Backup Saved: ${data.filename}`);
            setTimeout(() => setBackupStatus(''), 3000);
          } catch (e) {
            setBackupStatus('Backup Failed');
          }
        };

        const handleSearch = async () => {
          if (!query.trim()) return;
          setLoading(true);
          setResults([]);
          try {
            const res = await fetch('/v1/memory/search', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                query,
                // buckets: ['notebook'], // Removed to allow global search (inbox, journals, etc.)
                max_chars: tokenBudget * 4, // Approx chars
                token_budget: tokenBudget, // For backend slicer if supported
                provenance: sovereignBias ? 'sovereign' : 'all'
              })
            });

            const data = await res.json();

            if (data.results) {
              setResults(data.results);
              setContext(data.context || '');
              setMetadata(data.metadata); // Capture metadata
            } else {
              setResults([]);
              setContext('No results found.');
              setMetadata(null);
            }

          } catch (e) {
            console.error(e);
            setContext('Error searching memories.');
          } finally {
            setLoading(false);
          }
        };

        const copyContext = () => {
          navigator.clipboard.writeText(context);
        };

        return (
          <div className="glass-panel" style={{ margin: '2rem', padding: '2rem', height: 'calc(100% - 4rem)', display: 'flex', flexDirection: 'column', gap: '1rem' }}>
            <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
              <h2>Memory Search</h2>

              {/* Helper Controls */}
              <div style={{ display: 'flex', gap: '0.5rem', alignItems: 'center' }}>
                {/* Backup Button (Feature 7) */}
                <button className="btn-primary" onClick={handleBackup} style={{ fontSize: '0.8rem', padding: '0.4rem' }}>
                   {backupStatus || 'Backup'}
                </button>

                {/* Dream Button (Restored) */}
                <button
                  className="btn-primary"
                  style={{ background: 'rgba(100, 108, 255, 0.1)', border: '1px solid var(--accent-primary)', fontSize: '0.8rem', padding: '0.4rem' }}
                  onClick={async () => {
                    const btn = document.activeElement as HTMLButtonElement;
                    if (btn) btn.disabled = true;
                    try {
                      const res = await fetch('/v1/dream', { method: 'POST' });
                      const data = await res.json();
                      alert(`Dream Cycle Complete:\nAnalyzed: ${data.analyzed}\nUpdated: ${data.updated}`);
                    } catch (e) {
                      alert('Dream Failed');
                      console.error(e);
                    } finally {
                      if (btn) btn.disabled = false;
                    }
                  }}
                >
                   Dream
                </button>

                {/* View Mode */}
                <button className="btn-primary" style={{ background: 'transparent', border: '1px solid var(--border-subtle)', fontSize: '0.8rem', padding: '0.4rem' }} onClick={() => setViewMode(viewMode === 'cards' ? 'raw' : 'cards')}>
                  {viewMode === 'cards' ? 'Raw' : 'Cards'}
                </button>
              </div>
            </div>

            {/* RAG IDE Controls (Features 8 & 9 & 10) */}
            <div className="glass-panel" style={{ padding: '1rem', display: 'flex', flexDirection: 'column', gap: '0.5rem', background: 'var(--bg-secondary)' }}>
              <div style={{ display: 'flex', gap: '2rem', alignItems: 'center' }}>
                {/* Active Mode Toggle */}
                <label style={{ display: 'flex', gap: '0.5rem', alignItems: 'center', cursor: 'pointer' }}>
                  <input type="checkbox" checked={activeMode} onChange={(e) => setActiveMode(e.target.checked)} />
                  <span style={{ fontSize: '0.9rem', fontWeight: 'bold', color: activeMode ? 'var(--accent-primary)' : 'var(--text-dim)' }}>
                     Live Search
                  </span>
                </label>

                {/* Sovereign Bias Toggle */}
                <label style={{ display: 'flex', gap: '0.5rem', alignItems: 'center', cursor: 'pointer' }}>
                  <input type="checkbox" checked={sovereignBias} onChange={(e) => setSovereignBias(e.target.checked)} />
                  <span style={{ fontSize: '0.9rem', color: sovereignBias ? '#FFD700' : 'var(--text-dim)' }}>
                     Sovereign Bias
                  </span>
                </label>

                {/* Budget Slider */}
                <div style={{ flex: 1, display: 'flex', gap: '1rem', alignItems: 'center' }}>
                  <span style={{ fontSize: '0.8rem', whiteSpace: 'nowrap' }}>Budget: {tokenBudget} tokens</span>
                  <input
                    type="range"
                    min="512"
                    max="131072"
                    step="512"
                    value={tokenBudget}
                    onChange={(e) => setTokenBudget(parseInt(e.target.value))}
                    style={{ flex: 1 }}
                  />
                </div>
              </div>

              {/* Context Visualization Bar */}
              <div style={{ width: '100%', height: '8px', background: 'var(--bg-tertiary)', borderRadius: '4px', overflow: 'hidden', position: 'relative' }}>
                <div style={{
                  width: `${metadata?.filledPercent || 0}%`,
                  height: '100%',
                  background: 'linear-gradient(90deg, var(--accent-primary), #a855f7)',
                  transition: 'width 0.3s ease'
                }} />
              </div>
              {metadata && (
                <div style={{ display: 'flex', justifyContent: 'space-between', fontSize: '0.75rem', color: 'var(--text-dim)' }}>
                  <span>Used: {metadata.tokenCount || 0} tokens | {metadata.charCount || 0} chars ({(metadata.filledPercent || 0).toFixed(1)}%)</span>
                  <span>Atoms: {metadata.atomCount || 0}</span>
                </div>
              )}
            </div>

            {/* Query Section */}
            <div style={{ display: 'flex', gap: '0.5rem' }}>
              <input
                className="input-glass"
                placeholder="Ask your memories..."
                value={query}
                onChange={(e) => setQuery(e.target.value)}
                onKeyDown={(e) => { if (e.key === 'Enter') handleSearch(); }}
              />
              <button className="btn-primary" onClick={handleSearch} disabled={loading}>
                Search
              </button>
            </div>

            {/* Results Section */}
            <div style={{ flex: 1, overflowY: 'auto', display: 'flex', flexDirection: 'column', gap: '1rem', paddingRight: '0.5rem' }}>

              {viewMode === 'raw' && (
                <div style={{ position: 'relative', height: '100%' }}>
                  <button
                    className="btn-primary"
                    style={{ position: 'absolute', top: '1rem', right: '1rem', padding: '0.4rem 0.8rem', fontSize: '0.8rem', zIndex: 10 }}
                    onClick={copyContext}
                  >
                    Copy All
                  </button>
                  <textarea
                    className="input-glass"
                    style={{ width: '100%', height: '100%', resize: 'none', fontFamily: 'monospace', fontSize: '0.9rem', lineHeight: '1.5' }}
                    value={context}
                    readOnly
                    placeholder="Raw context will appear here..."
                  />
                </div>
              )}

              {viewMode === 'cards' && results.map((r, idx) => (
                <div key={r.id || idx} className="card-result animate-fade-in" style={{ animationDelay: `${idx * 0.05}s` }}>
                  <div style={{ display: 'flex', justifyContent: 'space-between', marginBottom: '0.5rem' }}>
                    <div style={{ display: 'flex', gap: '0.5rem', alignItems: 'center' }}>
                      <span className={`badge ${r.provenance === 'sovereign' ? 'badge-sovereign' : 'badge-external'}`}>
                        {r.provenance || 'EXTERNAL'}
                      </span>
                      <span style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>
                        {(r.score || 0).toFixed(2)}
                      </span>
                    </div>
                    <span style={{ fontSize: '0.8rem', color: 'var(--text-secondary)', fontStyle: 'italic' }}>
                      {r.source}
                    </span>
                  </div>
                  <div style={{ whiteSpace: 'pre-wrap', fontSize: '0.95rem', lineHeight: '1.5', maxHeight: '300px', overflowY: 'auto' }}>
                    {r.content}
                  </div>
                </div>
              ))}

              {results.length === 0 && !loading && (
                <div style={{ textAlign: 'center', padding: '2rem', color: 'var(--text-secondary)' }}>
                  No memories found. Try a different query.
                </div>
              )}
            </div>
          </div>
        );
      };

      const ChatPage = () => {
        const [messages, setMessages] = useState<{ role: 'user' | 'assistant'; content: string }[]>([
          { role: 'assistant', content: 'Welcome to the Sovereign Chat. How can I help you today?' }
        ]);
        const [input, setInput] = useState('');
        const [loading, setLoading] = useState(false);

        // Model Config State
        const [modelDir, setModelDir] = useState('../models');
        const [availableModels, setAvailableModels] = useState<string[]>([]);
        const [selectedModel, setSelectedModel] = useState('');
        const [currentModel, setCurrentModel] = useState('');
        const [modelLoading, setModelLoading] = useState(false);

        // Scan Models
        const scanModels = async () => {
          try {
            const res = await fetch(`/v1/models?dir=${encodeURIComponent(modelDir)}`);
            if (!res.ok) throw new Error('Failed to scan');
            const models = await res.json();
            setAvailableModels(models);
            if (models.length > 0 && !selectedModel) setSelectedModel(models[0]);
          } catch (e) {
            console.error(e);
            alert('Failed to scan directory');
          }
        };

        // Load Model
        const loadModel = async () => {
          if (!selectedModel) return;
          setModelLoading(true);
          try {
            // If custom directory, we must pass it OR pass full path?
            // API /v1/inference/load accepts direct 'dir'.
            const res = await fetch('/v1/inference/load', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: selectedModel,
                dir: modelDir
              })
            });
            const data = await res.json();
            if (res.ok) {
              setCurrentModel(selectedModel);
              alert(`Model Loaded: ${selectedModel}`);
            } else {
              throw new Error(data.error);
            }
          } catch (e: any) {
            console.error(e);
            alert(`Load Failed: ${e.message}`);
          } finally {
            setModelLoading(false);
          }
        };

        const sendMessage = async () => {
          if (!input.trim() || loading) return;

          const userMsg = input.trim();
          setInput('');
          setMessages(prev => [...prev, { role: 'user', content: userMsg }]);
          setLoading(true);

          // Initial empty assistant message
          setMessages(prev => [...prev, { role: 'assistant', content: '' }]);

          try {
            const res = await fetch('/v1/chat/completions', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                messages: [
                  { role: 'system', content: 'You are a helpful assistant serving the Sovereign Context Engine.' },
                  ...messages.map(m => ({ role: m.role, content: m.content })),
                  { role: 'user', content: userMsg }
                ],
                stream: true
              })
            });

            if (!res.body) throw new Error('No response body');

            const reader = res.body.getReader();
            const decoder = new TextDecoder();
            let assistantContent = '';

            while (true) {
              const { done, value } = await reader.read();
              if (done) break;

              const chunk = decoder.decode(value);
              const lines = chunk.split('\n');

              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const dataStr = line.replace('data: ', '').trim();
                  if (dataStr === '[DONE]') break;

                  try {
                    const data = JSON.parse(dataStr);
                    const delta = data.choices[0]?.delta?.content || '';
                    assistantContent += delta;

                    setMessages(prev => {
                      const newMsgs = [...prev];
                      const last = newMsgs[newMsgs.length - 1];
                      if (last.role === 'assistant') {
                        last.content = assistantContent;
                      }
                      return newMsgs;
                    });
                  } catch (e) {
                    console.error('Error parsing SSE:', e);
                  }
                }
              }
            }

          } catch (e) {
            console.error(e);
            setMessages(prev => [...prev, { role: 'assistant', content: 'Error: Could not connect to inference engine.' }]);
          } finally {
            setLoading(false);
          }
        };

        return (
          <div style={{ display: 'grid', gridTemplateColumns: '1fr 3fr', height: '100%' }}>
            {/* Sidebar */}
            <div style={{ padding: '1rem', borderRight: '1px solid var(--border-subtle)', background: 'var(--bg-secondary)', display: 'flex', flexDirection: 'column', gap: '1rem', overflowY: 'auto' }}>

              {/* Model Config Panel */}
              <div>
                <h3>Model Config</h3>
                <div className="glass-panel" style={{ padding: '1rem', display: 'flex', flexDirection: 'column', gap: '0.8rem' }}>
                  <div>
                    <label style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Directory</label>
                    <div style={{ display: 'flex', gap: '0.5rem' }}>
                      <input className="input-glass" style={{ fontSize: '0.8rem', padding: '0.4rem' }} value={modelDir} onChange={(e) => setModelDir(e.target.value)} />
                      <button className="btn-primary" style={{ padding: '0.4rem' }} onClick={scanModels}>Scan</button>
                    </div>
                  </div>

                  {availableModels.length > 0 && (
                    <div>
                      <label style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Select Model</label>
                      <select
                        className="input-glass"
                        style={{ fontSize: '0.8rem', padding: '0.4rem' }}
                        value={selectedModel}
                        onChange={(e) => setSelectedModel(e.target.value)}
                      >
                        {availableModels.map(m => <option key={m} value={m}>{m}</option>)}
                      </select>
                      <button
                        className="btn-primary"
                        style={{ width: '100%', marginTop: '0.5rem', background: currentModel === selectedModel ? 'var(--bg-tertiary)' : 'var(--accent-primary)' }}
                        onClick={loadModel}
                        disabled={modelLoading}
                      >
                        {modelLoading ? 'Loading...' : currentModel === selectedModel ? 'Active' : 'Load Model'}
                      </button>
                    </div>
                  )}
                </div>
              </div>

              {/* Context Panel */}
              <div style={{ flex: 1 }}>
                <h3>Context</h3>
                <div className="glass-panel" style={{ padding: '1rem', height: '100%', display: 'flex', flexDirection: 'column', gap: '0.5rem', minHeight: '150px' }}>
                  <span style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Tokens: 0 / 4096</span>
                  <textarea className="input-glass" style={{ flex: 1, resize: 'none', fontSize: '0.8rem' }} placeholder="Paste context here..." />
                </div>
              </div>
            </div>

            {/* Chat Area */}
            <div style={{ display: 'flex', flexDirection: 'column', height: '100%' }}>
              <div style={{ flex: 1, padding: '2rem', overflowY: 'auto', display: 'flex', flexDirection: 'column', gap: '1rem' }}>
                {messages.map((m, idx) => (
                  <div key={idx} className={`glass-panel animate-fade-in`} style={{
                    padding: '1rem',
                    maxWidth: '80%',
                    alignSelf: m.role === 'user' ? 'flex-end' : 'flex-start',
                    background: m.role === 'user' ? 'rgba(100, 108, 255, 0.1)' : 'var(--glass-bg)',
                    whiteSpace: 'pre-wrap'
                  }}>
                    {m.content}
                  </div>
                ))}
                {loading && <div style={{ alignSelf: 'flex-start', color: 'var(--text-dim)', fontSize: '0.8rem', marginLeft: '1rem' }}>Thinking...</div>}
              </div>
              <div style={{ padding: '1rem', borderTop: '1px solid var(--border-subtle)' }}>
                <div style={{ display: 'flex', gap: '1rem' }}>
                  <textarea
                    className="input-glass"
                    rows={2}
                    placeholder="Type a message..."
                    style={{ resize: 'none' }}
                    value={input}
                    onChange={(e) => setInput(e.target.value)}
                    onKeyDown={(e) => { if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); sendMessage(); } }}
                  />
                  <button className="btn-primary" style={{ height: 'auto' }} onClick={sendMessage} disabled={loading}>Send</button>
                </div>
              </div>
            </div>
          </div>
        );
      };

      function App() {
        const [route, setRoute] = useState(window.location.hash || '#');

        // Simple hash router listener
        window.addEventListener('hashchange', () => setRoute(window.location.hash));

        return (
          <>
            <nav style={{ padding: '1rem', borderBottom: '1px solid var(--border-subtle)', display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
              <span style={{ fontWeight: 'bold', cursor: 'pointer' }} onClick={() => window.location.hash = '#'}>SCE</span>
              <div style={{ display: 'flex', gap: '1rem', fontSize: '0.9rem' }}>
                <a onClick={() => window.location.hash = '#search'} style={{ cursor: 'pointer', color: route === '#search' ? 'white' : 'gray' }}>Search</a>
                <a onClick={() => window.location.hash = '#chat'} style={{ cursor: 'pointer', color: route === '#chat' ? 'white' : 'gray' }}>Chat</a>
              </div>
            </nav>
            <main style={{ flex: 1, overflow: 'hidden' }}>
              {route === '#' || route === '' ? <Dashboard /> : null}
              {route === '#search' ? <SearchPage /> : null}
              {route === '#chat' ? <ChatPage /> : null}
            </main>
          </>
        );
      }

      export default App;
    tokens: 6869
    size: 19911
  - path: frontend\src\index.css
    content: |-
      :root {
        /* Premium Dark Theme Palette */
        --bg-primary: #0a0a0c;
        --bg-secondary: #121214;
        --bg-tertiary: #1c1c1f;

        --accent-primary: #646cff;
        --accent-hover: #7b83ff;
        --accent-glow: rgba(100, 108, 255, 0.4);

        --text-primary: #ffffff;
        --text-secondary: #a1a1aa;
        --text-dim: #52525b;

        --border-subtle: #27272a;
        --border-active: #3f3f46;

        --glass-bg: rgba(28, 28, 31, 0.7);
        --glass-border: rgba(255, 255, 255, 0.1);
        --glass-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37);

        --font-sans: 'Inter', system-ui, Avenir, Helvetica, Arial, sans-serif;
        --radius-sm: 4px;
        --radius-md: 8px;
        --radius-lg: 16px;
        --radius-full: 9999px;

        --transition-fast: 0.15s ease;
        --transition-normal: 0.3s ease;
      }

      body {
        margin: 0;
        background-color: var(--bg-primary);
        color: var(--text-primary);
        font-family: var(--font-sans);
        -webkit-font-smoothing: antialiased;
        min-height: 100vh;
      }

      #root {
        display: flex;
        flex-direction: column;
        height: 100vh;
      }

      /* Utilities */
      .glass-panel {
        background: var(--glass-bg);
        backdrop-filter: blur(12px);
        -webkit-backdrop-filter: blur(12px);
        border: 1px solid var(--glass-border);
        box-shadow: var(--glass-shadow);
        border-radius: var(--radius-lg);
      }

      .btn-primary {
        background: var(--accent-primary);
        color: white;
        border: none;
        padding: 0.6rem 1.2rem;
        border-radius: var(--radius-md);
        font-weight: 500;
        cursor: pointer;
        transition: all var(--transition-fast);
      }

      .btn-primary:hover {
        background: var(--accent-hover);
        box-shadow: 0 0 15px var(--accent-glow);
        transform: translateY(-1px);
      }

      .input-glass {
        background: rgba(0, 0, 0, 0.2);
        border: 1px solid var(--border-subtle);
        color: var(--text-primary);
        padding: 0.8rem 1rem;
        border-radius: var(--radius-md);
        outline: none;
        transition: border-color var(--transition-fast);
        width: 100%;
        font-size: 1rem;
      }

      .input-glass:focus {
        border-color: var(--accent-primary);
      }

      /* Animations */
      @keyframes fadeIn {
        from {
          opacity: 0;
          transform: translateY(10px);
        }

        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .animate-fade-in {
        animation: fadeIn var(--transition-normal) forwards;
      }

      .animate-fade-in {
        animation: fadeIn var(--transition-normal) forwards;
      }

      /* Scrollbar */
      ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
      }

      ::-webkit-scrollbar-track {
        background: rgba(0, 0, 0, 0.1);
      }

      ::-webkit-scrollbar-thumb {
        background: var(--border-active);
        border-radius: var(--radius-full);
      }

      ::-webkit-scrollbar-thumb:hover {
        background: var(--text-dim);
      }

      /* Components */
      .card-result {
        background: rgba(255, 255, 255, 0.03);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        padding: 1rem;
        transition: all var(--transition-fast);
      }

      .card-result:hover {
        background: rgba(255, 255, 255, 0.05);
        border-color: var(--accent-glow);
      }

      .badge {
        display: inline-block;
        padding: 0.2rem 0.5rem;
        border-radius: var(--radius-sm);
        font-size: 0.7rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.05em;
      }

      .badge-sovereign {
        background: rgba(100, 108, 255, 0.2);
        color: #8b92ff;
        border: 1px solid rgba(100, 108, 255, 0.3);
      }

      .badge-external {
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .code-block {
        background: #000;
        padding: 1rem;
        border-radius: var(--radius-md);
        font-family: monospace;
        font-size: 0.9rem;
        overflow-x: auto;
        border: 1px solid var(--border-subtle);
      }
    tokens: 1335
    size: 3478
  - path: frontend\src\main.tsx
    content: |
      import { StrictMode } from 'react'
      import { createRoot } from 'react-dom/client'
      import './index.css'
      import App from './App.tsx'

      createRoot(document.getElementById('root')!).render(
        <StrictMode>
          <App />
        </StrictMode>,
      )
    tokens: 84
    size: 230
  - path: frontend\tsconfig.app.json
    content: |
      {
        "compilerOptions": {
          "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
          "target": "ES2022",
          "useDefineForClassFields": true,
          "lib": ["ES2022", "DOM", "DOM.Iterable"],
          "module": "ESNext",
          "types": ["vite/client"],
          "skipLibCheck": true,

          /* Bundler mode */
          "moduleResolution": "bundler",
          "allowImportingTsExtensions": true,
          "verbatimModuleSyntax": true,
          "moduleDetection": "force",
          "noEmit": true,
          "jsx": "react-jsx",

          /* Linting */
          "strict": true,
          "noUnusedLocals": true,
          "noUnusedParameters": true,
          "erasableSyntaxOnly": true,
          "noFallthroughCasesInSwitch": true,
          "noUncheckedSideEffectImports": true
        },
        "include": ["src"]
      }
    tokens: 236
    size: 732
  - path: frontend\tsconfig.json
    content: |
      {
        "files": [],
        "references": [
          { "path": "./tsconfig.app.json" },
          { "path": "./tsconfig.node.json" }
        ]
      }
    tokens: 40
    size: 119
  - path: frontend\tsconfig.node.json
    content: |
      {
        "compilerOptions": {
          "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
          "target": "ES2023",
          "lib": ["ES2023"],
          "module": "ESNext",
          "types": ["node"],
          "skipLibCheck": true,

          /* Bundler mode */
          "moduleResolution": "bundler",
          "allowImportingTsExtensions": true,
          "verbatimModuleSyntax": true,
          "moduleDetection": "force",
          "noEmit": true,

          /* Linting */
          "strict": true,
          "noUnusedLocals": true,
          "noUnusedParameters": true,
          "erasableSyntaxOnly": true,
          "noFallthroughCasesInSwitch": true,
          "noUncheckedSideEffectImports": true
        },
        "include": ["vite.config.ts"]
      }
    tokens: 210
    size: 653
  - path: frontend\vite.config.ts
    content: |
      import { defineConfig } from 'vite'
      import react from '@vitejs/plugin-react'

      // https://vite.dev/config/
      export default defineConfig({
        plugins: [react()],
      })
    tokens: 60
    size: 161
  - path: LICENSE
    content: |-
      MIT License

      Copyright (c) 2026 External Context Engine

      Permission is hereby granted, free of charge, to any person obtaining a copy
      of this software and associated documentation files (the "Software"), to deal
      in the Software without restriction, including without limitation the rights
      to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
      copies of the Software, and to permit persons to whom the Software is
      furnished to do so, subject to the following conditions:

      The above copyright notice and this permission notice shall be included in all
      copies or substantial portions of the Software.

      THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
      IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
      FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
      AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
      LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
      OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
      SOFTWARE.
    tokens: 441
    size: 1079
  - path: package.json
    content: |-
      {
        "name": "@ece/core",
        "version": "1.0.0",
        "description": "External Context Engine Core Components",
        "main": "index.js",
        "type": "module",
        "scripts": {
          "start": "node engine/dist/index.js",
          "dev": "pnpm --filter sovereign-context-engine dev",
          "build": "pnpm --filter frontend build && pnpm --filter sovereign-context-engine build",
          "test": "jest",
          "lint": "eslint . --ext .ts,.js",
          "clean": "rimraf dist engine/dist frontend/dist"
        },
        "keywords": [
          "context",
          "ai",
          "memory",
          "external-context-engine",
          "sovereign"
        ],
        "author": "External Context Engine Team",
        "license": "MIT",
        "dependencies": {
          "@types/express": "^4.17.21",
          "@types/node": "^20.9.0",
          "axios": "^1.6.0",
          "body-parser": "^1.20.2",
          "cors": "^2.8.5",
          "dotenv": "^16.3.1",
          "express": "^4.18.2",
          "typescript": "^5.0.0",
          "ws": "^8.14.2"
        },
        "devDependencies": {
          "@types/jest": "^29.5.6",
          "eslint": "^8.53.0",
          "jest": "^29.7.0",
          "js-yaml": "^4.1.1",
          "nodemon": "^3.0.1",
          "rimraf": "^5.0.5",
          "ts-node": "^10.9.1"
        },
        "engines": {
          "node": ">=18.0.0"
        },
        "repository": {
          "type": "git",
          "url": "https://github.com/External-Context-Engine/ECE_Core.git"
        },
        "bugs": {
          "url": "https://github.com/External-Context-Engine/ECE_Core/issues"
        },
        "homepage": "https://github.com/External-Context-Engine/ECE_Core#readme"
      }
    tokens: 538
    size: 1430
  - path: plugins\whisper-recorder\package.json
    content: "{\r\n    \"name\": \"whisper-audio-recorder\",\r\n    \"version\": \"1.0.0\",\r\n    \"description\": \"Standalone audio recorder and transcriber using node-llama-cpp\",\r\n    \"main\": \"dist/index.js\",\r\n    \"type\": \"module\",\r\n    \"scripts\": {\r\n        \"build\": \"tsc\",\r\n        \"start\": \"node dist/index.js\",\r\n        \"record\": \"node dist/record.js\"\r\n    },\r\n    \"dependencies\": {\r\n        \"@mlc-ai/web-llm\": \"^0.2.1\",\r\n        \"@xenova/transformers\": \"^2.15.0\",\r\n        \"onnxruntime-node\": \"^1.17.0\",\r\n        \"node-audiorecorder\": \"^3.0.0\",\r\n        \"wav\": \"^1.0.2\",\r\n        \"dotenv\": \"^16.3.1\",\r\n        \"chalk\": \"^5.3.0\",\r\n        \"ws\": \"^8.16.0\"\r\n    },\r\n    \"devDependencies\": {\r\n        \"typescript\": \"^5.3.3\",\r\n        \"@types/node\": \"^20.10.0\",\r\n        \"@types/wav\": \"^1.0.4\"\r\n    }\r\n}"
    tokens: 285
    size: 776
  - path: plugins\whisper-recorder\src\index.ts
    content: "import { spawn } from 'child_process';\r\nimport path from 'path';\r\nimport fs from 'fs';\r\nimport { fileURLToPath } from 'url';\r\nimport { Transcriber } from './transcriber.js';\r\nimport readline from 'readline';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\nconst rl = readline.createInterface({\r\n    input: process.stdin,\r\n    output: process.stdout\r\n});\r\n\r\nconst RECORDING_SCRIPT = path.join(__dirname, 'recorder.js');\r\n\r\nasync function main() {\r\n    console.log('=== Whisper Audio Recorder ===');\r\n    console.log('1. Press ENTER to START recording.');\r\n\r\n    await new Promise<void>(resolve => rl.question('', () => resolve()));\r\n\r\n    console.log('Starting Recorder...');\r\n    const child = spawn('node', [RECORDING_SCRIPT], {\r\n        stdio: ['ignore', 'pipe', 'inherit'] // Pipe stdout to capture filename\r\n    });\r\n\r\n    let capturedFile = '';\r\n\r\n    child.stdout.on('data', (data) => {\r\n        const line = data.toString();\r\n        console.log(`[Recorder] ${line.trim()}`);\r\n        // Recorder script prints \"Saved: <path>\" on exit\r\n        const match = line.match(/Saved: (.+\\.wav)/);\r\n        if (match) {\r\n            capturedFile = match[1];\r\n        }\r\n    });\r\n\r\n    console.log('Recording in progress... Press ENTER to STOP.');\r\n\r\n    await new Promise<void>(resolve => rl.question('', () => resolve()));\r\n\r\n    console.log('Stopping Recorder...');\r\n    child.kill('SIGINT');\r\n\r\n    // Wait for child to exit\r\n    await new Promise<void>(resolve => child.on('exit', () => resolve()));\r\n\r\n    if (capturedFile && fs.existsSync(capturedFile.trim())) {\r\n        console.log(`\\nAnalyzing Audio: ${capturedFile}`);\r\n        // Transformers.js downloads model automatically\r\n        const transcriber = new Transcriber('Xenova/whisper-tiny.en');\r\n        try {\r\n            console.log('Running Whisper (WASM/ONNX)...');\r\n            const text = await transcriber.transcribe(capturedFile.trim());\r\n            console.log('\\n--- Transcript ---');\r\n            console.log(text);\r\n            console.log('------------------\\n');\r\n        } catch (e) {\r\n            console.error('Transcription failed:', e);\r\n        }\r\n    } else {\r\n        console.error('No capture file found.');\r\n    }\r\n\r\n    rl.close();\r\n}\r\n\r\nmain();\r\n"
    tokens: 805
    size: 2291
  - path: plugins\whisper-recorder\src\InferenceKernel.ts
    content: "import { CreateMLCEngine, MLCEngine } from \"@mlc-ai/web-llm\";\r\n\r\n/**\r\n * InferenceKernel (WebGPU/WASM Edition)\r\n * Uses @mlc-ai/web-llm (MLC) to run LLM inference.\r\n * \r\n * Note: Running this in Node.js requires a WebGPU implementation.\r\n * In a standard Node environment without a browser, this might fallback or fail \r\n * unless 'tvmjs' / 'navigator.gpu' polyfills are active.\r\n * \r\n * If running in Electron (Renderer), this works natively.\r\n * If running in pure Node, it assumes environment compatibility.\r\n */\r\nexport class InferenceKernel {\r\n    private engine: MLCEngine | null = null;\r\n\r\n    constructor(private modelId: string = \"Llama-3.1-8B-Instruct-q4f32_1-MLC\") { }\r\n\r\n    async init() {\r\n        console.log(`[Kernel] Initializing WebLLM for: ${this.modelId}`);\r\n        try {\r\n            // CreateEngine automatically selects the best available backend (WebGPU if available, or WASM fallback)\r\n            this.engine = await CreateMLCEngine(this.modelId, {\r\n                initProgressCallback: (report) => {\r\n                    console.log(`[Kernel] Loading: ${report.text}`);\r\n                }\r\n            });\r\n            console.log(`[Kernel] WebLLM Engine Ready.`);\r\n        } catch (e) {\r\n            console.error(`[Kernel] Initialization Failed (WebGPU missing?):`, e);\r\n            throw e;\r\n        }\r\n    }\r\n\r\n    async chat(message: string): Promise<string> {\r\n        if (!this.engine) throw new Error(\"Kernel not initialized\");\r\n\r\n        const messages = [\r\n            { role: \"system\", content: \"You are a helpful assistant.\" },\r\n            { role: \"user\", content: message }\r\n        ];\r\n\r\n        const reply = await this.engine.chat.completions.create({\r\n            messages: messages as any\r\n        });\r\n\r\n        return reply.choices[0].message.content || \"\";\r\n    }\r\n\r\n    /**\r\n     * Transcribe via Kernel?\r\n     * Current Architecture separates Transcriber (Whisper/Transformers.js) from Kernel (LLM/WebLLM).\r\n     * This method delegates or throws.\r\n     */\r\n    async transcribe(audioPath: string): Promise<string> {\r\n        throw new Error(\"Transcribe is handled by the sibling Transcriber class (Transformers.js).\");\r\n    }\r\n}\r\n"
    tokens: 775
    size: 2183
  - path: plugins\whisper-recorder\src\recorder.ts
    content: "import AudioRecorder from 'node-audiorecorder';\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n// Constants\r\nconst OUTPUT_DIR = path.join(__dirname, '../../recordings'); // plugins/whisper-recorder/recordings\r\n\r\nif (!fs.existsSync(OUTPUT_DIR)) {\r\n    fs.mkdirSync(OUTPUT_DIR, { recursive: true });\r\n}\r\n\r\n// Configuration for 16-bit PCM, 16kHz, Mono (Whisper Standard)\r\nconst options = {\r\n    program: 'sox',     // Server-side recording usually works best with SoX\r\n    silence: 0,\r\n    thresholdStart: 0.5,\r\n    thresholdStop: 0.5,\r\n    keepSilence: true,\r\n    device: null,       // Default device\r\n    bits: 16,\r\n    channels: 1,\r\n    encoding: 'signed-integer',\r\n    rate: 16000,\r\n    type: 'wav',\r\n};\r\n\r\n// Initialize\r\nconst audioRecorder = new AudioRecorder(options, console);\r\n\r\nconsole.log('Recording... Press Ctrl+C to stop.');\r\n\r\n// Create file stream\r\nconst timestamp = new Date().toISOString().replace(/[:.]/g, '-');\r\nconst filename = `recording_${timestamp}.wav`;\r\nconst filePath = path.join(OUTPUT_DIR, filename);\r\nconst fileStream = fs.createWriteStream(filePath, { encoding: 'binary' });\r\n\r\n// Start recording\r\naudioRecorder.start().stream().pipe(fileStream);\r\n\r\n// Handle exit\r\nprocess.on('SIGINT', () => {\r\n    console.log('Stopping recording...');\r\n    audioRecorder.stop();\r\n    console.log(`Saved: ${filePath}`);\r\n    process.exit();\r\n});\r\n"
    tokens: 537
    size: 1502
  - path: plugins\whisper-recorder\src\transcriber.ts
    content: "import { pipeline, env } from '@xenova/transformers';\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport wav from 'wav'; // Used to read WAV headers if needed, but transformers handles paths\r\n\r\n// Configure cache to avoid re-downloading to user home\r\nenv.localModelPath = path.join(process.cwd(), 'models');\r\nenv.allowRemoteModels = true;\r\n\r\n/**\r\n * Transcriber (WASM/ONNX Edition)\r\n * Uses @xenova/transformers to run Whisper.\r\n */\r\nexport class Transcriber {\r\n    private p: any = null;\r\n\r\n    constructor(private modelName: string = 'Xenova/whisper-tiny.en') { }\r\n\r\n    async init() {\r\n        console.log(`[Transcriber] Loading Model: ${this.modelName}...`);\r\n        // Define task and model\r\n        this.p = await pipeline('automatic-speech-recognition', this.modelName, {\r\n            quantized: true // Use INT8 quantized model for speed\r\n        });\r\n        console.log(`[Transcriber] Model Loaded.`);\r\n    }\r\n\r\n    async transcribe(wavPath: string): Promise<string> {\r\n        if (!this.p) await this.init();\r\n\r\n        console.log(`[Transcriber] Processing: ${wavPath}`);\r\n\r\n        if (!fs.existsSync(wavPath)) {\r\n            throw new Error(`File not found: ${wavPath}`);\r\n        }\r\n\r\n        try {\r\n            // @xenova/transformers accepts file paths directly in Node.js\r\n            // It uses 'wavefile' internally to parse.\r\n            const result = await this.p(wavPath, {\r\n                chunk_length_s: 30,\r\n                stride_length_s: 5,\r\n                language: 'english',\r\n                task: 'transcribe',\r\n                return_timestamps: true\r\n            });\r\n\r\n            // Result is { text: \"...\", chunks: [...] }\r\n            const text = result.text.trim();\r\n\r\n            // Save transcript\r\n            const txtPath = wavPath.replace('.wav', '.txt');\r\n            fs.writeFileSync(txtPath, text);\r\n            console.log(`[Transcriber] Saved: ${txtPath}`);\r\n\r\n            return text;\r\n\r\n        } catch (e) {\r\n            console.error(`[Transcriber] Error:`, e);\r\n            throw e;\r\n        }\r\n    }\r\n}\r\n"
    tokens: 723
    size: 2071
  - path: plugins\whisper-recorder\tsconfig.json
    content: "{\r\n    \"compilerOptions\": {\r\n        \"target\": \"ES2022\",\r\n        \"module\": \"NodeNext\",\r\n        \"moduleResolution\": \"NodeNext\",\r\n        \"outDir\": \"./dist\",\r\n        \"rootDir\": \"./src\",\r\n        \"strict\": true,\r\n        \"esModuleInterop\": true,\r\n        \"skipLibCheck\": true,\r\n        \"forceConsistentCasingInFileNames\": true\r\n    },\r\n    \"include\": [\r\n        \"src/**/*\"\r\n    ],\r\n    \"exclude\": [\r\n        \"node_modules\"\r\n    ]\r\n}"
    tokens: 131
    size: 432
  - path: pnpm-workspace.yaml
    content: "packages:\r\n  - 'engine'\r\n  - 'shared'\r\n  - 'frontend'\r\n\r\n"
    tokens: 19
    size: 57
  - path: QUICKSTART.md
    content: |-
      # Quick Start Guide

      ## Prerequisites

      - Node.js >= 18.0.0
      - pnpm package manager
      - Git
      - Available port 3000 (default)

      ## Installation

      1. Clone the repository:
      ```bash
      git clone https://github.com/External-Context-Engine/ECE_Core.git
      cd ECE_Core
      ```

      2. Install dependencies:
      ```bash
      pnpm install
      ```

      3. Set up configuration:
      ```bash
      # Copy and edit the configuration file
      cp sovereign.yaml.example sovereign.yaml
      # Edit sovereign.yaml with your configuration
      ```

      4. Ensure you have models in the `models/` directory (GGUF format)

      5. Start the engine:
      ```bash
      pnpm start
      ```

      ## Basic Usage

      Once started, the engine will be available at `http://localhost:3000`.

      ### API Endpoints
      - Health check: `GET /health`
      - Chat completions: `POST /v1/chat/completions`
      - Memory search: `POST /v1/memory/search`
      - Ingest content: `POST /v1/ingest`
      - List buckets: `GET /v1/buckets`
      - Backup database: `GET /v1/backup`

      ### File-Based Context
      The system automatically watches the `context/` directory for new files and ingests them. Supported formats include:
      - `.txt`, `.md`, `.json`, `.yaml`, `.yml`
      - `.js`, `.ts`, `.py`, `.html`, `.css`
      - And many other text-based formats

      ## Configuration

      The system is configured via `sovereign.yaml` which includes:
      - Model paths and settings
      - Network configuration (ports)
      - Memory and storage settings
      - Dreamer service intervals

      ## Development

      For development mode:
      ```bash
      npm run dev
      ```

      ## Building

      To build the executable:
      ```bash
      npm run build
      ```

      ## Services

      The engine includes several background services:
      - **Dreamer**: Self-organizing memory categorization (runs automatically)
      - **Mirror Protocol**: Creates physical copies of the AI brain
      - **File Watcher**: Monitors `context/` directory for changes
      - **Scribe**: Manages session state for conversation coherence
    tokens: 697
    size: 1822
  - path: README.md
    content: |-
      # ECE_Core - Sovereign Context Engine

      > **Executive Cognitive Enhancement (ECE)** - Personal external memory system as an assistive cognitive tool.

      **Status**: Active development | **Architecture**: UniversalRAG (Node.js + WebGPU + RocksDB)

      ---

      ##  Overview

      The ECE_Core is a modern **UniversalRAG** engine that transforms your local file system into a queriable, sovereign AI memory. It runs locally, ensuring 100% privacy, and uses a **Dual-Worker** architecture to handle chat and ingestion simultaneously without lag.

      ### Key Features
      - **Sovereign Provenance**: Your files are "Tier 1" knowledge. The system boosts them 2x over generic data.
      - **Dual-Worker System**: Dedicated workers for **Chat** (e.g., Qwen) and **Embeddings** (e.g., Gemma).
      - **Universal Ingestion**: Just drop files into the `Inbox` or `Notebook`. Text, code, and markdown are chemically "atomized" into vector memories.
      - **Thinking Context**: Uses a "Rolling Context" window that prioritizes relevant facts + recent history.
      - **Desktop Overlay**: A thin, transparent "Heads Up Display" for instant access to your specialized AI.

      ---

      ##  Architecture

      ### 1. Ingestion Pipeline ("The Refiner")
      - **Atomization**: Splits content into semantic "Atoms" (thoughts) rather than arbitrary chunks.
      - **Sanitization**: Strips null bytes, corrects encoding, and handles standard file types.
      - **Embedding**: Uses a dedicated 300M+ parameter model (separate from Chat) to vectorize atoms.
      - **Storage**: Persists to **CozoDB** (RocksDB backend) + **HNSW** Vector Index.

      ### 2. Cognitive Services
      - **ChatWorker**: Specialized worker for high-speed inference (supports streaming).
      - **EmbeddingWorker**: Dedicated worker for vector generation.
      - **ContextManager**: Middle-out context composer with:
          - **Dynamic Recency**: Adapts sort order based on temporal queries ("latest logs" vs "history").
          - **Safety Buffer**: Targets 3800 tokens to prevent overflow.
          - **Smart Slicing**: Truncates content at punctuation boundaries.

      ### 3. Application Layer
      - **API**: RESTful interface at `http://localhost:3000/v1/`.
      - **Frontend**: Modern React + Vite dashbaord.
      - **Desktop Overlay**: Lightweight Electron shell for "Always-on-Top" assistance.

      ---

      ##  Quick Start

      ### Prerequisites
      - Node.js >= 18.0.0
      - pnpm package manager (`npm i -g pnpm`)
      - Git

      ### 1. Installation
      ```bash
      git clone https://github.com/External-Context-Engine/ECE_Core.git
      cd ECE_Core
      pnpm install
      ```

      ### 2. Configuration (`.env`)
      The system uses a single `.env` file. A sample is provided.
      ```bash
      # Core
      PORT=3000
      API_KEY=ece-secret-key

      # Models (Absolute Paths or specific filenames in 'engine/models')
      LLM_MODEL_PATH=Qwen3-4B-Instruct.gguf
      LLM_EMBEDDING_MODEL_PATH=embeddinggemma-300m.gguf

      # Hardware
      LLM_GPU_LAYERS=33
      LLM_CTX_SIZE=4096
      LLM_EMBEDDING_CTX_SIZE=8192

      # Vision (Required for image processing)
      VISION_MODEL_PATH=C:/path/to/Qwen2-VL-2B-Instruct.gguf
      VISION_PROJECTOR_PATH=C:/path/to/mmproj-Qwen2-VL.gguf
      ```

      ### 3. Run Engine
      ```bash
      pnpm start
      ```
      *   Server: `http://localhost:3000`
      *   Health: `http://localhost:3000/health`

      ### 4. Run Desktop Overlay (Optional)
      ```bash
      cd desktop-overlay
      pnpm install
      pnpm start
      ```

      ---

      ##  Project Structure

      - **engine/**: The core logic (Node.js, Express, Llama.cpp).
          - `src/core/inference/`: Chat & Embedding Workers.
          - `src/services/ingest/`: Refiner & Atomizer.
          - `src/services/search/`: Vector Search & Routing.
      - **frontend/**: React + Vite web dashboard.
      - **desktop-overlay/**: Electron "Thin Client".
      - **archive/**: Deprecated code.

      ---

      ##  Development

      ### Build
      ```bash
      # Builds Engine, Frontend, and Types
      npm run build
      ```

      ### Test
      ```bash
      npm test
      ```

      ---

      ##  Documentation Standards

      - **`specs/doc_policy.md`**: Documentation standards.
      - **`specs/spec.md`**: Technical specification.
      - **`specs/plan.md`**: Roadmap.
      - **`specs/tasks.md`**: Current task list.

      ---

      ##  Utility Tools

      ### Codebase Scraper (`read_all.js`)
      Use this tool to consolidate an entire project into a digestable format for the engine.
      ```bash
      node read_all.js <path_to_project_root>
      ```
      **Output:** `combined_memory.yaml`
      **Usage:** Drop the resulting file into your `notebook/inbox` folder to ingest the entire codebase as a single knowledge source.

      ---

      ## Acknowledgments
      **"Your data, sovereign. Your tools, open. Your mind, augmented."**
    tokens: 1641
    size: 4411
  - path: read_all.js
    content: "/**\r\n * Universal Context Aggregation Tool\r\n *\r\n * This script recursively scans all text files in a project root,\r\n * aggregates their content into a single YAML file with configurable limits.\r\n * Designed to work in any codebase from the root directory.\r\n */\r\n\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\nimport yaml from 'js-yaml';\r\n\r\n// Configuration options\r\nconst CONFIG = {\r\n    // Token limits\r\n    tokenLimit: 1000000, // 1M tokens - increased for full codebase analysis\r\n    maxFileSize: 5 * 1024 * 1024, // 5MB max per file to prevent huge files\r\n    maxLinesPerFile: 5000, // Max 5000 lines per file to prevent huge content\r\n\r\n    // Output options\r\n    outputDir: 'codebase',\r\n    outputFile: 'combined_context.yaml',\r\n\r\n    // File inclusion/exclusion patterns\r\n    includeExtensions: [\r\n        // Code files\r\n        '.js', '.ts', '.jsx', '.tsx', '.py', '.java', '.cpp', '.c', '.h', '.cs',\r\n        '.go', '.rs', '.rb', '.php', '.html', '.css', '.scss', '.sass', '.less',\r\n        '.json', '.yaml', '.yml', '.xml', '.sql', '.sh', '.bash', '.zsh',\r\n        '.md', '.txt', '.csv', '.toml', '.ini', '.cfg', '.conf', '.env',\r\n        '.dockerfile', 'dockerfile', '.gitignore', '.npmignore', '.prettierignore',\r\n        // Configuration files\r\n        'makefile', 'cmakelists.txt', 'readme.md', 'readme.txt', 'readme',\r\n        'license', 'license.md', 'changelog', 'changelog.md', 'contributing',\r\n        'contributing.md', 'code_of_conduct', 'code_of_conduct.md'\r\n    ],\r\n\r\n    excludeExtensions: [\r\n        // Binary files\r\n        '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.svg', '.webp',\r\n        '.exe', '.bin', '.dll', '.so', '.dylib', '.zip', '.tar', '.gz', '.rar', '.7z',\r\n        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\r\n        '.mp3', '.mp4', '.avi', '.mov', '.wav', '.flac',\r\n        '.ttf', '.otf', '.woff', '.woff2',\r\n        // Build/cache files\r\n        '.o', '.obj', '.a', '.lib', '.out', '.class', '.jar', '.war', '.swp', '.swo',\r\n        '.lock', '.cache', '.log', '.tmp', '.temp', '.DS_Store', 'Thumbs.db'\r\n    ],\r\n\r\n    excludeDirectories: [\r\n        '.git', 'node_modules', 'archive', 'backups', 'logs', 'context', '.vscode',\r\n        '.idea', '.pytest_cache', '__pycache__', 'dist', 'build', 'target',\r\n        'venv', 'env', '.venv', '.env', 'Pods', 'Carthage', 'CocoaPods',\r\n        '.next', '.nuxt', 'public', 'static', 'assets', 'images', 'img', 'codebase',\r\n    ],\r\n\r\n    excludeFiles: [\r\n        'combined_context.yaml', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\r\n        'Gemfile.lock', 'Pipfile.lock', 'Cargo.lock', 'composer.lock',\r\n        'go.sum', 'go.mod', 'requirements.txt', 'poetry.lock',\r\n        // Database files\r\n        '*.db', '*.sqlite', '*.sqlite3', '*.fdb', '*.mdb', '*.accdb',\r\n        // Temporary files\r\n        '*~', '*.tmp', '*.temp', '*.cache', '*.swp', '*.swo'\r\n    ]\r\n};\r\n\r\n// Simple token counting function\r\nfunction countTokens(text) {\r\n    // A rough approximation: 1 token  4 characters or 1 word\r\n    const words = text.match(/\\b\\w+\\b/g) || [];\r\n    return words.length + Math.ceil(text.length / 4);\r\n}\r\n\r\n// Function to check if a path should be ignored based on configuration\r\nfunction shouldIgnore(filePath, rootDir) {\r\n    const fileName = path.basename(filePath).toLowerCase();\r\n    const dirName = path.dirname(filePath).split(path.sep).pop().toLowerCase();\r\n    const ext = path.extname(filePath).toLowerCase();\r\n\r\n    // Check if directory should be excluded\r\n    for (const excludeDir of CONFIG.excludeDirectories) {\r\n        if (dirName === excludeDir.toLowerCase()) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    // Check if file extension should be excluded\r\n    if (CONFIG.excludeExtensions.includes(ext)) {\r\n        return true;\r\n    }\r\n\r\n    // Check if file should be excluded by name patterns\r\n    for (const excludePattern of CONFIG.excludeFiles) {\r\n        if (matchesPattern(fileName, excludePattern.toLowerCase()) ||\r\n            matchesPattern(path.basename(filePath), excludePattern)) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    // Check if file is too large\r\n    try {\r\n        const stats = fs.statSync(filePath);\r\n        if (stats.size > CONFIG.maxFileSize) {\r\n            return true;\r\n        }\r\n    } catch (e) {\r\n        // If we can't stat the file, skip it\r\n        return true;\r\n    }\r\n\r\n    // Check if file extension should be included (if include list is specified)\r\n    if (CONFIG.includeExtensions.length > 0) {\r\n        const fullName = path.basename(filePath).toLowerCase();\r\n        if (!CONFIG.includeExtensions.includes(ext) && !CONFIG.includeExtensions.includes(fullName)) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    return false;\r\n}\r\n\r\n// Helper function to check if a filename matches a pattern (supports wildcards)\r\nfunction matchesPattern(fileName, pattern) {\r\n    if (pattern === fileName) return true;\r\n\r\n    if (pattern.startsWith('*') && fileName.endsWith(pattern.substring(1))) {\r\n        return true;\r\n    }\r\n\r\n    if (pattern.endsWith('*') && fileName.startsWith(pattern.substring(0, pattern.length - 1))) {\r\n        return true;\r\n    }\r\n\r\n    return false;\r\n}\r\n\r\n// Function to limit file content based on line count\r\nfunction limitFileContent(content) {\r\n    if (!content) return '';\r\n\r\n    const lines = content.split('\\n');\r\n    if (lines.length <= CONFIG.maxLinesPerFile) {\r\n        return content;\r\n    }\r\n\r\n    // Take the first and last parts of the file to preserve context\r\n    const header = lines.slice(0, CONFIG.maxLinesPerFile / 2).join('\\n');\r\n    const footer = lines.slice(-CONFIG.maxLinesPerFile / 2).join('\\n');\r\n\r\n    return `${header}\\n\\n... [CONTENT TRUNCATED - ${lines.length - CONFIG.maxLinesPerFile} LINES REMOVED] ...\\n\\n${footer}`;\r\n}\r\n\r\n// Function to aggregate all file contents from the project root\r\nexport function createFullCorpusRecursive(rootDir = process.cwd()) {\r\n    // Allow rootDir to be passed as command line argument\r\n    if (process.argv[2] && process.argv[2] !== 'json' && process.argv[2] !== 'yaml') {\r\n        rootDir = path.resolve(process.argv[2]);\r\n    }\r\n\r\n    const outputDir = path.join(rootDir, CONFIG.outputDir);\r\n    console.log(`Scanning project root: ${rootDir}`);\r\n\r\n    if (!fs.existsSync(outputDir)) {\r\n        console.log(`Output directory does not exist, creating: ${outputDir}`);\r\n        fs.mkdirSync(outputDir, { recursive: true });\r\n    }\r\n\r\n    const aggregatedData = {\r\n        project_structure: rootDir,\r\n        scan_config: {\r\n            tokenLimit: CONFIG.tokenLimit,\r\n            maxFileSize: CONFIG.maxFileSize,\r\n            maxLinesPerFile: CONFIG.maxLinesPerFile,\r\n            includeExtensions: CONFIG.includeExtensions,\r\n            excludeExtensions: CONFIG.excludeExtensions,\r\n            excludeDirectories: CONFIG.excludeDirectories,\r\n            excludeFiles: CONFIG.excludeFiles\r\n        },\r\n        files: []\r\n    };\r\n\r\n    let totalTokens = 0;\r\n\r\n    // Walk through all files in the project\r\n    function walkDirectory(currentPath) {\r\n        let items;\r\n        try {\r\n            items = fs.readdirSync(currentPath);\r\n        } catch (e) {\r\n            console.warn(`Could not read directory: ${currentPath} - ${e.message}`);\r\n            return;\r\n        }\r\n\r\n        for (const item of items) {\r\n            const itemPath = path.join(currentPath, item);\r\n            const relativePath = path.relative(rootDir, itemPath);\r\n\r\n            let stat;\r\n            try {\r\n                stat = fs.statSync(itemPath);\r\n            } catch (e) {\r\n                continue;\r\n            }\r\n\r\n            if (stat.isDirectory()) {\r\n                // Skip excluded directories\r\n                const dirName = item.toLowerCase();\r\n                if (CONFIG.excludeDirectories.some(exclude => dirName === exclude.toLowerCase())) {\r\n                    continue;\r\n                }\r\n\r\n                walkDirectory(itemPath);\r\n            } else {\r\n                // Check if file should be ignored\r\n                if (shouldIgnore(itemPath, rootDir)) {\r\n                    continue;\r\n                }\r\n\r\n                try {\r\n                    const rawContent = fs.readFileSync(itemPath, 'utf-8');\r\n                    const content = limitFileContent(rawContent);\r\n                    const fileTokens = countTokens(content);\r\n\r\n                    if (totalTokens + fileTokens > CONFIG.tokenLimit) {\r\n                        console.log(`Token limit reached. Skipping: ${relativePath}`);\r\n                        continue;\r\n                    }\r\n\r\n                    const fileData = {\r\n                        path: relativePath,\r\n                        content: content,\r\n                        tokens: fileTokens,\r\n                        size: Buffer.byteLength(rawContent, 'utf8')\r\n                    };\r\n\r\n                    aggregatedData.files.push(fileData);\r\n                    totalTokens += fileTokens;\r\n                    console.log(`Processed: ${relativePath} (${fileTokens} tokens)`);\r\n                } catch (e) {\r\n                    console.warn(`Could not read file: ${itemPath} - ${e.message}`);\r\n                    // Skip non-text files or files with read errors\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    walkDirectory(rootDir);\r\n\r\n    aggregatedData.metadata = {\r\n        total_files: aggregatedData.files.length,\r\n        total_tokens: totalTokens,\r\n        token_limit: CONFIG.tokenLimit,\r\n        token_limit_reached: totalTokens >= CONFIG.tokenLimit,\r\n        timestamp: new Date().toISOString(),\r\n        root_directory: rootDir,\r\n        config: CONFIG\r\n    };\r\n\r\n    // Write to YAML file in output directory\r\n    const outputFile = path.join(outputDir, CONFIG.outputFile);\r\n    const yamlContent = yaml.dump(aggregatedData, {\r\n        lineWidth: -1,\r\n        noRefs: true,\r\n        quotingType: '\"',\r\n        forceQuotes: false\r\n    });\r\n    fs.writeFileSync(outputFile, yamlContent);\r\n\r\n    console.log(\"Aggregation complete!\");\r\n    console.log(`Output file: ${outputFile}`);\r\n    console.log(`Total files processed: ${aggregatedData.metadata.total_files}`);\r\n    console.log(`Total tokens: ${aggregatedData.metadata.total_tokens}`);\r\n    console.log(`Scan completed at: ${new Date().toISOString()}`);\r\n\r\n    return aggregatedData;\r\n}\r\n\r\n// Alternative function to output JSON format\r\nexport function createFullCorpusRecursiveJSON(rootDir = process.cwd()) {\r\n    const result = createFullCorpusRecursive(rootDir);\r\n    const outputDir = path.join(rootDir, CONFIG.outputDir);\r\n    const outputFile = path.join(outputDir, CONFIG.outputFile.replace('.yaml', '.json'));\r\n\r\n    fs.writeFileSync(outputFile, JSON.stringify(result, null, 2));\r\n    console.log(`JSON output also saved to: ${outputFile}`);\r\n\r\n    return result;\r\n}\r\n\r\nexport { CONFIG };\r\n\r\n// Run if this file is executed directly\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst entryFile = process.argv[1];\r\n\r\nif (entryFile === __filename) {\r\n    console.log('Starting universal project aggregation...');\r\n\r\n    // Allow format selection via command line argument\r\n    const format = process.argv[3] || 'yaml';\r\n    let rootDir = process.argv[2];\r\n\r\n    // Check if first arg is format instead of dir\r\n    if (rootDir === 'json' || rootDir === 'yaml') {\r\n        rootDir = process.cwd();\r\n    } else {\r\n        rootDir = rootDir || process.cwd();\r\n    }\r\n\r\n    if (process.argv.includes('json') || format.toLowerCase() === 'json') {\r\n        createFullCorpusRecursiveJSON(rootDir);\r\n    } else {\r\n        createFullCorpusRecursive(rootDir);\r\n    }\r\n}\r\n"
    tokens: 4007
    size: 11617
  - path: shared\package.json
    content: "{\r\n    \"name\": \"@ece/shared\",\r\n    \"version\": \"1.0.0\",\r\n    \"private\": true,\r\n    \"main\": \"types/index.ts\"\r\n}"
    tokens: 41
    size: 109
  - path: shared\types\index.ts
    content: "/**\r\n * Core Data Structures for Sovereign Context Engine\r\n * Source of Truth for both Engine and Desktop Overlay\r\n */\r\n\r\n// ------------------------------------------------------------------\r\n// CONFIGURATION TYPES\r\n// ------------------------------------------------------------------\r\n\r\nexport interface ILLMConfig {\r\n    active: boolean;\r\n    path: string;\r\n    context_size: number;\r\n    gpu_layers: number;\r\n    temperature?: number;\r\n    projector_path?: string;\r\n}\r\n\r\nexport interface IAppConfig {\r\n    system_name: string;\r\n    ui: {\r\n        theme: 'dark' | 'light' | 'system';\r\n        transparency: boolean;\r\n        always_on_top: boolean;\r\n        shortcuts: {\r\n            toggle_overlay: string;\r\n        };\r\n    };\r\n    models: {\r\n        orchestrator: ILLMConfig;\r\n        main: ILLMConfig;\r\n        vision: ILLMConfig;\r\n    };\r\n    storage: {\r\n        db_path: string;\r\n        backup_path: string;\r\n    };\r\n    network: {\r\n        api_port: number;\r\n        websocket_port: number;\r\n    };\r\n}\r\n\r\n// ------------------------------------------------------------------\r\n// DATA TYPES\r\n// ------------------------------------------------------------------\r\n\r\nexport type ContextSource = 'file' | 'clipboard' | 'vision' | 'audio' | 'web';\r\n\r\nexport interface IContextItem {\r\n    id: string;            // UUID\r\n    content: string;       // The raw text/content\r\n    source: ContextSource; // Where did it come from?\r\n    timestamp: number;     // Unix Epoch\r\n    metadata: {\r\n        filePath?: string;\r\n        windowTitle?: string;\r\n        url?: string;\r\n        tags?: string[];\r\n    };\r\n    embedding?: number[];  // Vector representation (optional on client)\r\n}\r\n\r\nexport interface IChatMessage {\r\n    role: 'user' | 'assistant' | 'system';\r\n    content: string;\r\n    timestamp: number;\r\n    thoughts?: string;     // Chain of thought (reasoning)\r\n}\r\n"
    tokens: 602
    size: 1873
  - path: specs\context_assembly_findings.md
    content: |-
      # Context Assembly Findings & Optimization Report

      ## What Happened?
      During development and testing of the context assembly system, several important findings emerged regarding how context is retrieved, assembled, and presented to the LLM. This document captures the key findings and optimizations discovered during the process.

      ## The Cost
      - Initial context assembly was inefficient and slow
      - Poor relevance ranking in search results
      - Memory budget management issues
      - Inconsistent context presentation across different query types

      ## The Rule
      1. **The 70/30 Split:** When assembling context, allocate 70% of the character budget to Direct Matches (Keyword/Vector) and 30% to Associative Matches (Shared Tags).

      2. **Tag Harvesting:** Extract semantic tags from the Direct Matches to find "Neighboring" memories.

      3. **Unified Stream:** Present both Direct and Associative snippets in the same output stream, clearly labeled.

      4. **Memory Budget Management:**
         - Set a maximum character limit for context assembly (default 5000 chars)
         - Implement progressive loading for large context requests
         - Use sliding window approach for temporal context

      5. **Relevance Ranking:**
         - Use BM25 algorithm for keyword-based relevance
         - Implement semantic similarity for vector-based matching
         - Combine both approaches for hybrid search results

      6. **Performance Optimization:**
         - Cache frequent queries to improve response time
         - Implement pagination for large result sets
         - Use asynchronous loading where possible

      ## Key Findings
      - Direct matches provide the most relevant context for specific queries
      - Associative matches help with concept exploration and discovery
      - The combination of both approaches provides the most comprehensive context
      - Character budget management is crucial for performance and cost efficiency
    tokens: 702
    size: 1841
  - path: specs\doc_policy.md
    content: |-
      # Documentation Policy (Root Coda)

      **Status:** Active | **Authority:** Human-Locked

      ## Core Philosophy
      1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.
      2. **Synchronous Testing:** EVERY feature or data change MUST include a matching update to the Test Suite.
      3. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.
      4. **Brevity:** Text sections must be <500 characters.
      5. **Pain into Patterns:** Every major bug must become a Standard.
      6. **LLM-First Documentation:** Documentation must be structured for LLM consumption and automated processing.
      7. **Change Capture:** All significant system improvements and fixes must be documented in new Standard files.

      ## User-Facing Documentation

      ### `QUICKSTART.md` (Root)  **PRIMARY USER GUIDE**
      *   **Role:** First-time user onboarding and daily workflow reference.
      *   **Content:** Data ingestion methods, deduplication logic, backup/restore, search patterns.
      *   **Audience:** New users, daily reference for workflow.
      *   **Authority:** Canonical guide for how users interact with ECE.

      ### `README.md` (Root)
      *   **Role:** Project overview, installation, and quick start.
      *   **Content:** What ECE is, how to install, link to QUICKSTART.md.

      ## Data Ingestion Standards

      ### Unified Ingestion Flow
      ```
      
        INPUT METHODS (All paths lead to CozoDB)                        
      
        1. Drop files  context/           (Watcher auto-ingests)       
        2. Corpus YAML  context/          (read_all.js output)         
        3. API POST  /v1/ingest           (Programmatic)               
        4. Backup restore  backups/       (Session resume)             
      
                                  
      
        DEDUPLICATION LAYER                                             
      
         Hash match  Skip (exact duplicate)                           
         >80% Jaccard  Skip (semantic duplicate)                      
         50-80% Jaccard  New version (temporal folding)               
         <50% Jaccard  New document                                   
         >500KB  Reject (Standard 053: FTS poisoning)                
      
                                  
      
        CozoDB GRAPH  Mirror  context/mirrored_brain/                
      
      ```

      ### Corpus File Format (read_all.js output)
      ```yaml
      project_structure: "C:/path/to/project"
      files:
        - path: "src/index.js"
          content: "// file content..."
        - path: "README.md"
          content: "# Project..."
      metadata:
        total_files: N
        timestamp: "ISO-8601"
      ```

      ### Ingestion Rules
      1. **Max Content Size:** 500KB per file (Standard 053: CozoDB Pain Points)
      2. **Auto-Bucketing:** Top-level folder name = bucket; root files  `pending`
      3. **Corpus Detection:** Files with `project_structure:` + `files:` array are extracted
      4. **Temporal Folding:** Search shows latest version, history timestamps collapsed

      ## Structure

      ### 1. The Blueprint (`specs/spec.md`)
      *   **Role:** The single architectural source of truth.
      *   **Format:** "Visual Monolith".
      *   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.

      ### 2. Search Patterns (`specs/search_patterns.md`)
      *   **Role:** Document the new semantic search and temporal folding capabilities.
      *   **Format:** Examples and usage guidelines.
      *   **Content:** How to leverage semantic intent translation and temporal folding for optimal results.

      ### 3. Context Assembly Findings (`specs/context_assembly_findings.md`)
      *   **Role:** Document the critical findings from context assembly experiments showing retrieval layer bottlenecks.
      *   **Format:** Analysis and recommendations.
      *   **Content:** How retrieval layer optimization is more important than inference layer upgrades, with scaling recommendations for different model sizes.

      ### 4. Testing Standards (`TESTING_STANDARDS.md`)
      *   **Role:** Document the comprehensive testing policy for the ECE project.
      *   **Format:** Standards and policies for testing approach.
      *   **Content:** Single point of truth for all testing through the comprehensive suite.

      ### 5. Cleanup Reports (`CLEANUP_REPORT.md`)
      *   **Role:** Document codebase cleanup activities and improvements.
      *   **Format:** Summary of cleanup actions taken.
      *   **Content:** Details of test consolidation, duplicate file cleanup, and system improvements.

      ### 2. The Tracker (`specs/tasks.md`)
      *   **Role:** Current work queue.
      *   **Format:** Checklist.
      *   **Maintenance:** Updated by Agents after every major task.

      ### 3. The Roadmap (`specs/plan.md`)
      *   **Role:** Strategic vision.
      *   **Format:** Phased goals.

      ### 4. Standards (`specs/standards/*.md`)
      *   **Role:** Institutional Memory (The "Laws" of the codebase).
      *   **Trigger:** Created after any bug that took >1 hour to fix OR any systemic improvement that affects multiple components.
      *   **Format:** "The Triangle of Pain"
          1.  **What Happened:** The specific failure mode (e.g., "Bridge crashed on start").
          2.  **The Cost:** The impact (e.g., "3 hours debugging Unicode errors").
          3.  **The Rule:** The permanent constraint (e.g., "Force UTF-8 encoding on Windows stdout").

      ### 5. Root-Level Documents
      *   **Role:** System-wide protocols and policies.
      *   **Examples:** `SCRIPT_PROTOCOL.md`, `README.md`
      *   **Purpose:** Critical system-wide protocols that apply to the entire project.

      ### 6. Local Context (`*/README.md`)
      *   **Role:** Directory-specific context.
      *   **Limit:** 1 sentence explaining the folder's purpose.

      ### 7. System-Wide Standards
      *   **Universal Logging:** All system components must route logs to the central log collection system (Standard 013)
      *   **Single Source of Truth:** The log viewer at `/log-viewer.html` is the single point for all system diagnostics
      *   **Async Best Practices:** All async/await operations must follow proper patterns for FastAPI integration (Standard 014)
      *   **Browser Control Center:** All primary operations must be accessible through unified browser interface (Standard 015)
      *   **Detached Script Execution:** All data processing scripts must run in detached mode with logging to `logs/` directory (Standard 025)
      *   **Never Attached Mode:** Long-running services and scripts must NEVER be run in attached mode to prevent command-line blocking (Standard 035 in 30-OPS)
      *   **Script Running Protocol:** All long-running processes must execute in detached mode with output redirected to timestamped log files (Standard 035 in 30-OPS)
      *   **Ghost Engine Connection Management:** All memory operations must handle Ghost Engine disconnections gracefully with proper error reporting and auto-reconnection (Standard 026)
      *   **No Resurrection Mode:** System must support manual Ghost Engine control via NO_RESURRECTION_MODE flag (Standard 027)
      *   **Default No Resurrection:** Ghost Engine resurrection is disabled by default, requiring manual activation (Standard 028)
      *   **Consolidated Data Aggregation:** Single authoritative script for data aggregation with multi-format output (Standard 029)
      *   **Multi-Format Output:** Project aggregation tools must generate JSON, YAML, and text outputs for maximum compatibility (Standard 030)
      *   **Ghost Engine Stability:** CozoDB schema creation must handle FTS failures gracefully to prevent browser crashes (Standard 031)
      *   **Ghost Engine Initialization Flow:** Database initialization must complete before processing ingestion requests to prevent race conditions (Standard 032)
      *   **CozoDB Syntax Compliance:** All CozoDB queries must use proper syntax to ensure successful execution (Standard 033)
      *   **Node.js Monolith Migration:** System must migrate from Python/Browser Bridge to Node.js Monolith architecture (Standard 034)
      *   **Cortex Upgrade**: Local inference via `node-llama-cpp` for GGUF support (Standard 038)
      *   **Multi-Bucket Schema**: Memories support multiple categories via `buckets: [String]` (Standard 039)
      *   **Cozo Syntax Hardening**: Avoid `unnest` and complex list queries in CozoDB (Standard 040)
      *   **Timed Background Execution**: Model development scripts must run with timers in background mode, directing output to logs (Standard 049)
      *   **CozoDB Pain Points Reference**: Comprehensive gotchas and lessons learned for CozoDB queries (Standard 053)
      *   **Side-Channel Summarization**: Context injections >50% of budget must be summarized via ephemeral sequence (Standard 054)
      *   **Unified Data Ingestion**: All data enters via context/ directory, API, or backup restore with automatic deduplication (QUICKSTART.md)
      *   **Sequential LLM Access Protocol**: All LLM access must go through a global request queue to prevent resource contention (Standard 055)
      *   **LLM Access Serialization Implementation**: Complete audit and implementation of request queue for all LLM-accessing functions (Standard 056)
      *   **Priority-Based Request Queue System**: Implement priority classification and scheduling for different types of requests (Standard 057)
      ## LLM Protocol
      1. **Read-First:** Always read `specs/spec.md`, `SCRIPT_PROTOCOL.md`, AND `specs/standards/` before coding.
      2. **Drafting:** When asked to document, produce **Mermaid diagrams** and short summaries.
      3. **Editing:** Do not modify `specs/doc_policy.md` or `specs/spec.md` structure unless explicitly instructed.
      4. **Archival:** Move stale docs to `archive/` immediately.
      5. **Enforcement:** If a solution violates a Standard, reject it immediately.
      6. **Standards Evolution:** New standards should follow the "Triangle of Pain" format and be numbered sequentially (001, 002, etc.).
      7. **Cross-Reference:** When creating new standards, reference related existing standards to maintain consistency.
      8. **Detached Mode:** All LLM development scripts must run in detached mode (non-interactive) and log to files in the `logs/` directory with timestamped names (Standard 025).

      ## Windows-Specific Considerations
      1. **Safe Shell Execution:** On Windows, use the SafeShellExecutor for running commands to avoid console window issues.
      2. **Command Output:** Due to Windows process creation behavior, command outputs may not appear in the current session when running background processes.
      3. **Native Modules:** Windows may require additional build tools for native Node.js modules. Consider using prebuilt binaries or installing Visual Studio Build Tools.
      4. **Path Handling:** Always use Node.js path utilities (`path.join`, `path.resolve`) for cross-platform compatibility.

      ---
      *Verified by Architecture Council. Edits verified by Humans Only.*
    tokens: 4052
    size: 12138
  - path: specs\findings_2026_01_19_cozodb_parser_instability.md
    content: "# Finding: CozoDB Query Parser Instability in Hybrid Search\r\n\r\n**Date:** 2026-01-19\r\n**Status:** Open\r\n**Severity:** High\r\n**Component:** Engine / Search Service / CozoDB Driver\r\n\r\n## Description\r\nDuring the implementation of \"Sovereign Bias\" and \"UniversalRAG\", persistent `coercion_failed` and `query parser unexpected input` errors were encountered when executing complex Datalog queries via the Node.js CozoDB driver (`cozo-node`).\r\n\r\nSpecifically, the FTS (Full-Text Search) query combined with Vector Search logic fails with:\r\n```\r\nError: \"The query parser has encountered unexpected input / end of input at 20..20\"\r\n```\r\nThis occurs even when the query syntax appears valid and identical queries pass in isolated test scripts (`test_fts_simple.ts`).\r\n\r\n## Symptoms\r\n- `runTraditionalSearch` fails consistently when imported into the full engine context.\r\n- `vectorSearch` triggers `coercion_failed` or similar opaque errors.\r\n- The error `20..20` suggests the parser chokes on the projection variables (e.g., `?[id, score, content...]`), possibly due to:\r\n    1. Invisible character encoding issues in TypeScript template literals.\r\n    2. Conflict with reserved keywords (though `content` worked in isolation).\r\n    3. Memory corruption or uninitialized state in the `db` instance when running multiple heavy queries.\r\n\r\n## Workaround / Resolution\r\nTo restore stable system functionality, we have implemented the following temporary measures:\r\n1.  **Disabled Vector Search**: The `vectorSearch` call in `executeSearch` matches has been replaced with a `Promise.resolve([])` stub.\r\n2.  **Simplified FTS Queries**: Search queries are restricted to single-line strings to minimize parser ambiguity.\r\n3.  **Fallback Mechanism**: The system relies heavily on the `runTraditionalSearch` (FTS) and Engram (Lexical) layers until the driver instability is resolved.\r\n\r\n## Impact\r\n- Semantic retrieval (embedding-based) is currently inactive. Use `provenance` or `buckets` for filtering.\r\n- \"Dreamer\" and \"Recall\" features relying on purely semantic matches may see reduced accuracy.\r\n- \"Sovereign Bias\" logic remains implemented but operates only on FTS/Lexical results.\r\n\r\n## Next Steps\r\n- Investigate `cozo-node` binary compatibility with the current Node.js version.\r\n- Re-enable Vector Search incrementally using simplified, isolated queries.\r\n- Audit all Datalog queries for template literal normalization.\r\n"
    tokens: 914
    size: 2412
  - path: specs\llama_servers.md
    content: "# Starting Llama Servers with ECE_Core\r\n\r\nYou can start llama.cpp-based LLM servers for both inference and embeddings via the scripts in the ECE_Core project.\r\n\r\nThe `start-llama-server.bat` and `start-embed-server.bat` scripts live in the repo root (`./start-llama-server.bat` and `./start-embed-server.bat`).\r\n\r\nHow it works:\r\n- `scripts/generate_llama_env.py` reads `src.config.settings` and prints environment variables.\r\n- The batch scripts call the helper to load configuration values from `.env` or environment, and use them to start `llama-server.exe` with appropriate flags (context, threads, GPU layers, etc.).\r\n- For interactive model selection, the scripts call `select_model.py`; if the helper supplied `MODEL`, it will use this directly.\r\n\r\nInstructions:\r\n1. Set configuration values in `.env` (for example `LLM_MODEL_PATH`, `LLM_CONTEXT_SIZE`, `LLM_THREADS`, `LLM_GPU_LAYERS`, `LLM_EMBEDDINGS_*`, `LLAMA_SERVER_EXE_PATH`, etc.).\r\n2. Start the API server:\r\n   - Open a PowerShell window in the `ECE_Core` folder and run:\r\n\r\n```powershell\r\n.\\start-llama-server.bat\r\n```\r\n\r\n3. Start the Embeddings server:\r\n\r\n```powershell\r\n.\\start-embed-server.bat\r\n```\r\n\r\nTip: You can also specify a different set of values using environment variables directly or via a custom `.env` file, and you can override the model selection interactively with `select_model.py` if needed.\r\n\r\nBatching guidance (GPU/UBATCH) \r\n--------------------------------\r\nIf you serve many small requests concurrently, we recommend keeping continuous batching enabled (it improves throughput). However, ensure that the `UBATCH` (physical batch size) is large enough to fit typical requests, and also small enough to avoid OOM on your GPU.\r\n\r\nFor NVIDIA RTX 4090 (16 GB VRAM) laptops, a reasonable starting point is to set:\r\n\r\n```env\r\nLLAMA_SERVER_UBATCH_MAX=8192\r\nLLAMA_BATCH=2048\r\nLLAMA_PARALLEL=1\r\nLLAMA_CONT_BATCHING=True\r\n```\r\n\r\nAdjust `LLAMA_SERVER_UBATCH_MAX` up or down depending on model size:\r\n- Small embedding models (e.g., 300M): you can often set a higher UBATCH.\r\n- Larger models (4B+): start conservative (4096 or 2048) and raise if the load stays stable.\r\n\r\nUse `python scripts/generate_llama_env.py` to dump settings and confirm the final `LLAMA_UBATCH` value prior to starting the server. This helper respects `LLAMA_SERVER_UBATCH_MAX` and will cap the computed UBATCH accordingly.\r\n\r\nPre-flight token validation \r\n---------------------------------\r\nECE_Core includes a pre-flight validation in the API layer that checks the size of the assembled prompt (in tokens) against the configured `LLAMA_SERVER_UBATCH_SIZE` micro-batch. If the prompt tokens exceed the UBATCH the service returns an HTTP 400 response advising the user to reduce the context size or increase `LLAMA_SERVER_UBATCH_SIZE`. This prevents a llama.cpp GGML assertion (encoder requires n_ubatch >= n_tokens) and reduces 500 Internal Server Errors under heavy load.\r\n\r\nDebugging:\r\n\r\nAuto-tuning helper \r\n---------------------\r\nECE_Core ships with `scripts/auto_tune_llama.py` which can recommend `LLM_CONTEXT_SIZE`, `LLAMA_SERVER_UBATCH_SIZE`, and `LLAMA_SERVER_BATCH_SIZE` based on detected GPU VRAM and model file size. Run it with `python scripts/auto_tune_llama.py` to print recommended settings or `python scripts/auto_tune_llama.py --apply` to append conservative recommendations to `ece-core/.env` (backing up the original). This can be helpful when swapping models on limited VRAM machines like the RTX 4090.\r\n"
    tokens: 1344
    size: 3490
  - path: specs\plan.md
    content: "# Anchor Core Roadmap (V2.4)\r\n\r\n**Status:** Markovian Reasoning Deployed & Context Assembly Experiments Added\r\n**Focus:** Production Polish & Verification.\r\n\r\n## Phase 1: Foundation (Completed)\r\n- [x] Pivot to WebLLM/WebGPU stack.\r\n- [x] Implement CozoDB (WASM) for memory.\r\n- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).\r\n\r\n## Phase 2: Stabilization (Completed)\r\n- [x] Fix Model Loading (Quota/VRAM config).\r\n- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).\r\n- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).\r\n\r\n## Phase 2.5: Root Refactor (Completed)\r\n- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).\r\n- [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.\r\n- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.\r\n\r\n## Phase 3: Markovian Reasoning & Context Optimization (Completed)\r\n- [x] **Scribe Service**: Created `engine/src/services/scribe.js` for rolling state\r\n- [x] **Context Weaving**: Upgraded `inference.js` to auto-inject session state\r\n- [x] **Dreamer Service**: Enhanced `dreamer.js` with batch processing to prevent OOM errors\r\n- [x] **Semantic Translation**: Added intent translation via local SLM\r\n- [x] **Context Experiments**: Created `engine/tests/context_experiments.js` for optimal context window sizing\r\n- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).\r\n\r\n## Phase 3-8: [Archived] (Completed)\r\n*See `specs/tasks.md` for detailed historical phases.*\r\n\r\n## Phase 9: Node.js Monolith & Snapshot Portability (Completed)\r\n- [x] **Migration**: Move from Python/Browser Bridge to Node.js Monolith (Standard 034).\r\n- [x] **FTS Optimization**: Implement native CozoDB BM25 search.\r\n- [x] **Operational Safety**: Implement detached execution and logging protocols (Standard 035/036).\r\n- [x] **Snapshot Portability**: Create \"Eject\" (Backup) and \"Hydrate\" (Restore) workflow (Standard 037).\r\n\r\n## Phase 10: Cortex Upgrade (Completed)\r\n- [x] **Local Inference**: Integrate `node-llama-cpp` for GGUF support (Standard 038).\r\n- [x] **Multi-Bucket Schema**: Migrate from single `bucket` to `buckets: [String]` (Standard 039).\r\n- [x] **Dreamer Service**: Implement background self-organization via local LLM.\r\n- [x] **Cozo Hardening**: Resolve list-handling and `unnest` syntax errors (Standard 040).\r\n- [x] **ESM Interop**: Fix dynamic import issues for native modules in CJS.\r\n\r\n## Phase 11: Markovian Reasoning Engine (Completed)\r\n- [x] **Scribe Service**: Implement rolling session state compression (Standard 041).\r\n- [x] **Context Weaving**: Auto-inject Markovian state into inference.\r\n- [x] **Test Suite**: Create `engine/tests/suite.js` for API verification.\r\n- [x] **Benchmark Tool**: Create `engine/tests/benchmark.js` for accuracy testing.\r\n- [x] **Configuration Hardening**: Externalize paths, fix package.json, add validation.\r\n\r\n## Phase 12: Production Polish (In Progress)\r\n- [ ] **UI/UX Overhaul**: Implement \"Flight Recorder\" aesthetic for the dashboard.\r\n- [ ] **Chat Cockpit**: Enhance `interface/chat.html` with conversation history.\r\n- [ ] **Streaming Responses**: Implement SSE for real-time token streaming.\r\n- [ ] **Android Compatibility**: Ensure Node.js monolith runs in Termux.\r\n- [ ] **Clean Install Script**: Create one-click setup for new users.\r\n\r\n## Phase 13: Enterprise & Advanced RAG (Up Next)\r\n- [ ] **Backup System**: Robust snapshotting/restore (Feature 7).\r\n- [ ] **Smart Context**: Middle-Out \"Rolling Slicer\" logic (Feature 8).\r\n- [ ] **RAG IDE**: Live Context Visualization in UI (Feature 9).\r\n- [ ] **Provenance**: Trust Hierarchy switching (Feature 10).\r\n\r\n## Phase 14: Federation (Future)\r\n- [ ] **Device Sync**: Sync snapshots across devices (P2P or cloud).\r\n- [ ] **Local-First Cloud**: Optional encrypted backup.\r\n- [ ] **Multi-Model**: Support multiple models loaded simultaneously."
    tokens: 1480
    size: 3948
  - path: specs\search_patterns.md
    content: |-
      # Search Patterns & Query Syntax for ECE

      ## What Happened?
      The system needed a standardized approach for search queries to ensure consistent behavior across all search operations. This document defines the search patterns, query syntax, and optimization strategies for the ECE system.

      ## The Cost
      - Inconsistent search behavior across different components
      - Users experiencing different search results depending on which interface they used
      - Difficulty in optimizing search queries for performance
      - Lack of clear guidance on how to structure search queries for best results

      ## The Rule
      1. **Standardized Query Structure:** All search queries should follow the same basic structure:
         - Simple keyword search: Just enter the keywords you're looking for
         - Bucket filtering: Use `bucket:name` to filter results by bucket
         - Phrase matching: Use `"exact phrase"` to match exact phrases
         - Complex queries: Combine keywords, buckets, and phrases as needed

      2. **Search Optimization Strategies:**
         - **Broad Strategy:** For concept exploration and general information retrieval
         - **Precise Strategy:** For specific information and exact matches
         - **Hybrid Strategy:** For complex queries that need both concepts and specifics

      3. **Bucket-Based Organization:**
         - Use buckets to organize and filter search results
         - Common buckets include: `core`, `development`, `research`, `personal`, `codebase`
         - Create new buckets as needed for specific contexts or projects

      4. **Character Limit Considerations:**
         - Default character limit for search results is 5000 characters
         - This can be adjusted based on the specific needs of the search
         - Larger limits may impact performance but provide more context

      5. **Semantic Intent Translation:**
         - The system will automatically translate natural language queries to optimized search parameters
         - This includes identifying relevant buckets and search strategies
         - Users can override automatic classification if needed
    tokens: 759
    size: 1994
  - path: specs\spec.md
    content: "# ECE_Core - Technical Specification\r\n\r\n## Mission\r\n\r\nBuild a **personal external memory system** as an assistive cognitive tool using:\r\n- Redis + Neo4j tiered memory (pure graph architecture)\r\n- Markovian reasoning (chunked thinking)\r\n- Graph-R1 reasoning (iterative retrieval)\r\n- Local-first LLM integration (llama.cpp)\r\n- Plugin-based tool system (UTCP - Simple Tool Mode)\r\n\r\n**Current**: Neo4j + Redis architecture (SQLite removed)\r\n**Protocol**: Plugin System (migrated from MCP 2025-11-13)\r\n**Tools**: Tools loaded via `PluginManager` from `plugins/` directory:\r\n  - `web_search` - DuckDuckGo search with results\r\n  - `filesystem_read` - File and directory operations\r\n  - `shell_execute` - Shell command execution (with safety checks)\r\n  - `mgrep` - Semantic code & natural language file search (semantic `grep`) - Implemented as a standalone plugin in `plugins/mgrep/`\r\n\r\n## Architecture Overview\r\n\r\n### System Architecture (UniversalRAG)\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph \"Interface Layer\"\r\n        UI[Frontend (React)] -->|API| Server[Express Server]\r\n        Overlay[Desktop Overlay] -->|Loads| UI\r\n        Inbox[Inbox Directory] -.->|File Watch| Watcher[Watchdog Service]\r\n    end\r\n\r\n    subgraph \"Core Engine (Node.js)\"\r\n        Server --> Provider[LLM Provider]\r\n        Watcher --> Refiner[Refiner Service]\r\n        \r\n        subgraph \"Ingestion Pipeline\"\r\n            Refiner -->|Sanitize| Atomizer[Atomizer]\r\n            Atomizer -->|Chunks| EmbeddingWorker\r\n        end\r\n        \r\n        subgraph \"Inference System (Dual-Worker)\"\r\n            Provider -->|Routing| ChatWorker[ChatWorker (Chat Model)]\r\n            Provider -->|Routing| EmbeddingWorker[EmbeddingWorker (Vector Model)]\r\n        end\r\n        \r\n        subgraph \"Context Manager\"\r\n            Search[Vector Search] -->|Results| Slicer[Context Slicer]\r\n            Slicer -->|Assembly| Provider\r\n        end\r\n    end\r\n\r\n    subgraph \"Persistence Layer\"\r\n        EmbeddingWorker -->|Vectors| Cozo[CozoDB (RocksDB)]\r\n        Refiner -->|Metadata| Cozo\r\n    end\r\n\r\n    style ChatWorker fill:#f9f,stroke:#333\r\n    style EmbeddingWorker fill:#bbf,stroke:#333\r\n    style Cozo fill:#dfd,stroke:#333\r\n```\r\n\r\n### Context Assembly Flow\r\n\r\n```mermaid\r\ngraph LR\r\n    Query[User Query] --> Analysis{Temporal Analysis?}\r\n    \r\n    Analysis -->|Yes (\"recent\", \"now\")| WeightsTemp[Recency: 60% / Relev: 40%]\r\n    Analysis -->|No| WeightsStd[Recency: 30% / Relev: 70%]\r\n    \r\n    WeightsTemp --> Ranking\r\n    WeightsStd --> Ranking\r\n    \r\n    subgraph \"Search & Selection\"\r\n        Vectors[Vector Search] --> Ranking[Mixed Score Sort]\r\n        Ranking --> Budget{Token Budget < 3800?}\r\n    end\r\n    \r\n    Budget -->|Fit| Add[Add Atom]\r\n    Budget -->|Overflow| Slicing[Smart Slicing]\r\n    \r\n    subgraph \"Smart Slicing\"\r\n        Slicing --> Punctuation{Find . ! ? \\\\n}\r\n        Punctuation -->|Found| Cut[Truncate at Punctuation]\r\n        Punctuation -->|Not Found| HardCut[Hard Truncate]\r\n    end\r\n    \r\n    Add --> Resort[Chronological Re-Sort]\r\n    Cut --> Resort\r\n    HardCut --> Resort\r\n    \r\n    Resort --> FinalContext[Final Context Prompt]\r\n```\r\n\r\n### Cognitive Architecture: Agent-Based System\r\n\r\n**Verifier Agent** - Truth Verification\r\n- **Role**: Fact-checking via Empirical Distrust\r\n- **Method**: Provenance-aware scoring (primary sources > summaries)\r\n- **Goal**: Reduce hallucinations, increase factual accuracy\r\n\r\n**Distiller Agent** - Memory Compression & Context Rotation\r\n- **Role**: Memory summarization and compression + Context Rotation Protocol\r\n- **Method**: LLM-assisted distillation with salience scoring + context gist creation\r\n- **Goal**: Maintain high-value context, enable infinite context, prune noise\r\n\r\n**Archivist Agent** - Memory Maintenance & Context Management\r\n- **Role**: Knowledge base maintenance, freshness checks + Context Coordination\r\n- **Method**: Scheduled verification, stale node detection, context rotation oversight\r\n- **Goal**: Keep memory graph current and trustworthy, manage context windows\r\n\r\n**Memory Weaver** - Automated Relationship Repair\r\n- **Role**: Automated graph relationship repair and optimization\r\n- **Method**: Embedding-based similarity with audit trail (`auto_commit_run_id`)\r\n- **Goal**: Maintain graph integrity with full traceability\r\n\r\n### Reasoning Architecture: Graph-R1 + Markovian Reasoning\r\n\r\n**Graph-R1 Reasoning Pattern**:\r\n1. **Think** - High-level planning based on question\r\n2. **Generate Query** - Create Cypher query for Neo4j\r\n3. **Retrieve Subgraph** - Fetch relevant memories and relationships  \r\n4. **Rethink** - Plan next iteration based on retrieved context\r\n5. **Repeat** - Iterate until confident or max iterations reached\r\n\r\n**Markovian Memory**: Chunked context management for infinite windows\r\n- **Active Context**: Current working memory (in Redis)\r\n- **Gist Memory**: Compressed historical context (in Neo4j as `:ContextGist`)\r\n- **Rotation Protocol**: When active context approaches 55k tokens, compress oldest segments to gists\r\n\r\n### Tool Architecture: UTCP Plugin System\r\n\r\n**Current Implementation**: Plugin-based UTCP (Simple Tool Mode)\r\n- Discovery via `plugins/` directory\r\n- Safety layers with whitelist/blacklist\r\n- Human confirmation flows for dangerous operations\r\n\r\n**Available Tools**:\r\n- `web_search` - DuckDuckGo with result limits\r\n- `filesystem_read` - File operations with path restrictions\r\n- `shell_execute` - Command execution with safety checks\r\n- `mgrep` - Semantic code search with context\r\n\r\n## Infinite Context Pipeline\r\n\r\n### Phase 1: Hardware Foundation\r\n- **64k Context Windows**: All LLM servers boot with 65,536 token capacity\r\n- **GPU Optimization**: Full layer offload with Q8 quantized KV cache\r\n- **Flash Attention**: Enabled when available for optimal long-context performance\r\n\r\n### Phase 2: Context Rotation Protocol\r\n- **Monitoring**: ContextManager monitors total context length\r\n- **Trigger**: When context approaches 55k tokens (safety buffer for 64k window)\r\n- **Compression**: Distiller compresses old segments into \"Narrative Gists\"\r\n- **Storage**: Gists stored in Neo4j as `(:ContextGist)` nodes with `[:NEXT_GIST]` relationships\r\n- **Rewriting**: New context = `[System Prompt] + [Historical Gists Summary] + [Recent Context] + [New Input]`\r\n\r\n### Phase 3: Graph-R1 Enhancement\r\n- **Historical Retrieval**: GraphReasoner includes `:ContextGist` nodes in retrieval\r\n- **Continuity Maintenance**: Reasoning flow maintained across context rotations\r\n- **Temporal Awareness**: Reasoning considers chronological relationships in gists\r\n\r\n## API Specification\r\n\r\n### Core Endpoints (Port 8000)\r\n\r\n**Chat Interface**:\r\n- `POST /chat/stream` - Streaming conversation with full memory context\r\n- Request: `{\"session_id\": str, \"message\": str, \"stream\": bool}`\r\n- Response: Streaming SSE with full context injection\r\n\r\n**Memory Operations**:\r\n- `POST /memory/add` - Add memory to Neo4j graph\r\n- `POST /memory/search` - Semantic search with relationships  \r\n- `GET /memory/summaries` - Session summary retrieval\r\n- `POST /archivist/ingest` - Ingest content with distillation\r\n\r\n**Health & Info**:\r\n- `GET /health` - Server health check\r\n- `GET /v1/models` - Available models\r\n- `GET /health/memory` - Memory system status\r\n\r\n**MCP Integration** (when enabled):\r\n- `GET /mcp/tools` - Available memory tools\r\n- `POST /mcp/call` - Execute memory tools\r\n\r\n## Configuration\r\n\r\n### Required Parameters (in `.env` or config.yaml)\r\n- `NEO4J_URI` - Neo4j connection URI (default: bolt://localhost:7687)\r\n- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)\r\n- `LLM_MODEL_PATH` - Path to GGUF model file\r\n- `ECE_HOST` - Host for ECE server (default: 127.0.0.1)\r\n- `ECE_PORT` - Port for ECE server (default: 8000)\r\n\r\n### Optional Parameters\r\n- `ECE_REQUIRE_AUTH` - Enable API token authentication (default: false)\r\n- `ECE_API_KEY` - Static API key when auth enabled\r\n- `MCP_ENABLED` - Enable Model Context Protocol integration (default: true)\r\n- `VERIFIER_AGENT_ENABLED` - Enable truth-checking agent (default: true)\r\n- `ARCHIVIST_AGENT_ENABLED` - Enable memory maintenance agent (default: true)\r\n- `DISTILLER_AGENT_ENABLED` - Enable summarization agent (default: true)\r\n\r\n## Security\r\n\r\n### Authentication\r\n- Optional API token authentication (controlled by `ECE_REQUIRE_AUTH`)\r\n- Session isolation with UUID-based session IDs\r\n- Memory access limited to owner's session\r\n\r\n### Authorization\r\n- Path restrictions on filesystem operations\r\n- Command whitelisting for shell execution\r\n- Rate limiting on all endpoints\r\n- Input validation on all parameters\r\n\r\n### Data Protection\r\n- All data stored locally by default\r\n- End-to-end encryption for sensitive memories (optional)\r\n- Audit logging for all memory operations\r\n- Traceability for automated repairs and context rotations\r\n\r\n## Performance Optimization\r\n\r\n### Hardware Recommendations\r\n- **Minimum**: 16GB RAM, CUDA-capable GPU (RTX series)\r\n- **Recommended**: 32GB+ RAM, RTX 4090 or similar\r\n- **Context Windows**: 64k requires ~8GB VRAM for KV cache with 7B-14B models\r\n\r\n### Memory Management\r\n- **Hot Cache**: Redis for active session context (24h TTL)\r\n- **Cold Storage**: Neo4j for persistent memories with relationships\r\n- **Context Rotation**: Automatic compression of old context when approaching limits\r\n- **Caching Strategy**: L1 (Redis) for active context, L2 (Neo4j) for historical context\r\n\r\n## Integration Points\r\n\r\n### With Anchor CLI\r\n- HTTP API communication on configured port (default: 8000)\r\n- Streaming responses via Server-Sent Events\r\n- Memory operations through dedicated endpoints\r\n\r\n### With Browser Extension\r\n- HTTP API communication for context injection and memory saving\r\n- Streaming chat interface via Side Panel\r\n- Page content reading and memory ingestion\r\n\r\n### With LLM Servers\r\n- OpenAI-compatible API for LLM communication\r\n- Streaming response handling via SSE\r\n- Context window management with rotation protocol"
    tokens: 3580
    size: 9921
  - path: specs\standards\001-windows-console-encoding.md
    content: |-
      # Standard 001: Windows Console Encoding

      ## What Happened?
      The Python Bridge (`webgpu_bridge.py`) crashed immediately upon launch on Windows 11. The error was `UnicodeEncodeError: 'charmap' codec can't encode character...`.

      ## The Cost
      - 3 failed integration attempts.
      - "Integration Hell" state requiring full manual intervention.
      - Bridge stability compromised during demos.

      ## The Rule
      1. **Explicit Encoding:** All Python scripts outputting to stdout must explicitly handle encoding.
      2. **The Fix:** Include this snippet at the top of all entry points:
         ```python
         import sys
         if sys.platform == "win32":
             sys.stdout.reconfigure(encoding='utf-8')
         ```

      3. **Validation:** CI/CD or startup scripts must verify the bridge launches without encoding errors.
    tokens: 297
    size: 774
  - path: specs\standards\002-cache-api-security-policy.md
    content: |-
      # Standard 002: Cache API Security Policy

      ## What Happened?
      Browser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as "data in RAM" rather than "persistent storage", causing initialization failures.

      ## The Cost
      - Multiple failed model loading attempts
      - Browser security errors preventing WebGPU initialization
      - "Stealth Mode" required to bypass strict policies
      - Significant debugging time to understand browser security model

      ## The Rule
      1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:
         ```python
         class NoCacheStaticFiles(StaticFiles):
             async def __call__(self, scope, receive, send):
                 async def send_wrapper(message):
                     if message['type'] == 'http.response.start':
                         headers = message.get('headers', [])
                         headers.extend([
                             (b"Cache-Control", b"no-store, no-cache, must-revalidate, proxy-revalidate"),
                             (b"Pragma", b"no-cache"),
                             (b"Expires", b"0"),
                         ])
                         message['headers'] = headers
                     await send(message)
                 await super().__call__(scope, receive, send_wrapper)
         ```

      2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.

      3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components.
    tokens: 578
    size: 1579
  - path: specs\standards\003-webgpu-initialization-stability.md
    content: |-
      # Standard 003: WebGPU Initialization Stability

      ## What Happened?
      WebGPU failed to initialize properly in headless browsers, causing GPU access failures and preventing AI model execution. This occurred because browsers require visible windows for GPU access in some configurations.

      ## The Cost
      - Failed AI model execution in headless environments
      - Hours of debugging GPU initialization issues
      - Unreliable AI processing in automated systems
      - Need for complex workarounds to achieve stable GPU access

      ## The Rule
      1. **Minimized Window Approach:** Always use `--start-minimized` flag when launching headless browsers that require GPU access:
         ```bash
         start "Ghost Engine" /min msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222
         ```

      2. **GPU Buffer Configuration:** Implement 256MB override for Adreno GPUs and other constrained hardware:
         ```javascript
         // In WebGPU configuration
         const adapter = await navigator.gpu.requestAdapter({
             powerPreference: 'high-performance',
             forceFallbackAdapter: false
         });
         ```

      3. **Hardware Abstraction Layer:** Use clamp buffer techniques for Snapdragon/Mobile stability to prevent VRAM crashes.

      4. **Consciousness Semaphore:** Ensure resource arbitration between different components to prevent GPU memory conflicts.
    tokens: 501
    size: 1344
  - path: specs\standards\004-wasm-memory-management.md
    content: |-
      # Standard 004: WASM Memory Management

      ## What Happened?
      WASM applications experienced "memory access out of bounds" errors and crashes when handling large JSON payloads or complex database operations. This was particularly problematic in `sovereign-db-builder.html` and `unified-coda.html` where JSON parameters were passed to `db.run()`.

      ## The Cost
      - Crashes during database operations in browser-based CozoDB
      - "Maximum call stack size exceeded" errors with large JSON payloads
      - Unreliable memory operations in browser-based systems
      - Hours of debugging memory access violations in WASM

      ## The Rule
      1. **JSON Stringification:** Always properly stringify JSON parameters before passing to WASM functions:
         ```javascript
         // Before calling db.run() or similar WASM functions
         const jsonString = JSON.stringify(data);
         db.run(query, jsonString);
         ```

      2. **Payload Size Limits:** Implement size checks before processing large JSON payloads in browser workers:
         ```javascript
         if (JSON.stringify(payload).length > MAX_SAFE_SIZE) {
             // Handle large payloads differently or chunk them
         }
         ```

      3. **Error Handling:** Add timeout protection and fallback mechanisms for hanging WASM calls:
         ```javascript
         try {
             const result = await Promise.race([
                 db.run(query),
                 new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))
             ]);
         } catch (error) {
             // Handle timeout or memory errors gracefully
         }
         ```
    tokens: 561
    size: 1498
  - path: specs\standards\005-model-loading-configuration.md
    content: |-
      # Standard 005: Model Loading Configuration & Endpoint Verification

      ## What Happened?
      Model loading failed due to various configuration issues including "Cannot find model record" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.

      ## The Cost
      - Failed model initialization preventing AI functionality
      - Multiple 404 errors for specific model types
      - 503 and 405 errors during embedding and model download requests
      - Hours spent debugging model configuration issues
      - Unreliable model loading across different model types
      - Significant time wasted discovering that documented endpoints didn't exist in the backend
      - Frontend-backend integration failures due to missing API endpoints

      ## The Rule
      1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:
         ```python
         # Example mapping for problematic models
         MODEL_MAPPINGS = {
             'OpenHermes': 'Mistral-v0.3',
             'NeuralHermes': 'Mistral-v0.3',
             # Add other mappings as needed
         }
         ```

      2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:
         ```python
         # In webgpu_bridge.py - ensure flexible model name handling
         # Don't validate model names strictly on the bridge side
         ```

      3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:
         ```javascript
         // In frontend code
         const internalModelId = getModelInternalId(userModelName);
         const modelUrl = getModelUrl(internalModelId);
         ```
    tokens: 718
    size: 1903
  - path: specs\standards\006-model-url-construction.md
    content: |-
      # Standard 006: Model URL Construction for MLC-LLM Compatibility

      ## What Happened?
      The Anchor Console (`chat.html`) failed to load models with the error "TypeError: Failed to construct 'URL': Invalid URL", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.

      ## The Cost
      - 4+ hours debugging model loading failures
      - Confusion between working and failing components
      - Inconsistent model loading across different UI components
      - User frustration with non-functional chat interface
      - Multiple failed attempts with different URL construction approaches

      ## The Rule
      1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path:path}` endpoint to redirect MLC-LLM requests to local model files:
         ```python
         @app.get("/models/{model_name}/resolve/main/{file_path:path}")
         async def model_resolve_redirect(model_name: str, file_path: str):
             import os
             from fastapi.responses import FileResponse, JSONResponse

             models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models")
             actual_path = os.path.join(models_base, model_name, file_path)

             if os.path.exists(actual_path) and os.path.isfile(actual_path):
                 return FileResponse(actual_path)
             else:
                 return JSONResponse(status_code=404, content={
                     "error": f"File {file_path} not found for model {model_name}"
                 })
         ```

      2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.

      3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.

      4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.

      5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly.
    tokens: 841
    size: 2222
  - path: specs\standards\007-model-loading-transition.md
    content: |-
      # Standard 007: Model Loading Transition - Online-Only Implementation

      ## What Happened?
      The Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.

      The old implementation in `chat.html` was trying to:
      1. Check for local model files using the `/models/{model}/resolve/main/` pattern
      2. Download models through the bridge if not found locally
      3. Use a complex configuration with multiple model entries and local file resolution

      This approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.

      ## The Cost
      - Hours spent debugging model loading failures in `chat.html`
      - Confusion between working and failing components (anchor-mic.html vs chat.html)
      - Inconsistent model loading across different UI components
      - User frustration with non-functional chat interface
      - Time wasted on attempting to fix complex local model resolution logic
      - Delayed development due to complex debugging of the local file + bridge download approach

      ## The Rule
      1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:
         ```javascript
         // Use direct HuggingFace URLs like anchor-mic.html
         const appConfig = {
             model_list: [{
                 model: "https://huggingface.co/" + selectedModelId + "/resolve/main/",
                 model_id: selectedModelId,
                 model_lib: modelLib,  // WASM library URL
                 // ... other config
             }],
             useIndexedDBCache: false, // Disable caching to prevent issues
         };
         ```

      2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.

      3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:
    tokens: 849
    size: 2208
  - path: specs\standards\008-model-loading-online-only.md
    content: |-
      # Standard 008: Model Loading - Online-Only Approach for Browser Implementation

      ## What Happened?
      The Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.

      The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.

      ## The Cost
      - All models showing as unavailable in API tests
      - Confusion between working and failing components
      - Inconsistent model loading across different UI components
      - User frustration with limited model availability
      - Time wasted on attempting to fix complex local model resolution logic
      - Delayed development due to complex debugging of the local file + bridge download approach

      ## The Rule
      1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:
         ```javascript
         // Use direct HuggingFace URLs like anchor-mic.html
         const appConfig = {
             model_list: [{
                 model: window.location.origin + "/models/" + selectedModelId, // This will redirect to online source
                 model_id: selectedModelId,
                 model_lib: modelLib,  // WASM library URL
                 // ... other config
             }],
             useIndexedDBCache: false, // Disable caching to prevent issues
         };
         ```

      2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.

      3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.

      4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.

      5. **Bridge Redirect Endpoint**: Ensure the `/models/{model_name}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist.
    tokens: 909
    size: 2370
  - path: specs\standards\009-model-loading-configuration.md
    content: |-
      # Standard 009: Model Loading Configuration - Bridge vs Direct Online

      ## What Happened?
      The Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:

      1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources
      2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser

      The inconsistency occurred because:
      - Some components (like `anchor-mic.html`) work with direct online URLs
      - Other components (like `chat.html`) were configured for local file resolution
      - The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist

      ## The Cost
      - Confusion about which model loading approach to use
      - Inconsistent behavior across different UI components
      - Models working in some components but not others
      - Debugging time spent on understanding different loading mechanisms
      - Users experiencing different model availability depending on which UI they use

      ## The Rule
      1. **Consistent Model Configuration**: All UI components should use the same model loading approach:
         ```javascript
         // Recommended configuration pattern
         const modelConfig = {
             model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect
             model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID
             model_lib: modelLib,                                  // WASM library URL
         };
         ```

      2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:
         - First check for local files in the models directory
         - If local file doesn't exist, redirect to the corresponding HuggingFace URL:
           `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`

      3. **Fallback Handling**: Implement proper fallback when local files are not available:
         ```javascript
    tokens: 746
    size: 1972
  - path: specs\standards\012-context-utility-manifest.md
    content: |-
      # Standard 012: Context Utility Manifest - The Invisible Infrastructure

      ## What Happened?
      The Anchor Core system was originally conceived as a chat application, but has evolved into a unified cognitive infrastructure. The system now needs to transition from "active user input" to "passive observation" to function as truly invisible infrastructure like electricity - always present but never demanding attention.

      ## The Cost
      - UI bloat with multiple chat interfaces competing for user attention
      - Manual data entry required to populate context
      - Users having to copy/paste information instead of automatic capture
      - Architecture treating UI as primary rather than as debugging tool
      - Missing opportunity to create true "ambient intelligence"

      ## The Rule
      1. **Headless by Default**: All core functionality must operate without user interface interaction
         ```python
         # Core services run as background daemons
         daemon_services = [
             "memory_graph",      # CozoDB persistence
             "gpu_engine",        # WebLLM inference
             "context_capture",   # Screen/Audio observation
             "data_ingestion"     # Memory writing
         ]
         ```

      2. **Passive Observation**: System captures context automatically rather than waiting for user input
         - **Eyes**: Automated screen sampling and OCR
         - **Ears**: Continuous audio transcription (when enabled)
         - **Memory**: Automatic ingestion without user intervention

      3. **Architecture Priority**: `webgpu_bridge.py` is the nervous system; UIs are merely debugging/interaction tools
         - UIs are temporary visualization layers
         - Core logic exists independently of any UI
         - Background services operate without UI presence

      4. **Invisible Utility**: The system should function like electricity - always available, rarely noticed, essential infrastructure
         - Zero user interaction required for core functions
         - Automatic context capture and storage
         - Seamless integration with user's workflow
    tokens: 725
    size: 1957
  - path: specs\standards\014-async-await-best-practices.md
    content: |-
      # Standard 014: Async/Await Best Practices for FastAPI

      ## What Happened?
      The system had multiple "coroutine was never awaited" warnings due to improper async/await usage in the webgpu_bridge.py. These warnings occurred when async functions were called without being properly awaited or when they weren't integrated correctly with FastAPI's event loop system.

      ## The Cost
      - Runtime warnings cluttering the console output
      - Potential resource leaks from improperly handled async operations
      - Unpredictable behavior in WebSocket connections and API endpoints
      - Difficulty debugging real issues due to noise from async warnings

      ## The Rule
      1. **Proper Await Usage**: All async functions must be awaited when called within async contexts
         ```python
         # Correct
         await add_log_entry("source", "type", "message")

         # Incorrect
         add_log_entry("source", "type", "message")  # Creates unawaited coroutine
         ```

      2. **Event Loop Integration**: When scheduling tasks at module level, ensure they run within an active event loop:
         ```python
         # Correct - in startup event
         async def startup_event():
             await add_log_entry("System", "info", "Service started")

         # Incorrect - at module level before event loop starts
         # asyncio.create_task(add_log_entry(...))  # Will cause warning
         ```

      3. **FastAPI Event Handlers**: Use FastAPI's event system (`@app.on_event("startup")`) for initialization tasks that require async operations

      4. **Background Tasks**: For fire-and-forget async operations, use FastAPI's BackgroundTasks or properly scheduled asyncio tasks within request handlers

      5. **WebSocket Cleanup**: Always ensure proper cleanup of async resources in WebSocket exception handlers to prevent resource leaks

      6. **Exception Handling**: Wrap async operations in try/catch blocks that properly handle async exceptions and clean up resources
    tokens: 709
    size: 1862
  - path: specs\standards\017-file-ingestion-debounce-hash-checking.md
    content: |-
      # Standard 017: File Ingestion Debounce and Hash Checking

      ## What Happened?
      The Watchdog service was triggering excessive memory ingestion when modern editors (VS Code, Obsidian) would autosave files frequently. This caused "Memory Churn" in CozoDB with duplicate content being ingested repeatedly, fragmenting the database and spiking CPU usage.

      ## The Cost
      - High CPU usage from repeated ingestion of unchanged content
      - Database fragmentation from duplicate entries
      - Poor performance during active editing sessions
      - 2+ hours spent implementing debounce and hash checking to prevent "Autosave Flood"

      ## The Rule
      1. **Debounce File Events**: Implement a debounce mechanism that waits for a period of silence before processing file changes:
         ```python
         # Wait for debounce period before processing
         debounce_time = 2.0  # seconds
         ```

      2. **Content Hash Verification**: Calculate MD5 hash of file content before ingestion and compare with previously ingested version:
         ```python
         import hashlib
         current_hash = hashlib.md5(content).hexdigest()
         if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:
             # Skip ingestion - content hasn't changed
             return
         ```

      3. **Cancel Pending Operations**: Cancel any existing debounce timer when a new file event occurs for the same file:
         ```python
         if file_path in self.debounce_timers:
             self.debounce_timers[file_path].cancel()
         ```

      4. **Proper Cleanup**: Clean up debounce timer references after processing:
         ```python
         if file_path in self.debounce_timers:
             del self.debounce_timers[file_path]
         ```
    tokens: 612
    size: 1625
  - path: specs\standards\019-code-file-ingestion-comprehensive-context.md
    content: |-
      # Standard 019: Code File Ingestion for Comprehensive Context

      ## What Happened?
      The Watchdog service was only monitoring text files (.txt, .md, .markdown) but ignoring code files which represent a significant portion of developer context. This created an "Ingestion Blind Spot" where the system was blind to codebase context.

      ## The Cost
      - Limited context ingestion for developers
      - Missing important code-related information
      - 30 minutes spent updating watchdog.py to include code extensions

      ## The Rule
      1. **Expand File Extensions**: Include common programming language extensions in file monitoring:
         ```python
         enabled_extensions = {".txt", ".md", ".markdown", ".py", ".js", ".html", ".css",
                               ".json", ".yaml", ".yml", ".sh", ".bat", ".ts", ".tsx",
                               ".jsx", ".xml", ".sql", ".rs", ".go", ".cpp", ".c", ".h", ".hpp"}
         ```

      2. **Comprehensive Coverage**: Monitor all relevant text-based file types that contain context

      3. **Maintain Performance**: Ensure file size limits still apply to prevent performance issues with large code files

      This standard ensures that developer context is fully captured by including code files in passive ingestion.
    tokens: 456
    size: 1205
  - path: specs\standards\021-chat-session-persistence-context-continuity.md
    content: |-
      # Standard 021: Chat Session Persistence for Context Continuity

      ## What Happened?
      The anchor.py CLI client maintained conversation history only in memory. If the terminal was closed or the CLI crashed, the entire conversation history was lost. This created a "Lost Context" risk where valuable conversation history was not preserved.

      ## The Cost
      - Loss of conversation history on CLI crashes or termination
      - Broken loop between active chatting and long-term memory
      - 45 minutes spent implementing chat session persistence to context folder

      ## The Rule
      1. **Auto-Save Sessions**: Automatically save each chat message to a session file:
         ```python
         def save_message_to_session(role, content):
             # Create timestamped session file in context/sessions/
             # Append each message as it occurs
         ```

      2. **Session Directory**: Create a dedicated `context/sessions/` directory for chat logs:
         ```python
         SESSIONS_DIR = os.path.join(CONTEXT_DIR, "sessions")
         os.makedirs(SESSIONS_DIR, exist_ok=True)
         ```

      3. **Markdown Format**: Save conversations in markdown format for easy reading and processing:
         ```python
         # Format: ## Role\nContent\n\n for each message
         ```

      4. **Loop Closure**: Ensure that chat sessions become text files that the Watchdog service can monitor and ingest into long-term memory automatically.

      This standard ensures conversation history survives CLI crashes and becomes part of the persistent context.
    tokens: 558
    size: 1448
  - path: specs\standards\022-text-file-source-of-truth-cross-machine-sync.md
    content: |-
      # Standard 022: Text-File Source of Truth for Cross-Machine Sync

      ## What Happened?
      The CozoDB database lives in IndexedDB inside the headless browser profile, making it impossible to sync between machines. Chat history and learned connections were trapped in the browser instance and lost when switching laptops. The system needed a "Text-File Source of Truth" approach where the database is treated as a cache and all important data is stored in text files.

      ## The Cost
      - Lost conversation history when switching between machines
      - Inability to sync learned connections and context across devices
      - 1 hour spent implementing daily session files and text-file persistence

      ## The Rule
      1. **Database is Cache**: Treat CozoDB as a cache, not the source of truth:
         ```python
         # All important data must exist in text files first
         # Database is rebuilt from text files on each machine
         ```

      2. **Daily Session Files**: Create daily markdown files for chat persistence:
         ```python
         def ensure_session_file():
             date_str = datetime.now().strftime("%Y-%m-%d")
             filename = f"chat_{date_str}.md"
             # Creates daily consolidated session files
         ```

      3. **Text-File First**: All important information must be written to text files:
         ```python
         # Every chat message gets saved to markdown file
         # Files are automatically ingested by watchdog service
         # Creates infinite loop: Chat -> File -> Ingestion -> Memory -> Next Chat
         ```

      4. **Cross-Machine Sync**: Use Dropbox/Git for file synchronization:
         ```python
         # Text files sync automatically via Dropbox/Git
         # Database rebuilds from text files on each machine
         # Ensures consistent context across all devices
         ```
    tokens: 665
    size: 1703
  - path: specs\standards\024-context-ingestion-pipeline-fix.md
    content: |-
      # Standard 024: Context Ingestion Pipeline - Field Name Alignment Protocol

      ## What Happened?
      The context ingestion pipeline was failing silently due to field name mismatches between the watchdog service and the ghost engine. The watchdog was sending `filetype` but the memory ingestion endpoint expected `file_type`, and the ghost engine was looking for `msg.filetype` instead of `msg.file_type`. This caused the database to appear empty even though files were being processed, resulting in failed context searches.

      ## The Cost
      - 2+ hours spent debugging why context files weren't appearing in the database
      - Confusion from "Database appears empty!" messages in ghost engine logs
      - Failed context searches returning no results despite files existing in context directory
      - Misleading "Ingested" messages in watchdog logs that masked the actual field name mismatch
      - Users experiencing broken context retrieval functionality

      ## The Rule
      1. **Field Name Consistency**: All components in the ingestion pipeline must use consistent field names:
         - Watchdog sends: `file_type`, `source`, `content`, `filename`
         - Bridge expects: `file_type`, `source`, `content`, `filename`
         - Ghost engine receives: `file_type`, `source`, `content`, `filename`

      2. **Payload Validation**: Always validate that field names match across the entire pipeline:
         ```javascript
         // In ghost.html handleIngest function
         await runQuery(query, {
             data: [[id, ts, msg.content, msg.source || msg.filename, msg.file_type || "text"]]
         });
         ```

      3. **Source Identification**: The watchdog must send a proper source identifier instead of relying on default "unknown" values

      4. **Error Reporting**: Include detailed error messages when ingestion fails to help with debugging
    tokens: 673
    size: 1762
  - path: specs\standards\027-no-resurrection-mode.md
    content: |-
      # Standard 027: No Resurrection Mode for Manual Ghost Engine Control

      ## What Happened?
      The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues for users who wanted to use an existing browser window or manually control when the Ghost Engine connects. Users needed an option to disable the automatic resurrection protocol and connect the Ghost Engine manually when needed.

      ## The Cost
      - Unnecessary browser processes launched automatically
      - Resource usage when Ghost Engine not needed
      - Inability to use existing browser windows for Ghost Engine operations
      - Confusion when multiple browser instances were running
      - Users wanting more control over when the Ghost Engine connects

      ## The Rule
      1. **Environment Variable Control**: The system must support a `NO_RESURRECTION_MODE=true` environment variable to disable automatic Ghost Engine launching.

      2. **Conditional Launch**: When `NO_RESURECTION_MODE=true`, the system shall NOT automatically launch the Ghost Engine during startup.

      3. **Manual Connection**: In no resurrection mode, users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

      4. **Clear Messaging**: The system shall provide clear instructions to users when no resurrection mode is enabled, indicating they need to open ghost.html manually.

      5. **Resource Management**: When no resurrection mode is enabled, the system shall not attempt to kill browser processes during shutdown.

      6. **API Behavior**: API endpoints that require the Ghost Engine shall return appropriate 503 errors with clear messaging when the Ghost Engine is disconnected, regardless of resurrection mode setting.

      ## Implementation
      - Set environment variable: `set NO_RESURRECTION_MODE=true` before running `start-anchor.bat`
      - The Bridge will log a message indicating manual connection is required
      - Users open `http://localhost:8000/ghost.html` in their browser to connect the Ghost Engine
      - All functionality remains the same, just with manual control over Ghost Engine connection
    tokens: 818
    size: 2089
  - path: specs\standards\028-default-no-resurrection-mode.md
    content: |-
      # Standard 028: Configuration-Driven System with Default No Resurrection Mode

      ## What Happened?
      The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues with resource usage and prevented users from controlling when the Ghost Engine connects. The system now defaults to "No Resurrection Mode" where the Ghost Engine must be manually started by opening ghost.html in the browser. Additionally, ALL system variables are now abstracted to a central configuration file (config.json) to support future settings menu implementation.

      ## The Cost
      - Excessive resource usage from automatically launching headless browser
      - Browser processes that couldn't be controlled by the user
      - Confusion when multiple browser instances were running
      - Unnecessary complexity in the startup process
      - Users wanting more control over when the Ghost Engine connects
      - Hard-coded values throughout the codebase that made customization difficult

      ## The Rule
      1. **Default Behavior**: The system shall default to `NO_RESURRECTION_MODE=true`, meaning the Ghost Engine is not automatically launched.

      2. **Manual Connection**: Users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

      3. **Environment Override**: Users can set `NO_RESURRECTION_MODE=false` to return to auto-launching behavior.

      4. **Queued Operations**: When Ghost Engine is disconnected, operations shall be queued and processed when connection is established.

      5. **Clear Messaging**: The system shall provide clear instructions when Ghost Engine is not connected, indicating how to establish the connection.

      6. **Configurable Values**: All system parameters shall be configurable via the config.json file, including:
         - Server settings (port, host, CORS origins)
         - Ghost Engine settings (auto resurrection, browser paths, flags)
         - Logging configuration (max lines, directory, format)
         - Memory settings (max ingest size, default limits, char limits)
         - GPU management (enabled, concurrent ops, timeout)
         - Model loading (timeout, default model, base URL)
         - Watchdog settings (enabled, watch directory, allowed extensions, debounce time)

      7. **Detached Operation**: All scripts shall run in detached mode with logging to the logs/ directory as per Standard 025.

      ## Implementation
      - Default configuration sets `"ghost_engine.auto_resurrection_enabled": false`
      - The start-anchor.bat script defaults to NO_RESURRECTION_MODE=true
    tokens: 961
    size: 2502
  - path: specs\standards\029-consolidated-data-aggregation.md
    content: |-
      # Standard 029: Consolidated Data Aggregation with YAML Support

      ## What Happened?
      The system had multiple scripts performing similar functions for data aggregation and migration:
      - `migrate_history.py` - Legacy session migration to YAML
      - `read_all.py` in context directory - Data aggregation to JSON
      - Multiple overlapping data processing scripts

      This created redundancy and confusion about which script to use for data aggregation. The functionality has been consolidated into a single authoritative script: `context/Coding-Notes/Notebook/read_all.py` which now supports all three output formats (text, JSON, YAML).

      ## The Cost
      - Multiple scripts with overlapping functionality
      - Confusion about which script to use for data aggregation
      - Maintenance burden of multiple similar scripts
      - Inconsistent output formats across scripts
      - Redundant code that needed to be updated in multiple places

      ## The Rule
      1. **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation from the context directory.

      2. **Multi-Format Output**: The script must generate three output formats:
         - `combined_text.txt` - Human-readable text corpus
         - `combined_memory.json` - Structured JSON for database ingestion
         - `combined_memory.yaml` - Structured YAML for easier processing and migration

      3. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.

      4. **Encoding Handling**: The script must handle various file encodings using chardet for reliable processing.

      5. **Recursive Processing**: The script must process all subdirectories while respecting exclusion rules.

      6. **Metadata Preservation**: File metadata (path, timestamp) must be preserved in structured outputs.

      ## Implementation
      - Consolidated migrate_history.py functionality into read_all.py
      - Moved migrate_history.py to archive/tools/
      - Updated read_all.py to generate YAML output with proper multiline formatting
      - Used yaml.dump() with custom representer for multiline strings
      - Maintained all existing functionality while adding YAML support
      - Preserved the same exclusion rules and file type filtering
    tokens: 849
    size: 2229
  - path: specs\standards\030-multi-format-output.md
    content: |-
      # Standard 030: Multi-Format Output for Project Aggregation

      ## What Happened?
      The `read_all.py` script in the root directory was only generating text and JSON outputs for project aggregation. To improve compatibility with various processing tools and follow the documentation policy of supporting YAML format, the script was updated to also generate a YAML version of the memory records.

      ## The Cost
      - Limited output format options for downstream processing
      - Inconsistency with the documentation policy that prefers YAML for configuration and data exchange
      - Missing opportunity to provide easily readable structured data in YAML format
      - Users had to convert JSON to YAML if they needed that format

      ## The Rule
      1. **Multi-Format Output**: The `read_all.py` script must generate both JSON and YAML versions of memory records.

      2. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.

      3. **Consistent Naming**: Output files should follow consistent naming patterns:
         - `combined_text.txt` - Aggregated text content
         - `combined_memory.json` - Structured JSON memory records
         - `combined_text.yaml` - Structured YAML memory records

      4. **Custom Representers**: Use custom YAML representers to handle multiline strings appropriately with the `|` indicator.

      5. **Encoding Handling**: Ensure proper UTF-8 encoding for both input and output operations.

      ## Implementation
      - Updated `read_all.py` to import and use the `yaml` module
      - Added custom string representer for multiline content
      - Created separate YAML output file with proper formatting
      - Maintained all existing functionality while adding YAML support
      - Used `yaml.dump()` with appropriate parameters for clean output
    tokens: 690
    size: 1784
  - path: specs\standards\032-ghost-engine-initialization-flow.md
    content: |-
      # Standard 032: Ghost Engine Initialization and Ingestion Flow

      ## What Happened?
      The Ghost Engine was experiencing race conditions where memory ingestion requests were being processed before the database was fully initialized. This caused errors like "Cannot read properties of null (reading 'run')" and inconsistent ingestion behavior between the Bridge API logs and the Ghost Engine logs.

      ## The Cost
      - Database ingestion failures when Ghost Engine connected to Bridge before database initialization completed
      - Inconsistent logging between Bridge and Ghost Engine (Bridge showing success, Ghost Engine showing failures)
      - Race conditions where ingestion requests arrived before database was ready
      - Poor user experience with failed memory operations
      - Confusing error messages in the UI

      ## The Rule
      1. **Sequential Initialization**: The Ghost Engine must initialize the database completely before signaling readiness to the Bridge.

      2. **Database Readiness Checks**: All ingestion and search operations must verify that the database object is properly initialized before attempting operations.

      3. **Proper Error Handling**: When database is not ready, the Ghost Engine must return appropriate error messages to the Bridge instead of failing silently.

      4. **Synchronous Connection Flow**: WebSocket connection must follow: Connect  Initialize Database  Signal Ready  Process Requests.

      5. **Graceful Degradation**: If database initialization fails, the Ghost Engine must report the error to the Bridge and not attempt to process requests.

      6. **Message Type Handling**: The system must properly handle all message types including `engine_error` responses.

      ## Implementation
      - Modified WebSocket connection flow to initialize database before signaling readiness
      - Added database readiness checks in `handleIngest` and `handleSearch` functions
      - Implemented proper error responses when database is not ready
      - Added support for `engine_error` message type handling
      - Enhanced error logging with fallbacks to prevent "undefined" messages
      - Ensured sequential processing: Connect  DB Init  Ready Signal  Process Requests
    tokens: 812
    size: 2141
  - path: specs\standards\058_universal_rag_api.md
    content: "# Standard 058: UniversalRAG API & Modality-Aware Search\r\n\r\n**Status:** Active | **Type:** Architectural Constraint | **Created:** 2026-01-16\r\n\r\n## The Triangle of Pain\r\n\r\n1.  **What Happened:** The initial search API (`GET /v1/memory/search`) relied on URL parameters, making it impossible to support complex RAG queries involving modality routing (Buckets) and provenance filtering. Additionally, native database exports proved opaque and brittle, risking data lock-in.\r\n2.  **The Cost:** Agent confusion (\"wobbling\") due to ambiguous \"legacy support\" directives, inability to implement \"Deep Research\" features, and risk of losing historical context if the database engine changes.\r\n3.  **The Rule:** \r\n    *   **Strict POST:** All semantic search operations MUST use `POST /v1/memory/search` with a structured JSON body conforming to the `SearchRequest` interface.\r\n    *   **Universal Context Routing:** \"Buckets\" are strictly mapped to \"Modalities\" (e.g., `@code`, `@memory`, `@visual`).\r\n    *   **Sovereign Dump:** Backups MUST be human-readable JSON streams (`GET /v1/backup`), never binary database exports.\r\n\r\n## The Standard\r\n\r\n### 1. UniversalRAG Interface\r\nThe search endpoint is the \"Central Nervous System\" of the engine. It does not just \"look up keywords\"; it routes intent.\r\n\r\n```typescript\r\nexport interface SearchRequest {\r\n  query: string;           // Natural language intent\r\n  limit?: number;          // Default: 20\r\n  deep?: boolean;          // True = Trigger Dreamer/Epochal layers\r\n  buckets?: string[];      // Modalities: [\"@code\", \"@visual\", \"@memory\"]\r\n  provenance?: 'sovereign' | 'external' | 'all';\r\n}\r\n```\r\n\r\n### 2. Modality Mapping\r\nBuckets are not arbitrary folders. They define the *Type of Mind* required:\r\n*   `@code`  Source code focus (`.ts`, `.py`, `.rs`). Prioritizes structural understanding.\r\n*   `@memory`  Chat logs, Dreamer epochs, and episodic history. Prioritizes temporal continuity.\r\n*   `@visual`  Image descriptions and spatial data.\r\n\r\n### 3. Sovereign Backup Strategy\r\nData sovereignty means the user owns the format.\r\n*   **Format:** Single JSON object.\r\n*   **Structure:**\r\n    ```json\r\n    {\r\n      \"timestamp\": \"ISO-8601\",\r\n      \"stats\": { \"memory_count\": N, \"engram_count\": N },\r\n      \"memories\": [ ... ],\r\n      \"engrams\": [ ... ]\r\n    }\r\n    ```\r\n*   **Portability:** This format is database-agnostic. It can be re-ingested into SQLite, Postgres, or a new CozoDB instance.\r\n\r\n## Implementation Requirements\r\n*   **Routes:** `POST /v1/memory/search`, `GET /v1/backup`\r\n*   **Legacy Support:** `GET` search endpoints should redirect or instruct users to use `POST`.\r\n*   **Streaming:** Chat interfaces (`/v1/chat/completions`) must support SSE (Server-Sent Events) for real-time feedback.\r\n"
    tokens: 1020
    size: 2766
  - path: specs\standards\059_reliable_ingestion.md
    content: "# Standard 059: Reliable Ingestion (The \"Ghost Data\" Protocol)\r\n\r\n**Status:** Active\r\n**Trigger:** Ingestion API returning 200 OK while failing to persist data to CozoDB.\r\n\r\n## 1. The Pain (Ghost Data & Silent Failures)\r\n*   **symptom:** The `POST /v1/ingest` endpoint returned `200 OK` with a valid ID, but the data was never written to the database.\r\n*   **Cost:** 6 hours of debugging search logic logic when ingestion was the root cause.\r\n*   **Risk:** Silent data loss. Users believe memories are saved when they are discarded.\r\n\r\n## 2. The Solution (Trust but Verify)\r\n1.  **Read-After-Write (RAW):** Every ingestion operation MUST perform a read query immediately after the write operation, *within the same request scope*, to verify persistence.\r\n    *   *Implementation:* insert `?[count] := *memory{id}, count(id)` or `?[id] := *memory{id}, id = $id`\r\n2.  **Count Validation:** The API MUST NOT return `200 OK` unless the Verification Count > 0 (or specifically matches expected count).\r\n3.  **Explicit Failure:** If verification fails, the API MUST return `500 Internal Server Error` with a standard error code (`INGEST_VERIFY_FAILED`).\r\n4.  **Logging:** The Verification Count must be logged to the critical path log (Console or File) with the prefix `[INGEST_VERIFY]`.\r\n\r\n## 4. Schema Alignment\r\n*   **Strict Column Order:** CozoDB's `<- $data` insertion is positional. The API array order MUST match the `::columns memory` order exactly.\r\n*   **Migration Integrity:** Any schema change (adding columns) requires a corresponding update to the `ingest.ts` data array *and* a verified migration of existing data using the Safe Restart Protocol.\r\n*   **Nuclear Fallback:** If automated migration fails persistently (e.g. index locks) and data volume is zero or recoverable (inbox-based), the system MAY auto-reset the database (delete/recreate) to ensure service availability.\r\n\r\n## 5. Metadata Mandatory\r\n*   **Source ID:** `source_id` is mandatory for all atoms.\r\n*   **Sequence:** `sequence` is mandatory (default 0).\r\n## 6. The Cleanup Protocol (Encoding & Sanitization)\r\n*   **Null Byte Stripping:** Ingested content MUST be scrubbed of null bytes (`\\x00`) and replacement characters (`\\uFFFD`). These cause `node-llama-cpp` tokenizer to bloat text significantly (1 char -> multiple tokens), leading to context overflows.\r\n*   **BOM Detection:** The system MUST detect UTF-16 LE/BE Byte Order Marks (BOM) and decode buffers accordingly before processing.\r\n*   **Strict Truncation:** To preserve system stability, embedding workers MUST truncate inputs to a safe factor of the context window (Recommended: `1.2 * ContextSize` characters) to prevent OOM or logic crashes on dense inputs (e.g., minified code).\r\n\r\n## 7. The Inbox Zero Protocol (Recursive Ingestion)\r\n*   **Recursive Scanning:** The Ingestion Engine MUST scan subdirectories within the `inbox/` folder.\r\n*   **Smart Bucketing:**\r\n    *   Files at `inbox/root.md` -> Bucket: `inbox`.\r\n    *   Files at `inbox/project-a/note.md` -> Bucket: `project-a`.\r\n    *   *Purpose:* This allows users to pre-organize content without it getting lost in a generic \"inbox\" tag.\r\n*   **Transient Tag Cleanup:** The \"inbox\" tag is considered transient. The Dreamer/Organization Agents MUST remove the `inbox` tag after processing/tagging, but MUST preserve specific subfolder tags (e.g. `project-a`) to respect user intent.\r\n"
    tokens: 1320
    size: 3386
  - path: specs\standards\060_worker_system.md
    content: |

      # Standard 060: Worker System Architecture

      **Supersedes**: N/A (New Standard)
      **Effective Date**: 2026-01-16
      **Status**: Active

      ## 1. Dual-Worker Model
      To resolve concurrency issues (blocking during ingestion), ECE_Core uses a dedicated worker model.

      ### 1.1 ChatWorker
      - **File**: `src/core/inference/ChatWorker.ts`
      - **Role**: Handles conversational inference only.
      - **Model**: Loaded from `LLM_MODEL_PATH`.
      - **Context**: Managed via `LlamaChatSession`.

      ### 1.2 EmbeddingWorker
      - **File**: `src/core/inference/EmbeddingWorker.ts`
      - **Role**: Handles vector generation only.
      - **Model**: Loaded from `LLM_EMBEDDING_MODEL_PATH`.
      - **Context**: Managed via `LlamaEmbeddingContext`.
      - **Note**: If `LLM_EMBEDDING_MODEL_PATH` is unset, the system falls back to `HybridWorker` (shared model).

      ## 2. Provider Routing
      - `src/services/llm/provider.ts` is the orchestrator.
      - It detects the configuration state and spawns the appropriate workers.
      - **Dedicated Mode**: Spawns both workers. Routes `chat` -> ChatWorker, `embed` -> EmbeddingWorker.
      - **Shared Mode**: Spawns `HybridWorker`. Routes all traffic to it.

      ## 3. Communication Protocol
      - Workers communicate via `parentPort` messages.
      - **Types**: `loadModel`, `chat`, `getEmbeddings`.
      - **Error Handling**: Workers must wrap main logic in `try/catch` and send `type: 'error'` on failure.
    tokens: 496
    size: 1348
  - path: specs\standards\061_context_logic.md
    content: "\r\n# Standard 061: Context Management Logic\r\n\r\n**Supersedes**: N/A (New Standard)\r\n**Effective Date**: 2026-01-16\r\n**Status**: Active\r\n\r\n## 1. Rolling Context Assembly\r\nECE_Core uses a \"Middle-Out\" budgeting strategy to maximize context relevance while preserving narrative flow.\r\n\r\n### 1.1 Selection Pipeline\r\n1.  **Temporal Analysis**:\r\n    *   If query contains `[\"recent\", \"latest\", \"today\", \"now\", \"current\"]`:\r\n        *   **Recency Weight**: 60%\r\n        *   **Relevance Weight**: 40%\r\n    *   Otherwise:\r\n        *   **Recency Weight**: 30%\r\n        *   **Relevance Weight**: 70%\r\n2.  **Scoring**: Atoms are ranked by `MixedScore` (Relevance * W1 + Recency * W2).\r\n3.  **Budgeting**: Atoms fill the `TokenBudget` starting from highest score.\r\n\r\n### 1.2 Safety Constraints\r\n- **Token Buffer**: The target budget is effectively `min(ConfiguredBudget, 3800)` to provide a ~300 token safety margin against CJK/multibyte inflation and tokenizer mismatches.\r\n- **Smart Slicing**:\r\n    *   Atoms are NOT cut mid-sentence.\r\n    *   The slicer looks for punctuation (`.`, `!`, `?`, `\\n`) within the last 50-100 characters of the remaining budget.\r\n    *   If no punctuation is found, it falls back to a hard cut with `...`\r\n\r\n### 1.3 Assembly\r\n- **Re-Sorting**: After selection, atoms are re-sorted **Chronologically** to present a linear narrative to the LLM.\r\n- **Formatting**: Each atom is prefixed with `[Source: <filename>] (<ISO-Date>)`.\r\n"
    tokens: 541
    size: 1443
  - path: specs\standards\062_inference_stability.md
    content: "\r\n# Standard 062: Inference Worker Stability\r\n\r\n**Status:** Active\r\n**Context:** Local LLM/Embedding inference via `node-llama-cpp` or similar bindings.\r\n\r\n## 1. The Pain (Context Explosions)\r\n*   **Symptom:** Worker threads crashing with `Input is longer than context size` errors during background embedding.\r\n*   **Cause:** \"Dense Text\" (Minified code, base64, foreign languages) can have a 1:1 Character-to-Token ratio. A 6000-char chunk becomes 6000 tokens, overflowing a 2048-token context.\r\n*   **Risk:** System instability, lost data, and endless retry loops.\r\n\r\n## 2. The Solution (Dynamic Safety)\r\n### A. Context Awareness\r\n*   **Dynamic Configuration:** Workers MUST read the actual `CTX_SIZE` from load options, not assume 4096.\r\n\r\n### B. The \"Safe Ratio\" Rule\r\n*   **Logic:** Truncate input text *before* tokenization using a conservative safety factor.\r\n*   **Formula:** `SafeLength = floor(ContextSize * 1.2)`\r\n    *   Example: 2048 tokens * 1.2 = 2457 chars.\r\n*   **Blob Strategy:** For detected dense content (avg line len > 300), use an even stricter hard limit (e.g. 1500 chars) to guarantee safety.\r\n\r\n## 3. Worker Isolation\r\n*   **Error Containment:** A crash in a worker (e.g., CUDA error) MUST NOT crash the main process.\r\n*   **Queue Resilience:** If a batch fails, the worker should attempt to recover or return a partial result (e.g., empty embeddings for failed items) rather than hanging the queue indefinitely.\r\n\r\n## 4. The \"Ghost CUDA\" Patch\r\n*   **Symptom:** Setting `GPU_LAYERS=0` for a worker still results in CUDA initialization and VRAM usage (leading to OOM).\r\n*   **Cause:** `node-llama-cpp` by default eagerly initializes the best available backend (CUDA) even if `gpuLayers` is 0.\r\n*   **Fix:** Workers MUST explicitly check for `GPU_LAYERS=0` in their `init()` sequence and pass `gpu: { exclude: ['cuda'] }` to the loading configuration.\r\n*   **Rule:** \"Zero means Zero\". If the user requests 0 GPU layers, the CUDA backend should not even be loaded.\r\n"
    tokens: 780
    size: 1992
  - path: specs\standards\063_cozo_db_syntax.md
    content: "\r\n# Standard 063: CozoDB Syntax & Schema Patterns\r\n\r\n**Status:** Active\r\n**Context:** CozoDB (RocksDB Backend) via `cozo-node` binding.\r\n\r\n## 1. Syntax Criticals (The \"Parser Traps\")\r\nThe `cozo-node` parser is stricter/different than some Rust documentation implies.\r\n1.  **Vector Columns:** MUST use angle brackets with dimensions.\r\n    *    Correct: `embedding: <F32; 384>`\r\n    *    Incorrect: `embedding: [F32; 384]`, `embedding: Float32Array`\r\n2.  **Assignment Operator:** MUST be `<-` (no spaces).\r\n    *    Correct: `... <- $data`\r\n    *    Incorrect: `... < - $data` (Causes `eval::named_field_not_found`)\r\n3.  **Insertion Verb:** Use `:put`.\r\n    *    Correct: `:put memory { ... }`\r\n    *    Risky: `:insert`, `:replace` (Behavior varies by version/context)\r\n\r\n## 2. HNSW Index Creation\r\nThe `::index create` command is insufficient for HNSW. Use the dedicated `::hnsw` command.\r\n\r\n```cozoql\r\n::hnsw create memory:knn {\r\n    dim: 384,\r\n    m: 50,\r\n    ef_construction: 200,\r\n    fields: [embedding],\r\n    dtype: F32,\r\n    distance: L2\r\n}\r\n```\r\n\r\n## 3. Schema Evolution (The \"Safe Restart\")\r\nCozoDB does not support `ALTER TABLE` easily.\r\n*   **Protocol:** If the schema changes (e.g. adding `hash` column):\r\n    1.  Detect mismatch (Column count check).\r\n    2.  **Explicitly Drop Indices:** `::index drop memory:idxname` (Failure to do this locks the table drop).\r\n    3.  Drop Table: `:drop memory`.\r\n    4.  Recreate Table with new Schema.\r\n    5.  Recreate Indices.\r\n\r\n## 4. Query Reliability\r\n*   **Parameter Binding:** Always use `$var` binding.\r\n    *   `?[id] := *memory{id}, id = $id`\r\n*   **Read-After-Write:** See [Standard 059](059_reliable_ingestion.md).\r\n\r\n## 5. HNSW Vector Search (Verified Protocol)\r\nVector search via `cozo-node` has strict, non-obvious requirements that differ from CLI usage.\r\n\r\n### A. Explicit Index Query\r\nDo NOT use the `:vec_nearest` algorithm directly on the table (it forces a full table scan and has obscure syntax binding issues). Always query the Index.\r\n\r\n*    **Clean & Fast (O(log n)):**\r\n    ```typescript\r\n    // Use the ~table:index format\r\n    ?[id, dist] := ~memory:knn{id | query: vec($q), k: 100, ef: 200, bind_distance: d}, \r\n                   dist = d\r\n    ```\r\n*    **Slow & Error Prone (O(n)):**\r\n    ```typescript\r\n    ?[id, dist] := *memory{id, embedding}, :vec_nearest(embedding, $q, 100, dist)\r\n    ```\r\n\r\n### B. Type Casting (The \"List vs Vector\" Trap)\r\nJavaScript arrays (e.g. `[0.1, 0.2]`) passed as parameters (`$q`) are treated as *Lists* by Cozo. The HNSW index demands a *Vector*.\r\nYou MUST explicitly cast the input using `vec()` inside the query.\r\n\r\n*    Correct: `query: vec($queryVec)`\r\n*    Error (`Expected vector, got List`): `query: $queryVec`\r\n\r\n### C. Mandatory Parameters\r\n*   **`ef` (Expansion Factor):** This parameter is **REQUIRED** for HNSW index queries. Omitting it causes `Field 'ef' is required`.\r\n    *   *Recommendation:* Set `ef` to `2 * k` (e.g., if k=100, ef=200).\r\n*   **`k` (Limit):** Should be a literal integer or bound variable.\r\n\r\n### D. Output Variable Binding\r\nWhen binding the calculated distance, use a **Logic Variable** (no `$`), not a Parameter (`$`).\r\n*    Correct: `bind_distance: d` (where `d` is then used in projection)\r\n*    Error (`Unexpected input`): `bind_distance: $d`\r\n"
    tokens: 1250
    size: 3335
  - path: specs\standards\064-cozodb-query-stability.md
    content: "# Standard 064: CozoDB Query Structure & Stability\r\n\r\n**Category:** Engineering / Database\r\n**Status:** Draft\r\n**Date:** 2026-01-19\r\n\r\n## Context\r\nComplex Datalog queries, especially those involving `~memory:content_fts` (Full Text Search) and `~memory:knn` (Vector Search), have demonstrated instability in the Node.js environment. This manifests as opaque parser errors (`unexpected input`, `coercion_failed`).\r\n\r\n## Guidelines\r\n\r\n### 1. Query Simplicity\r\n- **Avoid Multiline Literals**: Where possible, keep queries single-line or strictly sanitized. Invisible newline characters in template literals can cause parser desync.\r\n  - **Bad**:\r\n    ```typescript\r\n    const q = `?[a, b] :=\r\n       *table{a, b}`;\r\n    ```\r\n  - **Good**:\r\n    ```typescript\r\n    const q = `?[a, b] := *table{a, b}`;\r\n    ```\r\n\r\n### 2. Variable Naming\r\n- Avoid variable names that collide with column names in complex projections if not strictly necessary. \r\n- Use distinct logic variables (e.g., `cont` vs `content`) during `bind` operations to prevent ambiguity.\r\n\r\n### 3. Vector & FTS Isolation\r\n- Do not assume `Promise.all` parallel execution of FTS and Vector queries is safe on the single `db` instance lock. \r\n- **Sequential Execution**: If instability persists, run queries sequentially rather than in parallel.\r\n- **Graceful degradation**: Always wrap vector/FTS queries in independent `try/catch` blocks. If one fails, the other should still return results.\r\n\r\n### 4. Parameter Binding\r\n- Always use `$param` binding for user input to prevent injection and parser errors.\r\n- **Sanitization**: Violently sanitize inputs for FTS. FTS parsers are fragile with symbols like `:`, `*`, `-`.\r\n\r\n## Implemented Workarounds (Current Codebase)\r\n- Vector Search is currently **DISABLED** in `services/search/search.ts` via `Promise.resolve([])`.\r\n- FTS Queries are **Single-Line**.\r\n"
    tokens: 700
    size: 1863
  - path: specs\standards\065-graph-associative-retrieval.md
    content: "# Standard 065: Graph-Based Associative Retrieval (Semantic-Lite)\r\n\r\n**Category:** Architecture / Search\r\n**Status:** Approved\r\n**Date:** 2026-01-19\r\n\r\n## Context\r\nTraditional Vector Search (HNSW) poses significant resource overhead (RAM/CPU) and can be unstable in local environments (CozoDB driver issues). Furthermore, for personal knowledge bases, \"fuzzy\" vector neighbors often hallucinate connections that lack explicit structural relevance.\r\n\r\n## The Strategy: \"Tag-Walker\"\r\nWe replace the Vector Layer with a **Graph-Based Associative Retrieval** protocol. This trades geometric distance for explicit graph traversals using `tags` and `buckets`.\r\n\r\n### Architecture\r\n| Feature | Vector Architecture | Tag-Walker Architecture |\r\n| --- | --- | --- |\r\n| **Storage** | Atoms + 768d Vectors (Float32) | Atoms + Strings |\r\n| **Index** | HNSW Index (Heavy) | Inverted Index (Light) |\r\n| **Logic** | \"Find nearest neighbors in embedding space\" | \"Traverse edges: Atom -> Tag -> Atom\" |\r\n\r\n### The Algorithm (70/30 Split)\r\n\r\n#### Phase 1: Anchor Search (70% Budget)\r\n**Goal:** Find \"Direct Hits\" using Weighted Keyword Search (BM25).\r\n1.  **Execute FTS**: Search for atoms matching the user query.\r\n2.  **Boosting**: Boost results that contain query terms in `tags` or `buckets` (2x boost).\r\n3.  **Selection**: Allocate **70%** of the context character budget to these results.\r\n\r\n#### Phase 2: Tag Harvest\r\n**Goal:** Identify \"Bridge Tags\" to find hidden context.\r\n1.  **Extract**: Collect all unique `tags` and `buckets` from the top X results of Phase 1.\r\n2.  **Filter**: Exclude generic system tags if necessary (though strict filtering is often not needed).\r\n3.  **Bridge**: These tags represent the *structural* context of the query.\r\n\r\n#### Phase 3: Neighbor Walk (30% Budget)\r\n**Goal:** Find \"Associative Hits\" (Hidden connections).\r\n1.  **Query**: Find atoms that share the **Harvested Tags** but *do not* contain the original query keywords (or are duplicates of Phase 1).\r\n    *   *Logic*: `atom -> has_tag -> tag -> has_tag -> neighbor_atom`\r\n2.  **Selection**: Allocate the remaining **30%** of the budget to these associative neighbors.\r\n\r\n### Implementation Guidelines (CozoDB)\r\n\r\n**Anchor Search Query (Simplified)**\r\n```cozo\r\n?[id, score, content, tags] := *memory{id, content, tags},\r\n                               ~memory:content_fts{id | query: $query, bind_score: score}\r\n```\r\n\r\n**Neighbor Walk Query**\r\n```cozo\r\n?[neighbor_id, neighbor_content] := *memory{id, tags},\r\n                                    member($tag, tags),        # Explode tags from source\r\n                                    *memory{id: neighbor_id, tags: n_tags},\r\n                                    member($tag, n_tags),      # Match neighbor tags\r\n                                    id != neighbor_id          # Exclude self\r\n```\r\n\r\n### The \"Lazy Tax\" Mitigation\r\nTo ensure graph connectivity even when users fail to tag notes manually, the **Dreamer Service** should be employed to auto-tag atoms during idle cycles, ensuring a dense node-edge-node graph.\r\n"
    tokens: 1133
    size: 3055
  - path: specs\standards\README.md
    content: "# The Sovereign Engineering Code (SEC)\r\n\r\nThis is the authoritative reference manual for the External Context Engine (ECE) project. Standards are organized by domain to facilitate navigation and understanding.\r\n\r\n## Domain 00: CORE (Philosophy & Invariants)\r\nPhilosophy, Privacy, and \"Local-First\" invariants that govern the fundamental principles of the system.\r\n\r\n### Standards:\r\n- [012-context-utility-manifest.md](00-CORE/012-context-utility-manifest.md) - Context utility manifest and philosophical foundations\r\n- [027-no-resurrection-mode.md](00-CORE/027-no-resurrection-mode.md) - Manual control via NO_RESURRECTION_MODE flag\r\n- [028-default-no-resurrection-mode.md](00-CORE/028-default-no-resurrection-mode.md) - Default behavior for Ghost Engine resurrection\r\n\r\n## Domain 10: ARCH (System Architecture)\r\nNode.js Monolith, CozoDB, Termux, Hardware limits, and system architecture decisions.\r\n\r\n### Standards:\r\n- [003-webgpu-initialization-stability.md](10-ARCH/003-webgpu-initialization-stability.md) - WebGPU initialization stability\r\n- [004-wasm-memory-management.md](10-ARCH/004-wasm-memory-management.md) - WASM memory management\r\n- [014-async-best-practices.md](10-ARCH/014-async-best-practices.md) - Async/await patterns for system integration\r\n- [014-gpu-resource-availability.md](10-ARCH/014-gpu-resource-availability.md) - GPU resource availability\r\n- [023-anchor-lite-simplification.md](10-ARCH/023-anchor-lite-simplification.md) - Anchor Lite architectural simplification\r\n- [031-ghost-engine-stability-fix.md](10-ARCH/031-ghost-engine-stability-fix.md) - CozoDB schema FTS failure handling\r\n- [032-ghost-engine-initialization-flow.md](10-ARCH/032-ghost-engine-initialization-flow.md) - Database initialization race condition prevention\r\n- [034-nodejs-monolith-migration.md](10-ARCH/034-nodejs-monolith-migration.md) - Migration to Node.js monolith architecture\r\n- [048-epochal-historian-recursive-decomposition.md](10-ARCH/048-epochal-historian-recursive-decomposition.md) - Epochal Historian & Recursive Decomposition (Epochs -> Episodes -> Propositions)\r\n- [051-service-module-path-resolution.md](10-ARCH/051-service-module-path-resolution.md) - Service Module Path Resolution for subdirectory services\r\n- [057-enterprise-library-architecture.md](10-ARCH/057-enterprise-library-architecture.md) - Enterprise Library Architecture (Logical Notebooks/Cartridges)\r\n\r\n## Domain 20: DATA (Data, Memory, Filesystem)\r\nSource of Truth, File Ingestion, Schemas, YAML Snapshots, and all data-related concerns.\r\n\r\n### Standards:\r\n- [017-file-ingestion-debounce-hash-checking.md](20-DATA/017-file-ingestion-debounce-hash-checking.md) - File ingestion with debouncing and hash checking\r\n- [019-code-file-ingestion-comprehensive-context.md](20-DATA/019-code-file-ingestion-comprehensive-context.md) - Comprehensive context for code file ingestion\r\n- [021-chat-session-persistence-context-continuity.md](20-DATA/021-chat-session-persistence-context-continuity.md) - Chat session persistence and context continuity\r\n- [022-text-file-source-of-truth-cross-machine-sync.md](20-DATA/022-text-file-source-of-truth-cross-machine-sync.md) - Text files as source of truth with cross-machine synchronization\r\n- [024-context-ingestion-pipeline-fix.md](20-DATA/024-context-ingestion-pipeline-fix.md) - Context ingestion pipeline fixes\r\n- [029-consolidated-data-aggregation.md](20-DATA/029-consolidated-data-aggregation.md) - Consolidated data aggregation approach\r\n- [030-multi-format-output.md](20-DATA/030-multi-format-output.md) - JSON, YAML, and text output support\r\n- [033-cozodb-syntax-compliance.md](20-DATA/033-cozodb-syntax-compliance.md) - CozoDB syntax compliance requirements\r\n- [037-database-hydration-snapshot-portability.md](20-DATA/037-database-hydration-snapshot-portability.md) - Database hydration and snapshot portability workflow\r\n- [052-schema-evolution-epochal-classification.md](20-DATA/052-schema-evolution-epochal-classification.md) - Schema Evolution & Epochal Classification for hierarchical memory organization\r\n- [053-cozodb-pain-points-reference.md](20-DATA/053-cozodb-pain-points-reference.md) - ** CRITICAL**: CozoDB pain points, gotchas, and lessons learned\r\n\r\n## Domain 30: OPS (Protocols, Safety, Debugging)\r\nAgent Safety (Protocol 001), Logging, Async handling, and operational procedures.\r\n\r\n### Standards:\r\n- [001-windows-console-encoding.md](30-OPS/001-windows-console-encoding.md) - Windows console encoding handling\r\n- [011-comprehensive-testing-verification.md](30-OPS/011-comprehensive-testing-verification.md) - Comprehensive testing and verification\r\n- [013-universal-log-collection.md](30-OPS/013-universal-log-collection.md) - Universal log collection system\r\n- [016-process-management-auto-resurrection.md](30-OPS/016-process-management-auto-resurrection.md) - Process management and auto-resurrection\r\n- [020-browser-profile-management-cleanup.md](30-OPS/020-browser-profile-management-cleanup.md) - Browser profile management and cleanup\r\n- [024-detached-logging-standard.md](30-OPS/024-detached-logging-standard.md) - Detached execution with logging\r\n- [025-script-logging-protocol.md](30-OPS/025-script-logging-protocol.md) - Script logging protocol (Protocol 001)\r\n- [035-never-attached-mode.md](30-OPS/035-never-attached-mode.md) - Never run services in attached mode (Detached Execution)\r\n- [036-log-file-management-protocol.md](30-OPS/036-log-file-management-protocol.md) - Log file management and rotation\r\n- [050-windows-background-process-behavior.md](30-OPS/050-windows-background-process-behavior.md) - Windows background process behavior and console window prevention\r\n\r\n## Domain 40: BRIDGE (APIs, Extensions, UI)\r\nExtensions, Ports, APIs, and all interface-related concerns.\r\n\r\n### Standards:\r\n- [010-bridge-redirect-implementation.md](40-BRIDGE/010-bridge-redirect-implementation.md) - Bridge redirect implementation\r\n- [015-browser-control-center.md](40-BRIDGE/015-browser-control-center.md) - Unified browser control center\r\n- [018-streaming-cli-client-responsive-ux.md](40-BRIDGE/018-streaming-cli-client-responsive-ux.md) - Responsive UX for streaming CLI clients\r\n- [026-ghost-engine-connection-management.md](40-BRIDGE/026-ghost-engine-connection-management.md) - Ghost Engine connection management\r\n"
    tokens: 2417
    size: 6271
  - path: specs\tasks.md
    content: "# Context-Engine Implementation Tasks\r\n\r\n## Current Work Queue\r\n\r\n## Active Sprint: Sovereign Desktop & Robustness (Jan 10, 2026)\r\n\r\n###  Critical (Immediate)\r\n- [x] **Fix \"JSON Vomit\" (Session Pollution):** Implement Side-Channel Separation for Intent Translation. (Standard 055)\r\n- [x] **Fix Search Crash:** Handle `null` returns from Intent Translation in `api.js`.\r\n- [x] **Fix \"No Sequences Left\":** Explicitly dispose Side-Channel sessions and increase sequence limit.\r\n- [x] **Sovereign Desktop UI:** Implement \"Frosted Glass\" transparent overlay. (Standard 056)\r\n- [x] **Vision Integration:** detailed screen capture via `desktopCapturer` in the Overlay.\r\n- [x] **Refactor Inference Monolith:** Deconstruct `inference.js` into modular TypeScript services (`provider.ts`, `context.ts`, `inference.ts`).\r\n- [x] **Magic Inbox:** Implement \"Drop-Zone\" pattern in `watcher.ts` (Watch -> Ingest -> Archive).\r\n- [x] **Hybrid Module Stability:** Revert to CJS with Dynamic Imports for robust ESM compatibility.\r\n\r\n###  High Priority (This Week)\r\n- [ ] **Backend Vision Pipeline:** Ensure `inference.js` correctly handles the `{type: image_url}` message format via `node-llama-cpp`.\r\n- [ ] **Context Assembly Speed:** Investigate caching strategies for repeated Large Contexts.\r\n- [ ] **Dreamer Upgrade:** Enable \"Deep Sleep\" logic for aggressive deduplication.\r\n\r\n###  Backlog (Feature Requests)\r\n- [ ] **Voice Input:** Whisper integration for the Desktop Overlay.\r\n- [ ] **Codebase Map:** Visual graph of the `context/` directory.\r\n- [ ] **MCP Server:** Expose ECE as a Model Context Protocol server.\r\n\r\n### Phase 17: Enterprise Library Architecture (In Progress)\r\n- [x] **Context Cartridges UI:** Implemented \"Loadout\" buttons in `index.html` (Architect/Python/Whitepaper).\r\n- [x] **Logical Notebooks:** Updated `context_packer.js` to treat `context/libraries/` as auto-tagged cartridges.\r\n- [x] **Watcher Upgrades:** Updated `watcher.js` to detect Library folders and apply `#{lib}_docs` buckets.\r\n- [x] **Stability Fix:** Patched `inference.js` (Sequences: 15) to prevent VRAM exhaustion with concurrent Dreamer/Search.\r\n- [x] **Whitepaper Context:** Injected `specs/` into the graph as a dedicated `specs` bucket.\r\n- [ ] **Dynamic Loadouts:** Move Loadout config from `sovereign.yaml` (Updated from index.html).\r\n- [ ] **Docs Update:** Create `README_LIBRARIES.md` explaining how to add new cartridges.\r\n\r\n### Phase 19: Enterprise & Advanced RAG (Planned)\r\n- [ ] **Feature 7: Backup & Restore**: Server-side DB dumps (`POST /v1/backup`) and Restore-on-Boot logic.\r\n- [ ] **Feature 8: Rolling Context Slicer**: Middle-Out context budgeting for `ContextManager` (Relevance vs Recency).\r\n- [ ] **Feature 9: Live Context Visualizer**: \"RAG IDE\" in Frontend with real-time budget slider and atom visualization.\r\n- [ ] **Feature 10: Sovereign Provenance**: Trust hierarchy (Sovereign vs External) with bias toggle in Search.\r\n\r\n### Phase 18: Monorepo & Configuration Unification (Active)\r\n- [x] **PNPM Migration:** Converted project to `pnpm` workspace (packages: engine, desktop-overlay, shared).\r\n- [x] **Shared Types:** Created `@ece/shared` for unified TypeScript interfaces.\r\n- [x] **Unified Config:** Implemented `sovereign.yaml` as Single Source of Truth for Models, UI, and Network.\r\n- [x] **Lifecycle Management:** Electron Main now automatically spawns/kills the Engine process.\r\n- [x] **Settings UI:** Added `Settings.tsx` overlay with IPC read/write to `sovereign.yaml`.\r\n- [ ] **Security Hardening:** Migrate IPC to `contextBridge` / `preload.js` (disable `nodeIntegration`).\r\n\r\n### Phase 16: Brain Link & Sovereign Desktop (Done)\r\n- [x] **Schema Introspection Fix**: Use `::columns memory` instead of broken `*columns{...}` query (Standard 053)\r\n- [x] **FTS Persistence**: FTS index now survives restarts (no more migration loop)\r\n- [x] **Brain Link UI**: Auto-context injection in `chat.html` with memory budget slider\r\n- [x] **Personal Memory Ingestion**: Created `add_personal_memories.js` for test data\r\n- [x] **Planning Document**: Created `specs/sovereign-desktop-app.md` with full architecture\r\n- [x] **Chat UI Overhaul**: Simplified chat.html - removed Brain Link (unreliable local), kept Manual Context\r\n- [x] **Streaming Tokens**: Real-time token streaming display as LLM generates response\r\n- [x] **Thinking/Answer Separation**: Model `<think>` blocks displayed separately with purple styling\r\n- [x] **User Message Fix**: User prompts now persist correctly in chat history\r\n\r\n### Phase 12: Production Polish (Completed)\r\n- [x] **Post-Migration Safety**: Implement emergency backups before schema changes (`db.js`).\r\n- [x] **API Fortification**: Add input validation for `ingest` and `search` endpoints (`api.js`).\r\n- [x] **Search Resiliency**: Fix bucket-filtering bypass in `executeSearch`.\r\n- [x] **Verification Suite**: 100% pass rate on `npm test`.\r\n- [x] **Chat Cockpit Enhancement**: Add conversation history persistence to `chat.html`\r\n- [x] **Streaming Responses**: Implement SSE for real-time token streaming\r\n- [x] **One-Click Install**: Create `setup.ps1` / `setup.sh` scripts\r\n\r\n### Phase 11: Markovian Reasoning Engine (Completed)\r\n- [x] **Scribe Service**: Created `engine/src/services/scribe.js` for rolling state\r\n- [x] **Context Weaving**: Upgraded `inference.js` to auto-inject session state\r\n- [x] **Test Suite**: Created `engine/tests/suite.js` for API verification\r\n- [x] **Benchmark Tool**: Created `engine/tests/benchmark.js` for accuracy testing\r\n- [x] **Config Fixes**: Externalized MODELS_DIR, fixed package.json typo\r\n- [x] **API Endpoints**: Added `/v1/scribe/*` and `/v1/inference/status`\r\n- [x] **Standard 041**: Documented Markovian architecture\r\n\r\n### Phase 13: Epochal Historian & Mirror Protocol Enhancement (Completed)\r\n- [x] **Epochal Historian Implementation**: Implement recursive decomposition (Epochs -> Episodes -> Propositions) in Dreamer service\r\n- [x] **Mirror Protocol Enhancement**: Update to prioritize Epoch-based structure in `context/mirrored_brain/[Bucket]/[Epoch]/[Memory_ID].md`\r\n- [x] **Documentation Updates**: Update `specs/spec.md`, `specs/search_patterns.md`, and `specs/context_assembly_findings.md` with Epochal Historian details\r\n- [x] **Watcher Shield**: Ensure file watcher ignores `context/mirrored_brain/` to prevent recursive loops\r\n\r\n### Phase 14: Path Resolution Fixes (Completed)\r\n- [x] **Service Module Path Corrections**: Fix relative import paths in all service files (search, ingest, scribe, dreamer, mirror, inference, watcher, safe-shell-executor)\r\n- [x] **Core Module References**: Correct paths from `'../core/db'` to `'../../core/db'` in services located in subdirectories\r\n- [x] **Configuration Imports**: Standardize all relative imports to properly reference core modules and configuration files\r\n- [x] **Module Loading Verification**: Verify all modules load without \"Cannot find module\" errors\r\n\r\n### Phase 15: Schema Evolution & Epochal Historian Enhancement (Completed)\r\n- [x] **Database Schema Update**: Add `epochs: String` field to memory table schema to store epochal classifications\r\n- [x] **Dreamer Service Update**: Modify database queries and updates to include epochs field in processing\r\n- [x] **Search Service Update**: Modify database queries to include epochs field in search operations\r\n- [x] **Mirror Service Update**: Ensure epochs field is properly handled in mirroring operations\r\n- [x] **Documentation Updates**: Update `specs/spec.md`, `specs/search_patterns.md`, and `specs/context_assembly_findings.md` with schema changes\r\n\r\n### Phase 16: Brain Link & Sovereign Desktop (In Progress)\r\n- [x] **Schema Introspection Fix**: Use `::columns memory` instead of broken `*columns{...}` query (Standard 053)\r\n- [x] **FTS Persistence**: FTS index now survives restarts (no more migration loop)\r\n- [x] **Brain Link UI**: Auto-context injection in `chat.html` with memory budget slider\r\n- [x] **Personal Memory Ingestion**: Created `add_personal_memories.js` for test data\r\n- [x] **Planning Document**: Created `specs/sovereign-desktop-app.md` with full architecture\r\n- [x] **Chat UI Overhaul**: Simplified chat.html - removed Brain Link (unreliable local), kept Manual Context\r\n- [x] **Streaming Tokens**: Real-time token streaming display as LLM generates response\r\n- [x] **Thinking/Answer Separation**: Model `<think>` blocks displayed separately with purple styling\r\n- [x] **User Message Fix**: User prompts now persist correctly in chat history\r\n- [ ] **Sovereign Desktop Prototype**: Electron overlay with hotkey activation\r\n- [ ] **Screen Capture Integration**: Add VL model for screen understanding\r\n- [ ] **Proactive Memory**: Auto-ingest screen context and conversation highlights\r\n- [ ] **Distribution**: Installer, auto-update, first-run wizard\r\n\r\n### Phase 10: Cortex Upgrade (Completed)\r\n- [x] **Multi-Bucket Schema**: Migrate from single `bucket` to `buckets: [String]` (Standard 039).\r\n- [x] **Dreamer Service**: Implement background self-organization via local LLM.\r\n- [x] **Cozo Hardening**: Resolve list-handling and `unnest` syntax errors (Standard 040).\r\n- [x] **ESM Interop**: Fix dynamic import issues for native modules in CJS.\r\n\r\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access\r\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat\r\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking\r\n- [x] **Session Tracking**: Add session file path display in CLI startup\r\n\r\n### Completed - Root Refactor \r\n- [x] **Kernel**: Implement `tools/modules/sovereign.js`.\r\n- [x] **Mic**: Refactor `root-mic.html` to use Kernel.\r\n- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.\r\n- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).\r\n- [x] **Docs**: Update all specs to reflect Root Architecture.\r\n\r\n### Completed - Hardware Optimization \r\n- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.\r\n- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.\r\n- [x] **Crash Prevention**: Context clamping for constrained drivers.\r\n- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.\r\n- [x] **Consciousness Semaphore**: Implemented resource arbitration in `sovereign.js`.\r\n\r\n### Completed - The Subconscious \r\n- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.\r\n- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.\r\n- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).\r\n- [x] **Memory Hygiene**: Implemented \"Forgetting Curve\" in `root-dreamer.html`.\r\n\r\n### Completed - Active Cognition \r\n- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.\r\n- [x] **User Control**: Add \"Auto-Save\" toggle to System Controls.\r\n- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.\r\n- [x] **Multimodal**: Add Drag-and-Drop Image support to Console.\r\n\r\n### Phase 4.1: The Neural Shell (Completed) \r\n**Objective:** Decouple Intelligence (Chat) from Agency (Terminal).\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 3.5:** Ghost Auto-Ignition (Headless auto-start with ?headless=true flag).\r\n- [x] **Phase 4:** Migration to C++ Native Runtime (Removing Chrome entirely).\r\n- [x] **Bridge Repair**: Debug and stabilize `extension-bridge` connectivity.\r\n- [x] **Neural Shell Protocol**: Implement `/v1/shell/exec` in `webgpu_bridge.py`.\r\n- [x] **The \"Coder\" Model**: Add `Qwen2.5-Coder-1.5B` to Model Registry.\r\n- [x] **Terminal UI**: Create `tools/neural-terminal.html` for natural language command execution.\r\n\r\n### Phase 4.2: Agentic Expansion (Deferred)\r\n- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.\r\n- [ ] **Voice Output**: Add TTS to Console.\r\n\r\n## Phase 5: The Specialist Array\r\n- [ ] **Dataset Generation**: Samsung TRM / Distillation.\r\n- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.\r\n- [ ] **Model Merging**: FrankenMoE construction.\r\n\r\n## Phase 6: GPU Resource Management (Completed)\r\n- [x] **GPU Queuing System**: Implement `/v1/gpu/lock`, `/v1/gpu/unlock`, and `/v1/gpu/status` endpoints with automatic queuing\r\n- [x] **Resource Conflict Resolution**: Eliminate GPU lock conflicts with proper queue management\r\n- [x] **503 Error Resolution**: Fix \"Service Unavailable\" errors by implementing proper resource queuing\r\n- [x] **Sidecar Integration**: Add GPU status monitoring to sidecar interface\r\n- [x] **Log Integration**: Add GPU resource management logs to centralized logging system\r\n- [x] **Documentation**: Update specs and standards to reflect GPU queuing system\r\n\r\n## Phase 7: Async/Await Best Practices (Completed)\r\n- [x] **Coroutine Fixes**: Resolve \"coroutine was never awaited\" warnings in webgpu_bridge.py\r\n- [x] **Event Loop Integration**: Properly integrate async functions with FastAPI's event loop\r\n- [x] **Startup Sequence**: Ensure logging system initializes properly with application lifecycle\r\n- [x] **Resource Management**: Fix resource cleanup in WebSocket handlers to prevent leaks\r\n- [x] **Error Handling**: Enhance async error handling with proper cleanup procedures\r\n- [x] **Documentation**: Create Standard 014 for async/await best practices\r\n\r\n## Phase 8: Browser-Based Control Center (Completed)\r\n- [x] **Sidecar UI**: Implement `tools/sidecar.html` with dual tabs for retrieval and vision\r\n- [x] **Context UI**: Implement `tools/context.html` for manual context retrieval\r\n- [x] **Vision Engine**: Create `tools/vision_engine.py` for Python-powered image analysis\r\n- [x] **Bridge Integration**: Update `webgpu_bridge.py` to serve UI and handle vision endpoints\r\n- [x] **Endpoint Implementation**: Add `/v1/vision/ingest`, `/v1/memory/search`, `/logs/recent` endpoints\r\n- [x] **File-based Logging**: Implement persistent logging to `logs/` directory with truncation\r\n- [x] **Documentation**: Update specs and standards to reflect new architecture\r\n\r\n### Phase 9: Anchor Lite Refactor (Completed)\r\n- [x] **Consolidation**: Simplified system to Single Source of Truth (`context/`) -> Single Index (CozoDB) -> Single UI (`context.html`).\r\n- [x] **Cleanup**: Archived unused tools (`db_builder`, `memory-builder`, `sidecar`, `mobile-chat`).\r\n- [x] **Engine Refactor**: Created headless `ghost.html` engine with WebSocket bridge.\r\n- [x] **Launch Logic**: Unified startup in `start-anchor.bat` and `webgpu_bridge.py`.\r\n- [x] **Standard 023**: Documented \"Anchor Lite\" architecture and \"Triangle of Pain\".\r\n\r\n### Phase 10: Context Ingestion Pipeline Fixes (Completed)\r\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\r\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\r\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\r\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\r\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\r\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\r\n\r\n### Phase 11: Script Running Protocol Implementation (Completed)\r\n- [x] **Protocol Creation**: Created `SCRIPT_PROTOCOL.md` with guidelines to prevent getting stuck in long-running loops\r\n- [x] **System Optimization**: Fixed database paths and search queries for better performance\r\n- [x] **Documentation Update**: Updated doc_policy to include protocol as allowed root document\r\n- [x] **Standards Creation**: Created Standards 035 and 036 for detached execution and log management\r\n- [x] **Startup Scripts**: Created proper detached startup scripts with logging\r\n\r\n## Backlog\r\n- [ ] **Federation Protocol**: P2P sync.\r\n- [ ] **Android App**: Wrapper for Root Coda."
    tokens: 6045
    size: 16112
  - path: specs\TROUBLESHOOTING.md
    content: "# Forensic Restoration & Annotated Cleaning\r\n\r\n## Objectives\r\n- Preserve raw data (`m.content`) for forensic and retrieval purposes.\r\n- Use `content_cleaned` for indexing and linking to avoid spam/garbage embedding.\r\n- Annotate sanitized technical context rather than removing it entirely (e.g., replacing ANSI codes with `[Context: Terminal Output]`).\r\n- Identify and quarantine `token-soup` nodes to avoid using them for embeddings or graph repairs.\r\n- Provide a regeneration path that normalizes and re-distills quarantined nodes.\r\n\r\n## Key Tools & Functions\r\n- `src/content_utils.normalize_technical_content(text)`\r\n  - Detects and annotates: ANSI, Unix/Windows paths, hex dumps;\r\n  - Produces a normalized text with semantic annotations instead of opaque noise.\r\n\r\n- `src/content_utils.clean_content(text, annotate_technical=False)`\r\n  - A conservative text cleaner; when `annotate_technical=True` it runs normalization first to preserve context tags.\r\n\r\n- `src/distiller_impl.Distiller.distill_moment`\r\n  - Integrates resilience logic: If the distiller detects token-soup, it attempts `normalize_technical_content()` and retries distillation prior to fallback sanitization.\r\n\r\n- Quarantine scripts\r\n  - `scripts/quarantine_token_soup.py`  Scans and optionally tags nodes as `#corrupted`.\r\n  - `scripts/quarantine_regenerate.py`  For quarantined nodes: normalizes raw content, redistills it, and optionally writes `content_cleaned`; it can also replace the `#corrupted` tag with `regenerated`.\r\n\r\n- Repair & Weaver\r\n  - `scripts/neo4j/repair/repair_missing_links_similarity_embeddings.py`  now supports `--exclude-tag` to skip quarantined nodes.\r\n  - `src/maintenance/weaver.py`  now passes `weaver_exclude_tag` to `run_repair` by default.\r\n\r\n## Typical Workflows\r\n\r\n1. Dry-run: identify quarantined nodes\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_token_soup.py --category summary --limit 500 --csv-out logs/token_soup_report.csv --sample 10 --use-cleaned\r\n```\r\n\r\n2. Tag quarantined nodes (write mode)\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_token_soup.py --category summary --limit 500 --write --csv-out logs/token_soup_report.csv --sample 10 --use-cleaned\r\n```\r\n\r\n3. Re-generate summaries for quarantined nodes\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_regenerate.py --tag '#corrupted' --limit 200 --csv-out logs/regenerate_report.csv --write\r\n```\r\n\r\n4. Run the weaver (repair) excluding corrupted nodes\r\n\r\n```pwsh\r\npython .\\scripts\\neo4j\\repair\\repair_missing_links_similarity_embeddings.py --dry-run --csv-out logs/weaver_review_chunked.csv --exclude-tag '#corrupted' --limit 200 --candidate-limit 100 --top-n 3 --export-top 25\r\n```\r\n\r\n## Rollback & Auditability\r\n- CSV logs are produced for every step to review proposed repairs and regeneration results.\r\n- Relationships created by the Weaver include `r.auto_commit_*` fields enabling rollback via existing scripts.\r\n\r\n## Future Directions\r\n- Integrate regeneration automatically within the Distiller or as a scheduled job under the Archivist.\r\n- Add a quarantine UI or a triage CLI to quickly inspect and approve re-distilled nodes.\r\n- Improve chunk-weighted averaging for embeddings and add more E2E tests for the regeneration process.\r\n\r\n## Summary\r\nThis design preserves both the raw, forensic truth and the usable, sanitized indexable text. We now have a robust path to identify token-soup failures, protect the graph's signal, and reprocess nodes to recover valid summaries with contextual tags.\r\n"
    tokens: 1296
    size: 3472
  - path: specs\vscode_integration.md
    content: "# VSCode Integration\r\n\r\n## Configure VSCode (example for 'Custom OpenAI endpoint')\r\n- Open `Settings`  `Extensions`  `Chat` or the settings for the Chat provider you use\r\n- Add a custom endpoint with URL: `http://localhost:8000/v1/chat/completions`\r\n- Model: `ece-core`\r\n- If API key is required, set a secret with key `Authorization` value `Bearer <API_KEY>` for the provider\r\n- Set `stream` to `true` where the provider supports it\r\n\r\n## Quick test with curl\r\n\r\n### Normal (non-streaming)\r\n```powershell\r\n$body = @{\r\n    model = 'ece-core'\r\n    messages = @(\r\n        @{ role = 'system'; content = 'You are a helpful assistant for VSCode.' },\r\n        @{ role = 'user'; content = 'List the top-level files in the repository' }\r\n    )\r\n} | ConvertTo-Json -Depth 4\r\n\r\nInvoke-RestMethod -Method Post -Uri 'http://localhost:8000/v1/chat/completions' -Body $body -ContentType 'application/json' -Headers @{ Authorization = 'Bearer <API_KEY_HERE>' }\r\n```\r\n\r\n### Streaming (SSE)\r\n```powershell\r\n$body = @{\r\n    model = 'ece-core'\r\n    messages = @(\r\n        @{ role = 'system'; content = 'You are a helpful assistant for VSCode.' },\r\n        @{ role = 'user'; content = 'Summarize the repository' }\r\n    )\r\n    stream = $true\r\n} | ConvertTo-Json -Depth 4\r\n\r\n# Using curl you can receive SSE chunks as they arrive:\r\ncurl -N -H \"Authorization: Bearer <API_KEY_HERE>\" -H \"Content-Type: application/json\" -X POST \"http://localhost:8000/v1/chat/completions\" -d $body\r\n```\r\n"
    tokens: 549
    size: 1469
metadata:
  total_files: 112
  total_tokens: 133981
  token_limit: 1000000
  token_limit_reached: false
  timestamp: "2026-01-19T18:43:38.481Z"
  root_directory: C:\Users\rsbiiw\Projects\ECE_Core
  config:
    tokenLimit: 1000000
    maxFileSize: 5242880
    maxLinesPerFile: 5000
    outputDir: codebase
    outputFile: combined_context.yaml
    includeExtensions:
      - .js
      - .ts
      - .jsx
      - .tsx
      - .py
      - .java
      - .cpp
      - .c
      - .h
      - .cs
      - .go
      - .rs
      - .rb
      - .php
      - .html
      - .css
      - .scss
      - .sass
      - .less
      - .json
      - .yaml
      - .yml
      - .xml
      - .sql
      - .sh
      - .bash
      - .zsh
      - .md
      - .txt
      - .csv
      - .toml
      - .ini
      - .cfg
      - .conf
      - .env
      - .dockerfile
      - dockerfile
      - .gitignore
      - .npmignore
      - .prettierignore
      - makefile
      - cmakelists.txt
      - readme.md
      - readme.txt
      - readme
      - license
      - license.md
      - changelog
      - changelog.md
      - contributing
      - contributing.md
      - code_of_conduct
      - code_of_conduct.md
    excludeExtensions:
      - .png
      - .jpg
      - .jpeg
      - .gif
      - .bmp
      - .ico
      - .svg
      - .webp
      - .exe
      - .bin
      - .dll
      - .so
      - .dylib
      - .zip
      - .tar
      - .gz
      - .rar
      - .7z
      - .pdf
      - .doc
      - .docx
      - .xls
      - .xlsx
      - .ppt
      - .pptx
      - .mp3
      - .mp4
      - .avi
      - .mov
      - .wav
      - .flac
      - .ttf
      - .otf
      - .woff
      - .woff2
      - .o
      - .obj
      - .a
      - .lib
      - .out
      - .class
      - .jar
      - .war
      - .swp
      - .swo
      - .lock
      - .cache
      - .log
      - .tmp
      - .temp
      - .DS_Store
      - Thumbs.db
    excludeDirectories:
      - .git
      - node_modules
      - archive
      - backups
      - logs
      - context
      - .vscode
      - .idea
      - .pytest_cache
      - __pycache__
      - dist
      - build
      - target
      - venv
      - env
      - .venv
      - .env
      - Pods
      - Carthage
      - CocoaPods
      - .next
      - .nuxt
      - public
      - static
      - assets
      - images
      - img
      - codebase
    excludeFiles:
      - combined_context.yaml
      - package-lock.json
      - yarn.lock
      - pnpm-lock.yaml
      - Gemfile.lock
      - Pipfile.lock
      - Cargo.lock
      - composer.lock
      - go.sum
      - go.mod
      - requirements.txt
      - poetry.lock
      - "*.db"
      - "*.sqlite"
      - "*.sqlite3"
      - "*.fdb"
      - "*.mdb"
      - "*.accdb"
      - "*~"
      - "*.tmp"
      - "*.temp"
      - "*.cache"
      - "*.swp"
      - "*.swo"
