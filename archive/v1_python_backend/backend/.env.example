# Example .env for ECE_Core
# Copy this to `configs/.env` (recommended) or to `.env` at repo root and fill in secrets locally (do NOT commit real secrets to Git)

# Server / App
# Recommended: create `configs/.env` and place secrets in it. The loader will prefer configs/.env over root .env
ECE_HOST=127.0.0.1
ECE_PORT=8000
ECE_API_KEY=
ECE_REQUIRE_AUTH=false

# Redis
REDIS_URL=redis://localhost:6379
REDIS_TTL=3600
REDIS_MAX_TOKENS=16000

# Neo4j
NEO4J_ENABLED=true
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=changeme
NEO4J_RECONNECT_ENABLED=true
NEO4J_RECONNECT_INITIAL_DELAY=5
NEO4J_RECONNECT_MAX_ATTEMPTS=6
NEO4J_RECONNECT_BACKOFF_FACTOR=2.0

# LLM (Model Configuration - these can be adjusted for your specific needs)
LLM_API_BASE=http://localhost:8080/v1
# Optional: if your embeddings / llama.cpp server is configured on a different port (e.g., 8081), set it to that port:
# LLM_API_BASE=http://localhost:8081/v1
LLM_MODEL_NAME=OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf
LLM_CONTEXT_SIZE=16384      # Adjust context size (in tokens) - 16384 recommended for 20B when GPU + KV cache allow
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.3
LLM_TOP_P=0.85
LLM_GPU_LAYERS=-1           # GPU layers: -1 for all layers, 0 for CPU only, positive number for specific layers
LLM_THREADS=12              # Number of CPU threads to use

# llama.cpp Server Configuration (for startup script)
LLAMA_SERVER_DEFAULT_PORT=8080
LLAMA_EMBED_SERVER_DEFAULT_PORT=8081
LLAMA_ALLOW_SELECT_MODEL=true    # Set to false to disable interactive model selection
LLAMA_CONT_BATCHING=true
LLAMA_BATCH_SIZE=2048            # Batch size for llama.cpp server (logical batch - tuned for throughput)
LLAMA_UBATCH_SIZE=2048           # Physical ubatch size for llama.cpp server (raised to avoid GGML n_ubatch assertions)
LLAMA_PARALLEL=1                 # Number of parallel sequences
LLAMA_CACHE_RAM=0                # Prompt cache size in MiB (0 disables prompt cache to avoid VRAM pressure)

# Vector DB
VECTOR_ENABLED=false
VECTOR_ADAPTER_NAME=redis
VECTOR_AUTO_EMBED=false

# Memory and Context Management
MAX_CONTEXT_TOKENS=24000         # Max tokens in total context
SUMMARIZE_THRESHOLD=14000        # Trigger summarization when Redis exceeds this

# Local debug
LOG_LEVEL=INFO

# ============================================================
# SECURITY - API Authentication
# ============================================================
# Generate a secure key: python -c "import secrets; print(secrets.token_urlsafe(32))"
ECE_API_KEY=your-secret-api-key-here-change-this
ECE_REQUIRE_AUTH=true

# ============================================================
# SECURITY - Audit Logging
# ============================================================
AUDIT_LOG_ENABLED=true
AUDIT_LOG_PATH=./logs/audit.log
AUDIT_LOG_TOOL_CALLS=true
AUDIT_LOG_MEMORY_ACCESS=false

# MEMORY WEAVER (Autonomous Repair) - MASTER SWITCH
# When true, the Archivist will allow automated repair cycles to commit changes.
# Default is false to keep the system in a dry-run, auditable-only mode.
WEAVER_COMMIT_ENABLED=false
WEAVER_THRESHOLD=0.55
