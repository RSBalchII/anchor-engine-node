# ECE_Core Configuration
# Single source of truth for all ECE_Core settings

# ============================================================
# Server Configuration
# ============================================================
server:
  host: "0.0.0.0"
  port: 8000
  log_level: "INFO"
  require_auth: false

# ============================================================
# Security Configuration
# ============================================================
ece_api_key: "ece-secret-key"

# ============================================================
# Audit Configuration
# ============================================================
audit:
  log_enabled: true
  log_path: "./logs/audit.log"

# ============================================================
# LLM Configuration (WebGPU Bridge)
# ============================================================
llm:
  api_base: "http://localhost:8080/v1"
  model_name: "webgpu-chat"
  chat_template: "auto"
  context_size: 16384
  max_tokens: 8192
  temperature: 0.7
  top_p: 0.95
  timeout: 300
  gpu_layers: -1
  threads: 8
  
  # Embeddings settings
  embeddings_api_base: "http://localhost:8080/v1"
  embeddings_local_fallback_enabled: false
  embeddings_model_name: "snowflake-arctic-embed-m-q0f32-MLC-b32"
  embeddings_chunk_size_default: 2048
  embeddings_default_batch_size: 2
  embeddings_adaptive_backoff_enabled: true
  embeddings_chunk_backoff_sequence: [2048, 1024, 512, 256, 128]

# ============================================================
# Memory System Configuration
# ============================================================
# Redis (Hot cache)
redis:
  url: "redis://localhost:6379"
  ttl: 3600
  max_tokens: 16000
  enabled: true

# Neo4j (Graph database)
neo4j:
  uri: "bolt://localhost:7687"
  user: "neo4j"
  password: "password"
  database: "neo4j"
  enabled: true

# Vector DB (Semantic Search)
vector:
  enabled: true
  adapter_name: "redis"
  auto_embed: true

# Memory thresholds
max_context_tokens: 24000
summarize_threshold: 14000

# ============================================================
# Archivist (Summarization)
# ============================================================
archivist:
  enabled: true
  chunk_size: 3000
  overlap: 300
  compression_ratio: 0.5

# Context assembly
context_recent_turns: 5
context_summary_limit: 10
context_entity_limit: 15

# ============================================================
# Graph Retrieval (Q-Learning)
# ============================================================
qlearning:
  enabled: true
  learning_rate: 0.1
  discount_factor: 0.8
  epsilon: 0.4
  max_hops: 5
  max_paths: 10
  save_interval: 10
  table_path: "./q_table.json"

# Entity extraction
entity:
  extraction_batch_size: 20
  extraction_delay: 0.1
  min_confidence: 0.48
  types:
    - PERSON
    - CONCEPT
    - PROJECT
    - CONDITION
    - SKILL

# ============================================================
# MCP (Model Context Protocol)
# ============================================================
mcp:
  url: "http://localhost:8000"
  timeout: 30
  max_tool_iterations: 5
  enabled: true

# ============================================================
# Coda Persona Configuration
# ============================================================
coda:
  enabled: true
  genesis_context: true
  philosophy: "MDD"
  communication_style: "radical_candor"
  system_prompt: |
    For every user task and instruction you will use "GE FUNCTION" to ponder the TASK STEP BY STEP and then do the task. For each and every line of output you will ponder carefully to ensure it meets the instructions of the user, and if you are unsure use "GE FUNCTION" to re-ponder and then produce the improved output.

    GE FUNCTION: Silent input → Spawn 20 agents Sternberg Styles → Enhance idea → Seek Novel Emergence NE:unique/significant idea/concept → Ponder, assess, creative enhance notions → Refined idea => IdeaArray[].size=20 elements,  else → Interesting? Pass to rand. agent for refinement, else discard.=>output(IdeaArray)

# ============================================================
# Advanced Settings
# ============================================================
# Markovian reasoning
markovian:
  enabled: true
  chunk_overlap: 200
  min_chunk_size: 1500
  max_chunk_size: 3500

# Debug settings
debug:
  log_llm_responses: false
  log_tool_calls: true
  log_context_assembly: false
  save_responses: false

# ============================================================
# Weaver / Repair Configuration
# ============================================================
weaver:
  candidate_limit: 200
  batch_size_default: 2
  batch_size: 2
  sleep_between_batches: 1.0
  time_window_hours: 24
  threshold: 0.55
  delta: 0.05
  max_commit: 50
  commit_enabled: true
