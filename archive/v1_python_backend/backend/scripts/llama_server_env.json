{
  "MODEL": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
  "MODEL_PATH": "",
  "CTX_SIZE": 16384,
  "THREADS": 12,
  "PORT": 8080,
  "EMBED_PORT": 8081,
  "LLAMA_SERVER": "C:\\Users\\rsbiiw\\Projects\\tools\\llama.cpp\\build\\bin\\Release\\llama-server.exe",
  "EMBEDDINGS_FLAG": "--embeddings",
  "GPU_LAYERS": -1,
  "TIMEOUT": 300,
  "N_THREADS": 12,
  "LLM_API_BASE": "http://localhost:8080/v1",
  "LLM_EMBEDDINGS_API_BASE": "http://127.0.0.1:8081/v1",
  "LLM_TEMPERATURE": 1.0,
  "LLM_TOP_P": 0.95,
  "LLM_TIMEOUT": 300,
  "LLAMA_ALLOW_SELECT_MODEL": true,
  "LLAMA_CONT_BATCHING": true,
  "LLAMA_FLASH_ATTN": "auto",
  "LLAMA_CACHE_K": "f16",
  "LLAMA_CACHE_V": "f16",
  "LLAMA_REPEAT_PENALTY": 1.1,
  "LLAMA_BATCH": 2048,
  "LLAMA_UBATCH": 2048,
  "LLAMA_PARALLEL": 1,
  "LLAMA_CACHE_RAM": 0,
  "LLM_GPU_LAYERS": -1,
  "LLM_THREADS": 12
}