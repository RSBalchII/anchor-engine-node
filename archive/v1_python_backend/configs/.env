# ECE_Core Configuration - Local overrides
# Copy this to `configs/.env` (recommended) or to `.env` at repo root and fill in secrets locally (do NOT commit real secrets to Git)

# Server / App
ECE_HOST=127.0.0.1
ECE_PORT=8000
ECE_API_KEY=your-secret-api-key-change-this-immediately
ECE_REQUIRE_AUTH=false

# Redis
REDIS_URL=redis://localhost:6379
REDIS_TTL=3600
REDIS_MAX_TOKENS=16000

# Neo4j - For local development
NEO4J_ENABLED=true
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-neo4j-password-change-this
NEO4J_RECONNECT_ENABLED=true
NEO4J_RECONNECT_INITIAL_DELAY=5
NEO4J_RECONNECT_MAX_ATTEMPTS=6
NEO4J_RECONNECT_BACKOFF_FACTOR=2.0

# LLM (Model Configuration - these can be adjusted for your specific needs)
LLM_API_BASE=http://localhost:8080/v1
# Note: This is just a default model name - actual model selection is done via select_model.py
LLM_MODEL_NAME=OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf
LLM_CONTEXT_SIZE=16384      # Adjust context size (in tokens) - 16384 recommended for 20B when GPU + KV cache allow
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.3
LLM_TOP_P=0.85
LLM_GPU_LAYERS=-1           # GPU layers: -1 for all layers, 0 for CPU only, positive number for specific layers
LLM_THREADS=24              # Number of CPU threads to use (recommend using all cores for your i9-13900HX)

# llama.cpp Server Configuration (for startup script)
LLAMA_SERVER_DEFAULT_PORT=8080
LLAMA_EMBED_SERVER_DEFAULT_PORT=8081
LLAMA_ALLOW_SELECT_MODEL=true    # Set to false to disable interactive model selection
LLAMA_CONT_BATCHING=true
LLAMA_BATCH_SIZE=2048            # Batch size for llama.cpp server (logical batch - tuned for throughput)
LLAMA_UBATCH_SIZE=2048           # Physical ubatch size for llama.cpp server (raised to avoid GGML n_ubatch assertions)
LLAMA_PARALLEL=1                 # Number of parallel sequences
LLAMA_CACHE_RAM=0                # Prompt cache size in MiB (0 disables prompt cache to avoid VRAM pressure)

# llama.cpp executable path - adjust this path based on where you build llama.cpp
# Common locations for Windows:
# Option 1: Default build location (if you build in tools/llama.cpp)
# LLAMA_SERVER=C:\Users\rsbiiw\Projects\tools\llama.cpp\build\bin\Release\llama-server.exe
# Option 2: Set to a default that will prompt user to build if not found
LLAMA_SERVER=llama-server.exe

# Vector DB
VECTOR_ENABLED=false
VECTOR_ADAPTER_NAME=redis
VECTOR_AUTO_EMBED=false

# Memory and Context Management
MAX_CONTEXT_TOKENS=24000         # Max tokens in total context
SUMMARIZE_THRESHOLD=14000        # Trigger summarization when Redis exceeds this

# Local debug
LOG_LEVEL=INFO

# ============================================================
# SECURITY - API Authentication
# ============================================================
# Generate a secure key: python -c "import secrets; print(secrets.token_urlsafe(32))"
ECE_API_KEY=your-secure-api-key-change-this-immediately
ECE_REQUIRE_AUTH=false  # Set to true in production

# ============================================================
# SECURITY - Audit Logging
# ============================================================
AUDIT_LOG_ENABLED=true
AUDIT_LOG_PATH=./logs/audit.log
AUDIT_LOG_TOOL_CALLS=true
AUDIT_LOG_MEMORY_ACCESS=false

# MEMORY WEAVER (Autonomous Repair) - MASTER SWITCH
# When true, the Archivist will allow automated repair cycles to commit changes.
# Default is false to keep the system in a dry-run, auditable-only mode.
WEAVER_COMMIT_ENABLED=false
WEAVER_THRESHOLD=0.55

# MCP Configuration (now archived, use plugins instead)
MCP_ENABLED=false
MCP_HOST=127.0.0.1
MCP_PORT=8421