<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anchor Console</title>
    <style>
        * {
            box-sizing: border-box;
        }

        body {
            background: #0f0f11;
            color: #ccc;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            min-height: 100vh;
            margin: 0;
            padding-top: 8px; /* Add small top padding to prevent cutoff */
            padding-bottom: 8px; /* Add small bottom padding */
        }

        /* --- Resizable Split Layout --- */
        #container {
            display: flex;
            width: 100%;
            min-height: calc(100vh - 16px); /* Account for body padding */
            height: auto;
            overflow: hidden;
            margin: 8px; /* Add margin to prevent cutoff */
            border-radius: 8px; /* Add slight rounding to match margin */
        }

        #sidebar {
            width: 320px;
            /* Default width */
            min-width: 200px;
            max-width: 80%;
            background: #151517;
            color: #d4d4d4;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #333;
            transition: width 0.1s ease;
        }

        /* Resize Handle */
        #resizer {
            width: 5px;
            cursor: col-resize;
            background: #333;
            transition: background 0.2s;
            z-index: 10;
        }

        #resizer:hover,
        #resizer.resizing {
            background: #00ff88;
        }

        #main {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: #1e1e1e;
            position: relative;
            min-width: 0;
        }

        /* --- Collapsible Details --- */
        details {
            margin-bottom: 10px;
            border-bottom: 1px solid #333;
        }

        summary {
            padding: 10px;
            cursor: pointer;
            font-weight: bold;
            user-select: none;
            background: #252526;
            list-style: none;
        }

        summary::-webkit-details-marker {
            display: none;
        }

        summary::after {
            content: '‚ñº';
            float: right;
            font-size: 0.8em;
            transition: transform 0.2s;
        }

        details[open] summary::after {
            transform: rotate(180deg);
        }

        .panel-content {
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        /* --- Chat Box --- */
        #chat-box {
            flex: 1;
            overflow-y: auto;
            margin-bottom: 20px;
            padding-right: 10px;
        }

        #chat-box::-webkit-scrollbar {
            width: 8px;
        }

        #chat-box::-webkit-scrollbar-track {
            background: #333;
        }

        #chat-box::-webkit-scrollbar-thumb {
            background: #555;
            border-radius: 4px;
        }

        .msg {
            padding: 12px;
            margin: 8px 0;
            border-radius: 6px;
            background: #333;
            max-width: 85%;
            word-wrap: break-word;
        }

        .user {
            background: #005f3b;
            /* Anchor Green */
            color: white;
            align-self: flex-end;
            margin-left: auto;
        }

        .assistant {
            background: #2d2d2d;
            border-left: 3px solid #00ff88;
        }

        .msg details {
            margin-top: 10px;
            font-size: 0.85rem;
            opacity: 0.7;
            border: none;
        }

        .msg pre {
            background: #151515;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 0.8rem;
        }

        h3 {
            margin: 0 0 10px 0;
        }

        #status-text {
            font-size: 0.9rem;
            color: #888;
            margin-bottom: 8px;
        }

        #progress-bar {
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            margin: 8px 0;
        }

        #progress {
            height: 100%;
            background: #00ff88;
            width: 0%;
            transition: width 0.2s;
        }

        #status-log {
            flex: 1;
            overflow-y: auto;
            font-family: 'Consolas', monospace;
            font-size: 0.75rem;
            margin-top: 10px;
            padding: 8px;
            background: #0f0f0f;
            border-radius: 4px;
            border: 1px solid #333;
        }

        #status-log div {
            margin: 2px 0;
            padding: 2px 0;
        }

        .info {
            color: #888;
        }

        .warn {
            color: #ffc107;
        }

        .error {
            color: #ff4444;
        }

        .success {
            color: #00ff88;
        }

        #input-area {
            display: flex;
            gap: 10px;
        }

        textarea {
            flex: 1;
            height: 60px;
            background: #3c3c3c;
            border: 1px solid #555;
            color: white;
            padding: 10px;
            border-radius: 4px;
            resize: vertical;
            font-family: 'Segoe UI', sans-serif;
        }

        textarea:focus {
            outline: none;
            border-color: #00ff88;
        }

        button {
            padding: 8px 20px;
            background: #2d2d2d;
            color: white;
            border: 1px solid #444;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            align-self: flex-end;
            transition: all 0.2s;
        }

        button:hover {
            border-color: #00ff88;
            color: #00ff88;
        }

        button:disabled {
            background: #222;
            border-color: #333;
            color: #555;
            cursor: not-allowed;
        }

        .streaming {
            border-right: 2px solid #00ff88;
            animation: blink 0.7s infinite;
        }

        @keyframes blink {
            50% {
                border-color: transparent;
            }
        }

        /* --- Drag & Drop --- */
        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }

        #image-preview-container {
            display: none;
            /* Hidden by default */
            margin-bottom: 5px;
            padding: 8px;
            background: #252526;
            border-radius: 6px;
            border: 1px solid #444;
            position: relative;
            width: fit-content;
        }

        #image-preview-container .image-preview {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 5px;
        }

        #image-preview-container img {
            max-height: 100px;
            max-width: 200px;
            border-radius: 4px;
            display: block;
            object-fit: contain;
        }

        #image-preview-container div {
            font-size: small;
            margin-top: 5px;
        }

        #image-preview-container button {
            margin-top: 5px;
            padding: 4px 8px;
            background: #a43131;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
        }

        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }
    </style>
</head>

<body>
    <div id="container">
        <div id="sidebar">
            <div
                style="padding: 10px; font-weight: bold; font-size: 16px; border-bottom: 1px solid #444; color: #00ff88;">
                ‚ö° ANCHOR CONSOLE
            </div>
            <small id="status-text" style="padding: 0 10px;">Initializing...</small>
            <div id="progress-bar" style="margin: 8px 10px 15px 10px;">
                <div id="progress"></div>
            </div>

            <!-- COLLAPSIBLE: Model Selection -->
            <details open>
                <summary>Model Selection</summary>
                <div class="panel-content">
                    <label for="hw-profile"
                        style="display:block; font-size: 11px; color: #ccc; margin-bottom: 4px;">Hardware
                        Profile:</label>
                    <select id="hw-profile" class="form-control"
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555; margin-bottom: 10px;">
                        <option value="lite">üîã Lite (Mobile / Snapdragon) - 2k Context</option>
                        <option value="mid" selected>üíª Mid (8GB VRAM) - 4k Context</option>
                        <option value="high">üöÄ High (16GB VRAM) - 16k Context</option>
                        <option value="ultra">‚ö° Ultra (24GB+ VRAM) - 32k Context</option>
                    </select>

                    <div style="margin-bottom: 10px; text-align: right;">
                        <button class="btn btn-secondary" onclick="clearModelCache()"
                            style="padding: 4px 8px; font-size: 0.8rem; background: #a43131; border: none;">üóëÔ∏è Clear
                            Cache</button>
                    </div>

                    <select id="model-select" disabled
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555;">
                        <option value="" disabled>-- Select a Model --</option>

                        <optgroup label="‚ú® SOTA (Latest)">
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 (7B Distill)
                                [Verified]</option>
                            <option value="mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek R1 (8B Llama
                                Distill) [Verified]</option>
                            <option value="mlc-ai/Qwen3-4B-q4f16_1-MLC">Qwen 3 4B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen3-8B-q4f16_1-MLC">Qwen 3 8B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC" selected>Qwen 2.5 7B (Instruct)
                                [Verified]</option>
                            <option value="mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC">Phi 3.5 Mini (3.8B)</option>
                        </optgroup>

                        <optgroup label="üëÅÔ∏è Vision Models">
                            <option value="mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC">Phi 3.5 Vision (4.2B)
                                [Multimodal]</option>
                            <option value="mlc-ai/Llama-3.2-11B-Vision-Instruct-q4f16_1-MLC">Llama 3.2 11B Vision
                                [Multimodal]</option>
                            <option value="mlc-ai/Llama-3.2-90B-Vision-Instruct-q3f16_1-MLC">Llama 3.2 90B Vision
                                [Multimodal]</option>
                        </optgroup>

                        <optgroup label="üíª Code Specialists">
                            <option value="mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 Coder 1.5B [Neural
                                Terminal]</option>
                            <option value="mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC">Qwen 2.5 Coder 7B [Advanced]
                            </option>
                        </optgroup>

                        <optgroup label="üåü Large Models">
                            <option value="mlc-ai/Llama-3-70B-Instruct-q3f16_1-MLC">Llama 3 70B (Instruct)</option>
                            <option value="mlc-ai/Llama-3.1-70B-Instruct-q3f16_1-MLC">Llama 3.1 70B (Instruct)</option>
                        </optgroup>

                        <optgroup label="üöÄ High Performance (Small)">
                            <option value="mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 1.5B (Instruct) [Lite
                                Choice]</option>
                            <option value="mlc-ai/SmolLM2-1.7B-Instruct-q4f16_1-MLC">SmolLM2 1.7B (Instruct)</option>
                            <option value="mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama 3.2 1B (Instruct)</option>
                            <option value="mlc-ai/Qwen3-0.6B-q4f16_1-MLC">Qwen 3 0.6B (Base) [Micro]</option>
                        </optgroup>

                        <optgroup label="üß™ Experimental / Other">
                            <option value="mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B</option>
                            <option value="mlc-ai/Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama 3.1 8B</option>
                            <option value="mlc-ai/gemma-2-2b-it-q4f16_1-MLC">Gemma 2 2B</option>
                            <option value="mlc-ai/gemma-2-9b-it-q4f16_1-MLC">Gemma 2 9B</option>
                            <option value="mlc-ai/gemma-3-2b-it-q4f16_1-MLC">Gemma 3 2B</option>
                            <option value="mlc-ai/TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B</option>
                            <option value="mlc-ai/Qwen2-7B-Instruct-q4f16_1-MLC">Qwen 2 7B</option>
                            <option value="mlc-ai/Mistral-7B-Instruct-v0.3-q4f16_1-MLC">Mistral 7B v0.3</option>
                            <option value="mlc-ai/Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC">Hermes 2 Pro Llama 3 8B</option>
                            <option value="mlc-ai/Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B</option>
                        </optgroup>

                        <option value="custom">-- Custom --</option>
                    </select>
                    <input id="custom-model-input" type="text" placeholder="Custom model ID"
                        style="width: 100%; padding: 5px; font-size: 11px;" disabled />
                    <button id="load-model-btn" disabled
                        style="width: 100%; padding: 6px; margin-top:5px; font-size: 11px;">Load Model</button>
                    <div style="font-size: 9px; color: #888; margin-top: 5px;">
                        * Models marked with * may require checking availability in MLC-AI repository
                    </div>
                </div>
            </details>

            <!-- COLLAPSIBLE: Controls -->
            <details>
                <summary>System Controls</summary>
                <div class="panel-content">
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px;">
                        <input type="checkbox" id="enable-bridge-toggle" onchange="toggleBridge(this.checked)">
                        <label for="enable-bridge-toggle" style="font-size: 11px; cursor: pointer;">Enable Bridge Connection</label>
                        <div id="bridge-status" style="font-size: 10px; color: #666; margin-left: 20px;">Disconnected
                        </div>
                    </div>
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px; display: flex; align-items: center; gap: 8px;">
                        <input type="checkbox" id="autosave-toggle" checked onchange="toggleAutoSave(this.checked)">
                        <label for="autosave-toggle" style="font-size: 11px; cursor: pointer;">üíæ Auto-Save
                            Memories</label>
                    </div>
                    <button id="clear-cache-btn"
                        style="width: 100%; padding: 6px; background: #a43131; border:none; margin-top:5px; font-size: 11px;">‚ö†Ô∏è
                        Delete Model Cache</button>
                    <button id="debug-gpu-btn"
                        style="width: 100%; padding: 6px; background: #555; border:none; margin-top: 5px; font-size: 11px;">‚ùì
                        Debug GPU</button>
                    <button id="force-unlock-btn"
                        style="width: 100%; padding: 6px; background: #da3633; border:none; margin-top: 5px; font-size: 11px; color: white;">üîì
                        Force Unlock GPU</button>
                </div>
            </details>

            <!-- COLLAPSIBLE: Logs -->
            <details open style="flex: 1; display: flex; flex-direction: column;">
                <summary>System Logs</summary>
                <div class="panel-content" style="flex: 1; display: flex; flex-direction: column; overflow: hidden;">
                    <button id="copy-logs-btn" style="width: 100%; padding: 4px; background: #333; font-size: 10px;">üìã
                        Copy Logs</button>
                    <div id="status-log"></div>
                </div>
            </details>
        </div>

        <div id="resizer"></div>

        <div id="main">
            <!-- Split view for Chat and Context -->
            <div style="flex: 1; display: flex; height: 100%; overflow: hidden;">
                <!-- Chat Column -->
                <div style="flex: 1; display: flex; flex-direction: column; border-right: 1px solid #333;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üí¨ Chat Stream</div>
                    <div id="chat-box"></div>

                    <!-- Input Area with Image Upload Button -->
                    <div id="input-container"
                        style="border-top: 1px solid #333; padding: 10px; display: flex; flex-direction: column;">
                        <div id="image-preview-container"></div>
                        <div id="input-area"
                            style="display: flex; gap: 10px; padding: 5px; border: 2px dashed transparent; border-radius: 6px; transition: all 0.2s;">
                            <button id="image-upload-btn" type="button"
                                style="padding: 8px; background: #2d2d2d; color: #888; border: 1px solid #444; border-radius: 4px; cursor: pointer; font-size: 16px;"
                                title="Upload Image">üìé</button>
                            <textarea id="input" disabled placeholder="Type your message..."></textarea>
                            <button id="send-btn" disabled>Send</button>
                        </div>
                    </div>
                </div>

                <!-- Context Column -->
                <div style="width: 40%; display: flex; flex-direction: column; background: #151515;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üß† Root Memory</div>
                    <div id="context-box"
                        style="flex: 1; overflow-y: auto; padding: 10px; font-family: monospace; font-size: 12px; white-space: pre-wrap; color: #aaa;">
                        <div style="text-align: center; margin-top: 50px; color: #555;">(Active retrievals will appear
                            here)</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // --- CACHE OVERRIDE: Prevent Cache API usage that causes "Tracking Prevention" errors ---
        // This is critical for WebLLM to work in environments with strict cache policies
        if ('caches' in window) {
            try {
                // Override the Cache API to prevent WebLLM from using it
                const originalAdd = Cache.prototype.add;
                const originalAddAll = Cache.prototype.addAll;
                const originalPut = Cache.prototype.put;

                Cache.prototype.add = function(request) {
                    console.warn("Cache.add blocked by Root Coda security override");
                    return Promise.resolve();
                };

                Cache.prototype.addAll = function(requests) {
                    console.warn("Cache.addAll blocked by Root Coda security override");
                    return Promise.resolve();
                };

                Cache.prototype.put = function(request, response) {
                    console.warn("Cache.put blocked by Root Coda security override");
                    return Promise.resolve();
                };

                console.log("üõ°Ô∏è Cache API overridden to prevent tracking prevention errors");
            } catch (e) {
                console.warn("Could not override Cache API:", e);
            }
        }

        // --- IMPORTS ---
        import { AnchorLogger, createStore, getWebGPUConfig, initCozo, GPUController } from './modules/anchor.js';
        import { MemoryAuditor } from './modules/agents.js';

        import { CozoDb } from './cozo_lib_wasm.js';
        import { loadAllFromIndexedDb, writeToIndexedDb } from './indexeddb.js';
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';
        import { CreateWebWorkerMLCEngine } from "https://esm.run/@mlc-ai/web-llm";
        import { marked } from "https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js";
        import { VisionController } from './modules/vision.js';

        env.allowLocalModels = false;

        // --- ROOT KERNEL SETUP ---
        const logger = new AnchorLogger('Root-Console');
        const auditor = new MemoryAuditor();

        const { state, subscribe } = createStore({
            status: "Initializing...",
            progress: 0,
            activeModel: null,
            autoSave: true
        });

        // --- UTILS: The Archivist ---
        async function saveTurn(role, content) {
            if (!state.autoSave || !window.db) return;

            // 1. Prepare Candidate
            const ts = Date.now();
            const id = 'msg_' + ts + '_' + role;
            const candidate = { id, timestamp: ts, role, content, source: 'root_console' };

            // 2. THE AUDIT
            const audit = auditor.audit(candidate);
            if (!audit.passed) {
                ui.log(`üõë Memory Rejected: ${audit.reason}`, "error");
                return; // Stop. Do not write to DB.
            }

            try {
                // 3. Commit (Only if passed)
                const query = ":put memory { id: $id, timestamp: $ts, role: $role, content: $content, source: 'root_console', embedding: null }";
                await window.db.run(query, JSON.stringify({ id, ts, role, content }));
                ui.log(`üíæ Memory Saved (${role})`, "debug");
            } catch (e) {
                ui.log(`Save Failed: ${e.message}`, "error");
            }
        }

        // Expose toggle handler globally
        window.toggleAutoSave = (checked) => {
            state.autoSave = checked;
            ui.log(`Auto-Save: ${checked ? 'ON' : 'OFF'}`, 'info');
        };

        // Bridge legacy UI logging to Sovereign Logger
        const ui = {
            log: (msg, type = 'info') => {
                // Bridge to Kernel Logger
                if (type === 'debug') type = 'info';
                if (logger[type]) logger[type](msg); else logger.info(msg);

                // Update on-screen log
                const el = document.getElementById('status-log');
                if (el) {
                    const div = document.createElement('div');
                    div.className = type;
                    div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
                    div.style.fontSize = '11px';
                    div.style.padding = '2px';
                    div.style.borderBottom = '1px solid #222';
                    el.insertBefore(div, el.firstChild);
                    if (el.children.length > 100) el.removeChild(el.lastChild);
                }
            },
            updateProgress: (pct, text) => {
                const bar = document.getElementById('progress');
                if (bar) bar.style.width = (pct * 100) + "%";
                if (text) {
                    const el = document.getElementById('status-text');
                    if (el) el.innerText = text;
                }
            },
            append: (role, text, meta = null) => {
                const box = document.getElementById('chat-box');
                const div = document.createElement('div');
                div.className = `msg ${role}`;
                div.innerHTML = text ? marked.parse(text) : "";

                if (meta && meta.trace && meta.trace.length > 0) {
                    const details = document.createElement('details');
                    const summary = document.createElement('summary');
                    summary.textContent = 'üìã Reasoning Trace';
                    const pre = document.createElement('pre');
                    pre.textContent = JSON.stringify(meta.trace, null, 2);
                    details.appendChild(summary);
                    details.appendChild(pre);
                    div.appendChild(details);
                }

                box.appendChild(div);
                box.scrollTop = box.scrollHeight;

                return {
                    div,
                    update: (newText, isMarkdown = true) => {
                        div.innerHTML = isMarkdown ? marked.parse(newText) : newText;
                        box.scrollTop = box.scrollHeight;
                    },
                    appendText: (chunk) => {
                        if (div.innerText.endsWith("...")) div.innerText = div.innerText.slice(0, -3);
                        div.innerText += chunk;
                        box.scrollTop = box.scrollHeight;
                    }
                };
            },
            appendContext: (title, details) => {
                const box = document.getElementById('context-box');
                const div = document.createElement('div');
                div.style.borderBottom = '1px solid #333';
                div.style.marginBottom = '10px';
                div.style.paddingBottom = '10px';

                const h4 = document.createElement('div');
                h4.style.fontWeight = 'bold';
                h4.style.color = '#00ff88';
                h4.style.marginBottom = '5px';
                h4.textContent = `[${new Date().toLocaleTimeString()}] ${title}`;

                const p = document.createElement('div');
                p.style.whiteSpace = 'pre-wrap';
                p.textContent = details;

                div.appendChild(h4);
                div.appendChild(p);
                box.appendChild(div);
                box.scrollTop = box.scrollHeight;
            }
        };

        // --- GLOBAL STATE ---
        let db;
        let embedder;
        let engine;
        let contextManager;
        let selectedModelId = null;

        // Custom App Config for Qwen-Coder and other new models not in the library default
        const appConfig = {
            model_list: [
                {
                    "model": "https://huggingface.co/mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC",
                    "model_id": "mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC",
                    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2.5-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                    "overrides": { "context_window_size": 4096 }
                },
                {
                    "model": "https://huggingface.co/mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC",
                    "model_id": "mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC",
                    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                    "overrides": { "context_window_size": 4096 }
                }
            ],
            useIndexedDBCache: true
        };

        // ... (Keep existing VisionController)

        // VisionController will handle all image-related functionality

        // --- UTILS: Response Pattern Matcher ---
        class ResponsePattern {
            static match(response) {
                let data = response;
                if (typeof response === 'string') {
                    try {
                        const clean = response.replace(/```json/g, '').replace(/```/g, '').trim();
                        if (clean.startsWith('{')) data = JSON.parse(clean);
                    } catch (e) { }
                }
                if (data?.rows && Array.isArray(data.rows)) return { type: 'DB_RESULT', rows: data.rows, count: data.rows.length };
                if (data?.error || (typeof response === 'string' && response.toLowerCase().includes('error:'))) return { type: 'ERROR', error: data?.error || response };
                if (data?.ok === false && data?.message) return { type: 'ERROR', error: data.message };
                return { type: 'RAW_TEXT', text: typeof response === 'string' ? response : JSON.stringify(response) };
            }
        }

        // --- LOGIC: Context Manager (SFS) ---
        class ContextManager {
            constructor(engine, db) {
                this.engine = engine;
                this.db = db;
                this.maxIterations = 3;
            }

            // NEW: Pure retrieval logic (returns raw data objects)
            async findRelevantMemories(userText) {
                // Initialize combined results map for deduplication
                const combined = new Map();

                // 1. GENERATE QUERY VECTOR (Semantic)
                let vectorResults = [];
                if (embedder) {
                    try {
                        const output = await embedder(userText, { pooling: 'mean', normalize: true });
                        const queryVec = Array.from(output.data);
                        // Query: Find top 10 nearest neighbors
                        const vecQuery = `
                            ?[id, content, dist, timestamp] := *memory{id: id, content: content, embedding: embedding, timestamp: timestamp},
                            !is_null(embedding),
                            dist = vec_l2(embedding, $vec)
                            :sort dist
                            :limit 10
                        `;
                        const res = await this.db.run(vecQuery, JSON.stringify({ vec: queryVec }));
                        const parsed = ResponsePattern.match(res);
                        if (parsed.rows) {
                            parsed.rows.forEach(r => {
                                const item = { id: r[0], content: r[1], dist: r[2], ts: r[3], source: 'semantic' };
                                combined.set(item.content, item); // Add semantic results first
                            });
                        }
                    } catch (e) { console.warn("Vector Search failed", e); }
                }

                // 2. BM25 SEARCH (Lexical) - REPLACES REGEX SEARCH
                // This replaces the manual "Regex" loop with CozoDB's native FTS using BM25
                try {
                    // Cozo FTS Syntax: ~index_name { yield_fields | query: $q }
                    const ftsQuery = `
                        ?[id, content, score, timestamp] := ~memory:content_fts{
                            id, content, timestamp |
                            query: $q,
                            k: 20  # Get top 20 matches by BM25 score
                        }
                        :order -score
                    `;

                    // Pass the raw user text. BM25 handles stop words and importance automatically.
                    const res = await this.db.run(ftsQuery, JSON.stringify({ q: userText }));

                    if (res.rows) {
                         res.rows.forEach(r => {
                            // Use BM25 score for ranking
                            const item = {
                                id: r[0],
                                content: r[1],
                                score: r[2],
                                ts: r[3],
                                source: 'BM25'
                            };

                            // Deduplicate: If Vector found it, maybe boost score?
                            if (!combined.has(item.content)) {
                                combined.set(item.content, item);
                            }
                        });
                    }
                } catch (e) {
                    // Fallback if index missing
                    console.warn("BM25 Search failed (Index missing?):", e);

                    // Fallback to original regex approach if BM25 index doesn't exist
                    const rawWords = userText.match(/[a-zA-Z0-9_\-]+/g) || [];
                    const stopWords = new Set(['the', 'and', 'is', 'in', 'at', 'of', 'on', 'for', 'to', 'it', 'this', 'that', 'what', 'who', 'how', 'why', 'when', 'where', 'tell', 'me', 'about', 'could', 'would']);

                    // Score words: Capitalized or Numbers get higher priority
                    const scoredWords = rawWords.map(w => {
                        let score = 0;
                        const clean = w.replace(/^[-_]+|[-_]+$/g, '');
                        if (clean.length <= 3 && !/^\d+$/.test(clean)) return null; // Skip short words unless numbers
                        if (stopWords.has(clean.toLowerCase())) return null;

                        // Priority Scoring
                        if (/^[A-Z]/.test(clean)) score += 2; // Capitalized (Proper Noun)
                        if (/^\d+$/.test(clean)) score += 3;  // Numbers (Dates/Years)

                        return { word: clean, score };
                    }).filter(w => w);

                    // Sort by score (descending) and take top 5
                    scoredWords.sort((a, b) => b.score - a.score);
                    const keywords = scoredWords.slice(0, 5).map(w => w.word);

                    if (keywords.length > 0) {
                        // Process top 5 keywords for better accuracy
                        for (const keyword of keywords) {
                            const kwQuery = `?[id, content, timestamp] := *memory{id: id, content: content, timestamp: timestamp}, regex_matches(content, '(?i)${keyword}') :sort -timestamp :limit 5`;
                            try {
                                const res = await this.db.run(kwQuery, "{}");
                                const parsed = ResponsePattern.match(res);
                                if (parsed.rows) {
                                    parsed.rows.forEach(r => {
                                        const item = { id: r[0], content: r[1], dist: 0, ts: r[2], source: 'lexical' };
                                        if (!combined.has(item.content)) {
                                            combined.set(item.content, item); // Add only if not already present (deduplicate)
                                        }
                                    });
                                }
                            } catch (e) { console.warn(`Keyword search failed for '${keyword}':`, e); }
                        }
                    }
                }

                // Return top 15 unique items
                return Array.from(combined.values()).slice(0, 15);
            }

            // EXISTING: Formats the data for the R1 Reasoning Loop
            async retrieveInitialContext(userText) {
                const topItems = await this.findRelevantMemories(userText);
                if (topItems.length === 0) return "";

                const paths = [];
                const clues = topItems.map((item, index) => {
                    const id = index + 1;
                    let title = "doc_" + id;
                    // Simple title extraction
                    if (item.content.length > 0) {
                        const safeTitle = item.content.substring(0, 30).replace(/[^a-zA-Z0-9 ]/g, '').trim().replace(/\s+/g, '_').toLowerCase();
                        if (safeTitle.length > 3) title = safeTitle;
                    }
                    const path = `/knowledge/${item.source}/${title}`;
                    paths.push(path);

                    let snippet = item.content.substring(0, 300).replace(/\n/g, ' ');
                    if (item.content.length > 300) snippet += "...";

                    return `[CLUE #${id}] [Path: ${path}] [Source: ${item.source || 'unknown'}]\nSnippet: "${snippet}"`;
                });

                ui.log(`‚úÖ Hybrid Retrieval: ${topItems.length} active clues.`, "success");
                const treeMap = `[CURRENT REALITY MAP]\n` + paths.map(p => `- ${p}`).join('\n');
                return `${treeMap}\n\n[CONTEXT CLUES]\n${clues.join('\n\n')}`;
            }

            // ... (Keep buildVirtualPrompt and executeR1Loop exactly as they were in combined_text.txt source 1976-1996)
            buildVirtualPrompt(systemPrompt, retrievedMemories, chatHistory, userText) {
                const now = new Date().toLocaleString('en-US', { timeZoneName: 'short' });
                const physicsInstruction = `
### COGNITIVE PROTOCOL:
1. **TRUST THE CLUES:** You are operating in a "Sovereign" environment. The [CONTEXT CLUES] provided are the Ground Truth.
2. **MINIMIZE ENTROPY:** If a Clue contains the answer, OUTPUT IT DIRECTLY. Do not generate "filler".
3. **PATH OF LEAST ACTION:** The most efficient answer is the correct one.
`;
                return `### SYSTEM TIME\n${now}\n\n### BACKGROUND INFORMATION\n${retrievedMemories ? retrievedMemories : "No specific context available."} \n\n### CHAT HISTORY\n${chatHistory.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\n')}\n\n### INSTRUCTION\n${systemPrompt}\n${physicsInstruction}\nBased on the Background Information above, answer the following question.\n\n### USER QUESTION\n${userText}`;
            }

            async executeR1Loop(userText, history) {
                let iteration = 0;
                let currentContext = await this.retrieveInitialContext(userText);
                if (currentContext) ui.appendContext("Reflex Retrieval", currentContext);
                while (iteration < this.maxIterations) {
                    iteration++;
                    const fullPrompt = this.buildVirtualPrompt(
                        "You are Root Coda, a Sovereign Agent.\nPROTOCOL: Trust clues. Answer directly. No refusals.",
                        currentContext,
                        history.slice(-5),
                        userText
                    );
                    ui.log(`üß† Reasoning (Iter ${iteration})...`, "info");
                    let reply = "";
                    try {
                        const response = await this.engine.chat.completions.create({
                            messages: [{ role: "user", content: fullPrompt }],
                            temperature: 0.1,
                            max_tokens: 100
                        });
                        reply = response.choices[0].message.content.trim();
                    } catch (genErr) {
                        const errMsg = genErr?.message || String(genErr);
                        if (errMsg.includes("disposed")) return { context: currentContext, finalAnswer: "‚ö†Ô∏è System Crash: GPU Driver lost." };
                        throw genErr;
                    }

                    if (!reply) reply = "";

                    if (reply.includes("NEED_CONTEXT:")) {
                        const searchTerm = reply.split("NEED_CONTEXT:")[1].trim();
                        ui.log(`ü§ñ Requested search: "${searchTerm}"`, "warn");
                        const extraData = await this.retrieveInitialContext(searchTerm);
                        if (extraData) {
                            currentContext += `\n--- Additional (${searchTerm}) ---\n${extraData}`;
                            ui.appendContext(`Requested: ${searchTerm}`, extraData);
                        }
                        continue;
                    }
                    return { context: currentContext, finalAnswer: reply };
                }
                return { context: currentContext, finalAnswer: null };
            }
        }

        // --- HANDLERS ---
        async function handleSend() {
            const input = document.getElementById('input');
            const text = input.value.trim();

            // Get image from VisionController
            const imageBase64 = vision ? vision.getImage() : null;

            // Allow sending if there is text OR an image
            if ((!text && !imageBase64) || !engine) return;

            input.value = "";
            input.disabled = true;
            document.getElementById('send-btn').disabled = true;

            // Display Logic
            if (imageBase64) {
                ui.append("user", `![Uploaded Image](${imageBase64})\n\n${text}`);
            } else {
                ui.append("user", text);
            }

            if (state.autoSave) await saveTurn("user", text + (imageBase64 ? " [Image Attached]" : ""));

            try {
                // Construct Message Payload
                let messages = [];
                let context = "";

                // Wrap GPU-heavy ops in lock
                await GPUController.withLock("Root-Console-Chat", async () => {
                    // If Image: Bypass R1 Loop (Vision models typically don't do R1 reasoning yet or complex context mixing)
                    // We use a direct shot for now.
                    if (imageBase64) {
                        messages = [
                            { role: "system", content: "You are a helpful assistant. Analyze the user's image and text." },
                            {
                                role: "user",
                                content: [
                                    { type: "text", text: text || "What is in this image?" },
                                    { type: "image_url", image_url: { url: imageBase64 } }
                                ]
                            }
                        ];
                        ui.log("üëÅÔ∏è Processing Visual Data...", "warn");
                    } else {
                        // Standard Text Path (R1 Loop)
                        const r1Result = await contextManager.executeR1Loop(text, []);
                        context = r1Result.context;
                        const finalAnswer = r1Result.finalAnswer;

                        if (finalAnswer && finalAnswer.includes("System Crash")) return ui.log("üõë Execution halted (GPU Crash).", "error");

                        ui.log("üß™ Synthesizing...", "warn");
                        const sysPrompt = `You are a helpful assistant with access to retrieved context. Use it to answer the user's question.\n\nCONTEXT:\n${context || "(No relevant context)"}`;
                        messages = [{ role: "system", content: sysPrompt }, { role: "user", content: text }];
                    }

                    const msgHandle = ui.append("assistant", "");

                    const stream = await engine.chat.completions.create({
                        messages: messages,
                        max_tokens: 1024,
                        temperature: 0.7,
                        stream: true
                    });

                    let fullAnswer = "";
                    for await (const chunk of stream) {
                        const delta = chunk.choices[0]?.delta?.content || "";
                        fullAnswer += delta;
                        msgHandle.update(fullAnswer);
                    }

                    // Cleanup - Clear the image from VisionController
                    if (imageBase64 && vision) {
                        vision.clear();
                    }

                    // --- NEW CODE START: Thought Separation ---
                    let finalContent = fullAnswer;
                    let thoughtContent = null;

                    // Regex to capture DeepSeek/Reasoning  blocks<think> blocks<think> blocks
                    const thinkMatch = fullAnswer.match(/<think>([\s\S]*?)<\/think>/);

                    if (thinkMatch) {
                        thoughtContent = thinkMatch[1].trim();
                        // Remove the thought block from the "assistant" memory to keep it clean
                        finalContent = fullAnswer.replace(/<think>[\s\S]*?<\/think>/, "").trim();
                    }

                    if (state.autoSave) {
                        // 1. Save the Thought (Path)
                        if (thoughtContent) {
                            await saveTurn("thought", thoughtContent);
                        }
                        // 2. Save the Answer (Result)
                        // We explicitly save the cleaned content as the assistant's official reply
                        await saveTurn("assistant", finalContent);
                    }
                    ui.log("‚úÖ Response generated.", "success");
                }); // End Lock

            } catch (e) {
                const msg = e.message || String(e) || "Unknown Error";
                ui.log(`Error: ${msg}`, "error");
                ui.append("assistant", `**Error:** ${msg}`);
            } finally {
                input.disabled = false;
                document.getElementById('send-btn').disabled = false;
                input.focus();
            }
        }

        // ARCHIVED MODEL LOADING FUNCTION - KEPT FOR FUTURE REFERENCE
        // This function was attempting to use local model files with bridge downloads
        // which was causing hangs and issues. See new online-only implementation below.
        async function loadModel_archived() {
            const select = document.getElementById('model-select');
            const customInput = document.getElementById('custom-model-input');
            const modelInput = select.value === 'custom' ? customInput.value : select.value;
            if (!modelInput) return alert("Please select a model.");

            selectedModelId = modelInput;
            document.getElementById('load-model-btn').disabled = true;

            try {
                // 0. THE BLOCKER (Model Load Lock) - Serialize model loading
                ui.log("‚è≥ Queueing for Model Load (Sequential)...", "info");

                await GPUController.withModelLoadLock("Root-Console-Init", async () => {
                    ui.log(`Initializing Engine (${selectedModelId})...`, "info");

                    // --- KERNEL: Hardware Config ---
                    const hwProfile = document.getElementById('hw-profile').value;
                    const gpuConfig = await getWebGPUConfig(hwProfile);

                    if (gpuConfig.isConstrained) {
                        ui.log(`‚ö†Ô∏è Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, "warn");
                    } else {
                        ui.log(`‚úÖ GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, "success");
                    }

                    // --- Config Generation ---
                    // (Simplified Logic for cleaner file)
                    const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/"; // Fixed URL
                    let modelLib = null;
                    const lowerId = selectedModelId.toLowerCase();
                    let qTag = "q4f16_1"; // Default

                    // Mapper
                    // SOTA / New
                    if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('deepseek-r1') && lowerId.includes('8b') && lowerId.includes('llama')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;

                    // Vision Models
                    else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;

                    // Large Models (70B)
                    else if (lowerId.includes('llama-3-70b')) modelLib = libBase + `Llama-3-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.1-70b')) modelLib = libBase + `Llama-3.1-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    // Vision Models
                    else if (lowerId.includes('llama-3.2-11b-vision')) modelLib = libBase + `Llama-3.2-11B-Vision-Instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.2-90b-vision')) modelLib = libBase + `Llama-3.2-90B-Vision-Instruct-q3f16_1-ctx4k_cs2k-webgpu.wasm`;

                    // Other Popular Models
                    else if (lowerId.includes('llama-3.1-8b')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('gemma-2-9b')) modelLib = libBase + `gemma-2-9b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('gemma-3-2b')) modelLib = libBase + `gemma-3-2b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('mistral-7b')) modelLib = libBase + `Mistral-7B-Instruct-v0.3-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('hermes-2-pro')) modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('hermes-3')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    // Small / Efficient
                    else if (lowerId.includes('qwen3-0.6b')) modelLib = libBase + `Qwen3-0.6B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.2-1b')) modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;

                    // Fallbacks
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B



                    // 1. Clear previous engine if exists
                    if (engine) {
                        ui.log("Unloading previous engine...", "info");
                        await engine.unload();
                        engine = null;
                        ui.log("Engine Unloaded", "success");
                    }



                    // Custom App Config for Qwen-Coder and other new models not in the library default
                    const appConfig = {
                        // Disable all caching to prevent "Tracking Prevention" issues
                        useIndexedDBCache: false,
                        cacheDir: null,
                        // Additional cache-busting settings
                        "mlc.cache": false,
                        "cache.enabled": false,

                        model_list: [
                            {
                                "model": window.location.origin + "/models/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC/resolve/main/",
                                "model_id": "mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC",
                                "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                                "vram_required_MB": 2000,
                                "low_resource_required": true,
                                "buffer_size_required_bytes": gpuConfig.maxBufferSize,
                                "overrides": {
                                    "context_window_size": gpuConfig.isConstrained ? 2048 : 4096
                                },
                                "tokenizer_files": [
                                    "tokenizer_config.json",
                                    "vocab.json",
                                    "merges.txt",
                                    "tokenizer.json"
                                ]
                            },
                            {
                                "model": window.location.origin + "/models/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC/resolve/main/",
                                "model_id": "mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC",
                                "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                                "vram_required_MB": 2000,
                                "low_resource_required": true,
                                "buffer_size_required_bytes": gpuConfig.maxBufferSize,
                                "overrides": {
                                    "context_window_size": gpuConfig.isConstrained ? 2048 : 4096
                                },
                                "tokenizer_files": [
                                    "tokenizer_config.json",
                                    "vocab.json",
                                    "merges.txt",
                                    "tokenizer.json"
                                ]
                            },
                            {
                                "model": window.location.origin + "/models/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC/resolve/main/",
                                "model_id": "mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC",
                                "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                                "vram_required_MB": 2000,
                                "low_resource_required": true,
                                "buffer_size_required_bytes": gpuConfig.maxBufferSize,
                                "overrides": {
                                    "context_window_size": gpuConfig.isConstrained ? 2048 : 4096
                                }
                            },
                            {
                                "model": window.location.origin + "/models/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC/resolve/main/",
                                "model_id": "mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC",
                                "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                                "vram_required_MB": 2000,
                                "low_resource_required": true,
                                "buffer_size_required_bytes": gpuConfig.maxBufferSize,
                                "overrides": {
                                    "context_window_size": gpuConfig.isConstrained ? 2048 : 4096
                                }
                            },
                            {
                                "model": window.location.origin + "/models/Qwen2.5-7B-Instruct-q4f16_1-MLC/resolve/main/",
                                "model_id": "mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC",
                                "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                                "vram_required_MB": 2000,
                                "low_resource_required": true,
                                "buffer_size_required_bytes": gpuConfig.maxBufferSize,
                                "overrides": {
                                    "context_window_size": gpuConfig.isConstrained ? 2048 : 4096
                                }
                            }
                        ]
                    };

                    // 2.5: CHECK AND DOWNLOAD MODEL LOCALLY IF NEEDED
                    const strippedId = selectedModelId.split('/').pop(); // "mlc-ai/foo" -> "foo"
                    // Sanitize the path to ensure it's safe for URL construction
                    const safeStrippedId = strippedId.replace(/[^a-zA-Z0-9._-]/g, '_');
                    const localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;
                    const check = await fetch(localModelUrl, { method: 'HEAD' });

                    if (!check.ok) {
                        ui.log(`‚¨áÔ∏è Model missing locally. Initiating On-Demand Pull...`, "warn");
                        const pullRes = await fetch('/v1/models/pull', { // Use relative path to current server
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({
                                model_id: selectedModelId,
                                url: `https://huggingface.co/${selectedModelId}`
                            })
                        });

                        if (pullRes.ok) {
                            ui.log("‚è≥ Downloading Model (Server-Side)...", "info");
                            // Poll for completion
                            while (true) {
                                await new Promise(r => setTimeout(r, 2000));
                                const statusRes = await fetch(`/v1/models/pull/status?id=${selectedModelId}`); // Use relative path to current server
                                const status = await statusRes.json();
                                if (status.status === "done") {
                                    ui.log("‚úÖ Download Complete!", "success");
                                    break;
                                } else if (status.status === "error") {
                                    throw new Error(`Download Failed: ${status.error}`);
                                } else {
                                    ui.updateProgress(0.1, `Downloading: ${status.progress} (${status.file})`);
                                }
                            }
                        } else {
                            // NEW: Handle Pull Request Failure (e.g. 404/500)
                            const errText = await pullRes.text();
                            throw new Error(`Bridge refused download (${pullRes.status}): ${errText}. Is the Bridge updated?`);
                        }
                    }

                    // 3. Initialize Engine with Custom Config
                    ui.log(`Initializing Engine (${selectedModelId})...`, 'info');

                    // Add the selected model to the config if it's not already in the appConfig model list
                    // This ensures that any selected model is properly configured with cache disabled
                    const modelExists = appConfig.model_list.some(model => model.model_id === selectedModelId);
                    if (!modelExists) {
                        // Add the selected model to the configuration with cache disabled
                        // Use full URL format to match the existing appConfig structure with resolve/main pattern
                        const strippedModelId = selectedModelId.split('/').pop(); // Extract just the model name part
                        // Ensure the model path is properly formatted (no special characters that could break URL parsing)
                        const safeModelPath = strippedModelId.replace(/[^a-zA-Z0-9._-]/g, '_'); // Sanitize path
                        appConfig.model_list.push({
                            model: window.location.origin + `/models/${safeModelPath}/resolve/main/`,
                            model_id: selectedModelId,
                            model_lib: modelLib,
                            vram_required_MB: 2000,
                            low_resource_required: true,
                            buffer_size_required_bytes: gpuConfig.maxBufferSize,
                            overrides: {
                                context_window_size: gpuConfig.isConstrained ? 2048 : 4096
                            }
                        });
                    }

                    // Note: CreateWebWorkerMLCEngine merges this with default prebuiltAppConfig
                    // Additional configuration to ensure no caching occurs
                    try {
                        engine = await CreateWebWorkerMLCEngine(
                            new Worker('./modules/llm-worker.js', { type: 'module' }),
                            selectedModelId,
                            {
                                initProgressCallback: (report) => {
                                    ui.updateProgress(report.progress, report.text);
                                    if (report.progress === 1) ui.log("‚úÖ Engine Ready", "success");
                                },
                                appConfig: appConfig, // Pass our custom config here!
                                // Additional cache-busting parameters
                                logLevel: "INFO",
                                // Force-disable cache mechanisms at the engine level
                                useIndexedDBCache: false,
                                // Additional parameters to ensure no caching
                                "mlc.cache": false,
                                "cache.enabled": false
                            }
                        );
                    } catch (cacheError) {
                        // If cache-related error occurs, try fallback approach
                        if (cacheError.message && (cacheError.message.includes("Cache") || cacheError.message.includes("cache"))) {
                            ui.log("üîÑ Cache blocked, trying fallback approach...", "warn");

                            // Try with minimal configuration to bypass cache issues
                            engine = await CreateWebWorkerMLCEngine(
                                new Worker('./modules/llm-worker.js', { type: 'module' }),
                                selectedModelId,
                                {
                                    initProgressCallback: (report) => {
                                        ui.updateProgress(report.progress, report.text);
                                        if (report.progress === 1) ui.log("‚úÖ Engine Ready", "success");
                                    },
                                    // Minimal config to avoid cache operations
                                    appConfig: {
                                        useIndexedDBCache: false,
                                        model_list: appConfig.model_list
                                    },
                                    logLevel: "INFO"
                                }
                            );
                        } else {
                            // If it's not a cache error, re-throw
                            throw cacheError;
                        }
                    }
                }); // Uses the default 5-minute timeout for model loading

                ui.log("üéâ Root Console Online", "success");
                await initializeContextManager();
                document.getElementById('input').disabled = false;
                document.getElementById('send-btn').disabled = false;
                document.getElementById('input').focus();

            } catch (e) {
                const errorMsg = e.message || String(e);
                ui.log(`Load Failed: ${errorMsg}`, "error");

                // Specific handling for cache-related errors
                if (errorMsg && (errorMsg.includes("Cache") || errorMsg.includes("cache"))) {
                    ui.log(`üö® CACHE ERROR: The browser is blocking cache access.`, "error");
                    ui.log(`üí° SOLUTION: Try using an Incognito/Private window, or check browser security settings.`, "warn");
                }

                // Provide suggestion for common model name issues
                if (errorMsg && errorMsg.includes("Network response was not ok")) {
                    ui.log(`üí° Hint: Model may not exist or be temporarily unavailable. Try another model.`, "warn");
                }
                if (errorMsg && errorMsg.includes("QuotaExceededError")) {
                    ui.log(`üí° STORAGE FULL: Click 'üóëÔ∏è Clear Cache' to reset your browser storage.`, "error");
                    // Auto-suggest cleanup
                    if (confirm("Storage Quota Exceeded. Would you like to run the Deep Cleanup tool now?")) {
                        clearModelCache();
                    }
                }
                document.getElementById('load-model-btn').disabled = false;
            }
        }

        // NEW MODEL LOADING FUNCTION - ONLINE ONLY
        // Based on the working anchor-mic.html implementation
        async function loadModel() {
            const select = document.getElementById('model-select');
            const customInput = document.getElementById('custom-model-input');
            const modelInput = select.value === 'custom' ? customInput.value : select.value;
            if (!modelInput) return alert("Please select a model.");

            selectedModelId = modelInput;
            document.getElementById('load-model-btn').disabled = true;

            try {
                // 0. THE BLOCKER (Model Load Lock) - Serialize model loading
                ui.log("‚è≥ Queueing for Model Load (Sequential)...", "info");

                await GPUController.withModelLoadLock("Root-Console-Init", async () => {
                    ui.log(`Initializing Engine (${selectedModelId})...`, "info");

                    // --- KERNEL: Hardware Config ---
                    const hwProfile = document.getElementById('hw-profile').value;
                    const gpuConfig = await getWebGPUConfig(hwProfile);

                    if (gpuConfig.isConstrained) {
                        ui.log(`‚ö†Ô∏è Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, "warn");
                    } else {
                        ui.log(`‚úÖ GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, "success");
                    }

                    // --- Config Generation ---
                    // (Simplified Logic for cleaner file)
                    const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/"; // Fixed URL
                    let modelLib = null;
                    const lowerId = selectedModelId.toLowerCase();
                    let qTag = "q4f16_1"; // Default

                    // Mapper
                    // SOTA / New
                    if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('deepseek-r1') && lowerId.includes('8b') && lowerId.includes('llama')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;

                    // Vision Models
                    else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;

                    // Large Models (70B)
                    else if (lowerId.includes('llama-3-70b')) modelLib = libBase + `Llama-3-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.1-70b')) modelLib = libBase + `Llama-3.1-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    // Vision Models
                    else if (lowerId.includes('llama-3.2-11b-vision')) modelLib = libBase + `Llama-3.2-11B-Vision-Instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.2-90b-vision')) modelLib = libBase + `Llama-3.2-90B-Vision-Instruct-q3f16_1-ctx4k_cs2k-webgpu.wasm`;

                    // Other Popular Models
                    else if (lowerId.includes('llama-3.1-8b')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('gemma-2-9b')) modelLib = libBase + `gemma-2-9b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('gemma-3-2b')) modelLib = libBase + `gemma-3-2b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('mistral-7b')) modelLib = libBase + `Mistral-7B-Instruct-v0.3-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('hermes-2-pro')) modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('hermes-3')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    // Small / Efficient
                    else if (lowerId.includes('qwen3-0.6b')) modelLib = libBase + `Qwen3-0.6B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.2-1b')) modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;

                    // Fallbacks
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B

                    // 1. Clear previous engine if exists
                    if (engine) {
                        ui.log("Unloading previous engine...", "info");
                        await engine.unload();
                        engine = null;
                        ui.log("Engine Unloaded", "success");
                    }

                    // Create appConfig using bridge redirect endpoint for model resolution
                    const appConfig = {
                        model_list: [{
                            model: window.location.origin + "/models/" + selectedModelId + "/resolve/main/",
                            model_id: selectedModelId,
                            model_lib: modelLib,
                            vram_required_MB: 2000,
                            low_resource_required: true,
                            buffer_size_required_bytes: gpuConfig.maxBufferSize,
                            overrides: {
                                context_window_size: gpuConfig.isConstrained ? 2048 : 4096
                            }
                        }],
                        useIndexedDBCache: false, // Disable caching to prevent issues
                    };

                    // Initialize Engine with the new online-only approach
                    ui.log(`Initializing Engine (${selectedModelId})...`, 'info');

                    try {
                        engine = await CreateWebWorkerMLCEngine(
                            new Worker('./modules/llm-worker.js', { type: 'module' }),
                            selectedModelId,
                            {
                                initProgressCallback: (report) => {
                                    ui.updateProgress(report.progress, report.text);
                                    if (report.progress === 1) ui.log("‚úÖ Engine Ready", "success");
                                },
                                appConfig: appConfig,
                                logLevel: "INFO",
                                useIndexedDBCache: false, // Force disable cache
                            }
                        );
                    } catch (error) {
                        ui.log(`Engine initialization failed: ${error.message}`, "error");
                        throw error;
                    }
                }); // Uses the default 5-minute timeout for model loading

                ui.log("üéâ Root Console Online", "success");
                await initializeContextManager();
                document.getElementById('input').disabled = false;
                document.getElementById('send-btn').disabled = false;
                document.getElementById('input').focus();

            } catch (e) {
                const errorMsg = e.message || String(e);
                ui.log(`Load Failed: ${errorMsg}`, "error");

                // Specific handling for cache-related errors
                if (errorMsg && (errorMsg.includes("Cache") || errorMsg.includes("cache"))) {
                    ui.log(`üö® CACHE ERROR: The browser is blocking cache access.`, "error");
                    ui.log(`üí° SOLUTION: Try using an Incognito/Private window, or check browser security settings.`, "warn");
                }

                // Provide suggestion for common model name issues
                if (errorMsg && errorMsg.includes("Network response was not ok")) {
                    ui.log(`üí° Hint: Model may not exist or be temporarily unavailable. Try another model.`, "warn");
                }
                if (errorMsg && errorMsg.includes("QuotaExceededError")) {
                    ui.log(`üí° STORAGE FULL: Click 'üóëÔ∏è Clear Cache' to reset your browser storage.`, "error");
                    // Auto-suggest cleanup
                    if (confirm("Storage Quota Exceeded. Would you like to run the Deep Cleanup tool now?")) {
                        clearModelCache();
                    }
                }
                document.getElementById('load-model-btn').disabled = false;
            }
        }

        async function clearModelCache() {
            if (!confirm("‚ö†Ô∏è STORAGE RESET REQUIRED\n\nYour browser's storage is full (QuotaExceededError). We need to perform a full cleanup.\n\nThis will:\n1. Delete ALL downloaded models.\n2. Clear the browser cache.\n3. Unregister cached service workers.\n\nClick OK to proceed.")) return;

            ui.log("üßπ Starting Deep Cleanup...", "warn");

            try {
                // 1. Clear IndexedDB
                const dbs = await window.indexedDB.databases();
                for (const db of dbs) {
                    ui.log(`Deleting DB: ${db.name}...`, "info");
                    window.indexedDB.deleteDatabase(db.name);
                }

                // 2. Clear Cache API (Crucial for large file downloads)
                if ('caches' in window) {
                    const cacheKeys = await caches.keys();
                    for (const key of cacheKeys) {
                        ui.log(`Deleting Cache: ${key}...`, "info");
                        await caches.delete(key);
                    }
                }

                // 3. Unregister Service Workers
                if ('serviceWorker' in navigator) {
                    const registrations = await navigator.serviceWorker.getRegistrations();
                    for (const registration of registrations) {
                        ui.log(`Unregistering SW: ${registration.scope}...`, "info");
                        await registration.unregister();
                    }
                }

                ui.log("‚úÖ Cleanup Complete. Reloading...", "success");
                alert("Storage cleared successfully. The page will now reload.");
                window.location.reload();
            } catch (e) {
                ui.log(`‚ùå Cleanup Failed: ${e.message}`, "error");
                alert("Automatic cleanup failed. Please manually clear your browser's 'Site Data' in settings.");
            }
        }

        let vision = null; // Global VisionController instance

        async function init() {
            try {
                ui.log("üöÄ Root Kernel Starting...", "info");

                // 0. STORAGE DIAGNOSTICS
                if (navigator.storage && navigator.storage.persist) {
                    const isPersisted = await navigator.storage.persist();
                    ui.log(`üíæ Storage Persistence: ${isPersisted ? "GRANTED" : "DENIED"}`, isPersisted ? "success" : "warn");
                }
                if (navigator.storage && navigator.storage.estimate) {
                    const quota = await navigator.storage.estimate();
                    const used = (quota.usage / 1024 / 1024).toFixed(2);
                    const total = (quota.quota / 1024 / 1024).toFixed(2);
                    ui.log(`üìä Storage Quota: ${used}MB / ${total}MB`, "info");
                }

                // 1. CozoDB
                await initCozo('./cozo_lib_wasm_bg.wasm');
                // Recovery Logic
                try {
                    const [keys] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });
                    if (keys.length > 0) {
                        db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        window.db = db;
                        ui.log("‚úÖ Root Graph Connected (Persistent)", "success");
                    } else {
                        db = CozoDb.new();
                        window.db = db;
                        ui.log("‚úÖ Root Graph Created (Memory)", "info");
                    }
                } catch (e) {
                    db = CozoDb.new();
                    window.db = db;
                    ui.log("‚ö†Ô∏è Fallback to Memory Graph", "warn");
                }

                // 2. Embedder (Optional)
                ui.updateProgress(0.3, "Loading Embedder...");
                const embedderPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', { device: 'wasm' });
                try {
                    embedder = await Promise.race([embedderPromise, new Promise((_, r) => setTimeout(() => r(new Error("Timeout")), 10000))]);
                    ui.log("‚úÖ Neural Embedder Ready", "success");
                } catch (e) { ui.log("‚ö†Ô∏è Embedder Skipped (Timeout)", "warn"); }

                // 3. Ensure FTS Index for BM25 Search
                try {
                    // Create FTS index for BM25 search on content field if it doesn't exist
                    await db.run("::fts create memory:content_fts { extractor: content, tokenizer: Simple, filters: [Lowercase, AlphaNumOnly, Stemmer('english')] }", "{}");
                    ui.log("‚úÖ FTS Index Created for BM25 Search", "success");
                } catch (e) {
                    // Index might already exist, which is fine
                    if (!e.message.includes('already exists') && !e.message.includes('conflicts')) {
                        ui.log("‚ö†Ô∏è FTS Index creation failed (may already exist): " + e.message, "warn");
                    } else {
                        ui.log("‚úÖ FTS Index already exists", "success");
                    }
                }

            } catch (e) {
                ui.log(`Init Error: ${e.message}`, "error");
            } finally {
                // Initialize VisionController
                vision = new VisionController();
                vision.setup('input-area', 'image-preview-container', 'input');

                // Ensure controls are unlocked even if init fails (partial functionality)
                ui.updateProgress(1.0, "Ready");

                // Fix: Copy Logs Button
                const copyBtn = document.getElementById('copy-logs-btn');
                if (copyBtn) {
                    copyBtn.onclick = () => {
                        const logs = document.getElementById('status-log').innerText;
                        navigator.clipboard.writeText(logs).then(() => ui.log("üìã Logs copied to clipboard", "success"));
                    };
                }

                document.getElementById('model-select').disabled = false;
                document.getElementById('load-model-btn').disabled = false;
                document.getElementById('load-model-btn').addEventListener('click', loadModel);

                // Input Handlers (idempotent)
                const sendBtn = document.getElementById('send-btn');
                const input = document.getElementById('input');

                if (sendBtn && input) {
                    sendBtn.onclick = handleSend; // Use property to avoid duplicates
                    input.onkeydown = (e) => {
                        if (e.key === 'Enter' && !e.shiftKey && !sendBtn.disabled) {
                            e.preventDefault();
                            handleSend();
                        }
                    };
                }

                // --- GHOST PROTOCOL (Headless Auto-Start) ---
                const urlParams = new URLSearchParams(window.location.search);
                if (urlParams.get('headless') === 'true') {
                    ui.log("üëª Ghost Mode Detected: Initiating Auto-Sequence...", "warn");

                    // 1. Auto-Connect Bridge
                    const bridgeToggle = document.getElementById('enable-bridge-toggle');
                    if (bridgeToggle && !bridgeToggle.checked) {
                        bridgeToggle.checked = true;
                        toggleBridge(true);
                        ui.log("üëª Bridge Connection: ENABLED", "success");
                    }

                    // 2. Auto-Load Model (Qwen-Coder 1.5B for speed/utility)
                    // We use a small delay to ensure CozoDB is ready
                    setTimeout(() => {
                        const targetModel = "mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC";
                        const selector = document.getElementById('model-select');

                        if (selector) {
                            selector.value = targetModel;
                            // If value didn't stick (because model list is dynamic), try setting custom
                            if (selector.value !== targetModel) {
                                selector.value = 'custom';
                                document.getElementById('custom-model-input').value = targetModel;
                            }

                            ui.log(`üëª Ghost Auto-Load: ${targetModel}`, "info");
                            loadModel(); // Trigger the existing load logic
                        }
                    }, 2000);
                }
            }
        }

        // Initialize context manager after all dependencies are loaded
        async function initializeContextManager() {
            if (engine && db) {
                contextManager = new ContextManager(engine, db);
                ui.log("üß† Context Manager Ready", "success");
                return true;
            }
            return false;
        }

        // --- BRIDGE LOGIC ---
        let bridgeWs = null;

        // GPU Status Check and Force Unlock Handler
        async function checkGPUStatus() {
            try {
                const res = await fetch("/v1/gpu/status", {
                    method: "GET",
                    headers: { "Authorization": "Bearer sovereign-secret" }
                });

                if (res.ok) {
                    const status = await res.json();
                    ui.log(`GPU Status: ${status.locked ? `LOCKED by ${status.owner}` : 'FREE'}. Queue: ${status.queue_depth} items.`, "info");
                    if (status.queued && status.queued.length > 0) {
                        ui.log(`Queued: ${status.queued.join(', ')}`, "info");
                    }

                    // Check model loading status
                    const modelLoadStatus = GPUController.getModelLoadStatus();
                    if (modelLoadStatus.hasPendingLoad || modelLoadStatus.queueSize > 0) {
                        ui.log(`Model Load Status: Queue: ${modelLoadStatus.queueSize}, Active: ${modelLoadStatus.activeLoaders.join(', ')}`, "info");
                    }

                    return status;
                } else {
                    ui.log(`Status check failed: ${res.status}`, "warn");
                    return null;
                }
            } catch (e) {
                ui.log(`Status check error: ${e.message}`, "warn");
                return null;
            }
        }

        document.getElementById('debug-gpu-btn').addEventListener('click', async () => {
            await checkGPUStatus();
        });

        document.getElementById('force-unlock-btn').addEventListener('click', async () => {
            if (!confirm("‚ö†Ô∏è Force Unlock GPU?\nOnly do this if the system is stuck waiting for a lock.")) return;

            try {
                ui.log("Sending Force Unlock signal...", "warn");

                // First try the standard reset
                let res = await fetch("/v1/gpu/reset", {
                    method: "POST",
                    headers: { "Authorization": "Bearer sovereign-secret" }
                });

                if (res.ok) {
                    ui.log("üîì GPU Lock Force Released (Standard).", "success");
                } else {
                    ui.log(`Standard unlock failed: ${res.status}. Trying emergency release...`, "warn");

                    // If standard unlock failed, try the emergency endpoint
                    res = await fetch("/v1/gpu/force-release-all", {
                        method: "POST",
                        headers: { "Authorization": "Bearer sovereign-secret" }
                    });

                    if (res.ok) {
                        ui.log("üîì GPU Locks Force Released (Emergency).", "success");
                    } else {
                        ui.log(`Emergency unlock failed: ${res.status}`, "error");
                    }
                }
            } catch (e) {
                ui.log(`Unlock Error: ${e.message}`, "error");
            }
        });

        window.toggleBridge = function (enabled) {
            if (enabled) {
                // Check if database is available (minimum requirement for bridge)
                if (!db) {
                    alert("Database not initialized. Please wait for CozoDB to load.");
                    document.getElementById('enable-bridge-toggle').checked = false;
                    return;
                }

                // Use the same host and port as the current page for WebSocket connection
                const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsHost = window.location.host; // This will be 'localhost:8000' when served from the bridge
                bridgeWs = new WebSocket(`${wsProtocol}//${wsHost}/ws/chat`);
                bridgeWs.onopen = () => { document.getElementById('bridge-status').innerText = "üü¢ Connected"; ui.log("Bridge Online", "success"); };
                bridgeWs.onmessage = async (e) => {
                    const msg = JSON.parse(e.data);

                    // 1. STANDARD CHAT REQUEST (e.g. from VS Code) - requires LLM engine
                    if (msg.type === 'chat') {
                        // Check if engine is available for chat requests
                        if (!engine) {
                            const errorResponse = {
                                id: msg.id,
                                error: "LLM engine not loaded. Bridge only supports memory queries without loaded model."
                            };
                            bridgeWs.send(JSON.stringify(errorResponse));
                            ui.log(`‚ùå Chat request failed: LLM engine not loaded`, "error");
                            return;
                        }

                        ui.log(`Bridge Chat: ${msg.id}`, "info");
                        try {
                            const completion = await engine.chat.completions.create({
                                messages: msg.data.messages,
                                stream: true
                            });
                            for await (const chunk of completion) {
                                bridgeWs.send(JSON.stringify({ id: msg.id, chunk }));
                            }
                            bridgeWs.send(JSON.stringify({ id: msg.id, done: true }));
                        } catch (err) {
                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));
                        }
                    }

                    // 2. MEMORY SEARCH REQUEST (From Chrome Extension) - works without LLM engine
                    else if (msg.type === 'memory_query') {
                        const queryText = msg.data.query;
                        ui.log(`üîé Bridge Memory Search: "${queryText}"`, "info");

                        // Initialize context manager if not ready (only for memory queries)
                        if (!contextManager) {
                            await initializeContextManager();
                        }

                        // Check if context manager is now available
                        if (!contextManager) {
                            const errorResponse = {
                                id: msg.id,
                                error: "Context manager not ready. Database may not be fully initialized."
                            };
                            bridgeWs.send(JSON.stringify(errorResponse));
                            ui.log(`‚ùå Context manager not ready for query: ${queryText}`, "error");
                            return;
                        }

                        try {
                            // Use the new decoupled method
                            const results = await contextManager.findRelevantMemories(queryText);

                            // Send pure JSON back to the bridge
                            bridgeWs.send(JSON.stringify({
                                id: msg.id,
                                result: results
                            }));
                            ui.log(`‚úÖ Returned ${results.length} memories`, "success");
                        } catch (err) {
                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));
                            ui.log(`‚ùå Search Failed: ${err.message}`, "error");
                        }
                    }

                    // 3. MEMORY INGESTION REQUEST (From Daemon Eyes) - adds content to memory graph
                    else if (msg.type === 'memory_ingest') {
                        const content = msg.data.content;
                        const source = msg.data.source || 'daemon';
                        const timestamp = msg.data.timestamp || Date.now();
                        const tags = msg.data.tags || [];

                        ui.log(`üß† Bridge Memory Ingest: "${content.substring(0, 50)}..." from ${source}`, "info");

                        try {
                            // Save to memory graph using the same mechanism as user messages
                            const id = 'daemon_' + timestamp + '_' + Math.random().toString(36).substr(2, 9);
                            const candidate = {
                                id,
                                timestamp,
                                role: 'memory',
                                content,
                                source,
                                tags: tags.join(',')
                            };

                            // THE AUDIT (same as user messages)
                            const audit = auditor.audit(candidate);
                            if (!audit.passed) {
                                ui.log(`üõë Memory Rejected: ${audit.reason}`, "error");
                                // Still send success response but log the rejection
                                bridgeWs.send(JSON.stringify({
                                    id: msg.id,
                                    status: "rejected",
                                    reason: audit.reason
                                }));
                                return;
                            }

                            // Commit to DB (same as saveTurn function)
                            const query = ":put memory { id: $id, timestamp: $ts, role: $role, content: $content, source: $source, tags: $tags }";
                            await window.db.run(query, JSON.stringify({
                                id,
                                ts: timestamp,
                                role: 'memory',
                                content,
                                source,
                                tags: tags.join(',')
                            }));

                            ui.log(`üíæ Memory Saved from ${source} (${content.length} chars)`, "success");

                            // Send success response back to daemon
                            bridgeWs.send(JSON.stringify({
                                id: msg.id,
                                status: "success",
                                message: "Memory ingested successfully"
                            }));
                        } catch (err) {
                            bridgeWs.send(JSON.stringify({
                                id: msg.id,
                                status: "error",
                                error: err.message
                            }));
                            ui.log(`‚ùå Ingest Failed: ${err.message}`, "error");
                        }
                    }

                    // 4. MEMORY SEARCH REQUEST (From ask_memory.py tool) - searches memory graph
                    else if (msg.type === 'memory_search') {
                        const queryText = msg.query;
                        const reqId = msg.id;
                        ui.log(`üîç External Memory Search: "${queryText}"`, "info");

                        try {
                            // Initialize context manager if not ready
                            if (!contextManager) {
                                await initializeContextManager();
                            }

                            // Check if context manager is available
                            if (!contextManager) {
                                const errorResponse = {
                                    id: reqId,
                                    type: 'memory_search_result',
                                    status: 'error',
                                    error: "Context manager not ready. Database may not be fully initialized."
                                };
                                bridgeWs.send(JSON.stringify(errorResponse));
                                ui.log(`‚ùå Context manager not ready for search: ${queryText}`, "error");
                                return;
                            }

                            // Use the existing ContextManager to find relevant memories
                            const results = await contextManager.findRelevantMemories(queryText);

                            // Format results for external consumption
                            let formattedContext = `[ANCHOR MEMORY CONTEXT]\n`;
                            formattedContext += `[Query: ${queryText}]\n\n`;

                            if (results && results.length > 0) {
                                formattedContext += `[FOUND ${results.length} RELEVANT MEMORIES]\n\n`;
                                results.forEach((result, index) => {
                                    formattedContext += `[MEMORY ${index + 1}]\n`;
                                    formattedContext += `Source: ${result.source || 'unknown'}\n`;
                                    formattedContext += `Content: ${result.content || result.text || ''}\n`;
                                    formattedContext += `Timestamp: ${result.timestamp || 'unknown'}\n`;
                                    formattedContext += `---\n\n`;
                                });
                            } else {
                                formattedContext += `[NO RELEVANT MEMORIES FOUND]\n`;
                            }

                            // Send formatted result back to the bridge
                            bridgeWs.send(JSON.stringify({
                                id: reqId,
                                type: 'memory_search_result',
                                status: 'success',
                                result: formattedContext
                            }));

                            ui.log(`‚úÖ Returned ${results.length} memories for external search`, "success");
                        } catch (err) {
                            const errorResponse = {
                                id: reqId,
                                type: 'memory_search_result',
                                status: 'error',
                                error: err.message
                            };
                            bridgeWs.send(JSON.stringify(errorResponse));
                            ui.log(`‚ùå External search failed: ${err.message}`, "error");
                        }
                    }
                };
            } else if (bridgeWs) { bridgeWs.close(); bridgeWs = null; document.getElementById('bridge-status').innerText = "Disconnected"; }
        };

        window.addEventListener('load', init);
    </script>
</body>

</html>