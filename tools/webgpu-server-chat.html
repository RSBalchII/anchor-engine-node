<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>WebGPU Chat Server (Worker)</title>
    <style>
        body {
            background: #001;
            color: #0f0;
            font-family: monospace;
            padding: 20px;
        }

        .log {
            margin-top: 10px;
            color: #8c8;
            font-size: 0.9em;
        }

        .success {
            color: #0f0;
        }

        .error {
            color: #f00;
        }

        #status {
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 20px;
        }

        input,
        select {
            background: #222;
            color: #fff;
            border: 1px solid #444;
            padding: 5px;
        }

        button {
            background: #060;
            color: #fff;
            border: none;
            padding: 5px 15px;
            cursor: pointer;
        }
    </style>
</head>

<body>
    <div id="status">ðŸ”´ Disconnected</div>
    <div>
        <label>Chat Model:</label>
        <input type="text" id="model-input" value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC" style="width: 400px;">
        <!-- Common options helper -->
        <select id="model-helper" onchange="document.getElementById('model-input').value = this.value">
            <option value="">-- Presets --</option>
            <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5-1.5B (Fast)</option>
            <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5-7B (Balanced)</option>
            <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek-R1-7B</option>
            <option value="DeepSeek-V2-Lite-Chat-q4f16_1-MLC">DeepSeek-V2-Lite</option>
            <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama-3.1-8B</option>
        </select>
    </div>
    <div style="margin-top: 10px;">
        <label>Bridge URL:</label>
        <input type="text" id="bridge-url" value="ws://localhost:8080/ws/chat" style="width: 300px;">
        <button id="connect-btn">Start Server</button>
    </div>
    <div id="logs" style="margin-top: 20px; border-top: 1px solid #333;"></div>

    <script type="module">
        import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        const statusDiv = document.getElementById('status');
        const logsDiv = document.getElementById('logs');
        const connectBtn = document.getElementById('connect-btn');
        const modelInput = document.getElementById('model-input');

        let engine = null;
        let ws = null;

        function log(msg, type = 'log') {
            const div = document.createElement('div');
            div.className = type;
            div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
            logsDiv.prepend(div);
        }

        connectBtn.onclick = async () => {
            connectBtn.disabled = true;
            const model = modelInput.value;

            try {
                log(`Loading WebGPU Engine (${model})...`);
                engine = await CreateMLCEngine(model, {
                    initProgressCallback: (report) => {
                        statusDiv.textContent = `ðŸŸ¡ Loading Model: ${report.text}`;
                    }
                });
                log("Engine Loaded.", "success");

                connectWebSocket();
            } catch (e) {
                log(`Engine Load Failed: ${e.message}`, "error");
                connectBtn.disabled = false;
            }
        };

        function connectWebSocket() {
            const bridgeUrl = document.getElementById('bridge-url').value;
            log(`Connecting to Bridge (${bridgeUrl})...`);
            ws = new WebSocket(bridgeUrl);

            ws.onopen = () => {
                statusDiv.textContent = "ðŸŸ¢ Bridge Connected (Ready for Requests)";
                log("Bridge Connected.", "success");
            };

            ws.onclose = () => {
                statusDiv.textContent = "ðŸ”´ Disconnected";
                log("Bridge Disconnected. Retrying in 3s...", "error");
                setTimeout(connectWebSocket, 3000);
            };

            ws.onmessage = async (event) => {
                const msg = JSON.parse(event.data);
                if (msg.type === 'chat') {
                    handleChatRequest(msg.id, msg.data);
                }
            };
        }

        async function handleChatRequest(reqId, data) {
            log(`Chat Request ${reqId.slice(0, 8)}...`);
            try {
                // OpenAI Compatibility Mapping
                const messages = data.messages;
                const stream = data.stream !== false; // Default to true if undefined

                const completion = await engine.chat.completions.create({
                    messages,
                    stream: true,
                    temperature: data.temperature || 0.7,
                    max_tokens: data.max_tokens || 4096
                });

                for await (const chunk of completion) {
                    // Forward chunk to bridge
                    ws.send(JSON.stringify({
                        id: reqId,
                        chunk: chunk
                    }));
                }

                // Signal done
                ws.send(JSON.stringify({
                    id: reqId,
                    done: true
                }));

                log(`Request ${reqId.slice(0, 8)} Complete.`, "success");

            } catch (e) {
                log(`Request Failed: ${e.message}`, "error");
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ id: reqId, error: e.message }));
                }
            }
        }
    </script>
</body>

</html>