<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sovereign WebGPU Chat</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        :root {
            --bg-color: #121212;
            --surface-color: #1e1e1e;
            --primary-color: #00e676; /* Green for GPU/Power */
            --text-color: #e0e0e0;
            --user-msg-bg: #2b5c87;
            --ai-msg-bg: #2d2d2d;
            --border-color: #333;
        }
        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
            overflow: hidden;
        }
        
        /* Header & Controls */
        header {
            background-color: var(--surface-color);
            padding: 15px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        .top-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        h1 { margin: 0; font-size: 1.2rem; display: flex; align-items: center; gap: 10px; }
        .status-badge {
            font-size: 0.8rem;
            padding: 2px 8px;
            border-radius: 10px;
            background: #333;
            color: #aaa;
        }
        .status-badge.ready { background: var(--primary-color); color: #000; font-weight: bold; }
        .status-badge.serving { background: #3b82f6; color: white; font-weight: bold; animation: pulse 2s infinite; }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }

        .controls {
            display: flex;
            gap: 10px;
            align-items: center;
            flex-wrap: wrap;
        }
        select {
            background: #333;
            color: white;
            border: 1px solid var(--border-color);
            padding: 8px;
            border-radius: 5px;
            flex: 1;
            min-width: 200px;
        }
        button#init-btn {
            background: var(--primary-color);
            color: #000;
            border: none;
            padding: 8px 20px;
            border-radius: 5px;
            font-weight: bold;
            cursor: pointer;
        }
        button#init-btn:disabled { opacity: 0.5; cursor: not-allowed; }

        /* Progress Bar */
        #progress-container {
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            display: none;
            margin-top: 5px;
        }
        #progress-bar {
            height: 100%;
            background: var(--primary-color);
            width: 0%;
            transition: width 0.2s;
        }
        #progress-text {
            font-size: 0.8rem;
            color: #888;
            margin-top: 5px;
            display: none;
        }

        /* System Prompt */
        details.settings-panel {
            background: #1a1a1a;
            border: 1px solid var(--border-color);
            border-radius: 5px;
            margin-top: 5px;
        }
        details.settings-panel summary {
            padding: 8px;
            cursor: pointer;
            font-size: 0.9rem;
            color: #aaa;
            user-select: none;
        }
        .settings-content {
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        .settings-content label {
            font-size: 0.8rem;
            color: #888;
        }
        .settings-content input, .settings-content textarea {
            width: 100%;
            box-sizing: border-box;
            background: #111;
            border: 1px solid var(--border-color);
            color: #ddd;
            padding: 8px;
            border-radius: 4px;
        }

        /* Chat Area */
        #chat-container {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .message {
            max-width: 85%;
            padding: 15px;
            border-radius: 10px;
            line-height: 1.6;
            word-wrap: break-word;
        }
        .user { align-self: flex-end; background-color: var(--user-msg-bg); color: white; border-bottom-right-radius: 2px; }
        .ai { align-self: flex-start; background-color: var(--ai-msg-bg); border: 1px solid var(--border-color); border-bottom-left-radius: 2px; }
        
        .system-log { align-self: center; background: transparent; color: #666; font-size: 0.8rem; border: 1px dashed #333; padding: 5px 10px; }
        
        /* Markdown */
        .ai pre { background: #111; padding: 10px; border-radius: 5px; overflow-x: auto; border: 1px solid #333; }
        .ai code { font-family: 'Consolas', monospace; font-size: 0.9em; }
        .ai p { margin: 0 0 10px 0; }
        .ai p:last-child { margin: 0; }

        /* Input Area */
        #input-area {
            background-color: var(--surface-color);
            padding: 15px;
            display: flex;
            gap: 10px;
            border-top: 1px solid var(--border-color);
            align-items: flex-end;
        }
        textarea#message-input {
            flex: 1;
            background-color: #333;
            border: 1px solid var(--border-color);
            border-radius: 10px;
            color: white;
            padding: 12px;
            resize: none;
            height: 50px; /* Start height */
            max-height: 150px;
            font-family: inherit;
        }
        textarea#message-input:focus { outline: 1px solid var(--primary-color); background-color: #2a2a2a; }
        
        button#send-btn {
            background-color: var(--primary-color);
            color: #000;
            border: none;
            border-radius: 10px;
            padding: 0 20px;
            height: 50px; /* Match textarea */
            font-weight: bold;
            cursor: pointer;
        }
        button#send-btn:disabled { opacity: 0.5; cursor: not-allowed; }

        /* Stats */
        #stats {
            position: fixed;
            bottom: 80px; /* Above input area */
            right: 20px;
            background: rgba(0,0,0,0.7);
            padding: 5px 10px;
            border-radius: 5px;
            font-size: 0.8rem;
            color: var(--primary-color);
            pointer-events: none;
            opacity: 0;
            transition: opacity 0.3s;
        }
        #stats.visible { opacity: 1; }

    </style>
</head>
<body>
    <header>
        <div class="top-bar">
            <h1>‚ö° Sovereign WebGPU <span id="status-badge" class="status-badge">Offline</span></h1>
            <div id="bridge-status" style="font-size: 0.8rem; color: #666;">Bridge: Disconnected</div>
        </div>
        
        <div class="controls">
            <select id="model-select" onchange="document.getElementById('custom-model-input').style.display = this.value === 'custom' ? 'block' : 'none'">
                <option value="mlc-ai/Llama-3.2-1B-Instruct-q4f32_1-MLC">Llama 3.2 1B Instruct (Fastest)</option>
                <option value="mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama 3.1 8B Instruct (Balanced)</option>
                <option value="mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B Instruct (Fast & Smart)</option>
                <option value="mlc-ai/Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B (Uncensored)</option>
                <option value="mlc-ai/Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes 3 Llama 3.2 3B (Fast Uncensored)</option>
                <option value="mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC">Phi 3.5 Mini Instruct (Compact)</option>
                <option value="mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC">Phi 3.5 Vision Instruct (Multimodal)</option>
                <option value="mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen 2.5 7B Instruct (Strong Reasoning)</option>
                <option value="mlc-ai/Qwen2.5-3B-Instruct-q4f16_1-MLC">Qwen 2.5 3B Instruct (Fast Reasoning)</option>
                <option value="mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 Coder 1.5B (Coding)</option>
                <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 Distill Qwen 7B (Reasoning)</option>
                <option value="mlc-ai/gemma-2-2b-it-q4f16_1-MLC">Gemma 2 2B Instruct (Efficient)</option>
                <option value="mlc-ai/gemma-2-9b-it-q4f16_1-MLC">Gemma 2 9B Instruct (High Quality)</option>
                <option value="custom">Custom (Enter ID below)</option>
            </select>
            <input type="text" id="custom-model-input" placeholder="Enter HuggingFace Model ID" style="display: none; flex: 1; min-width: 200px; padding: 8px; background: #333; color: white; border: 1px solid #333; border-radius: 5px;">
            <button id="init-btn">Load Model & Serve</button>
        </div>

        <div id="progress-container">
            <div id="progress-bar"></div>
        </div>
        <div id="progress-text"></div>

        <details class="settings-panel">
            <summary>‚öôÔ∏è Settings & System Prompt</summary>
            <div class="settings-content">
                <div>
                    <label>Bridge URL (for ECE Connection):</label>
                    <input type="text" id="bridge-url" value="ws://localhost:8080/ws/chat" placeholder="ws://localhost:8080/ws/chat">
                </div>
                <div style="display: flex; align-items: center; gap: 10px; margin-top: 10px;">
                    <label for="ece-mode-toggle" style="cursor: pointer;">Use ECE Backend (Memory):</label>
                    <input type="checkbox" id="ece-mode-toggle" style="width: auto;">
                </div>
                <div>
                    <label>System Prompt:</label>
                    <textarea id="system-prompt-text" placeholder="You are a helpful AI assistant..." style="min-height: 80px;">You are a helpful, sovereign AI assistant running entirely locally in the user's browser via WebGPU. You prioritize privacy, accuracy, and concise answers.</textarea>
                </div>
            </div>
        </details>
    </header>

    <div id="chat-container">
        <div class="message ai">
            Select a model and click "Load Model & Serve" to initialize the WebGPU engine and Bridge connection.
            <br><br>
            <i>Note: The first load will download model weights (2GB-6GB) to your browser cache. Subsequent loads will be instant.</i>
        </div>
    </div>

    <div id="stats">0 t/s</div>

    <div id="input-area">
        <textarea id="message-input" placeholder="Type a message..." disabled></textarea>
        <button id="send-btn" disabled>Send</button>
    </div>

    <script type="module">
        // Using the latest version of WebLLM to support newer models (Llama 3.1, Gemma 2, etc.)
        import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        // --- State ---
        let engine = null;
        let messages = [];
        let isGenerating = false;
        let ws = null;

        // --- Elements ---
        const modelSelect = document.getElementById('model-select');
        const customModelInput = document.getElementById('custom-model-input');
        const initBtn = document.getElementById('init-btn');
        const progressBar = document.getElementById('progress-bar');
        const progressContainer = document.getElementById('progress-container');
        const progressText = document.getElementById('progress-text');
        const statusBadge = document.getElementById('status-badge');
        const bridgeStatus = document.getElementById('bridge-status');
        const chatContainer = document.getElementById('chat-container');
        const input = document.getElementById('message-input');
        const sendBtn = document.getElementById('send-btn');
        const systemPromptText = document.getElementById('system-prompt-text');
        const statsDiv = document.getElementById('stats');
        const eceModeToggle = document.getElementById('ece-mode-toggle');

        // --- Settings Logic ---
        eceModeToggle.addEventListener('change', () => {
            useECEMode = eceModeToggle.checked;
            if (useECEMode) {
                statusBadge.textContent = "ECE Mode";
                statusBadge.style.background = "#9c27b0"; // Purple for ECE
                statusBadge.style.color = "white";
                appendMessage('system-log', "üß† Switched to ECE Mode. Chat will now use Backend Memory.");
            } else {
                statusBadge.textContent = "Local Mode";
                statusBadge.style.background = "var(--primary-color)";
                statusBadge.style.color = "black";
                appendMessage('system-log', "‚ö° Switched to Local Mode. Chat will use WebGPU.");
            }
        });

        // --- Persistence ---
        const savedModel = localStorage.getItem('webgpu_selected_model');
        if (savedModel) {
            if (document.querySelector(`option[value="${savedModel}"]`)) {
                modelSelect.value = savedModel;
            } else {
                modelSelect.value = 'custom';
                customModelInput.style.display = 'block';
                customModelInput.value = savedModel;
            }
        }
        
        const savedSystemPrompt = localStorage.getItem('webgpu_system_prompt');
        if (savedSystemPrompt) systemPromptText.value = savedSystemPrompt;

        modelSelect.addEventListener('change', () => {
            const val = modelSelect.value === 'custom' ? customModelInput.value : modelSelect.value;
            localStorage.setItem('webgpu_selected_model', val);
        });
        
        customModelInput.addEventListener('change', () => {
             localStorage.setItem('webgpu_selected_model', customModelInput.value);
        });

        systemPromptText.addEventListener('change', () => {
            localStorage.setItem('webgpu_system_prompt', systemPromptText.value);
        });

        // --- Initialization ---
        initBtn.addEventListener('click', async () => {
            const selectedModel = modelSelect.value === 'custom' ? customModelInput.value : modelSelect.value;
            if (!selectedModel) return alert("Please select or enter a model ID");

            initBtn.disabled = true;
            modelSelect.disabled = true;
            customModelInput.disabled = true;
            progressContainer.style.display = 'block';
            progressText.style.display = 'block';
            
            try {
                let engineConfig = { 
                    initProgressCallback: (report) => {
                        console.log(report);
                        progressText.textContent = report.text;
                        const match = report.text.match(/\[(\d+)\/(\d+)\]/);
                        if (match) {
                            const pct = (parseInt(match[1]) / parseInt(match[2])) * 100;
                            progressBar.style.width = `${pct}%`;
                        } else if (report.progress) {
                            progressBar.style.width = `${report.progress * 100}%`;
                        }
                    }
                };

                // Handle Custom Models & New Dropdown Models
                // We need to generate appConfig for any model that isn't in the default WebLLM registry.
                // We'll try to match a WASM library for the selected model.
                let modelLib = null;
                const lowerId = selectedModel.toLowerCase();

                // Heuristic to guess the model library based on name
                // Using v0_2_80 WASM libraries (Latest as of Dec 2025)
                const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/";
                
                if (lowerId.includes('llama-3.2-3b')) {
                    modelLib = libBase + "Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('llama-3.2-1b')) {
                    modelLib = libBase + "Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('llama-3.1-8b')) {
                    modelLib = libBase + "Llama-3_1-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('llama-3-8b')) {
                    modelLib = libBase + "Llama-3-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('hermes-3-llama-3.1')) {
                    modelLib = libBase + "Llama-3_1-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('hermes-3-llama-3.2-3b')) {
                    modelLib = libBase + "Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('phi-3.5-mini')) {
                    modelLib = libBase + "Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('phi-3.5-vision')) {
                    modelLib = libBase + "Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm";
                } else if (lowerId.includes('qwen2.5-7b')) {
                    modelLib = libBase + "Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm"; // Compatible
                } else if (lowerId.includes('qwen2.5-3b')) {
                    modelLib = libBase + "Qwen2.5-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('gemma-2-9b')) {
                    modelLib = libBase + "gemma-2-9b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('gemma-2-2b')) {
                    modelLib = libBase + "gemma-2-2b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('gemma-3')) {
                    modelLib = libBase + "gemma-3-1b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm";
                } else if (lowerId.includes('deepseek-r1')) {
                    modelLib = libBase + "Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm"; // R1 Distill is Qwen based
                }

                // Always generate appConfig for our models (since they are not in the default registry)
                // If no specific library was matched, fallback to Llama 3 (most common architecture)
                if (!modelLib) {
                     console.warn("No specific WASM library found for " + selectedModel + ". Using Llama-3 fallback.");
                     modelLib = libBase + "Llama-3-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm"; 
                }

                engineConfig.appConfig = {
                    model_list: [
                        {
                            "model": "https://huggingface.co/" + selectedModel + "/resolve/main",
                            "model_id": selectedModel,
                            "model_lib": modelLib,
                            "low_resource_required": true,
                        }
                    ]
                };
                console.log("Generated Model Config:", engineConfig.appConfig);

                engine = await CreateMLCEngine(
                    selectedModel,
                    engineConfig
                );

                // Success
                statusBadge.textContent = "Ready";
                statusBadge.classList.add('ready');
                progressContainer.style.display = 'none';
                progressText.style.display = 'none';
                initBtn.textContent = "Loaded & Serving";
                
                input.disabled = false;
                sendBtn.disabled = false;
                input.focus();
                
                appendMessage('ai', `**${selectedModel}** loaded successfully. GPU adapter active.`);
                
                // Connect to Bridge
                connectWebSocket();

            } catch (err) {
                console.error(err);
                let errorMsg = err.message;
                if (errorMsg.includes("Failed to execute 'add' on 'Cache'")) {
                    errorMsg = "Model files not found (404). This repository might not be a valid WebLLM/MLC model. Try a different model.";
                }
                progressText.textContent = `Error: ${errorMsg}`;
                progressText.style.color = '#ff4444';
                initBtn.disabled = false;
                modelSelect.disabled = false;
                customModelInput.disabled = false;
            }
        });

        // --- Bridge Logic ---
        function connectWebSocket() {
            const bridgeUrl = document.getElementById('bridge-url').value;
            bridgeStatus.textContent = `Bridge: Connecting to ${bridgeUrl}...`;
            ws = new WebSocket(bridgeUrl);

            ws.onopen = () => {
                bridgeStatus.textContent = "Bridge: Connected (Serving)";
                bridgeStatus.style.color = "#00e676";
                statusBadge.classList.add('serving');
                statusBadge.textContent = "Serving";
                appendMessage('system-log', "üîå Connected to Bridge. External tools can now use this model.");
            };

            ws.onclose = () => {
                bridgeStatus.textContent = "Bridge: Disconnected (Retrying...)";
                bridgeStatus.style.color = "#ff4444";
                statusBadge.classList.remove('serving');
                statusBadge.textContent = "Ready";
                setTimeout(connectWebSocket, 3000);
            };

            ws.onmessage = async (event) => {
                const msg = JSON.parse(event.data);
                // Handle Keep-Alive or other types if necessary
                if (msg.type !== 'chat') return;

                const request_id = msg.id;
                const reqData = msg.data;
                const { messages: reqMessages, stream, tools, tool_choice, ...options } = reqData;

                appendMessage('system-log', `üì• External Request: ${request_id.slice(0,8)}...`);

                try {
                    const completionParams = {
                        messages: reqMessages,
                        stream: !!stream,
                        tools: tools,
                        tool_choice: tool_choice,
                        ...options
                    };

                    if (stream) {
                        // Check for tool calls in the first chunk or accumulate them
                        // WebLLM might stream tool calls differently.
                        // For now, we just forward everything.
                        
                        // Visual Indicator for Tool Calls
                        if (tools && tools.length > 0) {
                             appendMessage('system-log', `üõ†Ô∏è Processing Request with ${tools.length} Tools...`);
                        }

                        const chunks = await engine.chat.completions.create({
                            ...completionParams,
                            stream: true
                        });

                        for await (const chunk of chunks) {
                            // Forward chunk to bridge
                            // Note: Bridge expects { id, chunk }
                            ws.send(JSON.stringify({
                                id: request_id,
                                chunk: chunk
                            }));
                        }

                        ws.send(JSON.stringify({
                            id: request_id,
                            done: true
                        }));

                    } else {
                        const reply = await engine.chat.completions.create({
                            ...completionParams,
                            stream: false
                        });

                        ws.send(JSON.stringify({
                            id: request_id,
                            chunk: { choices: [ { delta: { content: reply.choices[0].message.content } } ] }, // Simulate chunk for compatibility if needed, or just send result
                            // Actually bridge expects 'chunk' for stream, but for non-stream it might just wait?
                            // Let's look at bridge.py: it yields "data: {json.dumps(message['chunk'])}\n\n"
                            // So we should probably send the full response as a "chunk" or handle it differently.
                            // But wait, bridge.py ONLY handles 'chunk' or 'done'.
                            // So even for non-stream, we should probably send it as a chunk.
                        }));
                        
                        // Actually, let's just stick to streaming for now as that's what ECE uses.
                        // But if we must support non-stream, we send one chunk then done.
                         ws.send(JSON.stringify({
                            id: request_id,
                            chunk: {
                                id: reply.id,
                                object: "chat.completion.chunk",
                                created: reply.created,
                                model: reply.model,
                                choices: [{ index: 0, delta: { content: reply.choices[0].message.content }, finish_reason: reply.choices[0].finish_reason }]
                            }
                        }));

                        ws.send(JSON.stringify({
                            id: request_id,
                            done: true
                        }));
                    }
                    appendMessage('system-log', `‚úÖ Request ${request_id.slice(0,8)} completed.`);
                } catch (err) {
                    console.error("Bridge Error:", err);
                    ws.send(JSON.stringify({
                        id: request_id,
                        error: err.message
                    }));
                    appendMessage('system-log', `‚ùå Request failed: ${err.message}`);
                }
            };
        }

        // --- Chat Logic ---
        let useECEMode = false; // Default to Local Mode

        function appendMessage(role, text) {
            const div = document.createElement('div');
            div.className = `message ${role}`;
            div.innerHTML = role === 'system-log' ? text : marked.parse(text);
            chatContainer.appendChild(div);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return div;
        }

        async function sendMessage() {
            if (isGenerating) return;
            
            const text = input.value.trim();
            if (!text) return;

            // UI Updates
            input.value = '';
            appendMessage('user', text);
            isGenerating = true;
            sendBtn.disabled = true;
            statsDiv.classList.add('visible');

            // Prepare History
            if (messages.length === 0) {
                messages.push({ role: "system", content: systemPromptText.value });
            }
            messages.push({ role: "user", content: text });

            // Create AI Message Placeholder
            const aiMsgDiv = document.createElement('div');
            aiMsgDiv.className = 'message ai';
            chatContainer.appendChild(aiMsgDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;

            let fullResponse = "";
            let startTime = performance.now();
            let tokenCount = 0;

            try {
                if (useECEMode) {
                    // --- ECE Mode: Send to Backend ---
                    const response = await fetch('http://localhost:8000/v1/chat/completions', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            messages: messages,
                            stream: true,
                            model: "ece-default" // ECE ignores this usually, but good practice
                        })
                    });

                    if (!response.ok) throw new Error(`ECE Error: ${response.statusText}`);

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();

                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        
                        const chunk = decoder.decode(value);
                        const lines = chunk.split('\n');
                        
                        for (const line of lines) {
                            if (line.startsWith('data: ') && line !== 'data: [DONE]') {
                                try {
                                    const data = JSON.parse(line.slice(6));
                                    const content = data.choices[0]?.delta?.content || "";
                                    fullResponse += content;
                                    aiMsgDiv.innerHTML = marked.parse(fullResponse);
                                    chatContainer.scrollTop = chatContainer.scrollHeight;
                                } catch (e) { /* ignore parse errors */ }
                            }
                        }
                    }

                } else {
                    // --- Local Mode: Use WebGPU Engine ---
                    const chunks = await engine.chat.completions.create({
                        messages: messages,
                        stream: true,
                        temperature: 0.7
                    });

                    for await (const chunk of chunks) {
                        const content = chunk.choices[0]?.delta?.content || "";
                        fullResponse += content;
                        aiMsgDiv.innerHTML = marked.parse(fullResponse);
                        chatContainer.scrollTop = chatContainer.scrollHeight;
                        
                        if (content) {
                            tokenCount++;
                            const elapsed = (performance.now() - startTime) / 1000;
                            const tps = (tokenCount / elapsed).toFixed(1);
                            statsDiv.textContent = `${tps} t/s`;
                        }
                    }
                }

                messages.push({ role: "assistant", content: fullResponse });

            } catch (err) {
                aiMsgDiv.innerHTML += `<br><br><span style="color:red">Error: ${err.message}</span>`;
            } finally {
                isGenerating = false;
                sendBtn.disabled = false;
                statsDiv.classList.remove('visible');
            }
        }

        // --- Event Listeners ---
        sendBtn.addEventListener('click', sendMessage);
        
        input.addEventListener('keydown', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });

        // Auto-resize textarea
        input.addEventListener('input', function() {
            this.style.height = 'auto';
            this.style.height = (this.scrollHeight) + 'px';
            if (this.value === '') this.style.height = '50px';
        });

    </script>
</body>
</html>