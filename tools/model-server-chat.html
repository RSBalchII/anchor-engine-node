<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Coda Sovereign Console (Graph-R1)</title>
    <style>
        * {
            box-sizing: border-box;
        }

        body {
            background: #1e1e1e;
            color: #ccc;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            height: 100vh;
            margin: 0;
        }

        /* --- Resizable Split Layout --- */
        #container {
            display: flex;
            width: 100%;
            height: 100vh;
            overflow: hidden;
        }

        #sidebar {
            width: 320px;
            /* Default width */
            min-width: 200px;
            max-width: 80%;
            background: #1e1e1e;
            color: #d4d4d4;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #333;
            transition: width 0.1s ease;
            /* smooth resize stop */
        }

        /* Resize Handle */
        #resizer {
            width: 5px;
            cursor: col-resize;
            background: #333;
            transition: background 0.2s;
            z-index: 10;
        }

        #resizer:hover,
        #resizer.resizing {
            background: #0078d4;
        }

        #main {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: #2d2d2d;
            position: relative;
            min-width: 0;
            /* Prevent flex overflow */
        }

        /* --- Collapsible Details --- */
        details {
            margin-bottom: 10px;
            border-bottom: 1px solid #333;
        }

        summary {
            padding: 10px;
            cursor: pointer;
            font-weight: bold;
            user-select: none;
            background: #252526;
            list-style: none;
            /* Hide default triangle */
        }

        summary::-webkit-details-marker {
            display: none;
        }

        summary::after {
            content: '‚ñº';
            float: right;
            font-size: 0.8em;
            transition: transform 0.2s;
        }

        details[open] summary::after {
            transform: rotate(180deg);
        }

        .panel-content {
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        /* --- Mobile Responsive --- */
        @media (max-width: 768px) {
            #container {
                flex-direction: column;
            }

            #sidebar {
                width: 100% !important;
                /* Force full width */
                height: auto;
                max-height: 40vh;
                /* Limit height on mobile */
                overflow-y: auto;
                border-right: none;
                border-bottom: 2px solid #333;
            }

            #resizer {
                display: none;
                /* No horizontal resize on mobile */
            }
        }

        #chat-box {
            flex: 1;
            overflow-y: auto;
            margin-bottom: 20px;
            padding-right: 10px;
        }

        #chat-box::-webkit-scrollbar {
            width: 8px;
        }

        #chat-box::-webkit-scrollbar-track {
            background: #333;
        }

        #chat-box::-webkit-scrollbar-thumb {
            background: #555;
            border-radius: 4px;
        }

        .msg {
            padding: 12px;
            margin: 8px 0;
            border-radius: 6px;
            background: #333;
            max-width: 85%;
            word-wrap: break-word;
        }

        .user {
            background: #0078d4;
            color: white;
            align-self: flex-end;
            margin-left: auto;
        }

        .assistant {
            background: #2d2d2d;
            border-left: 3px solid #0078d4;
        }

        .msg details {
            margin-top: 10px;
            font-size: 0.85rem;
            opacity: 0.7;
        }

        .msg pre {
            background: #1e1e1e;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 0.8rem;
        }

        h3 {
            margin: 0 0 10px 0;
        }

        #status-text {
            font-size: 0.9rem;
            color: #888;
            margin-bottom: 8px;
        }

        #progress-bar {
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            margin: 8px 0;
        }

        #progress {
            height: 100%;
            background: #0078d4;
            width: 0%;
            transition: width 0.2s;
        }

        #status-log {
            flex: 1;
            overflow-y: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.75rem;
            margin-top: 10px;
            padding: 8px;
            background: #1a1a1a;
            border-radius: 4px;
            border: 1px solid #333;
        }

        #status-log div {
            margin: 2px 0;
            padding: 2px 0;
        }

        .info {
            color: #888;
        }

        .warn {
            color: #d7ba7d;
        }

        .error {
            color: #f48771;
        }

        .success {
            color: #89d185;
        }

        #input-area {
            display: flex;
            gap: 10px;
        }

        textarea {
            flex: 1;
            height: 60px;
            background: #3c3c3c;
            border: 1px solid #555;
            color: white;
            padding: 10px;
            border-radius: 4px;
            resize: vertical;
            font-family: 'Segoe UI', sans-serif;
        }

        textarea:focus {
            outline: none;
            border-color: #0078d4;
        }

        button {
            padding: 8px 20px;
            background: #0078d4;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            align-self: flex-end;
        }

        button:hover {
            background: #1084d7;
        }

        .msg.assistant {
            background: #2d2d2d;
            align-self: flex-start;
        }

        .msg.system {
            background: #3c3c3c;
            align-self: center;
            font-style: italic;
            font-size: 11px;
        }

        /* Old details/summary styles, replaced by new ones above */
        /*
        details {
            margin-top: 5px;
            background: #252526;
            border: 1px solid #444;
            border-radius: 4px;
            font-size: 11px;
            overflow: hidden;
        }

        summary {
            padding: 5px 10px;
            cursor: pointer;
            list-style: none;
            display: flex;
            align-items: center;
            gap: 5px;
            opacity: 0.8;
            font-weight: bold;
        }

        summary:hover {
            opacity: 1;
            background: #2a2a2b;
        }

        summary::before {
            content: '‚ñ∂';
            font-size: 8px;
            transition: transform 0.2s;
        }

        details[open] summary::before {
            transform: rotate(90deg);
        }
        */

        details pre {
            margin: 0;
            padding: 10px;
            background: #1e1e1e;
            white-space: pre-wrap;
            word-break: break-all;
            max-height: 200px;
            overflow-y: auto;
        }

        .streaming {
            border-right: 2px solid var(--accent);
            animation: blink 0.7s infinite;
        }

        @keyframes blink {
            50% {
                border-color: transparent;
            }
        }

        button:disabled {
            background: #555;
            cursor: not-allowed;
        }
    </style>
</head>

<body>
    <div id="container">
        <div id="sidebar">
            <div style="padding: 10px; font-weight: bold; font-size: 16px; border-bottom: 1px solid #444;">
                ‚ö° Sovereign Coda
            </div>
            <small id="status-text" style="padding: 0 10px;">Initializing...</small>
            <div id="progress-bar" style="margin: 8px 10px 15px 10px;">
                <div id="progress"></div>
            </div>

            <!-- COLLAPSIBLE: Model Selection -->
            <details open>
                <summary>Model Selection</summary>
                <div class="panel-content">
                    <label for="hw-profile"
                        style="display:block; font-size: 11px; color: #ccc; margin-bottom: 4px;">Hardware
                        Profile:</label>
                    <select id="hw-profile" class="form-control"
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555; margin-bottom: 10px;">
                        <option value="lite" selected>üîã Lite (Snapdragon / Integrated / Mobile)</option>
                        <option value="mid">üíª Mid-Tier (8GB VRAM / Older Gaming Laptops)</option>
                        <option value="high">üöÄ High-Perf (16GB VRAM / RTX 4090 Laptop)</option>
                        <option value="ultra">‚ö° Ultra (24GB+ VRAM / Desktop Workstation)</option>
                    </select>
                    <select id="model-select" disabled
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555;">
                        <option value="" disabled selected>-- Select a Model --</option>
                        <optgroup label="Official MLC (Verified)">
                            <option value="mlc-ai/Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2-7B (Standard)</option>
                            <option value="mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC">Llama-3-8B</option>
                            <option value="mlc-ai/Mistral-7B-Instruct-v0.2-q4f16_1-MLC">Mistral-7B-v0.2</option>
                        </optgroup>
                        <optgroup label="Snapdragon Optimized (Fast & Safe)">
                            <option value="mlc-ai/Qwen3-4B-q4f16_1-MLC">Qwen3-4B (Highly Versatile)</option>
                            <option value="mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama-3.2-1B (Recommended)</option>
                            <option value="mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5-1.5B (Fastest)</option>
                            <option value="mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC">Qwen2.5-Coder-1.5B</option>
                        </optgroup>
                        <optgroup label="New / High Power">
                            <option value="mlc-ai/Qwen2.5-14B-Instruct-q4f16_1-MLC">Qwen2.5-14B (Powerful)</option>
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek-R1-7B (Qwen)
                            </option>
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC">DeepSeek-R1-14B (Qwen)
                            </option>
                            <option value="mlc-ai/DeepSeek-V2-Lite-Chat-q4f16_1-MLC">DeepSeek-V2-Lite (Experimental)
                            </option>
                            <option value="mlc-ai/Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes-3-Llama-3.2-3B</option>
                        </optgroup>
                        <optgroup label="Experimental / Legacy">
                            <option value="mlc-ai/OpenHermes-2.5-Mistral-7B-q4f16_1-MLC">OpenHermes-2.5-Mistral-7B
                            </option>
                            <option value="mlc-ai/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC">NeuralHermes-2.5-Mistral-7B
                                (q3)</option>
                            <option value="mlc-ai/Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2-7B</option>
                            <option value="mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama-3.1-8B</option>
                            <option value="mlc-ai/gemma-2-9b-it-q4f16_1-MLC">Gemma-2-9B</option>
                            <option value="mlc-ai/gemma-2-2b-it-q4f16_1-MLC">Gemma-2-2B</option>
                            <!-- Custom input handled via logic -->
                        </optgroup>
                        <optgroup label="Specialized">
                            <option value="mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC">Qwen2.5-Coder-1.5B</option>
                            <option value="mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC">Phi-3.5-vision</option>
                        </optgroup>
                        <option value="custom">-- Custom --</option>
                    </select>
                    <input id="custom-model-input" type="text" placeholder="Custom model ID"
                        style="width: 100%; padding: 5px; font-size: 11px;" disabled />
                    <button id="load-model-btn" disabled
                        style="width: 100%; padding: 6px; background: #0078d4; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 11px;">Load
                        Model</button>
                </div>
            </details>

            <!-- COLLAPSIBLE: Controls & Quota -->
            <details>
                <summary>System Controls</summary>
                <div class="panel-content">
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px;">
                        <input type="checkbox" id="enable-bridge-toggle" onchange="toggleBridge(this.checked)">
                        <label for="enable-bridge-toggle" style="font-size: 11px; cursor: pointer;">Enable Wave Terminal
                            Bridge (ws:8080)</label>
                        <div id="bridge-status" style="font-size: 10px; color: #666; margin-left: 20px;">Disconnected
                        </div>
                    </div>
                    <button id="attempt-indexeddb-load-btn"
                        style="width: 100%; padding: 6px; background: #6c757d; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 11px;">Attempt
                        Load IndexedDB</button>
                    <button id="clear-cache-btn"
                        style="width: 100%; padding: 6px; background: #a43131; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 11px;">‚ö†Ô∏è
                        Delete All Model Cache</button>
                    <button id="debug-gpu-btn"
                        style="width: 100%; padding: 6px; background: #555; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 11px; margin-top: 5px;">
                        ‚ùì Debug GPU Status
                    </button>
                    <small style="display:block;color:#aaa;">Use if WASM panicked or Quota Exceeded.</small>
                </div>
            </details>

            <!-- COLLAPSIBLE: Logs -->
            <details open style="flex: 1; display: flex; flex-direction: column;">
                <summary>System Logs</summary>
                <div class="panel-content" style="flex: 1; display: flex; flex-direction: column; overflow: hidden;">
                    <button id="copy-logs-btn"
                        style="width: 100%; padding: 4px; background: #333; color: #ccc; border: 1px solid #555; border-radius: 4px; cursor: pointer; font-size: 10px;">üìã
                        Copy Logs</button>
                    <div id="status-log"
                        style="flex: 1; overflow-y: auto; border-top: 1px solid #444; margin-top: 5px; min-height: 100px;">
                    </div>
                </div>
            </details>
        </div>

        <!-- RESIZE HANDLE -->
        <div id="resizer"></div>



        <div id="main">
            <!-- Split view for Chat and Context -->
            <div style="flex: 1; display: flex; height: 100%; overflow: hidden;">
                <!-- Chat Column -->
                <div style="flex: 1; display: flex; flex-direction: column; border-right: 1px solid #333;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üí¨ Chat Stream</div>
                    <div id="chat-box" style="flex: 1; overflow-y: auto; padding-right: 10px;"></div>
                    <div id="input-area" style="padding: 10px; border-top: 1px solid #333;">
                        <textarea id="input" disabled placeholder="Waiting for Engine..."></textarea>
                        <button id="send-btn" disabled>Send</button>
                    </div>
                </div>

                <!-- Context Column -->
                <div style="width: 40%; display: flex; flex-direction: column; background: #222;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üß† Context / Memory</div>
                    <div id="context-box"
                        style="flex: 1; overflow-y: auto; padding: 10px; font-family: monospace; font-size: 12px; white-space: pre-wrap; color: #aaa;">
                        <div style="text-align: center; margin-top: 50px; color: #555;">(Memory retrievals will appear
                            here)</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // --- IMPORTS ---
        import initCozo, { CozoDb } from './cozo_lib_wasm.js';
        import { loadAllFromIndexedDb, writeToIndexedDb } from './indexeddb.js';
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";
        import { marked } from "https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js";

        env.allowLocalModels = false;

        // --- GLOBAL STATE ---
        let db;
        let embedder;
        let engine;
        let contextManager;
        let selectedModelId = null;

        // --- BROADCAST CHANNEL FOR LOG-VIEWER ---
        const logChannel = new BroadcastChannel('sovereign-logs');
        // Additional mission-control channel (low-latency JSON messages)
        const codaChannel = new BroadcastChannel('coda_logs');

        // Helper to forward system/chat logs to mission-control channel
        function postCoda(msg, meta = {}) {
            try {
                codaChannel.postMessage(Object.assign({ source: 'Sovereign-Console', time: new Date().toISOString(), message: msg }, meta));
            } catch (e) {
                console.warn('coda post failed', e.message);
            }
        }

        // --- UI HELPERS ---
        const ui = {
            log: (msg, type = 'info') => {
                const el = document.getElementById('status-log');
                const div = document.createElement('div');
                div.className = type;
                div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
                div.style.fontSize = '11px';
                div.style.padding = '2px';
                div.style.borderBottom = '1px solid #222';
                el.insertBefore(div, el.firstChild);
                if (el.children.length > 100) el.removeChild(el.lastChild);

                // Send to log-viewer (legacy channel)
                logChannel.postMessage({ source: 'system', msg, type, time: new Date().toLocaleTimeString() });
                // Also post to Coda Mission Control channel for unified view
                try {
                    codaChannel.postMessage({ source: 'Sovereign-Console', message: msg, type, timestamp: new Date().toISOString() });
                } catch (e) {
                    console.warn('codaChannel post failed', e);
                }
            },
            append: (role, text, meta = null) => {
                const box = document.getElementById('chat-box');
                const div = document.createElement('div');
                div.className = `msg ${role}`;
                div.innerHTML = text ? marked.parse(text) : "";

                if (meta && meta.trace && meta.trace.length > 0) {
                    const details = document.createElement('details');
                    const summary = document.createElement('summary');
                    summary.textContent = 'üìã Reasoning Trace';
                    const pre = document.createElement('pre');
                    pre.textContent = JSON.stringify(meta.trace, null, 2);
                    details.appendChild(summary);
                    details.appendChild(pre);
                    div.appendChild(details);
                }

                box.appendChild(div);
                box.scrollTop = box.scrollHeight;

                // Return a handle for updating this message
                return {
                    div,
                    update: (newText, isMarkdown = true) => {
                        div.innerHTML = isMarkdown ? marked.parse(newText) : newText;
                        box.scrollTop = box.scrollHeight;
                    },
                    appendText: (chunk) => {
                        // For simple non-streaming render updates
                        if (div.innerText.endsWith("...")) div.innerText = div.innerText.slice(0, -3);
                        div.innerText += chunk;
                        box.scrollTop = box.scrollHeight;
                    }
                };
            },
            appendContext: (title, details) => {
                const box = document.getElementById('context-box');
                const div = document.createElement('div');
                div.style.borderBottom = '1px solid #333';
                div.style.marginBottom = '10px';
                div.style.paddingBottom = '10px';

                const h4 = document.createElement('div');
                h4.style.fontWeight = 'bold';
                h4.style.color = '#0078d4';
                h4.style.marginBottom = '5px';
                h4.textContent = `[${new Date().toLocaleTimeString()}] ${title}`;

                const p = document.createElement('div');
                p.style.whiteSpace = 'pre-wrap';
                p.textContent = details;

                div.appendChild(h4);
                div.appendChild(p);
                box.appendChild(div);
                box.scrollTop = box.scrollHeight;
            },
            appendReasoning: (label) => {
                const box = document.getElementById('chat-box');
                const details = document.createElement('details');
                details.open = true;
                const summary = document.createElement('summary');
                summary.textContent = `üìã ${label}`;
                const pre = document.createElement('pre');
                pre.className = "streaming";
                details.appendChild(summary);
                details.appendChild(pre);
                box.appendChild(details);
                box.scrollTop = box.scrollHeight;

                let content = "";
                return {
                    details,
                    pre,
                    update: (chunk) => {
                        content += chunk;
                        pre.textContent = content;
                        box.scrollTop = box.scrollHeight;
                    },
                    finish: (finalText) => {
                        if (finalText) pre.textContent = finalText;
                        pre.className = "";
                        details.open = false; // Collapse on finish
                    }
                };
            },
            updateProgress: (pct, text) => {
                document.getElementById('progress').style.width = (pct * 100) + "%";
                if (text) document.getElementById('status-text').innerText = text;
            }
        };

        // --- UTILS: Response Pattern Matcher ---
        class ResponsePattern {
            static match(response) {
                let data = response;
                // Parse if string
                if (typeof response === 'string') {
                    try {
                        const clean = response.replace(/```json/g, '').replace(/```/g, '').trim();
                        if (clean.startsWith('{')) {
                            data = JSON.parse(clean);
                        }
                    } catch (e) { }
                }

                // 1. Database Result (rows array)
                if (data?.rows && Array.isArray(data.rows)) {
                    return {
                        type: 'DB_RESULT',
                        rows: data.rows, // Use parsed data
                        count: data.rows.length
                    };
                }

                // 1.b Database Result (ok: true with rows inside?) - Handle specific Cozo return shapes if needed

                // 2. Search Plan (LLM JSON)
                if (data?.query && typeof data.query === 'string' && data?.hypothesis) {
                    return {
                        type: 'SEARCH_PLAN',
                        query: data.query,
                        hypothesis: data.hypothesis,
                        confidence: typeof data.confidence === 'number' ? data.confidence : 0.5
                    };
                }

                // 3. Error
                if (data?.error || (typeof response === 'string' && response.toLowerCase().includes('error:'))) {
                    return {
                        type: 'ERROR',
                        error: data?.error || response
                    };
                }

                // 3.b Cozo Error
                if (data?.ok === false && data?.message) {
                    return {
                        type: 'ERROR',
                        error: data.message
                    };
                }

                // 4. LLM Chat Response
                if (data?.choices && Array.isArray(data.choices) && data.choices.length > 0) {
                    const content = data.choices[0]?.message?.content || data.choices[0]?.text || '';
                    return {
                        type: 'LLM_RESPONSE',
                        text: content
                    };
                }

                // 5. Default / Raw Text
                return {
                    type: 'RAW_TEXT',
                    text: typeof response === 'string' ? response : JSON.stringify(response)
                };
            }
        }

        // --- LOGIC: Context Manager (The "Context OS") ---
        class ContextManager {
            constructor(engine, db) {
                this.engine = engine;
                this.db = db;
                this.maxIterations = 3;
                this.history = []; // Debug trace
            }

            // 1. The "Reflex": Fast, programmatic search
            // Finds obvious keywords immediately (e.g., "Blueberry-Omega")
            async retrieveInitialContext(userText) {
                ui.log("üîç Reflex: Scanning for keywords...", "info");

                // Extract words > 3 chars as rough keywords, excluding common stops
                const rawWords = userText.match(/\b\w{3,}\b/g) || [];
                const stopWords = new Set(['the', 'and', 'was', 'for', 'are', 'with', 'what', 'when', 'where', 'who', 'how', 'why', 'that', 'this']);
                const keywords = rawWords.filter(w => !stopWords.has(w.toLowerCase()));

                if (keywords.length === 0) return "";

                // Construct simple OR query for CozoDB
                // We use 'str_includes' for substring matching
                const conditions = keywords.map(w => `str_includes(content, '${w}')`).join(' or ');
                const query = `?[id, content, timestamp] := *memory{id, content, timestamp}, ${conditions}`;

                try {
                    const result = await this.db.run(query);
                    // ui.log(`DEBUG: DB Result: ${JSON.stringify(result)}`, "debug");

                    let parsed = null;
                    try {
                        parsed = ResponsePattern.match(result);
                    } catch (parseErr) {
                        // Ignore parsing errors
                    }

                    // SAFE CHECK: Ensure rows array exists before checking length
                    if (parsed && parsed.rows && Array.isArray(parsed.rows) && parsed.rows.length > 0) {
                        ui.log(`üîç Reflex found ${parsed.rows.length} matches for keywords: [${keywords.join(', ')}]`, "success");
                        // Format as a "Retrieved Memories" block
                        // Row format: [id, content, timestamp]
                        return parsed.rows.slice(0, 10).map(row =>
                            `- [Memory]: ${row[1]}`
                        ).join('\n');
                    }
                } catch (e) {
                    if (!e.message.includes("length")) {
                        ui.log(`Reflex search error: ${e.message}`, "warn");
                    }
                }
                return "";
            }

            // 2. The "Virtual Markdown" Builder
            // Assembles the "God Prompt" that the model sees as its reality
            buildVirtualPrompt(systemPrompt, retrievedMemories, chatHistory, userText) {
                return `
--- SYSTEM OS ---
${systemPrompt}

--- üß† LONG-TERM MEMORY (Reflex Retrieval) ---
${retrievedMemories ? retrievedMemories : "No specific memories found yet (Model may request more)."}

--- üìú RECENT CHAT CONTEXT ---
${chatHistory.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\n')}

--- üë§ CURRENT USER INPUT ---
${userText}
`;
            }

            // 3. The R1 Loop Wrapper
            // Allows the model to ask for MORE info if Reflex wasn't enough
            async executeR1Loop(userText, history) {
                this.history = [];
                let iteration = 0;

                // Step A: Reflex Search (Immediate)
                let currentContext = await this.retrieveInitialContext(userText);
                if (currentContext) {
                    ui.appendContext("Reflex Retrieval", currentContext);
                }

                while (iteration < this.maxIterations) {
                    iteration++;

                    // Build the "God Prompt"
                    const fullPrompt = this.buildVirtualPrompt(
                        "You are Sovereign Coda. Answer the user based on context. If you need more info to answer, output ONLY: 'NEED_CONTEXT: <search_term>'. Otherwise, provide the final answer.",
                        currentContext,
                        history.slice(-5), // Last 5 turns 
                        userText
                    );

                    ui.log(`üß† R1 Thinking (Iteration ${iteration})...`, "info");

                    let reply = "";
                    try {
                        // Call Model (Non-streaming for logic check)
                        const response = await this.engine.chat.completions.create({
                            messages: [{ role: "user", content: fullPrompt }],
                            temperature: 0.1, // Precision needed
                            max_tokens: 100 // REDUCED FROM 500 to save VRAM on Snapdragon
                        });

                        reply = response.choices[0].message.content.trim();
                    } catch (genErr) {
                        ui.log(`üí• Engine Crash: ${genErr.message}`, "error");
                        if (genErr.message.includes("Instance reference") || genErr.message.includes("disposed")) {
                            alert("GPU CRASHED (D3D12).\n\nPlease CLOSE this window and run 'launch-edge-vulkan.bat' to use the Vulkan backend.");
                            return { context: currentContext, finalAnswer: "‚ö†Ô∏è System Crash: GPU Driver lost. Please run `launch-edge-vulkan.bat`." };
                        }
                        throw genErr;
                    }

                    // Check if Model is asking for help (The Wrapper Logic)
                    if (reply.includes("NEED_CONTEXT:")) {
                        const searchTerm = reply.split("NEED_CONTEXT:")[1].trim();
                        ui.log(`ü§ñ Model requested semantic search: "${searchTerm}"`, "warn");

                        // Perform Semantic Search (simulated via keyword reflex for now, can be vector later)
                        const extraData = await this.retrieveInitialContext(searchTerm);

                        if (extraData) {
                            currentContext += `\n--- Additional Context (${searchTerm}) ---\n${extraData}`;
                            ui.appendContext(`Requested: ${searchTerm}`, extraData);
                            ui.log(`‚úÖ Found additional context for "${searchTerm}"`, "success");
                        } else {
                            currentContext += `\n--- Additional Context (${searchTerm}) ---\n(No results found)`;
                            ui.log(`‚ùå No data found for "${searchTerm}"`, "warn");
                        }

                        continue; // Loop again with new data
                    }

                    // If no request, this is the final answer (or close to it)
                    // We return the Context and the Reply (if it acts as the answer)
                    // But typically handleSend wants to stream the FINAL answer. 
                    // To keep compatible with handleSend logic, we return the accumulated context.
                    return { context: currentContext, finalAnswer: reply };
                }

                return { context: currentContext, finalAnswer: null };
            }

            // TODO: ingestAndCompress(largeText) implementation for 20k->1k distillation
        }

        // --- HANDLERS ---
        async function handleSend() {
            const input = document.getElementById('input');
            const text = input.value.trim();
            if (!text || !engine) return;

            input.value = "";
            input.disabled = true;
            document.getElementById('send-btn').disabled = true;

            ui.append("user", text);

            try {
                // 1. Run Context Manager (Reflex + R1 Loop)
                const { context, finalAnswer } = await contextManager.executeR1Loop(text, []);

                // CHECK FOR CRASH
                if (finalAnswer && finalAnswer.includes("System Crash")) {
                    ui.log("üõë Execution halted due to GPU Crash.", "error");
                    return; // Stop here, do not try to synthesize
                }

                // 2. Final Synthesis with Streaming
                ui.log("üß™ Synthesizing final answer...", "warn");

                const sysPrompt = `You are a helpful assistant with access to retrieved context from the user's memory system.

Use the following context to answer the user's question. Be specific and cite the memory when relevant.

CONTEXT:
${context || "(No relevant context found)"}

If context is insufficient, say so clearly.`;

                const msgHandle = ui.append("assistant", "");

                const stream = await engine.chat.completions.create({
                    messages: [
                        { role: "system", content: sysPrompt },
                        { role: "user", content: text }
                    ],
                    max_tokens: 500,
                    temperature: 0.7,
                    stream: true
                });

                let fullAnswer = "";
                for await (const chunk of stream) {
                    const delta = chunk.choices[0]?.delta?.content || "";
                    fullAnswer += delta;
                    msgHandle.update(fullAnswer);
                }

                ui.log("‚úÖ Response generated.", "success");

            } catch (e) {
                ui.log(`Error: ${e.message}`, "error");
                ui.append("system", `Error: ${e.message}`);
            } finally {
                input.disabled = false;
                document.getElementById('send-btn').disabled = false;
                input.focus();
            }
        }

        // --- MAIN INIT ---
        async function loadModel() {
            const select = document.getElementById('model-select');
            const customInput = document.getElementById('custom-model-input');
            const modelInput = select.value === 'custom' ? customInput.value : select.value;

            if (!modelInput) return alert("Please select a model.");

            // Standardize IDs
            const simpleId = modelInput.split('/').pop();
            selectedModelId = simpleId;

            document.getElementById('model-select').disabled = true;
            document.getElementById('custom-model-input').disabled = true;
            document.getElementById('load-model-btn').disabled = true;
            try {
                ui.log(`Loading Reasoning Engine (${simpleId})...`, "info");

                // --- Robust Model Library Mapping (from legacy) ---
                let modelLib = null;
                const lowerId = simpleId.toLowerCase();
                const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/";

                let qTag = "q4f16_1";
                if (lowerId.includes("q4f32_1")) qTag = "q4f32_1";
                else if (lowerId.includes("q4f16_0")) qTag = "q4f16_0";
                else if (lowerId.includes("q8f16_0")) qTag = "q8f16_0";
                else if (lowerId.includes("q0f32")) qTag = "q0f32";
                else if (lowerId.includes("q0f16")) qTag = "q0f16";
                else if (lowerId.includes("q3f16_1")) {
                    // WARNING: There is no specific q3f16_1 WASM for Mistral v0.3 in v0_2_80.
                    // We fallback to using the q4f16_1 WASM which covers the same architecture.
                    qTag = "q4f16_1";
                }

                if (lowerId.includes('llama-3.2-3b')) {
                    modelLib = libBase + `Llama-3.2-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('llama-3.2-1b')) {
                    modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('llama-3.1-8b')) {
                    // Fallback to Llama-3-8B-Instruct if 3.1 specific WASM is missing in v0_2_80
                    modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('phi-3.5-mini')) {
                    modelLib = libBase + `Phi-3.5-mini-instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('phi-3.5-vision')) {
                    modelLib = libBase + `Phi-3.5-vision-instruct-${qTag}-ctx4k_cs2k-webgpu.wasm`;
                } else if (lowerId.includes('qwen2.5-7b')) {
                    // Use Qwen2 as safe fallback for Qwen2.5 if specific WASM missing
                    modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('qwen2.5-3b')) {
                    modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('qwen3-4b')) {
                    modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Fallback map
                } else if (lowerId.includes('qwen2.5-coder-1.5b') || lowerId.includes('qwen2.5-1.5b') || lowerId.includes('qwen2-1.5b')) {
                    modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('gemma-2-9b')) {
                    modelLib = libBase + `gemma-2-9b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('gemma-2-2b')) {
                    modelLib = libBase + `gemma-2-2b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('mistral') || lowerId.includes('hermes')) {
                    // Hermes/Mistral Logic
                    if (lowerId.includes('hermes-3-llama-3.2-3b')) {
                        modelLib = libBase + `Llama-3.2-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    } else {
                        // VERIFIED: Use v0.3 WASM for all 7B Hermes/Mistral variants
                        modelLib = libBase + `Mistral-7B-Instruct-v0.3-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    }
                }

                if (lowerId.includes('deepseek-r1-distill-llama')) {
                    // Use Llama-3-8B library for architecture compatibility
                    modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                } else if (lowerId.includes('deepseek-r1') || lowerId.includes('qwen')) {
                    // Switch back to Qwen2-7B library (Verified Existing)
                    if (!modelLib) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                }

                // FINAL FALLBACK: Prevent null crash
                if (!modelLib) {
                    ui.log("‚ö†Ô∏è No specific WASM driver found. Defaulting to Qwen2-7B driver.", "warn");
                    modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                }

                // NOTE: For Mistral/Hermes, we omit specific modelLib mapping to let WebLLM defaults try to resolve,
                // or rely on previous Mistral mapping if valid. If explicit mapping failed 404, better to try default.

                ui.log(`Wait for it... Resolving Model Lib: ${modelLib}`, "warn");

                // Dynamic VRAM Calculation
                let vramVal = 2000; // default for small models (increased stability)
                if (lowerId.includes('72b')) vramVal = 48000;
                else if (lowerId.includes('32b')) vramVal = 24000;
                else if (lowerId.includes('14b')) vramVal = 12000;
                else if (lowerId.includes('8b') || lowerId.includes('9b')) vramVal = 8000;
                else if (lowerId.includes('7b')) vramVal = 6000;
                else if (lowerId.includes('3b') || lowerId.includes('4b')) vramVal = 4000;

                ui.log(`Configuring VRAM Requirement: ${vramVal}MB`, "info");

                // Construct Config with BOTH IDs to ensure lookup succeeds for primary OR fallback

                // --- WebGPU Probe (Move up to inform config) ---
                let maxBuffer = 128 * 1024 * 1024;
                try {
                    const adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' }) || await navigator.gpu.requestAdapter();
                    if (adapter) maxBuffer = adapter.limits.maxStorageBufferBindingSize;
                } catch (e) { }
                ui.log(`Configuring WebGPU Buffer: ${Math.round(maxBuffer / 1024 / 1024)}MB`, "info");

                const hwProfile = document.getElementById('hw-profile').value;
                let appConfigOverlay = {};

                if (hwProfile === 'lite') {
                    ui.log("üîã Loading Lite Profile (Snapdragon): Nuclear Mode (Chunk=32, Context=512)", "warn");
                    appConfigOverlay = {
                        prefillChunkSize: 32,     // STRICTEST safety
                        contextWindowSize: 512,   // MINIMUM viable context
                        slidingWindowSize: 512,
                        useIndexedDBCache: true
                    };
                } else if (hwProfile === 'mid') {
                    ui.log("üíª Loading Mid-Tier Profile (8GB VRAM): Balanced (Chunk=256, Context=4096)", "info");
                    appConfigOverlay = {
                        prefillChunkSize: 256,    // Moderate safety for older GPUs
                        contextWindowSize: 4096,  // Standard context
                        slidingWindowSize: undefined, // Standard
                        useIndexedDBCache: true
                    };
                } else if (hwProfile === 'high') {
                    ui.log("üöÄ Loading High-Perf Profile (16GB VRAM): Fast (Chunk=Max, Context=16k)", "info");
                    appConfigOverlay = {
                        prefillChunkSize: -1,     // Max throughput
                        contextWindowSize: 16384, // High context
                        slidingWindowSize: undefined,
                        useIndexedDBCache: true
                    };
                } else { // 'ultra' or default
                    ui.log("‚ö° Loading Ultra Profile (24GB+ VRAM): Unleashed (Chunk=Max, Context=32k+)", "info");
                    appConfigOverlay = {
                        prefillChunkSize: -1,
                        contextWindowSize: 32768, // Massive context
                        slidingWindowSize: undefined,
                        useIndexedDBCache: true
                    };
                }
                const snapdragonId = "snapdragon-" + simpleId;
                const appConfig = {
                    "useIndexedDBCache": appConfigOverlay.useIndexedDBCache,
                    "requiredLimits": { "maxStorageBufferBindingSize": maxBuffer }, // Double tap
                    "model_list": [
                        {
                            "model": "https://huggingface.co/" + modelInput + "/resolve/main/",
                            "model_id": snapdragonId,
                            "model_lib": modelLib,
                            "vram_required_MB": vramVal,
                            "low_resource_required": true,
                            "required_features": ["shader-f16"],
                            "context_window_size": appConfigOverlay.contextWindowSize,
                            "sliding_window_size": appConfigOverlay.slidingWindowSize,
                            "prefill_chunk_size": appConfigOverlay.prefillChunkSize,
                            "buffer_size_required_bytes": maxBuffer // <--- THE OVERRIDE
                        },
                        {
                            "model": "https://huggingface.co/mlc-ai/" + simpleId + "/resolve/main/",
                            "model_id": simpleId,
                            "model_lib": modelLib,
                            "vram_required_MB": vramVal,
                            "low_resource_required": true,
                            "required_features": ["shader-f16"],
                            "context_window_size": appConfigOverlay.contextWindowSize,
                            "sliding_window_size": appConfigOverlay.slidingWindowSize,
                            "prefill_chunk_size": appConfigOverlay.prefillChunkSize,
                            "buffer_size_required_bytes": maxBuffer // <--- THE OVERRIDE
                        }
                    ]
                };
                ui.log(`üöÄ Using Snapdragon Override: ${snapdragonId}`, "info");


                // If modelLib is null (e.g. Mistral), we should let CreateMLCEngine handle it or provide a default?
                // "model_lib": modelLib || (libBase + `Mistral-7B-Instruct-v0.2-${qTag}-sw4k_cs1k-webgpu.wasm`), 
                // We'll trust the logic: if we didn't map it, assume generic Mistral or let it fail gracefully/fallback.
                // Actually, best to map OpenHermes to Mistral v0.2 if we can, but we couldn't verify file.
                // Let's rely on Qwen/Llama mappings being correct.

                try {
                    // --- WebGPU Optimization: Pre-configure Device ---
                    let adapter = null;
                    let device = null;

                    if (navigator.gpu) {
                        adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' }) || await navigator.gpu.requestAdapter();
                        if (adapter) {
                            const features = [];
                            if (adapter.features.has("shader-f16")) features.push("shader-f16");

                            // Explicitly request exactly the buffer size the hardware allows
                            // This prevents WebLLM from requesting 1024MB and triggering the 128MB fallback.
                            device = await adapter.requestDevice({
                                requiredFeatures: features,
                                requiredLimits: {
                                    maxStorageBufferBindingSize: maxBuffer
                                }
                            });
                            ui.log("‚úÖ WebGPU Device pre-allocated (Snapdragon Optimized)", "success");
                        }
                    }

                    ui.log(`Attempting to load Reasoning Engine...`, "info");

                    // Use snapdragonId and our PRE-CREATED device
                    engine = await webllm.CreateMLCEngine(snapdragonId, {
                        appConfig,
                        device, // <--- THE FIX: Bypass WebLLM's internal device request
                        initProgressCallback: (report) => {
                            ui.updateProgress(0.5 + (report.progress * 0.4), report.text);
                        }
                    });

                } catch (primaryErr) {
                    if (primaryErr.message.includes("add' on 'Cache'") || primaryErr.message.includes("quota")) {
                        ui.log("‚ùå STORAGE QUOTA EXCEEDED! Browser cache is full.", "error");
                        alert("Your Browser Storage is FULL.\n\nPlease click 'Delete Model Cache' in the System Controls panel, then try again.");
                        throw primaryErr; // Stop trying fallback, it will fail too
                    }
                    ui.log(`Primary load failed (${primaryErr.message}), trying fallback to ${simpleId}...`, "warn");

                    // Fallback: Try creating engine with simpleId
                    engine = await webllm.CreateMLCEngine(simpleId, {
                        appConfig,
                        initProgressCallback: (report) => {
                            ui.updateProgress(0.5 + (report.progress * 0.4), report.text);
                        }
                    });
                }

                ui.log(`‚úÖ Engine Ready`, "success");
                ui.updateProgress(1.0, "Ready!");

                // Initialize Context Manager (The Brain)
                contextManager = new ContextManager(engine, db);

                // Enable input
                document.getElementById('input').disabled = false;
                document.getElementById('send-btn').disabled = false;
                document.getElementById('input').focus();

                ui.log("üéâ Sovereign Console Online", "success");
                ui.append("assistant", `**Welcome to Sovereign Coda Console with ${simpleId}!**\n\nI'm ready to query your memory system using Graph-R1 reasoning. Ask me anything about the memories you've ingested.\n\nExample: \"What happened in July 2025?\"`);

                // Setup input handlers
                document.getElementById('send-btn').addEventListener('click', handleSend);
                document.getElementById('input').addEventListener('keydown', (e) => {
                    if (e.key === 'Enter' && !e.shiftKey && !document.getElementById('send-btn').disabled) {
                        e.preventDefault();
                        handleSend();
                    }
                });
            } catch (e) {
                ui.log(`Model Load Failed: ${e.message}`, "error");
                document.getElementById('model-select').disabled = false;
                document.getElementById('custom-model-input').disabled = false;
                document.getElementById('load-model-btn').disabled = false;
            }
        }

        async function init() {
            try {
                ui.log("üöÄ Sovereign Console Starting...", "info");
                ui.updateProgress(0.1, "Initializing CozoDB...");

                // Initialize CozoDB
                await initCozo('./cozo_lib_wasm_bg.wasm');
                // Probe IndexedDB before attempting an automatic WASM load (malformed blobs can cause WASM panics)
                try {
                    const [keys, items] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });
                    ui.log(`IndexedDB probe found ${keys.length} entries in 'cozo_store'.`, 'info');
                    if (keys.length === 0) {
                        db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        ui.log("‚úÖ CozoDB Connected (IndexedDB)", "success");
                    } else {
                        // Keys exist. Try to load safe, but catch panic.
                        try {
                            ui.log(`Attempting safe IDB load (${keys.length} items)...`, 'info');
                            db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                            ui.log("‚úÖ CozoDB Connected (IndexedDB)", "success");
                        } catch (loadErr) {
                            ui.log(`Automatic load failed (${loadErr.message}); falling back to empty in-memory DB.`, 'warn');
                            db = CozoDb.new();
                            await createSchema();
                            ui.log('Created FRESH in-memory CozoDB (Schema Initialized). Use Import to restore data.', 'info');
                        }
                    }
                } catch (probeErr) {
                    ui.log(`IndexedDB probe failed: ${probeErr.message} ‚Äî creating fallback in-memory DB`, 'warn');
                    db = CozoDb.new();
                    await createSchema();
                }
                ui.updateProgress(0.3, "Loading Embedder (WASM)...");

                // Initialize Embedder with Timeout (15s)
                // If this hangs (CDN/Network issue), we skip it so the Console remains usable.
                const embedderPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', { device: 'wasm' });

                try {
                    embedder = await Promise.race([
                        embedderPromise,
                        new Promise((_, reject) => setTimeout(() => reject(new Error("Timeout")), 15000))
                    ]);
                    ui.log("‚úÖ Embedder Ready (all-MiniLM-L6-v2)", "success");
                } catch (err) {
                    ui.log(`‚ö†Ô∏è Embedder Init Failed/Timed Out: ${err.message}. Context features disabled.`, "warn");
                    embedder = null; // System proceeds without Embedder
                }

                ui.updateProgress(0.5, "Ready for Model Selection...");

                // Enable model selection
                document.getElementById('model-select').disabled = false;
                document.getElementById('load-model-btn').disabled = false;

                // Setup model selection handlers
                document.getElementById('model-select').addEventListener('change', (e) => {
                    const customInput = document.getElementById('custom-model-input');
                    if (e.target.value === 'custom') {
                        customInput.disabled = false;
                        customInput.focus();
                    } else {
                        customInput.disabled = true;
                    }
                });

                document.getElementById('copy-logs-btn').addEventListener('click', () => {
                    const logs = document.getElementById('status-log').innerText;
                    navigator.clipboard.writeText(logs).then(() => {
                        const btn = document.getElementById('copy-logs-btn');
                        const original = btn.textContent;
                        btn.textContent = "‚úÖ Copied!";
                        setTimeout(() => btn.textContent = original, 2000);
                    }).catch(err => {
                        console.error('Failed to copy: ', err);
                        ui.log('Failed to copy logs to clipboard', 'error');
                    });
                });

                document.getElementById('load-model-btn').addEventListener('click', loadModel);

                ui.log("‚ö†Ô∏è Select a model and click 'Load Model'", "info");

                document.getElementById('attempt-indexeddb-load-btn').addEventListener('click', async () => {
                    try {
                        ui.log('User requested manual IndexedDB load. This may show WASM panics in console but will be handled.', 'info');
                        const db2 = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        db = db2;
                        ui.log('Manual IndexedDB load succeeded.', 'success');
                    } catch (e) {
                        ui.log(`Manual IndexedDB load failed (check console for WASM panic): ${e && e.message ? e.message : String(e)}`, 'error');
                        ui.log('If this happens, inspect and remove any small/corrupt blobs in the Sovereign DB Builder and retry.', 'warn');
                    }
                });

            } catch (e) {
                // Specific handling for WebGPU on Snapdragon/ARM
                if (e.message.includes("compatible GPU")) {
                    ui.log(`‚ùå WebGPU Error: ${e.message}`, "error");
                    ui.append("system", `
<div style="border: 2px solid #d7ba7d; background: #2d2d2d; padding: 10px; border-radius: 5px; margin: 10px 0;">
    <h4 style="margin:0 0 10px 0; color: #d7ba7d;">‚ö†Ô∏è Snapdragon / WebGPU Issue Detected</h4>
    <p>Your browser is blocking WebGPU access. This is common on Snapdragon X Elite devices.</p>
    <p><strong>Fix:</strong></p>
    <ol style="margin-left: 20px;">
        <li>Open a new tab and go to: <code style="background:#444; padding:2px;">edge://flags/#enable-unsafe-webgpu</code></li>
        <li>Set it to <strong>Enabled</strong>.</li>
        <li><strong>Restart the browser</strong> completely.</li>
    </ol>
</div>
`);
                } else {
                    ui.append("system", `**Initialization Error**\n\`\`\`\n${e.message}\n${e.stack}\n\`\`\``);
                }
            }
        }

        // --- GLOBAL EVENT LISTENERS (Run immediately) ---
        const clearBtn = document.getElementById('clear-cache-btn');
        if (clearBtn) {
            clearBtn.addEventListener('click', async () => {
                if (!confirm("Are you sure you want to DELETE all cached model weights? This will require re-downloading models.")) return;
                try {
                    // Determine UI logging method
                    const log = (msg, type) => (typeof ui !== 'undefined' && ui.log) ? ui.log(msg, type) : console.log(msg);

                    log("Deleting model cache...", "warn");

                    // 1. Cache Storage API (Main Weights)
                    const cacheNames = await caches.keys();
                    let deleted = 0;
                    for (const name of cacheNames) {
                        // WebLLM usually uses "webllm/..." or just user provided names if custom
                        // We will delete ANYTHING that looks like a model cache to be safe
                        if (name.includes("webllm") || name.includes("param") || name.includes("wasm")) {
                            await caches.delete(name);
                            log(`Deleted cache: ${name}`, "success");
                            deleted++;
                        }
                    }

                    // 2. Clear Transformers.js cache (if used)
                    if (cacheNames.includes('transformers-cache')) {
                        await caches.delete('transformers-cache');
                        log("Deleted transformers-cache", "success");
                        deleted++;
                    }

                    if (deleted === 0) {
                        log("No obvious model caches found. Trying to list all...", "info");
                        // Desperate mode: list all and ask/delete
                        for (const name of cacheNames) {
                            log(`Found generic cache: ${name}`, "warn");
                            // Uncomment to nuke all: await caches.delete(name);
                        }
                    }

                    log(`Cache clear routine finished. (${deleted} caches removed).`, "success");
                    alert(`Deleted ${deleted} cache(s). Page will now reload.`);
                    window.location.reload();

                } catch (e) {
                    console.error(e);
                    alert(`Error clearing cache: ${e.message}`);
                }

            });
        }

        // --- RESIZE HANDLE LOGIC ---
        const resizer = document.getElementById('resizer');
        const sidebar = document.getElementById('sidebar');
        let isResizing = false;

        if (resizer && sidebar) {
            resizer.addEventListener('mousedown', (e) => {
                isResizing = true;
                resizer.classList.add('resizing');
                document.body.style.cursor = 'col-resize';
                document.body.style.userSelect = 'none'; // Prevent text selection
            });

            document.addEventListener('mousemove', (e) => {
                if (!isResizing) return;
                // Calculate new width
                const newWidth = e.clientX;
                // Constraints (matches CSS min/max)
                if (newWidth > 200 && newWidth < window.innerWidth * 0.8) {
                    sidebar.style.width = newWidth + 'px';
                }
            });

            document.addEventListener('mouseup', () => {
                if (isResizing) {
                    isResizing = false;
                    resizer.classList.remove('resizing');
                    document.body.style.cursor = '';
                    document.body.style.userSelect = '';
                }
            });
        }

        // --- BRIDGE LOGIC (Wave Terminal Integration) ---
        let bridgeWs = null;
        let bridgeReconnectTimer = null;

        window.toggleBridge = function (enabled) {
            const statusEl = document.getElementById('bridge-status');
            if (enabled) {
                if (!engine) {
                    alert("Please load a model first!");
                    document.getElementById('enable-bridge-toggle').checked = false;
                    return;
                }
                statusEl.textContent = "Connecting...";
                connectBridge();
            } else {
                if (bridgeWs) {
                    bridgeWs.close();
                    bridgeWs = null;
                }
                clearTimeout(bridgeReconnectTimer);
                statusEl.textContent = "Disconnected";
            }
        };

        function connectBridge() {
            const statusEl = document.getElementById('bridge-status');
            const bridgeUrl = "ws://localhost:8080/ws/chat"; // Default bridge port

            ui.log(`üåâ Connecting to Bridge (${bridgeUrl})...`, "info");
            bridgeWs = new WebSocket(bridgeUrl);

            bridgeWs.onopen = () => {
                statusEl.textContent = "üü¢ Connected";
                statusEl.style.color = "#0f0";
                ui.log("‚úÖ Bridge Connected (Ready for Wave Terminal)", "success");
            };

            bridgeWs.onclose = () => {
                const toggle = document.getElementById('enable-bridge-toggle');
                if (toggle && toggle.checked) {
                    statusEl.textContent = "üî¥ Disconnected (Retrying...)";
                    statusEl.style.color = "#f00";
                    ui.log("‚ö†Ô∏è Bridge Disconnected. Is 'python webgpu_bridge.py' running? Retrying in 5s...", "warn");
                    bridgeReconnectTimer = setTimeout(connectBridge, 5000);
                } else {
                    statusEl.textContent = "Disconnected";
                    statusEl.style.color = "#666";
                }
            };

            bridgeWs.onmessage = async (event) => {
                const msg = JSON.parse(event.data);
                if (msg.type === 'chat') {
                    handleBridgeChatRequest(msg.id, msg.data);
                }
            };
        }

        document.getElementById('debug-gpu-btn').addEventListener('click', async () => {
            let msg = "WebGPU Debug Info:\n\n";
            if (!navigator.gpu) {
                msg += "‚ùå navigator.gpu is UNDEFINED. Browser does not support WebGPU or it is disabled.\n";
            } else {
                msg += "‚úÖ navigator.gpu exists.\n";
                try {
                    const adapter = await navigator.gpu.requestAdapter();
                    msg += adapter ? `‚úÖ Default Adapter: ${adapter.limits.maxBufferSize}\n` : "‚ùå Default Adapter: NULL\n";
                } catch (e) { msg += `‚ùå Default Adapter Error: ${e.message}\n`; }

                try {
                    const lowAdapter = await navigator.gpu.requestAdapter({ powerPreference: 'low-power' });
                    msg += lowAdapter ? `‚úÖ Low-Power Adapter: ${lowAdapter.limits.maxBufferSize}\n` : "‚ùå Low-Power Adapter: NULL\n";
                } catch (e) { msg += `‚ùå Low-Power Adapter Error: ${e.message}\n`; }
            }
            alert(msg);
        });

        async function handleBridgeChatRequest(reqId, data) {
            ui.log(`üì• Bridge Request: ${reqId.slice(0, 8)}...`, "info");
            try {
                // OpenAI Compatibility Mapping
                let messages = data.messages || [];

                // Sanitize: Ensure System Prompt is FIRST (WebLLM strict requirement)
                // 1. Extract all system messages
                const systemMsgs = messages.filter(m => m.role === 'system');
                const otherMsgs = messages.filter(m => m.role !== 'system');

                // 2. Combine into single system prompt (or take last one if conflict)
                let finalSystemMsg = null;
                if (systemMsgs.length > 0) {
                    // Start with default or first
                    finalSystemMsg = { role: 'system', content: systemMsgs.map(m => m.content).join("\n\n") };
                }

                // 3. Reconstruct array
                const cleanMessages = [];
                if (finalSystemMsg) cleanMessages.push(finalSystemMsg);
                cleanMessages.push(...otherMsgs);

                // Force stream to true for bridge efficiency
                const completion = await engine.chat.completions.create({
                    messages: cleanMessages,
                    stream: true,
                    temperature: data.temperature || 0.7,
                    max_tokens: data.max_tokens || 4096,
                    // Respect current profile settings if needed, but usually request overrides
                });

                for await (const chunk of completion) {
                    if (bridgeWs && bridgeWs.readyState === WebSocket.OPEN) {
                        bridgeWs.send(JSON.stringify({
                            id: reqId,
                            chunk: chunk
                        }));
                    }
                }

                // Signal done
                if (bridgeWs && bridgeWs.readyState === WebSocket.OPEN) {
                    bridgeWs.send(JSON.stringify({
                        id: reqId,
                        done: true
                    }));
                }
                ui.log(`üì§ Bridge Request Complete.`, "success");

            } catch (e) {
                ui.log(`‚ùå Bridge Request Failed: ${e.message}`, "error");
                if (bridgeWs && bridgeWs.readyState === WebSocket.OPEN) {
                    bridgeWs.send(JSON.stringify({ id: reqId, error: e.message }));
                }
            }
        }

        // Start when page loads
        window.addEventListener('load', init);

    </script>
</body>

</html>
```