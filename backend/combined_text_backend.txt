--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\README.md (Section: BACKEND_ROOT) ---

# ECE_Core Backend

> **Executive Cognitive Enhancement (ECE)** - Backend component for the Context-Engine

**Philosophy**: Your mind, augmented. Your data, sovereign. Your tools, open.

---

## Architecture: The Brain

The ECE_Core backend is the cognitive engine of the Context-Engine system. It manages the memory system, reasoning engine, and cognitive orchestration.

### Memory Architecture
- **Neo4j** (port 7687) - PRIMARY STORAGE
  - All memories, summaries, relationships in graph format
  - ContextGist nodes for historical compressed context
  - Node types: `(:Memory)`, `(:Event)`, `(:Person)`, `(:Idea)`, `(:Code)`, `(:ContextGist)`
  - Relationship types: `[:RELATED_TO]`, `[:MENTIONS]`, `[:CAUSED_BY]`, `[:NEXT_GIST]`
- **Redis** (port 6379) - ACTIVE SESSION CACHE
  - Hot cache for active conversations (24h TTL)
  - Graceful fallback to Neo4j if unavailable

### Cognitive Architecture
- **Verifier Agent**: Truth-checking via Empirical Distrust
- **Distiller Agent**: Memory summarization, compression + Context Rotation Protocol
- **Archivist Agent**: Knowledge base maintenance and freshness + Context Continuity
- **Memory Weaver**: Automated relationship repair with full traceability

### Reasoning Architecture
- **Graph-R1 Reasoning Pattern**: "Think → Query → Retrieve → Rethink" iteration
- **Markovian Reasoner**: Infinite-length task handling with state preservation
- **Hybrid Retrieval**: Vector + Graph + Full-text search with ContextGist integration

---

## Quick Start

### Installation
```bash
cd backend
pip install -e .
```

### Configuration
- **Primary Config**: `backend/.env` (from `.env.example`)
- **LLM Settings**: Context size, GPU layers, model path
- **Memory Settings**: Redis/Neo4j connection strings
- **Agent Settings**: Enable/disable Verifier, Archivist, Distiller

### Run Server
```bash
# Start ECE_Core server
python launcher.py
# Server runs on http://localhost:8000
```

---

## Key Features

### ✅ Infinite Context Pipeline
- **64k Context Windows**: All servers configured with 65,536 token capacity
- **Context Rotation Protocol**: Automatic rotation when context approaches 55k tokens
- **Intelligent Distillation**: Old context compressed to "Narrative Gists" using Distiller agent
- **Historical Continuity**: ContextGist nodes maintain reasoning across rotations

### ✅ Cognitive Agents
- **Truth Verification**: Provenance-aware fact-checking
- **Memory Hygiene**: Automatic summarization and maintenance
- **Relationship Repair**: Automated graph integrity maintenance with audit trail

### ✅ Tool Architecture
- **Plugin System**: UTCP-based tool system in `plugins/` directory
- **Safety Layers**: Whitelist/blacklist with human confirmation for dangerous operations
- **MCP Integration**: Memory tools via integrated MCP endpoints

---

## Development

### Run Tests
```bash
python -m pytest tests/
```

### Package Distribution
```bash
python -m build
```

---

## Documentation

- `specs/spec.md` - Technical architecture and design
- `specs/plan.md` - Vision, roadmap, and strategic priorities
- `specs/tasks.md` - Implementation backlog and current tasks
- `specs/TROUBLESHOOTING.md` - Operational debugging and error resolution

---

## Research Foundation

- **Graph-R1**: Memory retrieval patterns with iterative graph traversal
- **Markovian Reasoning**: Chunked thinking with state preservation
- **Hierarchical Reasoning Model (HRM)**: Multi-level context processing
- **Empirical Distrust**: Primary source supremacy for verification

---

## Target Users

- **Cognitive Enhancement**: Users needing personal external memory systems
- **Privacy-Conscious**: Users wanting 100% local, zero-telemetry systems
- **AI Developers**: Users needing extensible, memory-enhanced workflows

---

## Acknowledgments

Built for the cognitive architecture that bridges human and machine intelligence.

**"Your data, sovereign. Your tools, open. Your mind, augmented."**

---

## Need Help?

- **Architecture Questions**: See `specs/spec.md`
- **Vision & Roadmap**: See `specs/plan.md`
- **Current Tasks**: See `specs/tasks.md`
- **Troubleshooting**: See `specs/TROUBLESHOOTING.md`

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\README.md ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\spec.md (Section: BACKEND_SPECS) ---

# ECE_Core - Technical Specification

## Mission

Build a **personal external memory system** as an assistive cognitive tool using:
- Redis + Neo4j tiered memory (pure graph architecture)
- Markovian reasoning (chunked thinking)
- Graph-R1 reasoning (iterative retrieval)
- Local-first LLM integration (llama.cpp)
- Plugin-based tool system (UTCP - Simple Tool Mode)

**Current**: Neo4j + Redis architecture (SQLite removed)
**Protocol**: Plugin System (migrated from MCP 2025-11-13)
**Tools**: Tools loaded via `PluginManager` from `plugins/` directory:
  - `web_search` - DuckDuckGo search with results
  - `filesystem_read` - File and directory operations
  - `shell_execute` - Shell command execution (with safety checks)
  - `mgrep` - Semantic code & natural language file search (semantic `grep`) - Implemented as a standalone plugin in `plugins/mgrep/`

## Architecture Overview

### Memory Architecture: Neo4j + Redis Tiered System

**Neo4j (port 7687)** - PRIMARY STORAGE
- All memories, summaries, relationships in graph format
- Node types: `(:Memory)`, `(:Event)`, `(:Person)`, `(:Idea)`, `(:Code)`, `(:ContextGist)`
- Relationship types: `[:RELATED_TO]`, `[:MENTIONS]`, `[:CAUSED_BY]`, `[:NEXT_GIST]`
- Full-text search capabilities with Cypher
- Temporal reasoning with creation timestamps

**Redis (port 6379)** - ACTIVE SESSION CACHE
- Hot cache for active conversations (24h TTL)
- Session state management
- Temporary context assembly
- Graceful fallback to Neo4j if unavailable

### Cognitive Architecture: Agent-Based System

**Verifier Agent** - Truth Verification
- **Role**: Fact-checking via Empirical Distrust
- **Method**: Provenance-aware scoring (primary sources > summaries)
- **Goal**: Reduce hallucinations, increase factual accuracy

**Distiller Agent** - Memory Compression & Context Rotation
- **Role**: Memory summarization and compression + Context Rotation Protocol
- **Method**: LLM-assisted distillation with salience scoring + context gist creation
- **Goal**: Maintain high-value context, enable infinite context, prune noise

**Archivist Agent** - Memory Maintenance & Context Management
- **Role**: Knowledge base maintenance, freshness checks + Context Coordination
- **Method**: Scheduled verification, stale node detection, context rotation oversight
- **Goal**: Keep memory graph current and trustworthy, manage context windows

**Memory Weaver** - Automated Relationship Repair
- **Role**: Automated graph relationship repair and optimization
- **Method**: Embedding-based similarity with audit trail (`auto_commit_run_id`)
- **Goal**: Maintain graph integrity with full traceability

### Reasoning Architecture: Graph-R1 + Markovian Reasoning

**Graph-R1 Reasoning Pattern**:
1. **Think** - High-level planning based on question
2. **Generate Query** - Create Cypher query for Neo4j
3. **Retrieve Subgraph** - Fetch relevant memories and relationships  
4. **Rethink** - Plan next iteration based on retrieved context
5. **Repeat** - Iterate until confident or max iterations reached

**Markovian Memory**: Chunked context management for infinite windows
- **Active Context**: Current working memory (in Redis)
- **Gist Memory**: Compressed historical context (in Neo4j as `:ContextGist`)
- **Rotation Protocol**: When active context approaches 55k tokens, compress oldest segments to gists

### Tool Architecture: UTCP Plugin System

**Current Implementation**: Plugin-based UTCP (Simple Tool Mode)
- Discovery via `plugins/` directory
- Safety layers with whitelist/blacklist
- Human confirmation flows for dangerous operations

**Available Tools**:
- `web_search` - DuckDuckGo with result limits
- `filesystem_read` - File operations with path restrictions
- `shell_execute` - Command execution with safety checks
- `mgrep` - Semantic code search with context

## Infinite Context Pipeline

### Phase 1: Hardware Foundation
- **64k Context Windows**: All LLM servers boot with 65,536 token capacity
- **GPU Optimization**: Full layer offload with Q8 quantized KV cache
- **Flash Attention**: Enabled when available for optimal long-context performance

### Phase 2: Context Rotation Protocol
- **Monitoring**: ContextManager monitors total context length
- **Trigger**: When context approaches 55k tokens (safety buffer for 64k window)
- **Compression**: Distiller compresses old segments into "Narrative Gists"
- **Storage**: Gists stored in Neo4j as `(:ContextGist)` nodes with `[:NEXT_GIST]` relationships
- **Rewriting**: New context = `[System Prompt] + [Historical Gists Summary] + [Recent Context] + [New Input]`

### Phase 3: Graph-R1 Enhancement
- **Historical Retrieval**: GraphReasoner includes `:ContextGist` nodes in retrieval
- **Continuity Maintenance**: Reasoning flow maintained across context rotations
- **Temporal Awareness**: Reasoning considers chronological relationships in gists

## API Specification

### Core Endpoints (Port 8000)

**Chat Interface**:
- `POST /chat/stream` - Streaming conversation with full memory context
- Request: `{"session_id": str, "message": str, "stream": bool}`
- Response: Streaming SSE with full context injection

**Memory Operations**:
- `POST /memory/add` - Add memory to Neo4j graph
- `POST /memory/search` - Semantic search with relationships  
- `GET /memory/summaries` - Session summary retrieval
- `POST /archivist/ingest` - Ingest content with distillation

**Health & Info**:
- `GET /health` - Server health check
- `GET /v1/models` - Available models
- `GET /health/memory` - Memory system status

**MCP Integration** (when enabled):
- `GET /mcp/tools` - Available memory tools
- `POST /mcp/call` - Execute memory tools

## Configuration

### Required Parameters (in `.env` or config.yaml)
- `NEO4J_URI` - Neo4j connection URI (default: bolt://localhost:7687)
- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)
- `LLM_MODEL_PATH` - Path to GGUF model file
- `ECE_HOST` - Host for ECE server (default: 127.0.0.1)
- `ECE_PORT` - Port for ECE server (default: 8000)

### Optional Parameters
- `ECE_REQUIRE_AUTH` - Enable API token authentication (default: false)
- `ECE_API_KEY` - Static API key when auth enabled
- `MCP_ENABLED` - Enable Model Context Protocol integration (default: true)
- `VERIFIER_AGENT_ENABLED` - Enable truth-checking agent (default: true)
- `ARCHIVIST_AGENT_ENABLED` - Enable memory maintenance agent (default: true)
- `DISTILLER_AGENT_ENABLED` - Enable summarization agent (default: true)

## Security

### Authentication
- Optional API token authentication (controlled by `ECE_REQUIRE_AUTH`)
- Session isolation with UUID-based session IDs
- Memory access limited to owner's session

### Authorization
- Path restrictions on filesystem operations
- Command whitelisting for shell execution
- Rate limiting on all endpoints
- Input validation on all parameters

### Data Protection
- All data stored locally by default
- End-to-end encryption for sensitive memories (optional)
- Audit logging for all memory operations
- Traceability for automated repairs and context rotations

## Performance Optimization

### Hardware Recommendations
- **Minimum**: 16GB RAM, CUDA-capable GPU (RTX series)
- **Recommended**: 32GB+ RAM, RTX 4090 or similar
- **Context Windows**: 64k requires ~8GB VRAM for KV cache with 7B-14B models

### Memory Management
- **Hot Cache**: Redis for active session context (24h TTL)
- **Cold Storage**: Neo4j for persistent memories with relationships
- **Context Rotation**: Automatic compression of old context when approaching limits
- **Caching Strategy**: L1 (Redis) for active context, L2 (Neo4j) for historical context

## Integration Points

### With Anchor CLI
- HTTP API communication on configured port (default: 8000)
- Streaming responses via Server-Sent Events
- Memory operations through dedicated endpoints

### With Browser Extension
- HTTP API communication for context injection and memory saving
- Streaming chat interface via Side Panel
- Page content reading and memory ingestion

### With LLM Servers
- OpenAI-compatible API for LLM communication
- Streaming response handling via SSE
- Context window management with rotation protocol

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\spec.md ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\plan.md (Section: BACKEND_SPECS) ---

# ECE_Core - Implementation Plan

## Vision & Philosophy

**Mission**: Build a personal external memory system that extends human cognitive capabilities while preserving sovereignty and privacy.

**Core Values**:
1. **Local-First**: Your data, your hardware, your control
2. **Cognitive Enhancement**: Augment human reasoning, don't replace human judgment
3. **Truth-Seeking**: Empirical verification over confident hallucination
4. **Sustainable Growth**: Infinite context without infinite complexity

## Strategic Objectives

### Primary Goal: Infinite Context Pipeline
Achieve truly **infinite context capability** through the harmonious integration of:
1. **Hardware Foundation**: 64k+ context windows on local hardware
2. **Smart Rotation Protocol**: Automatic context compression and archival
3. **Continuous Reasoning**: Seamless reasoning flow across context boundaries

### Secondary Goals:
- **Memory Sovereignty**: Complete data ownership and processing on user's hardware
- **Cognitive Enhancement**: Assist with executive function and memory management
- **Tool Integration**: Reliable, safe integration with local system tools
- **Privacy Preservation**: Zero telemetry, zero cloud dependency, 100% local

## Implementation Phases

### Phase 1-4: Foundation (COMPLETED)
- [x] Neo4j + Redis tiered memory architecture (SQLite completely removed)
- [x] Plugin-based tool system (UTCP integration, MCP archived)  
- [x] Cognitive agents (Verifier, Archivist, Distiller)
- [x] Traceability & rollback for automated repairs
- [x] Security hardening (API auth, audit logs)
- [x] MCP integration into main ECE server
- [x] PyInstaller packaging

### Phase 5: Infinite Context Pipeline (IN PROGRESS)
- [x] Phase 1: Hardware Foundation - 64k context windows (COMPLETED Dec 2025)
- [x] Phase 2: Context Rotation Protocol - Automatic context compression (COMPLETED Dec 2025) 
- [x] Phase 3: Graph-R1 Enhancement - Historical context retrieval (COMPLETED Dec 2025)
- [ ] Phase 4: Performance Optimization - Vector adapters + hot replicas (IN PROGRESS)

### Phase 6: Consolidation & Hardening (PLANNED)
- [ ] Complete documentation reset to `specs/` policy
- [ ] Comprehensive security audit
- [ ] Performance benchmarking and optimization
- [ ] User experience refinements

### Phase 7: Expansion (FUTURE)
- [ ] Vector adapter + C2C hot-replica for semantic retrieval
- [ ] Compressed summaries + passage recall (EC-T-133)
- [ ] SLM benchmarking and ALScore measurements
- [ ] Mobile and cross-platform deployment

## Technical Implementation Priorities

### Current Focus (Phase 5-6): Infinite Context & Optimization
1. **Context Management**:
   - Optimize context rotation algorithms for better compression fidelity
   - Enhance ContextGist creation with richer metadata
   - Improve historical context retrieval performance

2. **Memory Architecture**:
   - Vector adapter integration for hybrid search (graph + vector)
   - C2C (Context-to-Context) hot replica system for instant availability
   - Memory weaving optimization for relationship maintenance

3. **Performance**:
   - Implement ALScore for algorithmic latency measurement
   - Optimize graph queries for large-scale deployments
   - Profile and optimize memory usage patterns

### Future Priorities (Phase 7+)  
- **Scalability**: Horizontal partitioning for multi-user deployments
- **Multimodal**: Image and audio input capabilities
- **Federation**: Secure sharing across multiple Context-Engine instances
- **Edge Deployment**: Optimized for embedded and mobile devices

## Research Foundation

### Validated Approaches
- **Graph-R1 Reasoning**: Iterative graph traversal significantly improves memory retrieval accuracy
- **Empirical Distrust**: Provenance-aware verification reduces hallucinations by 60%+
- **Markovian Memory**: Chunked thinking with state preservation enables infinite context
- **Empirical Validation**: Real-world testing confirms cognitive enhancement benefits

### Emerging Research Areas
- **Quantum-Inspired Retrieval**: Exploring quantum-like superposition in memory search
- **Continuous Learning**: Methods to update knowledge graph while system operates
- **Distributed Consciousness**: Multi-node cognitive architecture patterns
- **Cognitive Load Measurement**: Quantifying productivity impact of memory augmentation

## Competitive Advantages

### vs Cloud AI Systems
- **Local Processing**: 100% data sovereignty, no privacy concerns
- **Infinite Context**: No artificial limits on conversation length or document processing
- **Personal Memory**: Long-term relationship with user's evolving knowledge graph
- **Tool Integration**: Native access to local files, systems, and applications

### vs Other Local Solutions  
- **Graph Architecture**: Superior relationship tracking and context retrieval vs simple vector stores
- **Cognitive Agents**: Automated maintenance vs manual memory management
- **Infinite Context**: Unique hardware+logic context rotation pipeline
- **Modular Design**: Easy extensibility vs monolithic architecture

## Success Metrics

### Technical Metrics
- **Context Window**: Achieved 64k effective capacity with infinite rotation capability
- **Memory Accuracy**: >95% retrieval accuracy for stored information
- **Response Latency**: <2s for context-rich queries
- **System Uptime**: >99% availability for local deployment

### User Experience Metrics
- **Session Length**: Users engaging in conversations >1 hour continuously
- **Memory Retention**: Users successfully retrieving information from weeks/months ago
- **Productivity Impact**: Measurable improvement in task completion and context management
- **Privacy Satisfaction**: 100% of data remaining local to user's device

## Risk Assessment

### Technical Risks
- **Memory Scalability**: Mitigated by context rotation and compression algorithms
- **Performance Degradation**: Managed through active maintenance and pruning
- **Hardware Requirements**: Addressed through optimization and varied deployment options

### Adoption Risks
- **Complexity**: Mitigated through excellent documentation and CLI automation
- **Privacy Concerns**: Addressed by 100% local processing default
- **Tool Reliability**: Managed through safety layers and human confirmation

### Competitive Risks
- **Cloud AI Services**: Differentiated through sovereignty and infinite context
- **Other Local Solutions**: Ahead with Graph-R1 and infinite context capabilities

## Timeline & Milestones

### Phase 5 Milestones (Infinite Context Pipeline) - COMPLETED
- [x] Hardware Foundation (64k windows) - Dec 2025
- [x] Context Rotation Protocol - Dec 2025
- [x] Graph-R1 Historical Context - Dec 2025
- [x] Continuity Maintenance - Dec 2025

### Phase 6 Milestones (Consolidation) - IN PROGRESS
- [ ] Documentation Reset - Dec 2025
- [ ] Security Audit - Jan 2026
- [ ] Performance Benchmarking - Jan 2026

### Phase 7 Milestones (Expansion) - PLANNED
- [ ] Vector Adapter Integration - Feb 2026
- [ ] Compressed Summary Architecture - Mar 2026
- [ ] SLM Benchmarking Framework - Apr 2026

## Ethical Framework

### Core Principles
1. **User Sovereignty**: All data belongs to and remains with the user
2. **Cognitive Liberty**: Enhancement without control or manipulation  
3. **Transparency**: Clear visibility into how the system processes information
4. **Autonomy**: Tools that enhance human decision-making, not replace it

### Implementation Guidelines
- Open source codebase with MIT license
- Local processing by default, no telemetry
- Clear audit trail for all automated operations
- Human confirmation for all autonomous changes

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\plan.md ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\tasks.md (Section: BACKEND_SPECS) ---

# ECE_Core - Implementation Tasks

## Active Work Queue (Phase 5-6: Infinite Context & Consolidation)

### [IN PROGRESS] Vector Adapter & C2C Implementation (EC-V-201)
- [ ] Implement VectorAdapter abstract interface (EC-V-201.01)
  - [ ] Define base class with required methods: `query_vector`, `insert_vector`, `delete_vector`
  - [ ] Implement Redis-based VectorAdapter with HNSW indexing
  - [ ] Implement FAISS VectorAdapter for local deployments
  - [ ] Performance benchmarking against graph-only retrieval
  - [ ] Integration testing with existing ContextManager

- [ ] C2C (Context-to-Context) Hot-Replica System (EC-V-201.02)
  - [ ] Define hot-replica synchronization protocol for vector indices
  - [ ] Implement real-time vector index updates during memory ingestion
  - [ ] Cross-validation between graph and vector retrieval methods  
  - [ ] Automatic failover from vector to graph when needed for reliability

### [IN PROGRESS] Compressed Summary Architecture (EC-CS-133)
- [ ] Implement compressed summary generation pipeline (EC-CS-133.01)
  - [ ] Design salience scoring algorithm for context gist compression
  - [ ] Implement passage recall mechanism from compressed representations  
  - [ ] Optimize compression ratios vs. information retention balance
  - [ ] Integration with ContextGist rotation system in Neo4j
  - [ ] Performance testing with 100k+ token context windows

### [PLANNED] Performance Optimization (EC-PERF-202)
- [ ] Implement ALScore (Augmentation Latency Score) measurement framework (EC-PERF-202.01)
  - [ ] Define latency benchmarks for different context window sizes
  - [ ] Create tool execution time measurements and analysis
  - [ ] Implement memory retrieval accuracy and completeness metrics
  - [ ] Establish optimization recommendations based on ALScore results

## Current Development (Phase 6: Consolidation)

### [IN PROGRESS] Security Audit & Hardening (EC-SEC-301)
- [ ] Complete comprehensive security audit of all HTTP endpoints
- [ ] Implement additional input validation and sanitization layers
- [ ] Perform penetration testing on authenticated endpoints
- [ ] Verify proper isolation between user sessions in multi-user scenarios

### [IN PROGRESS] Documentation Reset (EC-DOC-302)  
- [ ] Migrate all documentation to `specs/` policy structure
- [ ] Update README.md and CHANGELOG.md to match new architecture
- [ ] Consolidate all wiki-style docs into spec.md, plan.md, tasks.md format
- [ ] Archive obsolete documentation files appropriately

### [IN PROGRESS] Performance Benchmarking (EC-PM-303)
- [ ] Create standardized benchmark suite for infinite context operations
- [ ] Measure context rotation performance with 30k+ token inputs
- [ ] Compare memory retrieval speeds with and without historical gists
- [ ] Profile memory usage patterns during long-running sessions

## Upcoming Priorities (Phase 7: Expansion)

### [PLANNED] Small Model Benchmarking (EC-SLM-401)
- [ ] Establish SLM (Small Language Model) benchmarking framework
- [ ] Implement ALScore measurements for different model architectures
- [ ] Create optimization recommendations for various hardware configurations
- [ ] Evaluate gemma-2, phi-3, and mistral-nemo performance in infinite context

### [PLANNED] Memory Weaver Enhancements (EC-MW-402)  
- [ ] Automated relationship repair with improved similarity algorithms
- [ ] Enhanced audit trail with comprehensive rollback capabilities
- [ ] Performance optimization for large-scale graph repairs
- [ ] Integration with new vector adapter for hybrid repairs

### [PLANNED] Mobile Deployment Preparation (EC-MOB-403)
- [ ] Create ARM64 build pipeline for Raspberry Pi and mobile devices
- [ ] Optimize memory architecture for constrained resource environments
- [ ] Implement offline-only mode for air-gapped deployments
- [ ] Create native mobile applications for iOS and Android

## Tool Integration Tasks

### [IN PROGRESS] UTCP Plugin System Enhancement (EC-UTCP-250)
- [ ] Implement dynamic plugin loading and unloading
- [ ] Add safety sandboxing for external plugins
- [ ] Create plugin marketplace integration
- [ ] Implement plugin-specific rate limiting and resource quotas

### [PLANNED] Advanced Tool Integration (EC-ADV-251)
- [ ] OS-level tool integration (filesystem, clipboard, window management)
- [ ] IDE integration (VS Code, Vim, Emacs) for context injection
- [ ] Email client integration for inbox management
- [ ] Calendar integration for scheduling and time management

## Maintenance Tasks

### [ONGOING] System Health (EC-OPS-001)
- [ ] Monitor Neo4j performance under large graph conditions
- [ ] Track Redis memory usage and implement eviction policies
- [ ] Profile context rotation performance with historical data
- [ ] Maintain backup and recovery procedures for Neo4j/Redis

### [MONTHLY] Dependency Updates (EC-OPS-002)
- [ ] Update Python dependencies with security scanning
- [ ] Verify compatibility with latest llama.cpp releases
- [ ] Test with new GGUF model formats and quantizations
- [ ] Update HuggingFace model references and fallback URLs

### [QUARTERLY] Architecture Review (EC-ARCH-003)
- [ ] Performance analysis of graph vs vector retrieval
- [ ] Memory utilization optimization for long-running instances
- [ ] User experience improvements based on feedback
- [ ] Scalability assessment for enterprise deployment

## Completed Recently (Phase 5: Infinite Context Pipeline)

### [COMPLETED] Phase 1: Hardware Foundation (EC-HW-101)
- [x] Upgrade all LLM servers to 64k context windows (Dec 2025)
- [x] Implement Flash Attention support and optimization (Dec 2025)
- [x] Configure KV cache with Q8 quantization for memory efficiency (Dec 2025)

### [COMPLETED] Phase 2: Context Rotation Protocol (EC-CRP-102)
- [x] Implement ContextManager monitoring for 55k token threshold (Dec 2025)
- [x] Integrate Distiller for intelligent content compression (Dec 2025)
- [x] Create Neo4j storage for ContextGist nodes with chronological links (Dec 2025)
- [x] Implement context reconstruction with [System] + [Gists] + [Recent] + [New] (Dec 2025)

### [COMPLETED] Phase 3: Graph-R1 Enhancement (EC-GR1-103)  
- [x] Update GraphReasoner to include ContextGist node retrieval (Dec 2025)
- [x] Implement historical context integration in reasoning loop (Dec 2025)
- [x] Maintain reasoning continuity across context rotation boundaries (Dec 2025)

### [COMPLETED] System Integration & Testing (EC-INT-104)
- [x] End-to-end testing of infinite context pipeline (Dec 2025)
- [x] Performance benchmarking with 30k+ token inputs (Dec 2025)
- [x] Memory continuity verification across rotation boundaries (Dec 2025)

## Known Issues & Technical Debt

### Performance Issues
- [ ] Neo4j query optimization needed for large-scale graph traversal (EC-PERF-001)
- [ ] Redis memory usage monitoring and automated cleanup required (EC-PERF-002) 
- [ ] Context rotation timing optimization to minimize user experience disruption (EC-PERF-003)

### Reliability Issues
- [ ] Fallback mechanisms needed when Neo4j is temporarily unavailable (EC-REL-001)
- [ ] Retry logic for failed ContextGist creation during peak load periods (EC-REL-002)
- [ ] Graceful degradation when ContextGist retrieval fails (EC-REL-003)

### Usability Issues  
- [ ] Progress indicators needed during large context rotation operations (EC-USAB-001)
- [ ] User notifications for automatic context rotation events (EC-USAB-002)
- [ ] Configurable rotation thresholds based on model capabilities (EC-USAB-003)

## Research Tasks

### Active Research Projects
- [ ] Evaluation of different compression algorithms for ContextGist generation (EC-RES-001)
- [ ] Comparison of rotation strategies (oldest-first vs. least-relevant-first) (EC-RES-002)
- [ ] Investigation of hybrid retrieval effectiveness (graph + vector + keyword) (EC-RES-003)

### Planned Research Projects
- [ ] Long-term memory stability testing over 6+ month usage periods (EC-RES-004)
- [ ] Cognitive load measurement with infinite vs. finite context systems (EC-RES-005)
- [ ] User productivity impact assessment with comprehensive usage analytics (EC-RES-006)

---

**Current Status**: Active development on vector adapter integration and performance optimization
**Last Updated**: 2025-12-08
**Next Priority**: Complete vector adapter implementation for hybrid retrieval

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\specs\tasks.md ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\app_factory.py (Section: BACKEND_PYTHON) ---

"""Small wrapper to provide a clean app import for tests and tools.
This module avoids pulling in legacy `src.main` content and provides a
single `app` object to import in tests or external tooling.
"""
from src.bootstrap import create_app
from src.config import settings
import logging

logging.basicConfig(level=getattr(logging, settings.ece_log_level), format='%(asctime)s - %(levelname)s - %(message)s')

def create_app_with_routers():
	"""Create the FastAPI app and include all API routers.

	Use this factory to create an app instance after pytest autouse fixtures have run
	to ensure that patches (fake LLM, fake Redis) are applied before the app lifecycle
	creates real clients.
	"""
	app = create_app()
	from src.api import (
	memory_router,
	reason_router,
	health_router,
	# openai_router,
	plugins_router,
	audit_router,
    plan_router,
	)

	# Import browser bridge plugin
	from src.plugins.browser_bridge.plugin import router as browser_bridge_router
	# Import Coda Chat Recipe
	from src.recipes.coda_chat import router as coda_chat_router
	# Import Archivist Recipe
	from src.recipes.archivist import router as archivist_router

	from fastapi import Depends
	from src.security import verify_api_key

	app.include_router(health_router)  # Public
	app.include_router(health_router)  # Public
	# app.include_router(openai_router, dependencies=[Depends(verify_api_key)])
	app.include_router(reason_router, dependencies=[Depends(verify_api_key)])
	app.include_router(plugins_router, dependencies=[Depends(verify_api_key)])
	app.include_router(audit_router, dependencies=[Depends(verify_api_key)])
	app.include_router(plan_router, dependencies=[Depends(verify_api_key)])

	# Include browser bridge router
	app.include_router(browser_bridge_router)

	# Temporarily remove dependencies to debug 403
	app.include_router(coda_chat_router, prefix="/chat") #, dependencies=[Depends(verify_api_key)])

	# Include Archivist Recipe
	app.include_router(archivist_router, prefix="/archivist", dependencies=[Depends(verify_api_key)])

	return app


# Backwards compatible app instance for import-time use (rare)
app = None


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\app_factory.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\bootstrap.py (Section: BACKEND_PYTHON) ---

"""Application bootstrap: provide create_app() with lifecycle initialization.

This module centralizes the app startup and shutdown lifecycle so `main.py`
becomes a thin routing module while the heavy initialization logic lives
here. The components are stored in `app.state` to be accessible from routes.
"""
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, Any
from src.config import settings
from src.security import audit_logger
from src.memory import TieredMemory
from src.context import ContextManager
from src.intelligent_chunker import IntelligentChunker
from src.distiller import Distiller
from src.graph import GraphReasoner, MarkovianReasoner
from src.agents import VerifierAgent, ArchivistAgent
from src.agents.planner import PlannerAgent
try:
    from plugins.manager import PluginManager
except Exception:
    PluginManager = None
from src.tool_call_models import ToolCallParser, ToolCallValidator
from src.tools import ToolExecutor

logger = logging.getLogger(__name__)


def create_app() -> FastAPI:
    """Create FastAPI app with initialized components stored in app.state."""
    app = FastAPI(title="ECE_Core", version=settings.ece_version)

    @asynccontextmanager
    async def lifespan(app: FastAPI):
        logger.info("Starting ECE_Core with Markovian reasoning... (bootstrap)")
        memory = TieredMemory()
        await memory.initialize()

        mem_status = []
        if memory.redis and memory.redis.redis:
            mem_status.append("Redis")
        if memory.neo4j and memory.neo4j.neo4j_driver:
            mem_status.append("Neo4j")
        mem_str = " + ".join(mem_status) if mem_status else "No backends connected!"
        logger.info(f"Memory initialized ({mem_str})")

        # Import LLMClient here so unit tests can patch `src.llm.LLMClient` before booting the app
        from src.llm import LLMClient
        llm = LLMClient()
        logger.info("LLM client ready")
        context_mgr = ContextManager(memory, llm)
        logger.info("Context manager ready")
        chunker = IntelligentChunker(llm)
        logger.info("Intelligent chunker ready")
        distiller = Distiller(llm)
        logger.info("Distiller ready")
        graph_reasoner = GraphReasoner(memory, llm)
        logger.info("Graph reasoner ready (memory retrieval)")
        markov_reasoner = MarkovianReasoner(llm)
        logger.info("Markovian reasoner ready (chunked processing)")
        verifier_agent = VerifierAgent(memory, llm)
        logger.info("Verifier agent ready (Empirical Distrust)")
        archivist_agent = ArchivistAgent(memory, verifier_agent)
        # Start archivist background loop
        await archivist_agent.start()
        logger.info("Archivist agent ready (Maintenance Loop)")

        tool_parser = ToolCallParser()
        logger.info("Tool call parser ready (Pydantic validation)")
        tool_validator = None
        mcp_client = None
        plugin_manager = None

        # Initialize plugin manager (preferred) or MCP client for tools
        if PluginManager:
            plugin_manager = PluginManager(settings.__dict__)
            discovered = plugin_manager.discover()
            if discovered:
                logger.info(f"Plugin manager loaded plugins: {', '.join(discovered)}")
                tools = plugin_manager.list_tools()
                if tools:
                    tools_dict = {tool['name']: tool for tool in tools}
                    tool_validator = ToolCallValidator(tools_dict)
                    logger.info("Tool validator ready (via plugins)")
            else:
                logger.warning("Plugin manager enabled but no plugins discovered (tools disabled)")
        else:
            logger.warning("PluginManager not available (tools disabled)")

        # Initialize MCP client if configured
        if settings.mcp_enabled:
            try:
                from src.mcp_client import MCPClient as _MCPClient
                mcp_client = _MCPClient()
                logger.info("MCP client initialized for %s", mcp_client.base_url)
            except Exception as e:
                logger.warning("MCP client could not be initialized: %s", e)

        # Store components in app.state
        app.state.memory = memory
        app.state.llm = llm
        app.state.context_mgr = context_mgr
        app.state.chunker = chunker
        app.state.distiller = distiller
        app.state.graph_reasoner = graph_reasoner
        app.state.markov_reasoner = markov_reasoner
        app.state.verifier_agent = verifier_agent
        app.state.archivist_agent = archivist_agent
        app.state.plugin_manager = plugin_manager
        app.state.audit_logger = audit_logger
        app.state.tool_parser = tool_parser
        app.state.tool_validator = tool_validator
        # Planner agent
        planner_agent = PlannerAgent(llm)
        app.state.planner = planner_agent

        logger.info(f"ECE_Core running at http://{settings.ece_host}:{settings.ece_port}")
        try:
            yield
        finally:
            logger.info("Shutting down (bootstrap)...")
            try:
                await archivist_agent.stop()
            except Exception:
                pass
            try:
                await memory.close()
            except Exception:
                pass
            try:
                await llm.close()
            except Exception:
                pass

    app = FastAPI(title="ECE_Core", version=settings.ece_version, lifespan=lifespan)

    # DEBUG: Log all requests
    @app.middleware("http")
    async def log_requests(request, call_next):
        logger.info(f"Incoming request: {request.method} {request.url}")
        logger.info(f"Headers: {request.headers}")
        response = await call_next(request)
        logger.info(f"Response status: {response.status_code}")
        return response

    # Configure CORS - Permissive for Debugging
    app.add_middleware(
        CORSMiddleware,
        allow_origin_regex='.*',  # Allow ANY origin matching this regex
        allow_credentials=True,   # Allow cookies/auth headers
        allow_methods=["*"],
        allow_headers=["*"],
    )

    return app


def get_components(app: FastAPI) -> dict:
    """Return a dict of initialized components for convenience in routes.
    """
    return {
        "memory": getattr(app.state, "memory", None),
        "llm": getattr(app.state, "llm", None),
        "context_mgr": getattr(app.state, "context_mgr", None),
        "chunker": getattr(app.state, "chunker", None),
        "distiller": getattr(app.state, "distiller", None),
        "graph_reasoner": getattr(app.state, "graph_reasoner", None),
        "markov_reasoner": getattr(app.state, "markov_reasoner", None),
        "verifier_agent": getattr(app.state, "verifier_agent", None),
        "archivist_agent": getattr(app.state, "archivist_agent", None),
        "plugin_manager": getattr(app.state, "plugin_manager", None),
        "tool_parser": getattr(app.state, "tool_parser", None),
        "tool_validator": getattr(app.state, "tool_validator", None),
        "audit_logger": getattr(app.state, "audit_logger", None),
    }


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\bootstrap.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\chat_templates.py (Section: BACKEND_PYTHON) ---

"""
Chat template management for different LLM models.
Supports various chat formats including Qwen3, Gemma3, and standard OpenAI format.
"""
from typing import List, Dict, Optional
from jinja2 import Template


class ChatTemplate:
    """Base class for chat templates"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        raise NotImplementedError()


class OpenAIChatTemplate(ChatTemplate):
    """Standard OpenAI chat format"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # For API compatibility, we just return the messages as-is with system prompt added
        formatted_messages = []
        if system_prompt:
            formatted_messages.append({"role": "system", "content": system_prompt})
        formatted_messages.extend(messages)
        return formatted_messages


class Qwen3ChatTemplate(ChatTemplate):
    """Qwen3 chat template format - direct implementation"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Add system prompt to messages if provided
        all_messages = []
        if system_prompt and not (messages and messages[0].get("role") == "system"):
            all_messages.append({"role": "system", "content": system_prompt})
        all_messages.extend(messages)

        result = []

        if tools:
            # Include tools in the system message
            result.append("SYSTEM")
            if all_messages and all_messages[0].get("role") == "system":
                result.append(all_messages[0]["content"])
                result.append("")  # Empty line
            result.append("In this environment you have access to a set of tools you can use to answer the user's question. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.")
            result.append("")
            result.append("Tool Use Rules")
            result.append("Here are the rules you should always follow to solve your task:")
            result.append("1. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.")
            result.append("2. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.")
            result.append("3. If no tool call is needed, just answer the question directly.")
            result.append("4. Never re-do a tool call that you previously did with the exact same parameters.")
            result.append("5. When you have sufficient information to answer the user's question, answer directly without using more tools.")
            result.append("Now Begin!")
            result.append("")
            result.append("# Tools")
            result.append("")
            result.append("You may call one or more functions to assist with the user query.")
            result.append("")
            result.append("You are provided with function signatures within <tools></tools> XML tags:")
            result.append("<tools>")
            for tool in tools:
                import json
                result.append(json.dumps(tool))
            result.append("</tools>")
            result.append("")
            result.append("To use a tool, respond with: TOOL_CALL: tool_name(param1=value1, param2=value2)")
            result.append("Do not use any other format for tool calls. Do not generate JSON objects for tools.")
            result.append("")


            # Process remaining messages (skip the system message we already handled)
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for i, message in enumerate(all_messages[start_idx:]):
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user" or (role == "system" and i > 0):  # Additional system messages after first
                    result.append(f"USER")
                    result.append(content)
                    result.append("")
                elif role == "assistant":
                    result.append("ASSISTANT")
                    result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append("USER")
                    result.append(content)
                    result.append("")

        # No tools version
        if all_messages and all_messages[0].get("role") == "system":
            result.append("SYSTEM")
            result.append(all_messages[0]["content"])
            result.append("USER")
        else:
            result.append("SYSTEM")
            result.append("You are a helpful assistant.")
            result.append("USER")

        # Process remaining messages
        start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
        for message in all_messages[start_idx:]:
            role = message.get("role", "")
            content = message.get("content", "")

            if role == "user":
                result.append(content)
                result.append("ASSISTANT")
            elif role == "assistant":
                result.append(content)
                result.append("")
            elif role == "tool":
                result.append(content)
                result.append("USER")

        return "\n".join(result).strip()


class Qwen3ThinkingChatTemplate(ChatTemplate):
    """Qwen3 chat template with explicit thinking token support"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Add system prompt to messages if provided
        all_messages = []
        if system_prompt and not (messages and messages[0].get("role") == "system"):
            all_messages.append({"role": "system", "content": system_prompt})
        all_messages.extend(messages)

        # Build the formatted conversation with explicit thinking tokens
        result = []

        if tools:
            # Include tools in the system message
            result.append("SYSTEM")
            if all_messages and all_messages[0].get("role") == "system":
                result.append(all_messages[0]["content"])
                result.append("")  # Empty line
            result.append("In this environment you have access to a set of tools you can use to answer the user's question. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.")
            result.append("")
            result.append("Tool Use Rules")
            result.append("Here are the rules you should always follow to solve your task:")
            result.append("1. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.")
            result.append("2. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.")
            result.append("3. If no tool call is needed, just answer the question directly.")
            result.append("4. Never re-do a tool call that you previously did with the exact same parameters.")
            result.append("5. When you have sufficient information to answer the user's question, answer directly without using more tools.")
            result.append("Now Begin!")
            result.append("")
            result.append("# Tools")
            result.append("")
            result.append("You may call one or more functions to assist with the user query.")
            result.append("")
            result.append("You are provided with function signatures within <tools></tools> XML tags:")
            result.append("<tools>")
            for tool in tools:
                import json
                result.append(json.dumps(tool))
            result.append("</tools>")
            result.append("")
            result.append("To use a tool, respond with: TOOL_CALL: tool_name(param1=value1, param2=value2)")
            result.append("Do not use any other format for tool calls. Do not generate JSON objects for tools.")
            result.append("")


            # Process remaining messages (skip the system message we already handled)
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for i, message in enumerate(all_messages[start_idx:]):
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user" or (role == "system" and i > 0):  # Additional system messages after first
                    result.append(f"USER")
                    result.append(content)
                    result.append("")
                elif role == "assistant":
                    result.append("ASSISTANT")
                    result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append("USER")
                    result.append(content)
                    result.append("")

        else:
            # No tools version - with explicit thinking token support
            if all_messages and all_messages[0].get("role") == "system":
                result.append("SYSTEM")
                result.append(all_messages[0]["content"])
                result.append("USER")
            else:
                result.append("SYSTEM")
                result.append("You are a helpful assistant.")
                result.append("USER")

            # Process remaining messages
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for message in all_messages[start_idx:]:
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user":
                    result.append(content)
                    result.append("ASSISTANT")
                elif role == "assistant":
                    result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append(content)
                    result.append("USER")

        return "\n".join(result).strip()


class Gemma3ChatTemplate(ChatTemplate):
    """Gemma-3 chat template for creative writing - with minimal tool support"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Gemma uses the <start_of_turn> and <end_of_turn> format
        result = []

        # Handle system prompt and tools in a simple way for Gemma
        combined_system = ""
        if system_prompt:
            combined_system = system_prompt

        if tools and len(tools) > 0:
            # Add minimal tool instructions to system message
            tool_desc = "\n\nYou have access to tools. To use a tool, respond with: TOOL_CALL: tool_name(param1=value1, param2=value2)"
            combined_system += tool_desc

        if combined_system:
            result.append(f"<start_of_turn>system")
            result.append(combined_system)
            result.append(f"<end_of_turn>")

        # Process messages
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")

            if role == "user":
                result.append(f"<start_of_turn>user")
                result.append(content)
                result.append(f"<end_of_turn>")
            elif role == "assistant":
                result.append(f"<start_of_turn>model")
                result.append(content)
                result.append(f"<end_of_turn>")
            elif role == "tool":
                # Handle tool responses by presenting them back to the user
                result.append(f"<start_of_turn>user")
                result.append(f"Tool result: {content}")
                result.append(f"<end_of_turn>")
            elif role == "system" and not system_prompt:  # Additional system messages
                result.append(f"<start_of_turn>system")
                result.append(content)
                result.append(f"<end_of_turn>")

        # Add final model turn marker for generation
        result.append(f"<start_of_turn>model")

        return "\n".join(result)


class MinimalChatTemplate(ChatTemplate):
    """Minimal template for simpler models that get confused by complex formatting"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        result = []

        # Simple system message
        if system_prompt:
            result.append(f"SYSTEM: {system_prompt}")

        # Add minimal tool instruction if tools are available
        if tools:
            result.append("You can use tools. Format: TOOL_CALL: tool_name(param=value)")

        # Process messages in a simple format
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")

            if role == "user":
                result.append(f"USER: {content}")
            elif role == "assistant":
                result.append(f"ASSISTANT: {content}")
            elif role == "tool":
                result.append(f"TOOL RESULT: {content}")

        # Add final prompt for assistant response
        result.append("ASSISTANT:")

        return "\n".join(result)


class ChatTemplateManager:
    """Manager for different chat templates"""

    def __init__(self):
        self.templates = {
            "openai": OpenAIChatTemplate(),
            "qwen3": Qwen3ChatTemplate(),
            "qwen3-thinking": Qwen3ThinkingChatTemplate(),  # Enhanced template with thinking token support
            "gemma3": Gemma3ChatTemplate(),  # Added Gemma 3 template
            "minimal": MinimalChatTemplate(),  # Minimal template for simpler models
        }

    def get_template(self, template_name: str) -> ChatTemplate:
        return self.templates.get(template_name, self.templates["openai"])

    def register_template(self, name: str, template: ChatTemplate):
        self.templates[name] = template


# Global template manager instance
chat_template_manager = ChatTemplateManager()

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\chat_templates.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\circuit_breaker.py (Section: BACKEND_PYTHON) ---

"""
Circuit breaker pattern for ECE_Core external dependencies.
Prevents cascading failures when Neo4j or Redis are slow/down.
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Optional, Callable, Any
from enum import Enum

logger = logging.getLogger(__name__)

class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"  # Normal operation
    OPEN = "open"  # Failing - reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    """
    Circuit breaker for external service calls.
    
    Usage:
        breaker = CircuitBreaker(failure_threshold=5, timeout=60)
        result = await breaker.call(my_async_function, arg1, arg2)
    """
    
    def __init__(
        self,
        failure_threshold: int = 5,
        timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = CircuitState.CLOSED
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to try resetting."""
        if self.last_failure_time is None:
            return True
        
        return datetime.now() > self.last_failure_time + timedelta(seconds=self.timeout)
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute function with circuit breaker protection.
        
        Args:
            func: Async function to call
            *args, **kwargs: Arguments to pass to function
        
        Returns:
            Function result
        
        Raises:
            CircuitBreakerError: If circuit is open
            Exception: Original exception if circuit allows call
        """
        # If circuit is OPEN, check if we should try again
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logger.info("Circuit breaker transitioning to HALF_OPEN")
            else:
                raise CircuitBreakerError("Circuit breaker is OPEN")
        
        try:
            # Attempt the call
            result = await func(*args, **kwargs)
            
            # Success - reset failure count
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
                logger.info("Circuit breaker reset to CLOSED")
            
            self.failure_count = 0
            return result
        
        except self.expected_exception as e:
            # Track failure
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            
            logger.warning(f"Circuit breaker failure {self.failure_count}/{self.failure_threshold}: {e}")
            
            # Open circuit if threshold reached
            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                logger.error(f"Circuit breaker opened after {self.failure_count} failures")
            
            raise

class CircuitBreakerError(Exception):
    """Raised when circuit breaker is open."""
    pass

# ============================================================================
# PRE-CONFIGURED CIRCUIT BREAKERS
# ============================================================================

neo4j_breaker = CircuitBreaker(
    failure_threshold=5,
    timeout=60,
    expected_exception=Exception
)

redis_breaker = CircuitBreaker(
    failure_threshold=3,
    timeout=30,
    expected_exception=Exception
)

llm_breaker = CircuitBreaker(
    failure_threshold=10,
    timeout=120,
    expected_exception=Exception
)

# ============================================================================
# DECORATOR FOR EASY USAGE
# ============================================================================

def circuit_breaker(breaker: CircuitBreaker):
    """
    Decorator to apply circuit breaker to async functions.
    
    Usage:
        @circuit_breaker(neo4j_breaker)
        async def query_neo4j(...):
            ...
    """
    def decorator(func: Callable):
        async def wrapper(*args, **kwargs):
            return await breaker.call(func, *args, **kwargs)
        return wrapper
    return decorator


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\circuit_breaker.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\config.py (Section: BACKEND_PYTHON) ---

"""
Comprehensive configuration management for ECE_Core.
Organized by component/file for easy maintenance.
"""
from pydantic_settings import BaseSettings
from pydantic import ConfigDict, model_validator
from pathlib import Path
try:
    import yaml
except Exception:
    yaml = None
from dotenv import load_dotenv
from typing import Optional
import json
from urllib.parse import urlparse
import os

try:
    from importlib.metadata import version as _get_pkg_version
    try:
        _pkg_version = _get_pkg_version("ece-core")
    except Exception:
        _pkg_version = None
except Exception:
    _pkg_version = None

class Settings(BaseSettings):
    """
    Configuration organized by component.
    All settings can be overridden via environment variables or .env file.
    """
    
    # ============================================================
    # LLM_CLIENT.PY - Local GGUF Model Settings
    # ============================================================
    llm_api_base: str = "http://localhost:8080/v1"  # llama.cpp server
    # Optional: specify a distinct base URL for embeddings (useful when embedding server runs separately on port 8081)
    llm_embeddings_api_base: Optional[str] = "http://127.0.0.1:8081/v1"
    # Optional: specific model name for embeddings (embedding-capable model like qwen3-embedding-4b)
    llm_embeddings_model_name: Optional[str] = ""
    # Control whether a local GGUF model should be used as a fallback for embeddings
    llm_embeddings_local_fallback_enabled: bool = False
    # Embeddings chunk tuning
    llm_embeddings_chunk_size_default: int = 2048  # default char-based chunk size for long docs (reduced to avoid embedding server 500s)
    llm_embeddings_min_chunk_size: int = 128  # smallest allowed chunk size
    # Sequence of backoff chunk sizes to try when server reports input too large
    llm_embeddings_chunk_backoff_sequence: list[int] = [2048, 1024, 512, 256, 128]
    # Enable adaptive backoff for embeddings (parse server messages and try smaller chunk sizes automatically)
    llm_embeddings_adaptive_backoff_enabled: bool = True
    # Default batch size (number of documents per embeddings API request). Lower default to avoid server overloads.
    llm_embeddings_default_batch_size: int = 2
    # Model selection (name/path used by the LLM client and helper script)
    # Default production-tuned LLM settings for OpenAI-20B on 16GB RTX 4090
    llm_model_name: str = "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf"  # Model name/path used by launcher
    llm_model_path: str = ""  # Optional local model path (GGUF) — leave blank to avoid loading if not present
    # Optimized for 16GB RTX 4090 with 64k context window to maximize memory utilization
    # 64k context results in ~8GB KV cache which fits comfortably with model weight on RTX 4090
    llm_context_size: int = 65536  # OpenAI-20B-NEOPlus IQ4_NL - 64k context window (synchronized with hardware capabilities)
    # Tuned to keep responses snappy while avoiding large GPU occupancy
    llm_max_tokens: int = 4096  # Increased generation length for long reasoning chains
    llm_temperature: float = 1.0  # Reka default: higher temperature to support novel reasoning
    llm_top_p: float = 0.95  # Nucleus sampling threshold tuned for Reka
    llm_timeout: int = 300  # Request timeout seconds
    # Offload all layers to GPU for maximum inference speed on RTX 4090
    llm_gpu_layers: int = -1  # Use -1 to pin all layers to GPU (where supported)
    llm_threads: int = 12  # CPU threads
    llm_concurrency: int = 4  # How many concurrent LLM calls to allow
    llm_local_embeddings: bool = True  # Load local model with embeddings enabled when used as fallback
    # Character->token heuristics for chunk sizing; tokens ~ 4 chars, useful for server token limits
    llm_chars_per_token: int = 4
    # Fraction of server context to use for chunking (e.g., 0.5 = use half of the model context for chunk size)
    llm_chunk_context_ratio: float = 0.5
    # Stop tokens configuration for Reka model
    llm_stop_tokens: Optional[list] = ["< sep >", ""]  # Added Reka stop token
    # Chat template configuration - specify which template to use for formatting conversations
    llm_chat_template: str = "openai"  # Changed from qwen3-thinking to openai for simpler, more predictable tool handling
    # Batch size optimizations for 16GB VRAM to maximize context window
    llm_batch_size: int = 1024  # Optimized to conserve VRAM/RAM during processing
    llm_ubatch_size: int = 1024 # Optimized to conserve VRAM/RAM during processing

    @property
    def resolved_chat_template(self) -> str:
        """Auto-detect chat template based on model name if set to 'auto'"""
        if self.llm_chat_template.lower() == 'auto':
            model_name = getattr(self, 'llm_model_name', '').lower()
            if 'gemma' in model_name:
                return 'gemma3'
            elif 'qwen' in model_name:
                return 'qwen3-thinking'
            elif 'llama' in model_name or 'llm' in model_name:
                # Llama models usually use standard chatml format
                return 'openai'  # Using openai format as standard for Llama
            elif 'moe' in model_name or '4x' in model_name or 'california' in model_name:
                # For your specific MoE model, use standard format since it's llama3.2 based
                return 'openai'  # Llama3.2 based models use standard chat format
            else:
                return 'openai'  # Default fallback
        return self.llm_chat_template
    # (duplicate 'llm_chars_per_token' removed)

    # ============================================================
    # LLAMA.CPP SERVER & EMBEDDING SERVER RUNTIME CONFIG
    # Path to the built llama-server executable (explicit path overrides auto-detection)
    llama_server_exe_path: Optional[str] = None
    # Default ports for API and embedding servers
    llama_server_default_port: int = 8080
    llama_embed_server_default_port: int = 8081
    # Allow interactive model selection via `select_model.py` if model not configured in settings
    llama_allow_select_model: bool = True
    # Additional server runtime tuning flags exposed for convenience
    llama_server_cont_batching: bool = True
    llama_server_flash_attn: str = "auto"
    llama_server_cache_type_k: str = "f16"
    llama_server_cache_type_v: str = "f16"
    llama_server_repeat_penalty: float = 1.1
    # llama.cpp server batch tuning
    # Batch tuning: set a larger logical batch size to allow batching while keeping micro batches small
    # Defaults tuned for RTX 4090 16GB VRAM: large logical batch, small ubatch to avoid VRAM spikes
    llama_server_batch_size: int = 2048  # logical max batch size (llama-server --batch-size)
    llama_server_ubatch_size: int = 2048  # physical ubatch size (llama-server --ubatch-size) - raised for RTX 4090 stability
    llama_server_parallel: int = 1  # number of parallel sequences/slots (llama-server --parallel)
    # Optional cap for UBATCH to avoid allocating more memory than GPU can handle
    llama_server_ubatch_max: Optional[int] = None
    # Optional: configure prompt cache ram in MiB; default 0 disables prompt cache (good for VRAM-constrained setups)
    llama_cache_ram: int = 0
    # Optional stop tokens to instruct the model to terminate completions
    llm_stop_tokens: Optional[list[str]] = None
    
    # ============================================================
    # MEMORY.PY - Tiered Memory Settings
    # ============================================================
    # Redis (Hot/Working Memory)
    redis_url: str = "redis://localhost:6379"
    redis_ttl: int = 3600  # Session TTL in seconds
    redis_max_tokens: int = 32000  # Max tokens in Redis before flush (increased from 16000 for larger buffer)
    
    # Memory thresholds
    max_context_tokens: int = 60000  # Max tokens in total context (synchronized with 64k hardware window, leaving 5k buffer for output)
    summarize_threshold: int = 48000  # Trigger summarization when Redis exceeds this (allowing much longer conversations before forcing rotation)
    
    # ============================================================
    # CONTEXT_MANAGER.PY - Context Assembly
    # ============================================================
    # Archivist settings (summarization)
    archivist_enabled: bool = True
    archivist_chunk_size: int = 8000  # Tokens per chunk for summarization (increased to preserve more detail with 64k context)
    archivist_overlap: int = 500  # Overlap between chunks (increased for better continuity with larger chunks)
    archivist_compression_ratio: float = 0.5  # Target 50% of original size (reduced aggressiveness from 0.3)
    
    # Context tiers
    context_recent_turns: int = 50  # Recent conversation turns to include (increased from 10 to support 50+ exchanges)
    context_summary_limit: int = 20  # Max historical summaries to include (increased from 8)
    context_entity_limit: int = 50  # Max entity-based memories (increased from 15)

    # Defaults for memory provenance & freshness
    memory_default_provenance_score: float = 0.5  # Default provenance when metadata is unknown (0.0-1.0)
    memory_default_freshness_score: float = 1.0  # Freshness score at creation (1.0 = brand new)
    # Distiller caching settings
    memory_distill_cache_enabled: bool = True
    memory_distill_cache_ttl: int = 86400  # in seconds

    # ============================================================
    # ARCHIVIST - Auto-Purge (Janitor) Settings
    # ============================================================
    # When enabled, Archivist will periodically scan for and optionally delete
    # contaminated memory nodes that match the configured markers.
    archivist_auto_purge_enabled: bool = False
    archivist_auto_purge_interval_seconds: int = 600  # default 10 minutes
    archivist_auto_purge_dry_run: bool = True  # default to dry-run to avoid accidental deletes
    # A set of markers used to detect contaminated nodes. Lower-cased.
    archivist_auto_purge_markers: list[str] = [
        "thinking_content",
        "combined_text",
        "prompt-logs",
        "prompt_logs",
        "calibration_run",
        "dry-run",
        "dry_run",
        "[planner]",
        "--- start of file:",
        "(anchor) ps ",
        "[info] http",
        '"thinking_content":'
    ]
    
    # ============================================================
    # QLEARNING_RETRIEVER.PY - Graph Retrieval
    # ============================================================
    qlearning_enabled: bool = True
    qlearning_learning_rate: float = 0.1
    qlearning_discount_factor: float = 0.9
    qlearning_epsilon: float = 0.3  # Exploration rate
    qlearning_max_hops: int = 3  # Graph traversal depth
    qlearning_max_paths: int = 5  # Max paths to explore
    qlearning_save_interval: int = 10  # Save Q-table every N queries
    qlearning_table_path: str = "./q_table.json"
    
    # ============================================================
    # EXTRACT_ENTITIES.PY - Entity Extraction
    # ============================================================
    entity_extraction_batch_size: int = 20  # Process N turns at a time
    entity_extraction_delay: float = 0.1  # Delay between LLM calls (rate limiting)
    entity_min_confidence: float = 0.5  # Min confidence for entity extraction
    entity_types: list[str] = ["PERSON", "CONCEPT", "PROJECT", "CONDITION", "SKILL"]  # Specify entity types explicitly
    
    # ============================================================
    # NEO4J - Knowledge Graph (Optional)
    # ============================================================
    neo4j_enabled: bool = True  # Enable Neo4j for memory storage and retrieval
    neo4j_uri: str = "bolt://localhost:7687"  # Neo4j connection URI
    neo4j_user: str = "neo4j"  # Neo4j username
    neo4j_password: str = os.getenv("NEO4J_PASSWORD", "password")  # Neo4j password from environment variable
    neo4j_max_connection_pool_size: int = 50  # Max connection pool size
    neo4j_connection_timeout: int = 30  # Connection timeout in seconds
    # ============================================================
    # NEO4J RECONNECT (resilience settings for critical DB errors)
    # If Neo4j has a critical error at startup, attempt to reconnect in background.
    neo4j_reconnect_enabled: bool = True
    neo4j_reconnect_initial_delay: int = 5  # seconds before first reconnect attempt
    neo4j_reconnect_max_attempts: int = 6  # attempts before stopping
    neo4j_reconnect_backoff_factor: float = 2.0  # exponential backoff multiplier

    # ============================================================
    # VECTOR DB (Optional)
    # ============================================================
    vector_enabled: bool = False  # Enable vector DB usage for semantic search
    vector_adapter_name: str = "redis"  # Name of adapter to use (redis, faiss, pinecone)
    vector_auto_embed: bool = False  # Autogenerate embeddings for new memories (via llm_client)
    
    # ============================================================
    # MAIN.PY - ECE Server
    # ============================================================
    ece_host: str = "127.0.0.1"
    ece_port: int = 8000
    ece_log_level: str = "INFO"
    # Optional full URL form for MCP server (e.g., http://localhost:8008)
    mcp_url: Optional[str] = None
    # Versioning
    # Detect the package version if installed; otherwise allow ECE_VERSION env var or fallback to 'dev'
    ece_version: str = os.getenv('ECE_VERSION', _pkg_version or 'dev')
    ece_cors_origins: list[str] = ["*"]  # CORS allowed origins
    
    # ============================================================
    # SECURITY - API Authentication
    # ============================================================
    ece_api_key: Optional[str] = None
    ece_require_auth: bool = False
    # ------------------------------------------------------------------
    # MCP Server configuration
    # ------------------------------------------------------------------
    mcp_enabled: bool = False
    mcp_host: str = "127.0.0.1"
    mcp_port: int = 8421
    # Optional: support YAML `server.host` / `server.port` semantics via SERVER_HOST / SERVER_PORT env vars
    server_host: Optional[str] = None
    server_port: Optional[int] = None
    # Per-protocol API key for MCP (if required). Fallback to ece_api_key if not set.
    mcp_api_key: Optional[str] = None
    
    # ============================================================
    # SECURITY - Audit Logging
    # ============================================================
    audit_log_enabled: bool = True
    audit_log_path: str = "./logs/audit.log"
    audit_log_tool_calls: bool = True
    audit_log_memory_access: bool = False
    
    # ============================================================
    # ANCHOR - CLI Client
    # ============================================================
    anchor_session_id: str = "anchor-session"
    anchor_timeout: int = 300
    
    # ============================================================
    # GRAPH_REASONER.PY - Graph-R1 Reasoning
    # ============================================================
    reasoning_max_iterations: int = 5  # Markovian thinking iterations
    reasoning_enabled: bool = True
    # ============================================================
    # TOOL EXECUTION - Maximum iterations when processing tool calls
    # This controls how many tool-execute/regenerate cycles are allowed
    # Default kept in sync with ToolExecutor default (3)
    # New recommended setting name: tool_max_iterations (for non-MCP generic use)
    tool_max_iterations: int = 3
    # Backward compatibility: keep the old MCP-prefixed name in place
    mcp_max_tool_iterations: int = 3
    
    # ============================================================
    # Computed Properties
    # ============================================================
    @property
    def archivist_max_summary_tokens(self) -> int:
        """Target summary size based on compression ratio"""
        return int(self.summarize_threshold * self.archivist_compression_ratio)
    
    @property
    def llm_model(self) -> str:
        """Model name for API calls (backward compatibility)"""
        return self.llm_model_name
    
    # ============================================================
    # Pydantic Config
    # ============================================================
    # Pydantic v2 model config: ignore extra environment variables to avoid validation errors
    model_config = ConfigDict(extra="ignore", env_file=".env", env_file_encoding="utf-8", case_sensitive=False)

    @model_validator(mode='after')
    def _post_init(self):
        """After validation, derive host/port settings from URL fields if present.

        This allows the YAML config to specify `mcp: { url: "http://localhost:8008" }` and
        `server: { host: "0.0.0.0", port: 8000 }` while preserving backward-compatible
        `mcp_host`/`mcp_port` and `ece_host`/`ece_port` fields.
        """
        # Parse mcp_url if present
        if getattr(self, 'mcp_url', None):
            try:
                parsed = urlparse(self.mcp_url)
                if parsed.hostname:
                    object.__setattr__(self, 'mcp_host', parsed.hostname)
                if parsed.port:
                    object.__setattr__(self, 'mcp_port', parsed.port)
            except Exception:
                pass

        # Apply server_host/server_port to ece_host/ece_port if provided
        if getattr(self, 'server_host', None):
            object.__setattr__(self, 'ece_host', self.server_host)
        if getattr(self, 'server_port', None):
            try:
                object.__setattr__(self, 'ece_port', int(self.server_port))
            except Exception:
                pass

        # Parse possible stringified list for llm_stop_tokens from environment/YAML fallback.
        try:
            val = getattr(self, 'llm_stop_tokens', None)
            if isinstance(val, str):
                s = val.strip()
                if s.startswith('[') and s.endswith(']'):
                    try:
                        # Convert single quotes to double quotes if needed, then parse JSON
                        j = s.replace("'", '"')
                        parsed = json.loads(j)
                        if isinstance(parsed, list):
                            object.__setattr__(self, 'llm_stop_tokens', parsed)
                    except Exception:
                        # Fall through to comma-splitting
                        items = [i.strip() for i in s.strip('[]').split(',') if i.strip()]
                        object.__setattr__(self, 'llm_stop_tokens', items)
                else:
                    # Comma-separated values
                    items = [i.strip() for i in s.split(',') if i.strip()]
                    object.__setattr__(self, 'llm_stop_tokens', items)
        except Exception:
            pass

        return self

    # ============================================================
    # MEMORY WEAVER (AUTONOMOUS REPAIR)
    # ============================================================
    weaver_enabled: bool = True
    # We recommend enabling real commits after a short observation period
    # for production readiness; default to enabled for Sovereign Brain mode.
    weaver_dry_run_default: bool = False
    weaver_threshold: float = 0.55
    weaver_delta: float = 0.05
    weaver_time_window_hours: int = 24
    weaver_max_commit: int = 50
    weaver_prefer_same_app: bool = True
    weaver_commit_enabled: bool = True
    # When present, nodes containing this tag in m.tags will be excluded from weaver runs
    weaver_exclude_tag: Optional[str] = '#corrupted'
    # Defaults related to weaver and repair scripts
    weaver_candidate_limit: int = 200  # candidate limit per summary in repair runs
    weaver_batch_size_default: int = 2  # default batch size used by repair/weaver when not overridden
    # Backwards compatible alias and convenience env name (WEAVER_BATCH_SIZE)
    # If set, this value takes precedence over `weaver_batch_size_default`.
    weaver_batch_size: int | None = None
    # Sleep seconds to wait between batches for safe GPU breathing room
    weaver_sleep_between_batches: float = 1.0

    # Matrix / Factory Configuration - spawn worker processes for heavy async tasks
    matrix_worker_count: int = 8  # Number of worker processes for Matrix/Weaver tasks

# Global settings instance
def _load_config_fallbacks() -> None:
    """Load configs from the recommended `configs/` directory and .env files.

    Behavior:
    - Loads configs/config.yaml if present, flattening keys into environment variables
      so that Pydantic BaseSettings picks them up.`
    - Loads .env from `configs/.env` if present, else root `.env`.
    - Does not override existing environment variables.
    """
    repo_root = Path(__file__).resolve().parents[1]
    configs_dir = repo_root / "configs"
    # 1) Load .env file from config dir if present (otherwise root .env). Don't override existing env vars.
    candidate_envs = [configs_dir / ".env", repo_root / ".env"]
    for envf in candidate_envs:
        if envf.exists():
            try:
                load_dotenv(dotenv_path=str(envf), override=False)
            except Exception:
                pass
            break
    # 2) Load YAML defaults and set environment variables for missing keys
    if yaml is None:
        # PyYAML not installed; skip YAML defaults. If a YAML config exists, print a helpful hint.
        repo_root = Path(__file__).resolve().parents[1]
        configs_dir = repo_root / "configs"
        candidates = [configs_dir / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
        config_yaml = next((p for p in candidates if p.exists()), None)
        if config_yaml is not None:
            try:
                print(f"⚠️  PyYAML is not installed; found YAML config at {config_yaml}. Install PyYAML (pip install PyYAML) to load YAML-based defaults.")
            except Exception:
                # If printing fails in constrained env, just pass
                pass
        return
    # Support multiple locations for config.yaml to preserve compatibility with existing scripts
    candidates = [configs_dir / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
    config_yaml = next((p for p in candidates if p.exists()), None)
    if config_yaml is not None:
        try:
            raw = yaml.safe_load(config_yaml.read_text()) or {}
        except Exception:
            raw = {}
        def _flatten(cfg: dict, prefix: str = None):
            for k, v in (cfg or {}).items():
                key = f"{(prefix + '_') if prefix else ''}{k}".upper()
                if isinstance(v, dict):
                    yield from _flatten(v, key)
                else:
                    yield key, v
        for ek, ev in _flatten(raw):
            # Only set env var if not already present
            if os.environ.get(ek) is None and ev is not None:
                os.environ[ek] = str(ev)

_load_config_fallbacks()

try:
    settings = Settings()
except Exception:
    # During tests or in constrained environments, extra env vars can cause validation errors.
    # Fall back to a constructed default settings object to avoid blocking collection.
    settings = Settings.construct()

# Legacy exports for backward compatibility


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\config.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\config_loader.py (Section: BACKEND_PYTHON) ---

"""
Configuration Loader for ECE_Core and Anchor

Loads configuration from YAML files with environment variable substitution.
Provides typed configuration objects with validation.
"""
try:
    import yaml
except Exception:
    yaml = None
import os
import re
import logging
from pathlib import Path
from src.utils.config_finder import find_config_path
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field, validator

logger = logging.getLogger(__name__)


class ConfigLoader:
    """
    Configuration loader with environment variable substitution.
    
    Supports ${ENV_VAR} syntax in YAML files.
    Example: password: "${NEO4J_PASSWORD}"
    """
    
    def __init__(self, config_path: Optional[Path] = None):
        """
        Initialize config loader.
        
        Args:
            config_path: Path to config.yaml. If None, looks in current directory.
        """
        if config_path is None:
            # Ask the config_finder for the canonical location if present
            candidate = find_config_path()
            if candidate:
                config_path = Path(candidate)
            else:
                # Look for a default in the ece-core directory for backward compatibility
                config_path = Path(__file__).parent.parent / "config.yaml"
        
        self.config_path = Path(config_path)
        self._config: Optional[Dict[str, Any]] = None
    
    def load(self) -> Dict[str, Any]:
        """
        Load configuration from YAML file.
        
        Returns:
            Dictionary with configuration
        """
        if not self.config_path.exists():
            logger.warning(f"Config file not found: {self.config_path}")
            logger.warning("Using default configuration")
            return {}
        
        try:
            if yaml is None:
                logger.warning("PyYAML not installed; skipping YAML-based config loading")
                return {}
            with open(self.config_path, 'r') as f:
                content = f.read()
            
            # Substitute environment variables
            content = self._substitute_env_vars(content)
            
            # Parse YAML
            config = yaml.safe_load(content)
            
            logger.info(f"Loaded configuration from {self.config_path}")
            self._config = config
            return config
            
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return {}
    
    def _substitute_env_vars(self, content: str) -> str:
        """
        Substitute ${ENV_VAR} patterns with environment variable values.
        
        Args:
            content: YAML content with potential ${VAR} patterns
            
        Returns:
            Content with substitutions made
        """
        def replace_env(match):
            env_var = match.group(1)
            value = os.getenv(env_var)
            
            if value is None:
                # Check for default value: ${VAR:-default}
                if ':-' in env_var:
                    var_name, default = env_var.split(':-', 1)
                    value = os.getenv(var_name, default)
                else:
                    logger.warning(f"Environment variable {env_var} not set")
                    # Keep placeholder for optional values
                    return match.group(0)
            
            return value
        
        # Replace ${VAR} and ${VAR:-default}
        pattern = re.compile(r'\$\{([^}]+)\}')
        return pattern.sub(replace_env, content)
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        Get configuration value by dot-notation key.
        
        Args:
            key: Dot-notation key (e.g., "server.port")
            default: Default value if key not found
            
        Returns:
            Configuration value or default
        """
        if self._config is None:
            self.load()
        
        if self._config is None:
            return default
        
        # Navigate nested dict with dot notation
        parts = key.split('.')
        value = self._config
        
        for part in parts:
            if isinstance(value, dict) and part in value:
                value = value[part]
            else:
                return default
        
        return value
    
    def reload(self) -> Dict[str, Any]:
        """
        Reload configuration from file.
        
        Returns:
            Updated configuration dictionary
        """
        return self.load()
    
    def print_config(self, hide_secrets: bool = True):
        """
        Print current configuration (for debugging).
        
        Args:
            hide_secrets: Whether to hide password fields
        """
        if self._config is None:
            self.load()
        
        config = self._config.copy() if self._config else {}
        
        if hide_secrets:
            # Redact sensitive fields
            config = self._redact_secrets(config)
        
        print("=" * 60)
        print("Current Configuration:")
        print("=" * 60)
        print(yaml.dump(config, default_flow_style=False, sort_keys=False))
        print("=" * 60)
    
    def _redact_secrets(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Redact sensitive configuration values.
        
        Args:
            config: Configuration dictionary
            
        Returns:
            Config with secrets redacted
        """
        redacted = {}
        secret_keys = {'password', 'secret', 'token', 'key', 'api_key'}
        
        for key, value in config.items():
            if isinstance(value, dict):
                redacted[key] = self._redact_secrets(value)
            elif any(secret in key.lower() for secret in secret_keys):
                redacted[key] = "***REDACTED***"
            else:
                redacted[key] = value
        
        return redacted


# Global config loader instance
_loader: Optional[ConfigLoader] = None


def get_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    Get configuration (singleton pattern).
    
    Args:
        config_path: Optional path to config file
        
    Returns:
        Configuration dictionary
    """
    global _loader
    
    if _loader is None:
        _loader = ConfigLoader(config_path)
        _loader.load()
    
    return _loader._config or {}


def reload_config() -> Dict[str, Any]:
    """
    Reload configuration from file.
    
    Returns:
        Updated configuration dictionary
    """
    global _loader
    
    if _loader is None:
        return get_config()
    
    return _loader.reload()


def get_value(key: str, default: Any = None) -> Any:
    """
    Get configuration value by key.
    
    Args:
        key: Dot-notation key (e.g., "server.port")
        default: Default value if not found
        
    Returns:
        Configuration value
    """
    global _loader
    
    if _loader is None:
        get_config()
    
    return _loader.get(key, default) if _loader else default


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\config_loader.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\content_utils.py (Section: BACKEND_PYTHON) ---

import html
import json
import re

EMOJI_REGEX = re.compile(
    "[\U0001F300-\U0001F6FF\U0001F900-\U0001F9FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]",
    flags=re.UNICODE,
)

JSON_LIKE_PATTERNS = [re.compile(p) for p in [r"\{\s*\".*\"\s*:\s*", r"\[\s*\{", r'"response_content"', r'"timestamp"']]
HTML_LIKE_PATTERNS = [re.compile(p) for p in [r'<\s*\/?\w+[^>]*>', r'<a\s+href=', r'<script\b', r'<div\b', r'<p\b']]

SPAM_KEYWORDS = ['erotik', 'click here', 'buy now', 'free', 'cheap', 'subscribe now']

TECHNICAL_KEYWORDS = ['error', 'exception', 'traceback', 'sudo', 'apt-get', 'npm', 'pip', 'docker', 'cargo', 'journal', 'systemd', 'kernel', 'trace', 'failed', 'stacktrace']


def is_json_like(text: str) -> bool:
    if not text:
        return False
    for p in JSON_LIKE_PATTERNS:
        if p.search(text):
            return True
    return False


def is_html_like(text: str) -> bool:
    if not text:
        return False
    for p in HTML_LIKE_PATTERNS:
        if p.search(text):
            return True
    return False


def remove_html_tags(text: str) -> str:
    return re.sub(r'<[^>]+>', ' ', text)


def strip_emojis(text: str) -> str:
    return EMOJI_REGEX.sub('', text)


def extract_text_from_json(content: str) -> str:
    try:
        obj = json.loads(content)
        if isinstance(obj, dict):
            for k in ('response_content', 'content', 'text', 'message', 'response'):
                if k in obj and isinstance(obj[k], str):
                    return obj[k]
            values = []
            for v in obj.values():
                if isinstance(v, str):
                    values.append(v)
            return ' '.join(values)
        if isinstance(obj, list):
            texts = []
            for el in obj:
                if isinstance(el, dict):
                    for k in ('response_content', 'content', 'text'):
                        if k in el and isinstance(el[k], str):
                            texts.append(el[k])
                elif isinstance(el, str):
                    texts.append(el)
            return ' '.join(texts)
    except Exception:
        return content
    return content


# ---------------------- Technical Normalization ----------------------
ANSI_ESCAPE_RE = re.compile(r"\x1b\[[0-9;]*[A-Za-z]")
WINDOWS_PATH_RE = re.compile(r"[A-Za-z]:\\\\")
UNIX_PATH_RE = re.compile(r"/(?:[\w\-\.@]+/)+[\w\-\.@]+")
HEXDUMP_RE = re.compile(r"(?:0x[0-9a-fA-F]{2,}|[0-9A-Fa-f]{2,}(?:\s+[0-9A-Fa-f]{2,}){4,})")


def contains_ansi_codes(text: str) -> bool:
    return bool(ANSI_ESCAPE_RE.search(text))


def contains_windows_path(text: str) -> bool:
    m = re.search(r"[A-Za-z]:", text)
    if not m:
        return False
    idx = m.end()
    if idx < len(text) and text[idx] in ('\\', '/'):
        return True
    return False


def contains_unix_path(text: str) -> bool:
    if '/usr/' in text or '/bin/' in text or '/home/' in text:
        return True
    return bool(UNIX_PATH_RE.search(text))


def contains_hex_dump(text: str) -> bool:
    return bool(HEXDUMP_RE.search(text))


def normalize_technical_content(text: str) -> str:
    """Normalize technical content by removing/annotating noisy artifacts while preserving semantic metadata.
    The function detects ANSI/color codes, OS paths, and hex dumps and inserts human-readable annotations
    such as [Context: Terminal Output], [OS: Linux], [OS: Windows], and [Binary Data Omitted].
    """
    if not text:
        return ''
    tags = []
    t = str(text)
    # ANSI sequences
    if contains_ansi_codes(t):
        t = ANSI_ESCAPE_RE.sub(' ', t)
        tags.append('[Context: Terminal Output]')
    # Windows paths
    if contains_windows_path(t):
        tags.append('[OS: Windows]')
        t = WINDOWS_PATH_RE.sub(lambda m: (m.group(0)[:60] + '...' if len(m.group(0)) > 70 else m.group(0)), t)
    # Unix paths
    if contains_unix_path(t):
        tags.append('[OS: Linux]')
        t = UNIX_PATH_RE.sub(lambda m: (m.group(0)[:80] + '...' if len(m.group(0)) > 80 else m.group(0)), t)
    # Hex dump / binary-like sequences
    if contains_hex_dump(t):
        tags.append('[Binary Data Omitted]')
        t = HEXDUMP_RE.sub('[binary_data]', t)
    # If there are HTML-like artifacts but we want a log context, annotate with [Context: HTML]
    if is_html_like(t):
        tags.append('[Context: HTML]')
        t = remove_html_tags(t)
    # Strip control characters
    t = re.sub(r'\x00|\x07|\x0b|\x0c', ' ', t)
    t = re.sub(r'\s+', ' ', html.unescape(t)).strip()
    if tags:
        annotation = ' '.join(sorted(set(tags))) + ' '
        t = annotation + t
    return t

# ---------------------- End Technical Normalization ----------------------


def clean_content(text: str, remove_emojis: bool = True, remove_non_ascii: bool = False, annotate_technical: bool = False) -> str:
    if not text:
        return ''
    t = text.strip()
    # If requested, apply technical normalization which will remove/annotate ANSI codes, paths, and hex dumps
    if annotate_technical:
        t = normalize_technical_content(t)
    if t.startswith('{') or t.startswith('[') or '"response_content"' in t:
        t2 = extract_text_from_json(t)
        if isinstance(t2, str) and t2:
            t = t2
    t = remove_html_tags(t)
    t = html.unescape(t)
    if remove_emojis:
        t = strip_emojis(t)
    if remove_non_ascii:
        t = ''.join([c for c in t if ord(c) < 128])
    t = re.sub(r'[^\w\s\.,;:\-\'"@#%\(\)\?\/\\]+', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def has_technical_signal(text: str) -> bool:
    """Detect strings that indicate the text is technical/log-like and should be preserved.
    This will detect shell prompts, package managers, version numbers, file paths, and error markers.
    """
    if not text:
        return False
    t = text.lower()
    # Quick patterns
    if 'sudo' in t or 'apt-get' in t or 'npm ' in t or 'pip ' in t or 'docker ' in t or 'cargo ' in t:
        return True
    # Version numbers
    if re.search(r'v\d+\.\d+(?:\.\d+)?', text):
        return True
    # File paths
    if re.search(r'\/\w+\/\w+', text) or re.search(r'[A-Za-z]:\\\\', text):
        return True
        # Allow square brackets so annotation tokens like [Context: Terminal Output] are preserved
        t = re.sub(r'[^\w\s\.,;:\-\'"@#%\(\)\[\]\?/\\]+', ' ', t)
    for k in TECHNICAL_KEYWORDS:
        if k in t:
            return True
    # Shell-like prompts or stack traces
    if re.search(r'\b(error|exception|traceback|failed)\b', t):
        return True
    if re.search(r'\b[\$#] ', text):
        return True
    return False


def is_token_soup(text: str, *, min_tokens: int = 3) -> bool:
    """Detect whether a piece of text is likely corrupted/garbled (token soup).

    Heuristics used:
    - High fraction of tokens that contain code-like characters (parentheses, ';', '{', '}', '->', etc.)
    - High fraction of tokens that are hexadecimal strings or long digit-only sequences
    - High fraction of tokens that are one-letter (indicative of binary tokens or low-quality text)
    - Low fraction of alphabetic words (few dictionary-like words)
    - Large runs of punctuation or non-letter characters
    These heuristics are intentionally conservative — we prefer to flag likely garbage and
    leave marginal cases untouched.
    """
    if not text or not text.strip():
        return False
    s = text.strip()
    # Quick filters
    if len(s) < 40:
        # short content unlikely to be long corrupted token soup
        return False
    tokens = re.findall(r"\S+", s)
    if len(tokens) < min_tokens:
        return False
    total = len(tokens)
    code_like = 0
    hex_like = 0
    one_letter = 0
    no_vowel = 0
    alpha_like = 0
    long_token = 0
    for t in tokens:
        if len(t) == 1:
            one_letter += 1
        if re.search(r'[(){}\[\]<>;=:\\|/\\\\@#%\$]', t):
            code_like += 1
        if re.fullmatch(r'0x[0-9a-fA-F]{8,}', t) or re.fullmatch(r'[A-Fa-f0-9]{16,}', t):
            hex_like += 1
        if re.search(r'[0-9]', t) and re.search(r'[A-Za-z]', t) is None and len(t) >= 8:
            hex_like += 1
        if len(t) >= 6 and not re.search(r'[aeiouAEIOU]', t):
            no_vowel += 1
        if re.fullmatch(r'[A-Za-z]+', t):
            alpha_like += 1
        if len(t) > 30:
            long_token += 1
    # Ratios
    code_like_ratio = code_like / total
    hex_ratio = hex_like / total
    one_letter_ratio = one_letter / total
    no_vowel_ratio = no_vowel / total
    alpha_ratio = alpha_like / total
    long_token_ratio = long_token / total
    # If we see many code-like tokens or hex-like tokens, it's probably corrupted
    if code_like_ratio > 0.25 or hex_ratio > 0.05 or one_letter_ratio > 0.25:
        return True
    # If very few alphabetic words and many tokens are long & vowel-free, flag
    if alpha_ratio < 0.25 and (no_vowel_ratio > 0.2 or long_token_ratio > 0.05):
        return True
    # If we see run of excessive punctuation
    punct_runs = re.findall(r'[^\w\s]{6,}', s)
    if len(punct_runs) > 0:
        return True
    return False


def sanitize_token_soup(text: str) -> str:
    """Return a sanitized version of a token-soup string. We aim to preserve any readable text
    but remove or collapse obvious code fragments, long hex/ids, and JSON-like containers.
    This is a best-effort function — it can't recover content that has been transformed beyond
    recognition, but it will remove obvious artifacts so downstream summarizers see cleaner input.
    """
    if not text:
        return ''
    t = text
    # Remove fenced code blocks
    t = re.sub(r'```.*?```', ' ', t, flags=re.DOTALL)
    # Remove common code patterns: function calls with arguments, memory copies
    t = re.sub(r'\b[A-Za-z_][A-Za-z0-9_]*\([^\)]*\)', ' ', t)
    # Remove long hex sequences
    t = re.sub(r'0x[0-9a-fA-F]{6,}', ' ', t)
    t = re.sub(r'\b[A-Fa-f0-9]{16,}\b', ' ', t)
    # Remove JSON-like structural content when large
    if len(t) > 200 and (t.strip().startswith('{') or t.strip().startswith('[')):
        t = extract_text_from_json(t)
    # Collapse multiple punctuation and whitespace
    t = re.sub(r'[<>\|\\]{1,}', ' ', t)
    t = re.sub(r'[^\w\s\.,;:\-\'"\(\)]+', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    # Truncate to a sensible length - we don't want to produce overly long sanitized strings
    if len(t) > 500:
        t = t[:500] + '...'
    return t


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\content_utils.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\context.py (Section: BACKEND_PYTHON) ---

"""Context Manager: Assembles context and manages overflow."""
import logging
from typing import Optional
from datetime import datetime, timezone
from src.memory import TieredMemory
from src.llm import LLMClient
from src.distiller import Distiller
from src.intelligent_chunker import IntelligentChunker
from src.config import settings

logger = logging.getLogger(__name__)

class ContextManager:
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        self.distiller = Distiller(llm)  # Context quality gate & extractor
        self.chunker = IntelligentChunker(llm)  # Large input processor
    
    async def build_context(self, session_id: str, user_input: str) -> str:
        """
        Build context from memory tiers with Archivist filtering.
        
    Tiers:
    1. Relevant memories (Neo4j) - Long-term memories relevant to the query
    2. Summaries (Neo4j) - Long-term compressed context
        3. Active (Redis) - Short-term recent conversation
        
        The Archivist filters and consolidates retrieved context to prevent bloat.
        If user_input is very large, IntelligentChunker processes it first.
        """
        # DEBUG: Log build_context entry
        logger.debug(f"=== build_context START for session {session_id} ===")
        logger.debug(f"User input length: {len(user_input)} chars")
        # logger.debug(f"User input: {user_input[:200]}...")
        
        # **NEW**: If user input is large, process it intelligently
        if len(user_input) > 4000:
            logger.info(f"Large input detected ({len(user_input):,} chars), processing with IntelligentChunker...")
            user_input_processed = await self.chunker.process_large_input(
                user_input=user_input,
                query_context=""  # First pass, no context yet
            )
            logger.info(f"Input compressed: {len(user_input):,} â†’ {len(user_input_processed):,} chars")
        else:
            user_input_processed = user_input
        
        # 1. Retrieve relevant long-term memories
        logger.debug("Retrieving relevant memories...")
        relevant_memories = await self._retrieve_relevant_memories(user_input_processed, limit=10)
        logger.debug(f"Retrieved {len(relevant_memories)} relevant memories")
        # logger.debug(f"Memories: {relevant_memories}")
        
        # Get summaries from Neo4j
        logger.debug("Retrieving summaries...")
        summaries = await self.memory.get_summaries(session_id, limit=8)
        logger.debug(f"Retrieved {len(summaries)} summaries")
        # logger.debug(f"Summaries: {summaries}")
        
        # 3. Get recent conversation
        logger.debug("Retrieving active context...")
        active_context = await self.memory.get_active_context(session_id)
        logger.debug(f"Active context length: {len(active_context)} chars")
        # logger.debug(f"Active context: {active_context[:200]}...")
        
        # 4. DISTILLER: Filter and format
        logger.debug("Running Distiller filter...")
        filtered = await self.distiller.filter_and_consolidate(
            query=user_input_processed,
            memories=relevant_memories,
            summaries=summaries,
            active_context=active_context
        )
        logger.debug(f"Distiller returned filtered context")
        # logger.debug(f"Filtered: {filtered}")
        
        # 5. Build final context from filtered results
        parts = []

        # A. Current datetime
        current_dt = datetime.now(timezone.utc)
        formatted_dt = current_dt.strftime("%B %d, %Y at %H:%M:%S UTC")
        parts.append(f"**Current Date & Time:** {formatted_dt}\n<current_datetime>{current_dt.isoformat()}</current_datetime>")

        # B. Recent conversation (This Session)
        # Keeping this early provides continuity
        if filtered["active_context"]:
            recent_turns = "\n".join(filtered["active_context"].split("\n")[-100:])  # Preserve more turns (from 40 to 100)
            logger.debug(f"Adding current conversation ({len(recent_turns)} chars)")
            parts.append(f"# Current Conversation (This Session):\n{recent_turns}")

        # C. Historical Summaries (Moved UP)
        if filtered.get("summaries"):
            hist_parts = []
            for s in filtered['summaries']:
                try:
                    import xml.sax.saxutils as saxutils
                    mp = f'<memory id="" source="neo4j" status="verified" date="">{saxutils.escape(str(s))}</memory>'
                    hist_parts.append(mp)
                except Exception:
                    continue
            parts.append('<historical_summaries>\n' + '\n'.join(hist_parts) + '\n</historical_summaries>')

        # D. Relevant Memories / RAG (Moved UP)
        # This acts as the "background knowledge" section
        if filtered.get("relevant_memories"):
            mem_parts = []
            for mem in filtered['relevant_memories']:
                try:
                    mid = mem.get('id') or mem.get('memory_id') or ''
                    meta = mem.get('metadata') or {}
                    if isinstance(meta, str):
                        import json as _json
                        try:
                            meta = _json.loads(meta)
                        except Exception:
                            meta = {}
                    src = meta.get('source') or mem.get('source') or 'neo4j'
                    status = meta.get('status') or 'unverified'
                    date = mem.get('timestamp') or meta.get('created_at') or ''
                    content = mem.get('content') or ''

                    import xml.sax.saxutils as saxutils
                    esc_content = saxutils.escape(str(content))
                    mp = f'<memory id="{mid}" source="{saxutils.escape(str(src))}" status="{saxutils.escape(str(status))}" date="{saxutils.escape(str(date))}">{esc_content}</memory>'
                    mem_parts.append(mp)
                except Exception:
                    continue
            parts.append("<retrieved_memory>\n" + "\n".join(mem_parts) + "\n</retrieved_memory>")

        # NEW: Context Rotation Protocol to maintain optimal window size for 64k limits
        # Check if active context is getting too large for efficient processing within 64k window
        active_context = filtered["active_context"]
        MAX_CONTEXT_BEFORE_ROTATION = 55000  # Leave buffer for new content and system prompt
        CONTEXT_GIST_THRESHOLD = 25000      # Size at which we consider rotating old context

        if len(active_context) > CONTEXT_GIST_THRESHOLD:
            logger.info(f"Active context ({len(active_context)} chars) exceeds threshold ({CONTEXT_GIST_THRESHOLD}), initiating rotation...")

            # Calculate safe rotation point that preserves recent context
            rotation_point = max(len(active_context) // 2, len(active_context) - 20000)  # Don't cut too aggressively
            old_portion = active_context[:rotation_point]
            recent_portion = active_context[rotation_point:]

            # Create a summary of the old portion using distillation
            try:
                # Distill the old conversation into a compact, meaningful summary
                gist_summary = await self.distiller.make_compact_summary(
                    memories=[{"content": old_portion}],
                    summaries=[],
                    active_context="",
                    new_input=""
                )

                if gist_summary and len(gist_summary.strip()) > 0:
                    logger.debug(f"Created gist summary ({len(gist_summary)} chars) from {len(old_portion)} chars of old context")

                    # Create "gist memory" in Neo4j for long-term reference with proper metadata
                    gist_memory_id = await self.memory.add_memory(
                        session_id=session_id,
                        content=gist_summary,
                        category="context_gist",
                        importance=8,  # High importance for context continuity
                        tags=["gist", "summary", "historical", "context_rotation"],
                        metadata={
                            "original_context_length": len(old_portion),
                            "type": "context_rotation_gist",
                            "rotation_point": rotation_point,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                    )
                    logger.info(f"Stored context gist in memory with ID: {gist_memory_id}")

                    # Reconstruct context with gist + recent context
                    rotated_context = f"## Prior Context Summary:\n{gist_summary}\n\n## Recent Conversation:\n{recent_portion}"

                    # Update the memory with the rotated context
                    await self.memory.save_active_context(session_id, rotated_context)

                    # Update filtered active_context to reflect the rotated version
                    active_context = rotated_context
                    filtered["active_context"] = rotated_context

                    logger.info(f"Context successfully rotated: {len(active_context)} chars total")
                else:
                    logger.debug("Gist summary was empty, skipping context rotation")
            except Exception as e:
                logger.warning(f"Failed to create gist during context rotation: {e}")
                # Fall back to basic trimming if distillation fails
                if len(active_context) > MAX_CONTEXT_BEFORE_ROTATION:
                    # Simply trim to keep only the most recent portion that fits
                    trimmed_portion = active_context[-MAX_CONTEXT_BEFORE_ROTATION:]
                    await self.memory.save_active_context(session_id, trimmed_portion)
                    active_context = trimmed_portion
                    filtered["active_context"] = trimmed_portion
                    logger.info(f"Fallback: Trimmed context to {len(active_context)} chars")

        # Optional: Append a distiller compact summary to the active context for future turns
        try:
            compact_summary = await self.distiller.make_compact_summary(relevant_memories, summaries, active_context, user_input_processed)
            if compact_summary and len(compact_summary.strip()) > 0:
                # Append only if not already present in the active_context
                if compact_summary.strip() not in (active_context or "")[-1200:]:
                    new_active = (active_context or "") + "\n" + compact_summary
                    await self.memory.save_active_context(session_id, new_active)
                    # Update filtered active_context to include compact summary so prompt contains it
                    filtered["active_context"] = (filtered.get("active_context") or "") + "\n" + compact_summary
        except Exception as e:
            logger.debug(f"Failed to append distiller summary to active context: {e}")

        # E. The User Prompt (Moved to LAST)
        # This ensures the model sees your actual command as the immediate task to perform
        parts.append(f"# What the User Just Said:\n{user_input}")

        return "\n\n".join(parts)
    
    async def _retrieve_relevant_memories(self, query: str, limit: int = 15) -> list:
        """
        ENHANCED: Hybrid memory retrieval (Vector + Full-Text).
        
        1. Vector Search (Semantic) - Finds conceptually related memories
        2. Full-text search (Lexical) - Finds exact keyword matches
        3. Fallback to recent - If no matches found
        """
        # DEBUG: Log memory retrieval
        logger.debug(f"=== _retrieve_relevant_memories START ===")
        logger.debug(f"Query: {query[:100]}...")
        
        memories = []
        seen_ids = set()

        # Strategy 1: Vector Search (Semantic)
        if self.memory.vector_adapter and getattr(self.memory, "_vector_enabled", False):
            try:
                logger.debug("Attempting vector search...")
                # Generate embedding
                embeddings = await self.llm.get_embeddings(query)
                if embeddings and len(embeddings) > 0:
                    embedding = embeddings[0] if isinstance(embeddings[0], list) else embeddings
                    vector_results = await self.memory.vector_adapter.query_vector(embedding, top_k=limit)
                    
                    for res in vector_results:
                        # Convert vector result to memory dict format
                        mem = {
                            "id": res.get("node_id"),
                            "memory_id": res.get("node_id"),
                            "content": res.get("metadata", {}).get("content"),
                            "category": res.get("metadata", {}).get("category"),
                            "importance": res.get("metadata", {}).get("importance", 5),
                            "score": res.get("score"),
                            "source": "vector",
                            "metadata": res.get("metadata", {})
                        }
                        # Filter memory items by provenance / origin rules
                        if not self._memory_is_allowed(mem):
                            continue
                        if mem["id"] and mem["id"] not in seen_ids:
                            memories.append(mem)
                            seen_ids.add(mem["id"])
                    logger.debug(f"Vector search returned {len(vector_results)} results")
            except Exception as e:
                logger.warning(f"Vector search failed: {e}")

        # Strategy 2: Full-text search on actual content
        # Extract key terms (words longer than 3 chars)
        words = query.lower().split()
        keywords = [w.strip('.,!?;:()[]{}') for w in words if len(w) > 3]
        
        # Try each significant keyword with full-text search
        lexical_memories = []
        for keyword in keywords[:5]:  # Top 5 keywords
            results = await self.memory.search_memories_neo4j(
                query_text=keyword,
                limit=limit
            )
            lexical_memories.extend(results)
        
            for m in lexical_memories:
                # ensure metadata is loaded as a dict
                if isinstance(m.get('metadata'), str):
                    try:
                        import json as _json
                        m['metadata'] = _json.loads(m['metadata'])
                    except Exception:
                        m['metadata'] = {}
                m["source"] = "lexical"
                if not self._memory_is_allowed(m):
                    continue
                if m['id'] not in seen_ids:
                    seen_ids.add(m['id'])
                    memories.append(m)
        
        # If we found memories, re-rank/sort them
        if memories:
            # Simple scoring: Vector score (if present) vs Lexical score (if present)
            def score_mem(m: dict):
                base_score = float(m.get('score', 0) or 0)
                imp = float(m.get('importance', 5)) / 10.0
                return base_score + imp * 0.2

            scored_sorted = sorted(memories, key=score_mem, reverse=True)
            return scored_sorted[:limit]
        
        # Strategy 3: Retrieve ContextGist memories that may contain historical context
        # These are compressed summaries of old context that was rotated out
        try:
            logger.debug("Checking for relevant ContextGist memories...")
            context_gist_query = f"context gist {query}" if len(query.split()) < 10 else query
            gist_memories = await self.memory.search_memories_neo4j(
                query_text=context_gist_query,
                category="context_gist",
                limit=5
            )

            for gist in gist_memories:
                # ensure metadata is loaded as a dict
                if isinstance(gist.get('metadata'), str):
                    try:
                        import json as _json
                        gist['metadata'] = _json.loads(gist['metadata'])
                    except Exception:
                        gist['metadata'] = {}
                gist["source"] = "context_gist"
                if not self._memory_is_allowed(gist):
                    continue
                if gist['id'] not in seen_ids:
                    seen_ids.add(gist['id'])
                    memories.append(gist)
                    logger.debug(f"Included ContextGist memory: {gist['id']}")
        except Exception as e:
            logger.warning(f"Failed to retrieve ContextGist memories: {e}")

        # Strategy 4: Recent memories (Fallback)
        if not memories:
            logger.debug("No matches found, falling back to recent memories")
            all_recent = []
            for category in ['event', 'idea', 'task', 'person', 'code', 'general', 'genesis', 'context_gist']:
                recent = await self.memory.get_recent_by_category(category, limit=3)
                all_recent.extend(recent)

            # Sort by importance and recency
            memories = sorted(
                all_recent,
                key=lambda x: (x.get('importance', 0), x.get('created_at', '')),
                reverse=True
            )[:limit]

        # Filter items through same provenance filter
        filtered_memories = []
        for m in memories:
            if self._memory_is_allowed(m):
                filtered_memories.append(m)

        return filtered_memories

    def _memory_is_allowed(self, mem: dict) -> bool:
        """Return True if a memory should be used for injection into prompts.

        Rules:
        - Allow if metadata.status == 'committed'
        - Allow if metadata.app_id exists and metadata.source is not a blacklisted file-based generator
        - Block if content or metadata indicate 'thinking_content' or '[PLANNER]' or 'dry-run'
        - Block if metadata.source indicates combined_text/prompt_logs or simple dev file sources
        """
        try:
            # Conservative defaults
            md = mem.get('metadata') or {}
            if isinstance(md, str):
                import json as _json
                try:
                    md = _json.loads(md)
                except Exception:
                    md = {}
            # If explicit status exists and it's committed, allow
            if md.get('status') == 'committed':
                return True
            # If explicit status exists and not committed, block
            if md.get('status') and md.get('status') != 'committed':
                return False
            # Block obvious dev/test file sources
            src = (md.get('source') or md.get('path') or '')
            if isinstance(src, str):
                lc = src.lower()
                blacklisted = ['combined_text', 'combined_text2', 'prompt-logs', 'prompt_logs', 'calibration_run', 'dry-run', 'dry_run', 'tests/', 'weaver']
                for b in blacklisted:
                    if b in lc:
                        return False
            # Block content and metadata containing 'thinking_content' or planner markers
            cont = (mem.get('content') or '')
            if isinstance(cont, str) and ('thinking_content' in cont or '[planner]' in cont.lower()):
                return False
            if 'thinking_content' in str(md).lower():
                return False
            # If we have a valid app_id, we can generally allow it as a verified source
            if md.get('app_id'):
                return True
            # Otherwise conservatively block to avoid file/text provenance contamination
            return False
        except Exception:
            # If anything goes wrong, be conservative and block the memory from retrieval
            return False
    
    async def update_context(self, session_id: str, user_input: str, assistant_response: str):
        current = await self.memory.get_active_context(session_id)
        new_turn = f"User: {user_input}\nAssistant: {assistant_response}\n"
        updated_context = current + "\n" + new_turn

        token_count = self.memory.count_tokens(updated_context)

        if token_count > settings.summarize_threshold:
            summary = await self._summarize_context(updated_context)
            await self.memory.flush_to_neo4j(session_id, summary, original_tokens=token_count)
            recent_turns = "\n".join(updated_context.split("\n")[-25:])  # Keep more recent turns (from 10 to 25)
            await self.memory.save_active_context(session_id, recent_turns)
        else:
            await self.memory.save_active_context(session_id, updated_context)
    
    async def _summarize_context(self, context: str) -> str:
        """
        CHUNKED Markovian summarization with Distiller annotation.
        
        Instead of choking on large context, process in chunks:
        1. Split context into digestible chunks
        2. Distiller annotates meaning for each chunk
        3. Combine annotations into final summary
        """
        # Token budget: For 8K context model
        # System (200) + Output (1000) + Safety (300) = 1500 reserved
        # Available for input: ~6500 tokens
        CHUNK_SIZE = 5000  # tokens (~20,000 chars)
        
        # Rough token estimation: ~4 chars per token
        char_chunk_size = CHUNK_SIZE * 4
        
        # If context fits in one chunk, process directly
        if len(context) <= char_chunk_size:
            return await self._summarize_single_chunk(context)
        
        # Otherwise, chunk and process iteratively
        print(f"ðŸ§© Large context detected ({len(context)} chars) - chunking...")
        
        chunks = []
        start = 0
        while start < len(context):
            end = min(start + char_chunk_size, len(context))
            chunk_text = context[start:end]
            chunks.append(chunk_text)
            start = end
        
        print(f"   Split into {len(chunks)} chunks")
        
        # Process each chunk with Distiller
        annotated_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"   Processing chunk {i+1}/{len(chunks)}...")
            
            # Distiller annotates the chunk's meaning
            annotation = await self.distiller.annotate_chunk(chunk, chunk_number=i+1, total_chunks=len(chunks))
            annotated_chunks.append(f"[Chunk {i+1}] {annotation}")
        
        # Combine all annotations into a coherent summary
        combined = "\n\n".join(annotated_chunks)
        
        # Final compression pass
        final_summary = await self._compress_annotations(combined)
        
        print(f"âœ… Chunked summary complete")
        return final_summary
    
    async def _summarize_single_chunk(self, context: str) -> str:
        """Summarize a single chunk of context - preserve granular details."""
        system_prompt = """You are a memory summarizer. Create a comprehensive summary that preserves granular details.

Focus on:
- EXACT facts, numbers, and entity names (never generalize)
- All decisions and conclusions reached
- Different perspectives or options discussed
- Open questions and follow-ups
- Technical details, configurations, or specifications
- Specific code snippets or examples

Preserve specificity. This summary will be the ONLY memory of this conversation."""

        summary = await self.llm.generate(
            prompt=f"Summarize this conversation:\n\n{context}",
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=1200  # Increased from 1000 to allow more detail
        )
        return summary
    
    async def _compress_annotations(self, combined_annotations: str) -> str:
        """Compress multiple chunk annotations into a final summary while preserving details."""
        system_prompt = """You are synthesizing multiple memory annotations into one coherent summary.

Each annotation represents a chunk of a larger conversation. Synthesize them into:
- A unified narrative with all important facts preserved
- Key facts, numbers, and entities from ALL chunks (be exhaustive)
- Important patterns and recurring themes across the full conversation
- All decisions, conclusions, and open questions
- Technical details and specifications that shouldn't be lost

Preserve granularity and specificity across all chunks."""

        # If annotations are still too large, truncate to most recent
        max_chars = 6000 * 4  # Increased from 4000 to ~6000 tokens
        if len(combined_annotations) > max_chars:
            combined_annotations = "...[earlier annotations truncated]...\n\n" + combined_annotations[-max_chars:]
        
        final = await self.llm.generate(
            prompt=f"Synthesize these chunk annotations:\n\n{combined_annotations}",
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=1200  # Increased from 1000 to preserve more detail
        )
        return final



--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\context.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller.py (Section: BACKEND_PYTHON) ---

"""Thin wrapper for the canonical Distiller implementation.

This module intentionally re-exports the canonical implementation in
`core.distiller_impl` so that external callers can import `core.distiller`
without depending on a specific implementation file.
"""
from .distiller_impl import (
	DistilledEntity,
	DistilledMoment,
	Distiller,
	distill_moment,
	annotate_chunk,
	_safe_validate_moment,
	filter_and_consolidate,
	make_compact_summary,
)

__all__ = [
	"DistilledEntity",
	"DistilledMoment",
	"Distiller",
	"distill_moment",
	"annotate_chunk",
	"_safe_validate_moment",
	"filter_and_consolidate",
	"make_compact_summary",
]


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller_impl.py (Section: BACKEND_PYTHON) ---

"""Canonical Distiller implementation used across ECE_Core.

This implementation provides a compact Distiller API that mirrors the
legacy summarization and entity extraction methods used in earlier versions: `distill_moment`,
`annotate_chunk`, `filter_and_consolidate`, `make_compact_summary`, and
validation helpers. The file is intentionally single-copy, deterministic,
and has minimal dependencies to simplify testing.
"""
from __future__ import annotations

import asyncio
import json
import logging
import re
import uuid
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional
from collections import OrderedDict
import hashlib as _hashlib
import json as _json
from typing import Tuple

_redis_client = None
_redis_connect_lock = asyncio.Lock()

from pydantic import BaseModel, Field, ValidationError, validator
from src.config import settings
from src.content_utils import clean_content, has_technical_signal, is_token_soup, sanitize_token_soup, normalize_technical_content

logger = logging.getLogger(__name__)

# Simple in-memory distillation cache to avoid repeated LLM calls during ingestion
_distill_cache: "OrderedDict[str, Any]" = OrderedDict()
_distill_cache_limit = 4096
_llm_semaphore: Optional[asyncio.Semaphore] = None


class DistilledEntity(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    text: str
    type: Optional[str] = None
    score: Optional[float] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @validator("text")
    def not_empty_text(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("Entity text must be non-empty")
        return v.strip()


class DistilledMoment(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    text: str
    summary: Optional[str] = None
    entities: List[DistilledEntity] = Field(default_factory=list)
    score: float = Field(default=0.5, description="Salience score (0.0-1.0)")
    created_at: datetime = Field(default_factory=datetime.utcnow)

    @validator("text")
    def not_empty_text(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("Moment text must be non-empty")
        return v.strip()


def _normalize_entity_dict(e: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(e, dict):
        return {}
    if "text" not in e and "name" in e:
        e = dict(e)
        e["text"] = e.pop("name")
    return e


def _simple_entity_extraction(text: str, max_entities: int = 10) -> List[DistilledEntity]:
    # Add technical entity extraction if a technical signal exists
    from src.content_utils import has_technical_signal
    entities: List[DistilledEntity] = []
    seen = set()
    if has_technical_signal(text):
        # extract version numbers, file paths, package names, and error codes
        version_re = re.compile(r'v\d+\.\d+(?:\.\d+)?')
        path_re = re.compile(r'\b(?:[A-Za-z0-9\-_/\\]+\/[A-Za-z0-9\-_.]+)\b')
        pkg_re = re.compile(r'\b(?:npm|pip|apt-get|docker|cargo)\b', re.IGNORECASE)
        for m in version_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='version'))
        for m in path_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='path'))
        for m in pkg_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='package'))
        # also fallback to proper nouns
        pattern = r"\b(?:[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b"
        matches = re.findall(pattern, text)
        for m in matches:
            k = m.strip().lower()
            if k in seen:
                continue
            seen.add(k)
            entities.append(DistilledEntity(text=m, type='proper_noun'))
        return entities[:max_entities]
    pattern = r"\b(?:[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b"
    matches = re.findall(pattern, text)
    out: List[DistilledEntity] = []
    for m in matches:
        key = m.strip().lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(DistilledEntity(text=m, type="proper_noun"))
        if len(out) >= max_entities:
            break
    return out


async def _maybe_await(v: Any) -> Any:
    if asyncio.iscoroutine(v):
        return await v
    return v


class Distiller:
    def __init__(self, llm_client: Optional[Any] = None):
        self.llm = llm_client

    async def _call_llm(self, text: str, skip_chunking: bool = False, max_entities: int = 10) -> Any:
        if not self.llm:
            raise RuntimeError("No LLM configured")
        global _llm_semaphore
        if _llm_semaphore is None:
            _llm_semaphore = asyncio.Semaphore(getattr(settings, 'llm_concurrency', 4))
        # Prefer the `generate` API when present (modern LLMs), but support `complete`
        # for legacy clients. Also ensure we use callable attributes (MagicMock
        # will report attributes even when not set). This prevents calling
        # auto-generated MagicMock attributes which return a MagicMock instance
        # rather than the configured AsyncMock return value.
        try:
            if hasattr(self.llm, "generate") and callable(getattr(self.llm, "generate", None)):
                # Allow LLM client to optionally force remote API usage; we rely on the LLM client
                # to raise ContextSizeExceededError when it determines the prompt would exceed server context
                async with _llm_semaphore:
                    return await _maybe_await(self.llm.generate(text))
            if hasattr(self.llm, "complete") and callable(getattr(self.llm, "complete", None)):
                return await _maybe_await(self.llm.complete(text))
        except Exception as e:
            # If the LLM indicates the context is too large and we have not yet chunked, perform chunking
            from src.llm import ContextSizeExceededError
            if isinstance(e, ContextSizeExceededError) and not skip_chunking:
                logger.debug("LLM reported ContextSizeExceeded; falling back to chunk-and-merge strategy")
                return await self._chunk_and_distill(text, max_entities=max_entities)
            raise
        # If we reach here, LLM didn't have expected interface
        raise RuntimeError("LLM missing expected method")
        raise RuntimeError("LLM missing expected method")

    async def distill_moment(self, text: str, chunk_index: Optional[int] = None, total_chunks: Optional[int] = None, max_entities: int = 10, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        if not text or not text.strip():
            raise ValueError("text must be non-empty")
        # Keep a copy of the raw text for potential normalizations
        raw_text = text
        # Clean text before distillation while preserving technical signals
        tech = has_technical_signal(text)
        if tech:
            # Preserve technical artifacts, but reduce obvious noise and annotate context
            text = clean_content(text, remove_emojis=False, remove_non_ascii=False, annotate_technical=True)
        else:
            text = clean_content(text, remove_emojis=True, remove_non_ascii=False, annotate_technical=False)
        # Detect token-soup/corrupted content, attempt normalization, then fallback to sanitize
        if is_token_soup(text):
            logger.warning("Detected token-soup content; attempting normalization to preserve technical context")
            # Attempt to normalize technical content (map ANSI, paths, and hex dumps to human tags) and retry
            try:
                normalized = normalize_technical_content(raw_text)
                cleaned_normalized = clean_content(normalized, remove_emojis=False, remove_non_ascii=False)
            except Exception as e:
                logger.debug(f"Normalization failed: {e}")
                cleaned_normalized = ''
            # If normalization produced a non-soup text, proceed with it
            if cleaned_normalized and not is_token_soup(cleaned_normalized):
                text = cleaned_normalized
            else:
                # Keep technical content if flagged; otherwise sanitize aggressively
                sanitized = text if tech else sanitize_token_soup(text)
                # Return a safe fallback summary and minimal entity extraction
                entities = _simple_entity_extraction(sanitized, max_entities=max_entities)
                summary = (sanitized[:300] + '...') if len(sanitized) > 300 else sanitized
                moment = DistilledMoment(text=sanitized, summary=summary, entities=entities, score=0.1)
                return moment.dict()
        entities: List[DistilledEntity] = []
        summary: Optional[str] = None
        if not self.llm:
            return {"summary": text[:200] + "...", "entities": []}
        # Check cache before calling LLM (avoid repeated distillations during ingestion)
        try:
            # Include metadata in the hash so that different metadata results can be cached separately
            content_hash = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
            cached = _distill_cache.get(content_hash)
            if cached:
                return cached
        except Exception:
            pass
        # Use metadata heuristics to avoid LLM calls for code/log-like artifacts
        try:
            md_path = (metadata or {}).get('path') if isinstance(metadata, dict) else None
            if md_path and isinstance(md_path, str):
                lower = md_path.lower()
                if lower.endswith(('.py', '.js', '.ts', '.java', '.go', '.rs', '.c', '.cpp', '.sh', '.md', '.log')) or '/logs/' in lower or lower.endswith('.log'):
                    entities = _simple_entity_extraction(text, max_entities=max_entities)
                    summary = (text[:400] + '...') if len(text) > 400 else text
                    moment = DistilledMoment(text=text, summary=summary, entities=entities, score=0.1)
                    return moment.dict()
        except Exception:
            pass
        try:
            raw = await self._call_llm(text, max_entities=max_entities)
            parsed = None
            if isinstance(raw, dict):
                parsed = raw
            elif isinstance(raw, str):
                try:
                    parsed = json.loads(raw)
                except Exception:
                    return {"summary": f"Error distilling chunk {chunk_index}. Raw: {str(raw)[:100]}...", "entities": []}
            if isinstance(parsed, dict):
                summary = parsed.get("summary") or parsed.get("title")
                score = float(parsed.get("score", 0.5))
                # Normalize score if 0-10
                if score > 1.0:
                    score = score / 10.0
                
                raw_entities = parsed.get("entities", [])
                for e in raw_entities[:max_entities]:
                    if isinstance(e, str):
                        entities.append(DistilledEntity(text=e))
                    elif isinstance(e, dict):
                        nd = _normalize_entity_dict(e)
                        try:
                            entities.append(DistilledEntity(**nd))
                        except ValidationError:
                            logger.debug("Invalid LLM entity: %s", e)
        except Exception:
            logger.exception("LLM call failed; falling back to simple extractor")
        if not entities:
            entities = _simple_entity_extraction(text, max_entities=max_entities)
            score = 0.5  # Default for fallback
            
        moment = DistilledMoment(text=text, summary=summary, entities=entities, score=score)
        # Write to in-memory cache with limited size
        try:
            _distill_cache[content_hash] = moment.dict()
            if len(_distill_cache) > _distill_cache_limit:
                # pop the oldest
                _distill_cache.popitem(last=False)
        except Exception:
            pass
        return moment.dict()

    async def _chunk_and_distill(self, text: str, max_entities: int = 10) -> Dict[str, Any]:
        """
        Chunk a large piece of text into smaller pieces and distill them individually, then combine summaries.
        This is a simple approach: summarize each chunk, collect summaries, then ask the LLM to summarize the summaries.
        """
        # Detect token-soup/corrupt content and attempt normalization before sanitizing
        if is_token_soup(text):
            logger.warning("Detected token-soup in _chunk_and_distill; attempting normalization")
            try:
                normalized = normalize_technical_content(text)
                cleaned_normalized = clean_content(normalized, remove_emojis=False, remove_non_ascii=False)
            except Exception as e:
                logger.debug(f"Normalization error in _chunk_and_distill: {e}")
                cleaned_normalized = ''
            if cleaned_normalized and not is_token_soup(cleaned_normalized):
                text = cleaned_normalized
            else:
                logger.warning("Normalization did not yield clean text; sanitizing token soup and returning fallback")
                sanitized = sanitize_token_soup(text)
                entities = _simple_entity_extraction(sanitized, max_entities=max_entities)
                summary = (sanitized[:300] + '...') if len(sanitized) > 300 else sanitized
                return {"summary": summary, "entities": [e.dict() for e in entities], "score": 0.1}

        # Estimate tokens and char conversion heuristic (approx 4 chars per token)
        # Prefer to use a chunk size that's smaller than the server context if we detected it
        chunk_tokens = settings.archivist_chunk_size
        if hasattr(self.llm, '_detected_server_context_size') and self.llm._detected_server_context_size:
            try:
                detected = int(self.llm._detected_server_context_size)
                # leave a buffer for prompt and final summary
                usable = max(256, detected - 512)
                if usable < chunk_tokens:
                    chunk_tokens = usable
            except Exception:
                pass
        overlap_tokens = settings.archivist_overlap
        chars_per_token = 4
        chunk_chars = chunk_tokens * chars_per_token
        overlap_chars = overlap_tokens * chars_per_token
        text_len = len(text)
        chunks = []
        start = 0
        while start < text_len:
            end = min(start + chunk_chars, text_len)
            # Try to split at newline within the window for semantic boundaries
            seg = text[start:end]
            if end < text_len:
                last_newline = seg.rfind('\n')
                if last_newline > int(chunk_chars * 0.5):
                    end = start + last_newline
                    seg = text[start:end]
            chunks.append(seg)
            # Advance, with overlap
            start = max(0, end - overlap_chars)
        logger.info(f"Chunked text into {len(chunks)} parts for distillation")
        # Distill each chunk
        chunk_summaries = []
        chunk_entities = []
        for i, c in enumerate(chunks):
            try:
                res = await self._call_llm(c, skip_chunking=True, max_entities=max_entities)
            except Exception as e:
                logger.warning(f"Failed to distill chunk {i} independently: {e}")
                continue
            parsed = None
            if isinstance(res, dict):
                parsed = res
            elif isinstance(res, str):
                try:
                    parsed = json.loads(res)
                except Exception:
                    parsed = {"summary": res}
            if isinstance(parsed, dict):
                chunk_summaries.append(parsed.get("summary") or parsed.get("text") or "")
                raw_entities = parsed.get("entities", []) or []
                for e in raw_entities:
                    if isinstance(e, dict):
                        nd = _normalize_entity_dict(e)
                        try:
                            chunk_entities.append(DistilledEntity(**nd))
                        except ValidationError:
                            logger.debug("Invalid LLM entity in chunk: %s", e)
                    elif isinstance(e, str):
                        chunk_entities.append(DistilledEntity(text=e))
        # Join summaries and ask for final summarization
        combined = "\n\n".join([s for s in chunk_summaries if s])
        # Build a compact instruction for final summarization
        final_prompt = f"Summarize the following chunk summaries into a concise JSON object with fields 'summary' and 'entities'. Summaries:\n\n{combined}"
        final_raw = await self._call_llm(final_prompt, skip_chunking=True, max_entities=max_entities)
        final_parsed = None
        if isinstance(final_raw, dict):
            final_parsed = final_raw
        elif isinstance(final_raw, str):
            try:
                final_parsed = json.loads(final_raw)
            except Exception:
                final_parsed = {"summary": final_raw}
        # Consolidate entities from chunk_entities and final_parsed entities
        entities = []
        if isinstance(final_parsed, dict):
            raw_entities = final_parsed.get("entities", []) or []
            for e in raw_entities:
                if isinstance(e, dict):
                    nd = _normalize_entity_dict(e)
                    try:
                        entities.append(DistilledEntity(**nd))
                    except ValidationError:
                        logger.debug("Invalid final entity: %s", e)
                elif isinstance(e, str):
                    entities.append(DistilledEntity(text=e))
        # Merge chunk_entities
        entities.extend(chunk_entities)
        entities = filter_and_consolidate(entities)
        summary = (final_parsed.get("summary") if isinstance(final_parsed, dict) else final_parsed.get("title") if isinstance(final_parsed, dict) else None) or (combined[:400] + '...')
        # Score: fallback average or default
        score = float(final_parsed.get("score", 0.5)) if isinstance(final_parsed, dict) and final_parsed.get("score") else 0.5
        return {"summary": summary, "entities": [e.dict() for e in entities], "score": score}

    async def annotate_chunk(self, text: str, chunk_number: Optional[int] = None, total_chunks: Optional[int] = None) -> str:
        moment = await self.distill_moment(text, chunk_index=chunk_number, total_chunks=total_chunks)
        entities = moment.get("entities", []) if isinstance(moment, dict) else moment.entities
        summary = moment.get("summary") if isinstance(moment, dict) else moment.summary
        ent_names = [e.get("text") if isinstance(e, dict) else e.text for e in entities]
        ent_str = ", ".join([n for n in ent_names if n])
        return (summary or text[:200]) + ("\n\nEntities: " + ent_str if ent_str else "")

    async def filter_and_consolidate(self, query: str, memories: List[Dict[str, Any]], summaries: List[Dict[str, Any]], active_turn: Optional[str] = None, active_context: Optional[str] = None) -> Dict[str, Any]:
        # Support both active_turn and active_context keywords (legacy vs new callers)
        active_turn = active_turn or active_context
        q_lower = (query or "").lower()
        # Also ensure we strip out test/dev/thinking content that may have slipped into memories
        def _is_memory_clean(m: dict) -> bool:
            if not m or not isinstance(m, dict):
                return False
            meta = m.get('metadata') or {}
            if isinstance(meta, str):
                try:
                    meta = json.loads(meta)
                except Exception:
                    meta = {}
            src = (meta.get('source') or meta.get('path') or '')
            if isinstance(src, str) and any(x in src.lower() for x in ('combined_text', 'prompt-logs', 'calibration_run', 'dry-run')):
                return False
            content = (m.get('content') or '')
            if isinstance(content, str) and ('thinking_content' in content or '[planner]' in content.lower()):
                return False
            return True

        relevant_memories = [m for m in (memories or []) if q_lower in (m.get("content", "") or "").lower() and _is_memory_clean(m)]
        # Preserve active context in output (maintain key for ContextManager)
        return {"summaries": summaries or [], "relevant_memories": relevant_memories, "active_context": active_turn or ""}

    async def make_compact_summary(self, memories: List[Dict[str, Any]], summaries: List[Dict[str, Any]], active_turn: Optional[str], new_input: Optional[str], max_sentences: int = 3) -> str:
        if new_input and new_input.strip():
            return new_input.strip()
        if summaries:
            texts = [s.get("summary") or s.get("text") for s in summaries]
            joined = " ".join([t for t in texts if t])
            sentences = re.split(r"(?<=[.!?])\s+", joined)
            return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])
        if memories:
            texts = [m.get("content") for m in memories if m.get("content")]
            joined = " ".join(texts)
            sentences = re.split(r"(?<=[.!?])\s+", joined)
            return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])
        return ""

    def _safe_validate_moment(self, moment_data: Dict[str, Any]) -> DistilledMoment:
        return DistilledMoment(**moment_data)


def filter_and_consolidate(entities: Iterable[DistilledEntity]) -> List[DistilledEntity]:
    by_key: Dict[str, DistilledEntity] = {}
    for e in entities:
        if not e or not e.text:
            continue
        key = e.text.strip().lower()
        existing = by_key.get(key)
        if not existing:
            by_key[key] = e
            continue
        if (existing.score or 0) < (e.score or 0):
            by_key[key] = e
    return list(by_key.values())


def make_compact_summary(moment: DistilledMoment, max_sentences: int = 3) -> str:
    if moment.summary and moment.summary.strip():
        return moment.summary.strip()
    sentences = re.split(r"(?<=[.!?])\s+", moment.text)
    return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])


_default_distiller = Distiller()


async def distill_moment(text: str, llm_client: Optional[Any] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Dict[str, Any]:
    """Global convenience function. Accepts metadata to help Distiller choose strategy.
    If a Redis cache is configured, the function will attempt to reuse distillation results.
    """
    d = _default_distiller if llm_client is None else Distiller(llm_client)
    # If Redis caching enabled, try to fetch first
    try:
        if getattr(settings, 'memory_distill_cache_enabled', False) and getattr(settings, 'redis_url', None):
            global _redis_client
            async with _redis_connect_lock:
                if _redis_client is None:
                    try:
                        import redis.asyncio as aioredis
                        _redis_client = aioredis.from_url(settings.redis_url, decode_responses=True)
                        await _redis_client.ping()
                    except Exception:
                        _redis_client = None
            if _redis_client is not None:
                key = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
                try:
                    val = await _redis_client.get(key)
                    if val:
                        # parse and return
                        return _json.loads(val)
                except Exception:
                    # Ignore Redis errors and fall back to in-memory cache
                    pass
    except Exception:
        pass
    result = await d.distill_moment(text, metadata=metadata, **kwargs)
    # Cache to Redis + in-memory cache if enabled
    try:
        content_hash = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
        _distill_cache[content_hash] = result
        if len(_distill_cache) > _distill_cache_limit:
            _distill_cache.popitem(last=False)
        if getattr(settings, 'memory_distill_cache_enabled', False) and getattr(settings, 'redis_url', None) and _redis_client:
            try:
                await _redis_client.set(content_hash, _json.dumps(result), ex=getattr(settings, 'memory_distill_cache_ttl', 86400))
            except Exception:
                pass
    except Exception:
        pass
    return result


async def annotate_chunk(text: str, llm_client: Optional[Any] = None, **kwargs: Any) -> str:
    d = _default_distiller if llm_client is None else Distiller(llm_client)
    return await d.annotate_chunk(text, **kwargs)


def _safe_validate_moment(moment_data: Dict[str, Any]) -> DistilledMoment:
    return _default_distiller._safe_validate_moment(moment_data)


__all__ = [
    "DistilledEntity",
    "DistilledMoment",
    "Distiller",
    "distill_moment",
    "annotate_chunk",
    "_safe_validate_moment",
    "filter_and_consolidate",
    "make_compact_summary",
]


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller_impl.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\exceptions.py (Section: BACKEND_PYTHON) ---

"""
Custom exceptions for ECE_Core and Anchor

Simple, focused exception hierarchy for better error handling.
"""


class ECEError(Exception):
    """Base exception for all ECE_Core errors"""
    pass


class ConfigurationError(ECEError):
    """Configuration loading or validation failed"""
    pass


class MemoryError(ECEError):
    """Memory system (Redis/Neo4j) errors"""
    pass


class LLMError(ECEError):
    """LLM communication errors"""
    pass


class ToolCallError(ECEError):
    """Tool call parsing or execution errors"""
    pass


class MCPError(ECEError):
    """MCP server connection or tool errors"""
    pass


class ValidationError(ECEError):
    """Input validation errors"""
    pass


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\exceptions.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\graph.py (Section: BACKEND_PYTHON) ---

"""
Graph Reasoner: Memory retrieval using Graph-R1 patterns
Implements: think → generate query → retrieve subgraph → rethink
Based on Graph-R1 paper: arxiv.org/abs/2507.21892

Note: This implements the paper's iterative graph traversal for MEMORY RETRIEVAL,
not complex reasoning. It helps find relevant memories by exploring graph connections.
"""
import json
from typing import List, Dict, Any, Optional
from src.llm import LLMClient
from src.memory import TieredMemory

class GraphReasoner:
    """
    Memory retrieval using Graph-R1 patterns.
    Implements iterative "think-query-retrieve-rethink" cycle for finding relevant memories.
    
    Note: "Reasoning" here means iterative graph traversal to find connected memories,
    not complex logical reasoning. It's a smart retrieval strategy.
    """
    
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        self.max_iterations = 5  # Markovian thinking: small fixed iterations
    
    async def reason(self, session_id: str, question: str) -> Dict[str, Any]:
        """
        Main reasoning loop: think → query → retrieve → rethink
        Returns final answer with reasoning trace
        """
        reasoning_trace = []
        current_thought = question
        retrieved_context = []
        
        for iteration in range(self.max_iterations):
            # Step 1: Think (high-level planning)
            thought = await self._think(current_thought, retrieved_context, iteration)
            reasoning_trace.append({
                "iteration": iteration,
                "thought": thought,
                "type": "planning"
            })
            
            # Step 2: Generate query from thought
            query = await self._generate_query(thought, question)
            reasoning_trace.append({
                "iteration": iteration,
                "query": query,
                "type": "query_generation"
            })
            
            # Step 3: Retrieve subgraph (from Neo4j memories)
            subgraph = await self._retrieve_subgraph(query, session_id)
            retrieved_context.append({
                "iteration": iteration,
                "subgraph": subgraph
            })
            reasoning_trace.append({
                "iteration": iteration,
                "retrieved": len(subgraph),
                "type": "retrieval"
            })
            
            # Step 4: Check if we can answer
            answer_attempt = await self._attempt_answer(
                question, 
                thought, 
                retrieved_context
            )
            
            if answer_attempt["confident"]:
                reasoning_trace.append({
                    "iteration": iteration,
                    "final_answer": answer_attempt["answer"],
                    "type": "answer"
                })
                return {
                    "answer": answer_attempt["answer"],
                    "reasoning_trace": reasoning_trace,
                    "iterations": iteration + 1,
                    "confidence": "high"
                }
            
            # Step 5: Rethink for next iteration
            current_thought = await self._rethink(
                thought, 
                retrieved_context, 
                question
            )
        
        # Max iterations reached - provide best attempt
        final_answer = await self._final_answer(question, retrieved_context)
        reasoning_trace.append({
            "iteration": self.max_iterations,
            "final_answer": final_answer,
            "type": "final_attempt"
        })
        
        return {
            "answer": final_answer,
            "reasoning_trace": reasoning_trace,
            "iterations": self.max_iterations,
            "confidence": "medium"
        }
    
    async def _think(self, current_thought: str, retrieved_context: List[Dict], iteration: int) -> str:
        """
        High-level planning step.
        Like HRM's abstract planning module.
        """
        context_summary = self._summarize_context(retrieved_context)
        
        prompt = f"""You are in iteration {iteration} of a reasoning process.

Current question/thought: {current_thought}

Retrieved context so far:
{context_summary}

What should you focus on next? Think step by step about:
1. What information is still missing?
2. What aspect of the question needs exploration?
3. What specific memory or knowledge would help?

Provide a concise plan (2-3 sentences)."""
        
        thought = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=150
        )
        
        return thought.strip()
    
    async def _generate_query(self, thought: str, original_question: str) -> str:
        """
        Generate specific query to retrieve relevant memories.
        """
        prompt = f"""Based on this reasoning step:
{thought}

And original question:
{original_question}

Generate a concise search query (keywords and concepts) to find relevant memories.
Query:"""
        
        query = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=50
        )
        
        return query.strip()
    
    async def _retrieve_subgraph(self, query: str, session_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve relevant memories from Neo4j (Moments + Entities + ContextGists).
        """
        # Extract potential categories and tags from query
        keywords = query.lower().split()

        # 1. Search Summaries (Legacy + New)
        summaries = await self.memory.get_summaries(session_id, limit=3)

        # 2. Search ContextGist memories (compressed historical context)
        # These contain summaries of old context that was rotated out
        context_gists = await self.memory.search_memories_neo4j(
            query_text=query,
            category="context_gist",
            limit=5
        )

        # 3. Search Moments & Entities via Cypher
        # We look for Moments with matching summary text OR linked to matching Entities
        cypher_query = """
        CALL db.index.fulltext.queryNodes("momentSearch", $query) YIELD node, score
        RETURN node.summary as content, score, "moment" as type, node.id as id
        UNION
        CALL db.index.fulltext.queryNodes("entitySearch", $query) YIELD node, score
        MATCH (m:Moment)-[:CONTAINS]->(node)
        RETURN m.summary as content, score, "moment_via_entity" as type, m.id as id
        ORDER BY score DESC
        LIMIT 5
        """

        # Fallback to simple keyword search if fulltext index fails or returns nothing
        # (Not implemented here for brevity, relying on index)

        try:
            records = await self.memory.execute_cypher(cypher_query, {"query": query})
        except Exception as e:
            # Fallback if index doesn't exist yet
            records = []

        # Combine results
        subgraph = []

        # Add ContextGists first (historical context)
        for gist in context_gists:
            subgraph.append({
                "type": "context_gist",
                "content": gist.get("content", gist.get("summary", "")),
                "id": gist.get("id", ""),
                "importance": gist.get("importance", 5),
                "metadata": gist.get("metadata", {})
            })

        # Add summaries (organized context)
        for summary in summaries:
            subgraph.append({
                "type": "summary",
                "content": summary["summary"],
                "timestamp": summary["timestamp"]
            })

        # Add direct moments and entities
        for rec in records:
            subgraph.append({
                "type": rec.get("type", "memory"),
                "content": rec.get("content", ""),
                "id": rec.get("id", ""),
                "score": rec.get("score", 0.5)
            })

        return subgraph
    
    async def _attempt_answer(
        self,
        question: str,
        current_thought: str,
        retrieved_context: List[Dict]
    ) -> Dict[str, Any]:
        """
        Attempt to answer based on current knowledge.
        Returns confidence and answer.
        """
        context_text = self._format_context(retrieved_context)

        # Check if we have ContextGist information that might be relevant
        has_context_gists = any(
            item.get("type") == "context_gist"
            for ctx in retrieved_context
            for item in ctx.get("subgraph", [])
        )

        prompt = f"""Question: {question}

Current reasoning: {current_thought}

Has historical context (ContextGists): {"YES" if has_context_gists else "NO"}

Retrieved context:
{context_text}

Can you answer the question with HIGH confidence based on this context?
If YES: Provide the answer.
If NO: Explain what information is still needed.

Format:
Confidence: [HIGH/LOW]
Answer or Reasoning: [your response]"""

        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=300
        )

        # Parse response
        lines = response.strip().split('\n')
        confident = "HIGH" in lines[0].upper() if lines else False
        answer = '\n'.join(lines[1:]).replace("Answer or Reasoning:", "").strip()

        return {
            "confident": confident,
            "answer": answer
        }
    
    async def _rethink(
        self,
        previous_thought: str,
        retrieved_context: List[Dict],
        original_question: str
    ) -> str:
        """
        Rethink based on what we've learned.
        Markovian: carry forward only essential state (textual summary).
        """
        context_summary = self._summarize_context(retrieved_context)

        # Check if we have ContextGist information that might provide historical context
        has_context_gists = any(
            item.get("type") == "context_gist"
            for ctx in retrieved_context
            for item in ctx.get("subgraph", [])
        )

        prompt = f"""Original question: {original_question}

Previous reasoning: {previous_thought}

Historical context available: {"YES" if has_context_gists else "NO"}

What we've learned:
{context_summary}

What should be the next focus?
- If historical context exists, consider if it might contain relevant background information
- If no historical context, focus on continuing to explore new information
Provide a refined thought (1-2 sentences)."""

        rethought = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=100
        )

        return rethought.strip()
    
    async def _final_answer(self, question: str, retrieved_context: List[Dict]) -> str:
        """
        Generate final answer after max iterations.
        """
        context_text = self._format_context(retrieved_context)
        
        prompt = f"""Question: {question}

All retrieved context:
{context_text}

Based on everything available, provide the best possible answer."""
        
        answer = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=500
        )
        
        return answer.strip()
    
    def _summarize_context(self, retrieved_context: List[Dict]) -> str:
        """Create concise summary of retrieved context."""
        if not retrieved_context:
            return "No context retrieved yet."
        
        summary_parts = []
        for ctx in retrieved_context[-3:]:  # Last 3 iterations
            subgraph = ctx.get("subgraph", [])
            summary_parts.append(
                f"Iteration {ctx['iteration']}: Retrieved {len(subgraph)} items"
            )
        
        return "\n".join(summary_parts)
    
    def _format_context(self, retrieved_context: List[Dict]) -> str:
        """Format all retrieved context for prompts."""
        if not retrieved_context:
            return "No context available."
        
        formatted = []
        for ctx in retrieved_context:
            subgraph = ctx.get("subgraph", [])
            for item in subgraph:
                formatted.append(f"[{item['type']}] {item['content'][:200]}...")
        
        return "\n\n".join(formatted) if formatted else "No specific context."


class MarkovianReasoner:
    """
    Simpler Markovian-style reasoning without graph retrieval.
    Just: think → summarize → repeat with small context window.
    """
    
    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.max_chunks = 5
    
    async def reason(self, task: str, initial_context: str = "") -> str:
        """
        Chunked reasoning with textual carryover.
        Each chunk processes only previous summary + current task.
        """
        carryover = initial_context
        
        for chunk in range(self.max_chunks):
            # Process one reasoning chunk
            chunk_result = await self._process_chunk(task, carryover, chunk)
            
            # Check if task complete
            if chunk_result["complete"]:
                return chunk_result["answer"]
            
            # Carry forward only summary (Markovian property)
            carryover = chunk_result["summary"]
        
        # Final synthesis
        final = await self._synthesize(task, carryover)
        return final
    
    async def _process_chunk(
        self, 
        task: str, 
        carryover: str, 
        chunk_num: int
    ) -> Dict[str, Any]:
        """
        Process one reasoning chunk.
        Small context window: task + previous summary only.
        """
        prompt = f"""Task: {task}

Previous reasoning:
{carryover if carryover else 'Starting fresh.'}

Chunk {chunk_num+1}/{self.max_chunks}:
1. What's one key step toward solving this?
2. Is the task complete? (YES/NO)
3. Summary for next chunk (if incomplete):

Your response:"""
        
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.4,
            max_tokens=300
        )
        
        # Simple parsing
        lines = response.strip().split('\n')
        complete = any("YES" in line.upper() for line in lines[:5])
        
        return {
            "complete": complete,
            "answer": response if complete else None,
            "summary": response  # Entire response becomes carryover
        }
    
    async def _synthesize(self, task: str, final_carryover: str) -> str:
        """Final synthesis after all chunks."""
        prompt = f"""Task: {task}

Reasoning completed:
{final_carryover}

Provide final answer:"""
        
        answer = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=400
        )
        
        return answer.strip()


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\graph.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\intelligent_chunker.py (Section: BACKEND_PYTHON) ---

"""
Intelligent Chunker: Decides how to process each chunk.

Instead of blindly annotating everything, the Chunker:
1. Analyzes chunk content type (code, prose, logs, etc.)
2. Determines if annotation alone suffices or full detail needed
3. Routes to appropriate processing strategy
"""
from typing import List, Dict, Tuple, Literal
from src.llm import LLMClient
import re


ChunkStrategy = Literal["annotation_only", "distilled", "full_detail"]


class IntelligentChunker:
    """
    Analyzes chunks and routes them to the optimal processing strategy.
    
    This is the "decider" that makes chunking intelligent, not just mechanical.
    """
    
    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.chunk_size = 4000  # chars per chunk
        
    async def process_large_input(
        self, 
        user_input: str,
        query_context: str = ""
    ) -> str:
        """
        Main entry point for processing large user inputs.
        
        Returns a compressed context suitable for the LLM.
        """
        # Detect if input is large enough to warrant chunking
        if len(user_input) < self.chunk_size:
            return user_input  # No chunking needed
        
        # Split into semantic chunks
        chunks = self._split_semantic_chunks(user_input)
        
        # Process each chunk with appropriate strategy
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            strategy = await self._determine_strategy(chunk, query_context)
            processed = await self._process_chunk(chunk, i+1, len(chunks), strategy)
            processed_chunks.append(processed)
        
        # Combine processed chunks
        combined = self._combine_processed_chunks(processed_chunks)
        
        return combined
    
    def _split_semantic_chunks(self, text: str) -> List[str]:
        """
        Split text into chunks at semantic boundaries.
        
        Prefers to split at:
        1. Paragraph breaks (double newline)
        2. Code block boundaries (```)
        3. Section headers (##, ###)
        4. Sentence boundaries (. followed by newline)
        
        Avoids splitting mid-sentence or mid-code-block.
        """
        chunks = []
        current_chunk = ""
        
        # Split on paragraph boundaries first
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            # If adding this paragraph exceeds chunk size, save current chunk
            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # Add final chunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    async def _determine_strategy(
        self, 
        chunk: str, 
        query_context: str
    ) -> ChunkStrategy:
        """
        Decide processing strategy for this chunk.
        
        Returns:
        - "annotation_only": Just extract meaning, don't send full text
        - "distilled": Compress the chunk, send summary + key details
        - "full_detail": Send entire chunk (code, specs, novel info)
        """
        # Heuristic checks (fast, no LLM needed)
        
        # Code blocks always get full detail
        if "```" in chunk or "def " in chunk or "class " in chunk:
            return "full_detail"

        # If a file path indicating code is present, treat as full detail
        if re.search(r"\b[A-Za-z]:[\\/][\w\-\./\\]+\.py\b", chunk):
            return "full_detail"
        if "Traceback (most recent call last)" in chunk:
            return "distilled"
        
        # Error logs get distilled
        if "ERROR:" in chunk or "Traceback" in chunk or "Exception" in chunk:
            return "distilled"
        
        # Short, simple confirmations get annotation only
        if len(chunk) < 200 and any(word in chunk.lower() for word in 
                                     ["yes", "ok", "agree", "sure", "understood"]):
            return "annotation_only"
        
        # Terminal output (lots of technical info) gets distilled
        if any(marker in chunk for marker in ["INFO:", "WARNING:", "slot ", "srv "]):
            return "distilled"
        
        # For ambiguous cases, ask the LLM (slower but accurate)
        prompt = f"""Analyze this chunk and determine if it needs:
A) annotation_only - Simple, repetitive, or already-known context
B) distilled - Long but compressible (logs, verbose explanations)
C) full_detail - Code, specs, novel information requiring full context

Query context: {query_context[:200]}

Chunk preview:
{chunk[:500]}

Answer with just the letter (A, B, or C):"""
        
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.1,
            max_tokens=5
        )
        
        # Parse response
        response = response.strip().upper()
        if 'A' in response:
            return "annotation_only"
        elif 'B' in response:
            return "distilled"
        else:
            return "full_detail"
    
    async def _process_chunk(
        self,
        chunk: str,
        chunk_num: int,
        total_chunks: int,
        strategy: ChunkStrategy
    ) -> Dict[str, str]:
        """
        Process chunk according to determined strategy.
        """
        if strategy == "annotation_only":
            annotation = await self._annotate_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "annotation_only",
                "content": annotation,
                "original_length": len(chunk)
            }
        
        elif strategy == "distilled":
            summary = await self._distill_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "distilled",
                "content": summary,
                "original_length": len(chunk)
            }
        
        else:  # full_detail
            annotation = await self._annotate_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "full_detail",
                "content": chunk,
                "annotation": annotation,
                "original_length": len(chunk)
            }
    
    async def _annotate_chunk(self, chunk: str, chunk_num: int, total: int) -> str:
        """
        Extract meaning/themes from chunk without full content.
        """
        prompt = f"""Chunk {chunk_num}/{total} - Extract key meaning:

{chunk[:3000]}

In 2-3 sentences, state:
1. Main theme/topic
2. Key entities mentioned
3. Any decisions/insights

Be concise:"""
        
        annotation = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=150
        )
        
        return annotation.strip()
    
    async def _distill_chunk(self, chunk: str, chunk_num: int, total: int) -> str:
        """
        Compress chunk while preserving important details.
        """
        prompt = f"""Chunk {chunk_num}/{total} - Distill this down:

{chunk[:3000]}

Provide a compressed version that:
- Keeps critical facts, errors, decisions
- Removes verbose/repetitive content
- Stays under 300 words

Also rate the SALIENCE (importance) from 0.0 to 1.0.
- 1.0 = Critical architecture/decision
- 0.5 = Routine info
- 0.1 = Noise/logs

Format: JSON {{ "summary": "...", "score": 0.8 }}
Distilled version:"""
        
        distilled = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=400
        )
        
        return distilled.strip()
    
    def _combine_processed_chunks(self, processed: List[Dict[str, str]]) -> str:
        """
        Combine processed chunks into final context.
        """
        parts = []
        
        for i, chunk_data in enumerate(processed):
            strategy = chunk_data['strategy']
            
            if strategy == "annotation_only":
                parts.append(f"[Chunk {i+1} summary] {chunk_data['content']}")
            
            elif strategy == "distilled":
                parts.append(f"[Chunk {i+1} distilled]\n{chunk_data['content']}")
            
            else:  # full_detail
                parts.append(
                    f"[Chunk {i+1} - FULL DETAIL]\n"
                    f"Note: {chunk_data['annotation']}\n"
                    f"Content:\n{chunk_data['content']}"
                )
        
        combined = "\n\n".join(parts)
        
        # Add metadata summary
        total_original = sum(c['original_length'] for c in processed)
        compression_ratio = len(combined) / total_original if total_original > 0 else 1
        
        header = f"""ðŸ§© Large context processed ({len(processed)} chunks)
Original: {total_original:,} chars â†’ Compressed: {len(combined):,} chars
Compression: {compression_ratio:.1%}

---

"""
        
        return header + combined


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\intelligent_chunker.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\llm.py (Section: BACKEND_PYTHON) ---

"""
Simple LLM client supporting both API servers and local GGUF models.

Supports:
- llama.cpp server (OpenAI-compatible API)
- Local GGUF files via llama-cpp-python
- MCP (Model Context Protocol) for reliable tool execution
"""
import httpx
import logging
import asyncio
from typing import Optional, List, Tuple, Dict
import math
from src.config import settings
import os
import re
import json
from src.chat_templates import chat_template_manager

logger = logging.getLogger(__name__)


class EmbeddingsAPIError(RuntimeError):
    """Raised when embedding API responds with an error. Contains parsed info if available.

    Attributes:
        status_code: numeric HTTP status code
        body: raw response body
        server_message: parsed message (if present)
        n_ctx: optional detected context size (tokens)
    """
    def __init__(self, message: str, status_code: Optional[int] = None, body: Optional[str] = None, server_message: Optional[str] = None, n_ctx: Optional[int] = None):
        super().__init__(message)
        self.status_code = status_code
        self.body = body
        self.server_message = server_message
        self.n_ctx = n_ctx


class LLMClient:
    """
    LLM client with automatic fallback:
    1. Try API server (llama.cpp server, LM Studio, etc.)
    2. Fall back to local GGUF loading if API unavailable
    """
    
    def __init__(self):
        # Normalize API base so we don't accidentally double-up /v1 segments
        api_base = settings.llm_api_base or ''
        api_base = api_base.rstrip('/')
        # If the configured base ended in /v1, strip it; we'll append /v1 endpoints below
        if api_base.endswith('/v1'):
            api_base = api_base[:-3]
        self.api_base = api_base
        # Embeddings may use a different base (embedding-only server on 8081). Prefer explicit embeddings base setting, fallback to api_base
        emb_base = getattr(settings, 'llm_embeddings_api_base', '') or settings.llm_api_base
        emb_base = str(emb_base).rstrip('/')
        if emb_base.endswith('/v1'):
            emb_base = emb_base[:-3]
        self.embeddings_base = emb_base
        # Resolve model name defensively to handle different possible config names
        # Some environments may provide either `llm_model_name` or `llm_model`.
        self.model = getattr(settings, 'llm_model_name', getattr(settings, 'llm_model', ''))
        self.model_path = settings.llm_model_path
        self.client = httpx.AsyncClient(timeout=settings.llm_timeout)

        # Chat template configuration
        # Use the resolved template which can auto-detect based on model name
        resolved_template = getattr(settings, 'resolved_chat_template', 'openai')
        self.chat_template_name = resolved_template
        self.chat_template = chat_template_manager.get_template(self.chat_template_name)
        
        # Lazy-load local model if needed
        self._local_llm = None
        self._use_local = False
        self._local_llm_embedding_enabled = False
        self._detected_model = None
        self._model_detection_attempted = False
        # Embeddings-specific detection
        self._detected_embeddings_model = None
        self._embeddings_model_detection_attempted = False
        # Detected server context size (tokens). May be populated by parsing API errors
        self._detected_server_context_size: Optional[int] = None
        # Force remote usage flag (skip local fallback)
        self.force_remote_api: bool = False
    
    async def detect_model(self) -> str:
        """
        Detect the actual model running on the API server.
        Makes a GET request to /v1/models endpoint.
        Returns: Model name or falls back to configured name.
        """
        if self._model_detection_attempted:
            return self._detected_model or self.model
        
        self._model_detection_attempted = True
        
        try:
            # Try to get models list from API
            response = await self.client.get(f"{self.api_base}/models")
            response.raise_for_status()
            result = response.json()
            
            if "data" in result and len(result["data"]) > 0:
                # Get first model or find best match
                models = result["data"]
                if isinstance(models[0], dict) and "id" in models[0]:
                    self._detected_model = models[0]["id"]
                    print(f"‚úÖ Detected model: {self._detected_model}")
                    try:
                        md = models[0]
                        context_keys = ['n_ctx_train', 'n_ctx', 'context', 'context_window', 'max_input_tokens', 'max_context_tokens', 'max_tokens']
                        for ck in context_keys:
                            if ck in md and isinstance(md[ck], int):
                                self._detected_server_context_size = md[ck]
                                print(f"üîé Detected server context size via model metadata: {self._detected_server_context_size}")
                                break
                    except Exception:
                        pass
                    return self._detected_model
        except Exception as e:
            print(f"‚ö†Ô∏è  Model detection failed: {e}")
        
        # Fallback to configured model
        self._detected_model = self.model
        print(f"üìã Using configured model: {self._detected_model}")
        return self._detected_model

    async def detect_embeddings_model(self) -> str:
        """
        Detect the model served by the embeddings base. Falls back to `settings.llm_embeddings_model_name` or general model.
        """
        if self._embeddings_model_detection_attempted:
            return self._detected_embeddings_model or settings.llm_embeddings_model_name or self.model
        self._embeddings_model_detection_attempted = True
        try:
            response = await self.client.get(f"{self.embeddings_base}/models")
            response.raise_for_status()
            result = response.json()
            if "data" in result and len(result["data"]) > 0:
                models = result["data"]
                if isinstance(models[0], dict) and "id" in models[0]:
                    self._detected_embeddings_model = models[0]["id"]
                    # Attempt to parse context window (tokens) from model metadata
                    md = models[0]
                    # Look for typical fields
                    context_keys = [
                        'n_ctx_train', 'n_ctx', 'context', 'context_window', 'max_input_tokens', 'max_context_tokens', 'max_tokens'
                    ]
                    for ck in context_keys:
                        if ck in md and isinstance(md[ck], int):
                            self._detected_server_context_size = md[ck]
                            print(f"üîé Detected embeddings model context tokens via model metadata: {self._detected_server_context_size}")
                            break
                    # If not found, attempt a details endpoint for the model
                    if not self._detected_server_context_size:
                        try:
                            model_detail_resp = await self.client.get(f"{self.embeddings_base}/models/{self._detected_embeddings_model}")
                            model_detail_resp.raise_for_status()
                            detail_json = model_detail_resp.json()
                            if isinstance(detail_json, dict):
                                for ck in context_keys:
                                    if ck in detail_json and isinstance(detail_json[ck], int):
                                        self._detected_server_context_size = detail_json[ck]
                                        print(f"üîé Detected embeddings model context tokens via model detail endpoint: {self._detected_server_context_size}")
                                        break
                        except Exception:
                            pass
                    print(f"‚úÖ Detected embeddings model: {self._detected_embeddings_model}")
                    return self._detected_embeddings_model
        except Exception as e:
            print(f"‚ö†Ô∏è  Embeddings model detection failed: {e}")
        # Fallback to configured embedding model or general model
        if settings.llm_embeddings_model_name:
            self._detected_embeddings_model = settings.llm_embeddings_model_name
            print(f"üìã Using configured embeddings model: {self._detected_embeddings_model}")
            return self._detected_embeddings_model
        self._detected_embeddings_model = self._detected_model or self.model
        print(f"üìã Using model for embeddings: {self._detected_embeddings_model}")
        return self._detected_embeddings_model
    
    def get_model_name(self) -> str:
        """Get the detected or configured model name (non-async version for printing)"""
        if self._detected_model:
            return self._detected_model
        return self.model
    
    def _init_local_model(self):
        """Initialize local GGUF model (lazy loading)"""
        if self._local_llm is not None:
            return
        
        try:
            from llama_cpp import Llama
            
            if not os.path.exists(self.model_path):
                print(f"‚ö†Ô∏è  Model not found: {self.model_path}")
                return
            
            print(f"üîß Loading local GGUF model: {self.model_path}")
            # Use setting to control whether the local model exposes embedding() API
            enable_embedding = getattr(settings, 'llm_local_embeddings', True)
            self._local_llm = Llama(
                model_path=self.model_path,
                n_ctx=settings.llm_context_size,
                n_gpu_layers=settings.llm_gpu_layers,
                n_threads=settings.llm_threads,
                verbose=False
                , embedding=enable_embedding
            )
            self._local_llm_embedding_enabled = enable_embedding
            print(f"‚úÖ Local model loaded")
        except ImportError:
            print("‚ö†Ô∏è  llama-cpp-python not installed. Install with: pip install llama-cpp-python")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load local model: {e}")

    def _parse_context_size_from_error(self, err_msg: str) -> Optional[int]:
        """
        Parse server error messages to extract numeric context size hints (n_ctx or n_ctx_slot).
        Returns numeric int context size if found, otherwise None.
        """
        if not err_msg:
            return None
        try:
            m = re.search(r"n_ctx_slot\s*[=:\s]\s*(\d+)", err_msg)
            if m:
                return int(m.group(1))
            m2 = re.search(r"context\s*(?:size|window)\s*:?\s*(\d+)", err_msg)
            if m2:
                return int(m2.group(1))
            m3 = re.search(r"task\.n_tokens\s*[=]\s*(\d+)", err_msg)
            if m3:
                return int(m3.group(1))
        except Exception:
            return None
        return None
    
    async def generate(self,
                      prompt: str,
                      max_tokens: Optional[int] = None,
                      temperature: float = None,
                      system_prompt: Optional[str] = None,
                      tools: Optional[List[Dict]] = None) -> str:
        """
        Generate completion using API server or local model.
        Automatically falls back to local if API fails.
        """
        temperature = temperature if temperature is not None else settings.llm_temperature
        max_tokens = max_tokens or settings.llm_max_tokens

        # Try API server first if not forced to use local
        api_exc = None
        if not self._use_local or self.force_remote_api:
            try:
                return await self._generate_api(prompt, max_tokens, temperature, system_prompt, tools)
            except Exception as e:
                api_exc = e
                print(f"‚ö†Ô∏è  API failed: {e}")
                # If we are forcing the remote API, do not fall back to local model
                if self.force_remote_api:
                    raise
                print(f"   Attempting fallback to local model...")

        # Try local model fallback (only if API failed or local use requested)
        try:
            return await self._generate_local(prompt, max_tokens, temperature, system_prompt)
        except Exception as local_exc:
            print(f"‚ö†Ô∏è  Local model failed: {local_exc}")
            # If there was also an API failure, raise a combined error for easier debugging
            if api_exc:
                raise RuntimeError(f"API error: {api_exc}; Local error: {local_exc}")
            # Otherwise, re-raise the local exception
            raise

    async def get_embeddings(self, texts: list[str] | str):
        """Return embeddings for a list of texts or single text using API or local model.

        Returns: List[List[float]] if input is list, else List[float] for single string.
        """
        if isinstance(texts, str):
            inputs = [texts]
        else:
            inputs = texts

        # Try using API
        try:
            # Determine the model for embeddings explicitly
            # 1) prefer detected embeddings model, 2) configured embedding model, 3) detected model for api_base, 4) fallback config
            model_for_embeddings = None
            if self._detected_embeddings_model:
                model_for_embeddings = self._detected_embeddings_model
            else:
                # try to detect embeddings model from embeddings base
                try:
                    model_for_embeddings = await self.detect_embeddings_model()
                except Exception:
                    model_for_embeddings = settings.llm_embeddings_model_name or self._detected_model or self.model

            payload = {"model": model_for_embeddings, "input": inputs}
            # Use dedicated embeddings base if available
            resp = await self.client.post(f"{self.embeddings_base}/v1/embeddings", json=payload)
            resp.raise_for_status()
            data = resp.json()
            # Data likely has structure {'data': [{'embedding': [...]}, ...]}
            if isinstance(data, dict) and "data" in data:
                embeddings = [d.get("embedding") for d in data["data"]]
                return embeddings
        except httpx.HTTPStatusError as http_ex:
            try:
                body_str = http_ex.response.text
            except Exception:
                body_str = "<no response body>"
            server_message = None
            try:
                body_json = http_ex.response.json()
                if isinstance(body_json, dict):
                    server_message = body_json.get('message') or body_json.get('error')
            except Exception:
                body_json = None
            # Try to parse n_ctx from the message if available
            n_ctx = None
            if server_message:
                n_ctx = self._parse_context_size_from_error(server_message)
            print(f"‚ö†Ô∏è  Embeddings API failed: {http_ex} (status={http_ex.response.status_code}): {body_str}")
            # Save any detected context size for adaptive chunking
            if n_ctx and not self._detected_server_context_size:
                self._detected_server_context_size = n_ctx
                print(f"üîé Detected server context size via embeddings API error: {n_ctx}")
            # If fallback is disabled, raise structured error for upstream re-chunking logic
            if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
                raise EmbeddingsAPIError("Embeddings API failed", status_code=http_ex.response.status_code, body=body_json or body_str, server_message=server_message, n_ctx=n_ctx)
        except Exception as e:
            # Generic exception - try to parse context size if possible and raise a structured error
            server_message = None
            n_ctx = None
            try:
                server_message = str(e)
                n_ctx = self._parse_context_size_from_error(server_message)
                if n_ctx and not self._detected_server_context_size:
                    self._detected_server_context_size = n_ctx
            except Exception:
                pass
            print(f"‚ö†Ô∏è  Embeddings API failed: {e}")
            if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
                raise EmbeddingsAPIError("Embeddings API failed", status_code=None, body=None, server_message=server_message, n_ctx=n_ctx)

        # Fallback to local model embeddings; llama-cpp-python may not expose embeddings method
        if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
            raise RuntimeError("Embeddings API failed and local fallback for embeddings is disabled")
        try:
            # if we haven't initialized the local model yet, load it
            self._init_local_model()
            # Ensure local model is embedding-enabled when we want to call embed()
            if not self._local_llm_embedding_enabled:
                # try re-initializing with embeddings enabled
                print("üîß Reinitializing local model with embedding support")
                try:
                    # Force recreate with embedding enabled
                    # Destroy previous instance reference first
                    self._local_llm = None
                    # Call init to create with the setting in config
                    self._init_local_model()
                except Exception:
                    pass
            if self._local_llm is not None:
                # llama-cpp-python may expose an embeddings API in newer versions as embed()
                if hasattr(self._local_llm, "embed"):
                    res = self._local_llm.embed(inputs)
                    # Expect res to be list of embeddings
                    return res
        except Exception as e:
            print(f"‚ö†Ô∏è  Local embedding failed: {e}")

        raise RuntimeError("No embeddings method available (API or local model)")

    async def get_embeddings_for_documents(self, texts: list[str], chunk_size: Optional[int] = None, batch_size: int | None = None, min_batch: int = 1, delay: float = 0.15, max_retries: int = 3, chars_per_token: Optional[int] = None, max_chunk_tokens: Optional[int] = None):
        """
        Obtain a single embedding vector per document, even when documents are longer than `chunk_size`.
        Strategy:
          - Split each document into chunks up to `chunk_size` characters
          - Call get_embeddings() for all chunks in adaptive batches to avoid server 500s
          - Average embeddings for all chunks belonging to a document to obtain a final vector
        Returns: list[embedding] aligned to input `texts` (None where embedding failed)
        """
        if not texts:
            return []

        # Determine chunk_size in characters from token-based context detection if needed
        if chars_per_token is None:
            chars_per_token = getattr(settings, 'llm_chars_per_token', 4)
        detected_n_ctx = max_chunk_tokens or self._detected_server_context_size or getattr(settings, 'llm_context_size', None)
        if detected_n_ctx is None:
            detected_n_ctx = 4096
        if chunk_size is None:
            # Use a configurable fraction of the embeddings model context for chunking
            ratio = getattr(settings, 'llm_chunk_context_ratio', 0.5)
            tokens_per_chunk = max(64, int(math.floor(detected_n_ctx * float(ratio))))
            char_chunk_size = int(tokens_per_chunk * chars_per_token)
            # Cap by the configured embeddings default chunk size to avoid excessively large requests
            default_char_chunk = getattr(settings, 'llm_embeddings_chunk_size_default', 4096)
            chunk_size = min(default_char_chunk, char_chunk_size)
            print(f"üîß Computed chunk_size: {chunk_size} chars (tokens_per_chunk={tokens_per_chunk}, detected_n_ctx={detected_n_ctx}, chars_per_token={chars_per_token}, ratio={ratio})")

        # Build chunks per document
        docs_chunks = []  # list of lists
        chunk_to_doc_index = []  # flattened mapping index->doc_idx
        all_chunks = []
        for doc_idx, text in enumerate(texts):
            if text is None:
                docs_chunks.append([])
                continue
            if len(text) <= chunk_size:
                docs_chunks.append([text])
                all_chunks.append(text)
                chunk_to_doc_index.append(doc_idx)
            else:
                # split by whitespace preserving words across chunks to avoid cutting words (basic approach)
                chunks = []
                start = 0
                while start < len(text):
                    end = min(start + chunk_size, len(text))
                    # try to break on last whitespace if possible (avoid tokenization here)
                    if end < len(text):
                        wh = text.rfind(' ', start, end)
                        if wh > start:
                            end = wh
                    chunk = text[start:end]
                    chunks.append(chunk)
                    all_chunks.append(chunk)
                    chunk_to_doc_index.append(doc_idx)
                    start = end
                docs_chunks.append(chunks)

        # Resolve batch_size default from settings if None
        if batch_size is None:
            batch_size = getattr(settings, 'llm_embeddings_default_batch_size', 4)

        # Function to embed in batches with graceful shrinking on failure
        async def _split_text_into_smaller_chunks(text, target_size):
            # Splits `text` into chunks <= target_size by splitting on whitespace around midpoint.
            if len(text) <= target_size:
                return [text]
            # naive splitting by whitespace, try to preserve words
            mid = len(text) // 2
            left_break = text.rfind(' ', 0, mid)
            right_break = text.find(' ', mid, len(text))
            if left_break == -1 and right_break == -1:
                # no whitespace found; just split at mid
                left = text[:mid]
                right = text[mid:]
            else:
                # prefer nearest break
                if left_break == -1:
                    split_at = right_break
                elif right_break == -1:
                    split_at = left_break
                else:
                    # pick the break closer to mid
                    split_at = left_break if (mid - left_break) <= (right_break - mid) else right_break
                left = text[:split_at]
                right = text[split_at:].lstrip()
            res_left = await _split_text_into_smaller_chunks(left, target_size)
            res_right = await _split_text_into_smaller_chunks(right, target_size)
            return res_left + res_right

        # Use class-level context parser

        async def _embed_all_chunks(chunk_list, initial_batch=batch_size, min_batch=min_batch, delay_time=delay, max_retries_local=max_retries):
            n = len(chunk_list)
            results = [None] * n
            i = 0
            cur_batch = initial_batch
            while i < n:
                if cur_batch <= 0:
                    cur_batch = 1
                end = min(i + cur_batch, n)
                batch = chunk_list[i:end]
                try:
                    # call underlying get_embeddings which returns list of embeddings aligned with input
                    # Debug: show attempt
                    # print(f"üîç Embedding batch size {len(batch)} (starting idx {i})")
                    embs = await self.get_embeddings(batch)
                    if not embs:
                        # treat as failure to trigger shrinking
                        raise RuntimeError("Empty embeddings returned")
                    for j, emb in enumerate(embs):
                        results[i + j] = emb
                    i = end
                    await asyncio.sleep(delay_time)
                except Exception as e:
                    err_txt = str(e).lower()
                    # Attempt to parse server context size and record it to adaptively backoff
                    detected_ctx = None
                    if isinstance(e, EmbeddingsAPIError) and getattr(e, 'n_ctx', None):
                        detected_ctx = e.n_ctx
                    else:
                        detected_ctx = self._parse_context_size_from_error(str(e))
                    if detected_ctx and not self._detected_server_context_size:
                        self._detected_server_context_size = detected_ctx
                        print(f"üîé Detected server context size: {detected_ctx}")
                    # If the server returned a structured error saying the request is too large and provided n_ctx, resplit using tokens-based chunking
                    if isinstance(e, EmbeddingsAPIError) and getattr(e, 'n_ctx', None):
                        n_ctx = e.n_ctx
                        # Only proceed if the server-reported context is smaller than our current chunking
                        # Convert current chunk_size (chars) back to token-estimate to compare
                        cur_token_estimate = max(1, int(chunk_size / max(1, chars_per_token)))
                        if n_ctx < cur_token_estimate:
                            print(f"üîÅ Rebuilding chunks: server context {n_ctx} smaller than current chunk token estimate {cur_token_estimate}")
                            # compute new chunk_size based on n_ctx
                            new_tokens_per_chunk = max(64, int(math.floor(n_ctx * 0.8)))
                            new_chunk_chars = int(new_tokens_per_chunk * chars_per_token)
                            # Re-run with new chunk size
                            return await self.get_embeddings_for_documents(texts, chunk_size=new_chunk_chars, batch_size=initial_batch, min_batch=min_batch, delay=delay_time, max_retries=max_retries_local, chars_per_token=chars_per_token, max_chunk_tokens=n_ctx)
                    # If server returned a size-related error, try to re-chunk the offending content into smaller text chunks
                    if 'too large' in err_txt or 'increase the physical batch size' in err_txt or 'exceeds the available context size' in err_txt:
                        print(f"‚ö†Ô∏è  Embedding server indicates request too large: {e}. Attempting to split chunks further.")
                        # Re-chunk the current batch: replace each chunk with multiple smaller ones and try them individually
                        # Use configured backoff sequence if adaptive backoff is enabled
                        backoff_seq = getattr(settings, 'llm_embeddings_chunk_backoff_sequence', [4096, 2048, 1024, 512, 256, 128])
                        backoff_seq = sorted(list(dict.fromkeys(backoff_seq)), reverse=True)
                        # Ensure monotonic decreasing order and filter sizes smaller than current chunk_size
                        cur_chunk_size = chunk_size
                        # If server context size known, use that to cap backoff sizes
                        if self._detected_server_context_size and getattr(settings, 'llm_embeddings_adaptive_backoff_enabled', True):
                            # heuristically convert tokens to approximate chars using factor 4
                            approx_chars = max(128, int(self._detected_server_context_size * 4))
                            backoff_seq = [s for s in backoff_seq if s <= approx_chars]
                            if not backoff_seq:
                                backoff_seq = [max(128, approx_chars // 4)]

                        for j_local, original_chunk in enumerate(batch):
                            if not original_chunk:
                                continue
                            doc_idx_local = i + j_local
                            # attempt to split this chunk into smaller character-based subchunks using backoff sequence
                            smaller_chunks = None
                            for target_size in backoff_seq:
                                if target_size >= len(original_chunk):
                                    # no-op, chunk is small enough
                                    continue
                                try:
                                    smaller = await _split_text_into_smaller_chunks(original_chunk, target_size)
                                    if len(smaller) > 1:
                                        smaller_chunks = smaller
                                        break
                                except Exception:
                                    continue
                            if smaller_chunks is None:
                                # fall back to simple halving if we couldn't find a split in the sequence
                                try:
                                    smaller_chunks = await _split_text_into_smaller_chunks(original_chunk, max(128, len(original_chunk) // 2))
                                except Exception:
                                    smaller_chunks = [original_chunk]
                            if len(smaller_chunks) == 1:
                                # couldn't split, we'll let normal per-item retry handle
                                continue
                            # Try to embed these smaller chunks
                            # Note: we'll call get_embeddings directly with smaller chunks
                            try:
                                sub_embs = await self.get_embeddings(smaller_chunks)
                                # average back to a single vector
                                if sub_embs and len(sub_embs) > 0:
                                    vec_len = len(sub_embs[0])
                                    sum_vec = [0.0]*vec_len
                                    for sv in sub_embs:
                                        if not sv:
                                            continue
                                        for vi in range(len(sv)):
                                            sum_vec[vi] += sv[vi]
                                    avg_vec = [x/len(sub_embs) for x in sum_vec]
                                    # assign back
                                    results[doc_idx_local] = avg_vec
                                else:
                                    # fallthrough to shrinking behavior below
                                    pass
                            except Exception as esub:
                                # If the re-chunk attempt fails, we'll fall back to shrinking batch size
                                print(f"‚ö†Ô∏è  Subchunk embedding failed: {esub}")
                                pass
                        # Continue loop but shrink batch if necessary
                    # shrink batch if possible
                    if cur_batch > min_batch:
                        old_batch = cur_batch
                        cur_batch = max(min_batch, cur_batch // 2)
                        # double wait to be polite when we hit errors
                        await asyncio.sleep(delay_time * 2)
                        continue
                    # min-batch failing - try per item with retries
                    for j in range(i, end):
                        tries = 0
                        while tries < max_retries_local:
                            try:
                                em = await self.get_embeddings([chunk_list[j]])
                                results[j] = (em[0] if isinstance(em, list) and len(em) > 0 else None)
                                break
                            except Exception as e2:
                                tries += 1
                                await asyncio.sleep(delay_time * (tries + 1))
                        if tries >= max_retries_local and results[j] is None:
                            # give up this chunk
                            results[j] = None
                    i = end
            return results

        # Embed all chunks
        chunk_embeddings = await _embed_all_chunks(all_chunks, initial_batch=batch_size, min_batch=min_batch, delay_time=delay, max_retries_local=max_retries)

        # Now aggregate per document by averaging
        doc_embeddings = []
        # build list of lists per doc
        per_doc_embs = [[] for _ in range(len(texts))]
        for idx, emb in enumerate(chunk_embeddings):
            doc_idx = chunk_to_doc_index[idx]
            if emb is not None:
                per_doc_embs[doc_idx].append(emb)

        for doc_chunks_emb in per_doc_embs:
            if not doc_chunks_emb:
                doc_embeddings.append(None)
            else:
                # average the vectors elementwise
                length = len(doc_chunks_emb)
                # handle variable-length vectors unlikely but guard
                vec_len = len(doc_chunks_emb[0])
                sum_vec = [0.0] * vec_len
                for v in doc_chunks_emb:
                    if not v:
                        continue
                    for i in range(vec_len):
                        sum_vec[i] += v[i]
                avg_vec = [x / length for x in sum_vec]
                doc_embeddings.append(avg_vec)

        return doc_embeddings
    
    async def _generate_api(self,
                           prompt: str,
                           max_tokens: int,
                           temperature: float,
                           system_prompt: Optional[str],
                           tools: Optional[List[Dict]] = None) -> str:
        """Generate using API server (llama.cpp, LM Studio, etc.)"""

        # Detect model if not already done
        if not self._model_detection_attempted:
            await self.detect_model()

        # Use chat template to format the conversation
        messages = [{"role": "user", "content": prompt}]
        formatted_input = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)

        # Determine API endpoint and payload format based on chat template
        # For templates that generate raw prompts (not OpenAI-style messages), use /completions endpoint
        if self.chat_template_name in ["qwen3", "qwen3-thinking", "gemma", "gemma2", "gemma3", "llama", "llama2", "llama3", "mistral", "phi3", "chatml"]:
            # For these templates, use the formatted input as a single prompt
            payload = {
                "model": self._detected_model or self.model,
                "prompt": formatted_input,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p
            }
            # Use the basic /completions endpoint for raw prompt inputs
            api_endpoint = f"{self.api_base}/completions"
        else:
            # For standard OpenAI format (or when template is unknown/default), use messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self._detected_model or self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p
            }
            # Use the standard /chat/completions endpoint for message-based inputs
            api_endpoint = f"{self.api_base}/chat/completions"

        # If stop tokens are configured, include them in the API payload
        if getattr(settings, 'llm_stop_tokens', None):
            payload["stop"] = settings.llm_stop_tokens

        model_display = self._detected_model or self.model
        print(f"üîç Sending to LLM API:")
        print(f"   URL: {api_endpoint}")
        print(f"   Model: {model_display} (detected: {self._detected_model is not None})")
        print(f"   Template: {self.chat_template_name}")
        print(f"   Endpoint: {api_endpoint.split('/')[-1]}")
        if "messages" in payload:
            print(f"   Messages: {len(payload['messages'])} messages")
        else:
            print(f"   Prompt length: {len(payload['prompt'])} chars")
        print(f"   Payload: {payload}")

        try:
            response = await self.client.post(
            api_endpoint,
            json=payload
        )
            response.raise_for_status()
        except httpx.HTTPStatusError as http_err:
            # Detect llama.cpp style context error and raise a specialized exception
            # The server returns a 400 with a message like: "the request exceeds the available context size, try increasing it"
            txt = http_err.response.text or ""
            try:
                j = http_err.response.json()
                if isinstance(j, dict) and j.get("error"):
                    txt = j.get("error")
            except Exception:
                pass
            # Try to parse numbers like 'n_ctx_slot = 8192' and 'task.n_tokens = 10225'
            n_ctx = None
            m_ctx = re.search(r"n_ctx_slot\s*=\s*(\d+)", txt)
            if m_ctx:
                n_ctx = int(m_ctx.group(1))
                self._detected_server_context_size = n_ctx
            if "exceeds the available context size" in txt.lower() or "request exceeds the available context size" in txt.lower():
                raise ContextSizeExceededError(f"Context too large trying to use model; details: {txt}", n_ctx=n_ctx, server_message=txt)
            # If no special handling, re-raise
            raise

        try:
            result = response.json()
            print(f"üîç API Response: {result}")

            # Handle OpenAI format (gpt models)
            if "choices" in result and len(result["choices"]) > 0:
                choice = result["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"]
                    if content:
                        return content

            # If we get here, response was malformed
            print(f"‚ö†Ô∏è  Unexpected response format: {result}")
            return ""
        except (KeyError, ValueError) as e:
            print(f"‚ùå Failed to parse API response: {e}")
            print(f"   Raw response: {response.text}")
            return ""


    async def _generate_local(self,
                             prompt: str,
                             max_tokens: int,
                             temperature: float,
                             system_prompt: Optional[str],
                             tools: Optional[List[Dict]] = None) -> str:
        """Generate using local GGUF model"""
        self._init_local_model()

        if self._local_llm is None:
            raise RuntimeError("Neither API nor local model available")

        # Use chat template to format the conversation for local generation
        messages = [{"role": "user", "content": prompt}]
        if self.chat_template_name in ["qwen3"]:
            # For Qwen3 template with local model, format as a single prompt
            full_prompt = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)
        else:
            # Standard format for local model
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"

        # Generate (synchronous call, but we're in async context)
        # Note: llama-cpp-python is sync, so we just call it directly
        # In production, might want to use asyncio.to_thread()
        output = self._local_llm(
            full_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=settings.llm_top_p,
            stop=getattr(settings, 'llm_stop_tokens', None),
            echo=False
        )

        return output["choices"][0]["text"].strip()


    async def stream_generate(self,
                             prompt: str,
                             max_tokens: Optional[int] = None,
                             temperature: float = None,
                             system_prompt: Optional[str] = None,
                             tools: Optional[List[Dict]] = None):
        """Stream generation token-by-token using API server."""
        temperature = temperature if temperature is not None else settings.llm_temperature
        max_tokens = max_tokens or settings.llm_max_tokens

        if not self._model_detection_attempted:
            await self.detect_model()

        # Use chat template to format the conversation for streaming
        messages = [{"role": "user", "content": prompt}]
        formatted_input = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)

        # Determine API endpoint and payload format based on chat template
        # For templates that generate raw prompts (not OpenAI-style messages), use /completions endpoint
        if self.chat_template_name in ["qwen3", "qwen3-thinking", "gemma", "gemma2", "gemma3", "llama", "llama2", "llama3", "mistral", "phi3", "chatml"]:
            # For these templates, use the formatted input as a single prompt
            payload = {
                "model": self._detected_model or self.model,
                "prompt": formatted_input,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p,
                "stream": True
            }
            # Use the basic /completions endpoint for raw prompt inputs
            api_endpoint = f"{self.api_base}/completions"
        else:
            # For standard OpenAI format (or when template is unknown/default), use messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self._detected_model or self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p,
                "stream": True
            }
            # Use the standard /chat/completions endpoint for message-based inputs
            api_endpoint = f"{self.api_base}/chat/completions"

        if getattr(settings, 'llm_stop_tokens', None):
            payload["stop"] = settings.llm_stop_tokens

        print(f"üîç Streaming from LLM API...")
        print(f"   Template: {self.chat_template_name}")
        print(f"   Endpoint: {api_endpoint.split('/')[-1]}")
        if "messages" in payload:
            print(f"   Messages: {len(payload['messages'])} messages")
        else:
            print(f"   Prompt length: {len(payload['prompt'])} chars")

        async with self.client.stream(
            "POST",
            api_endpoint,
            json=payload
        ) as response:
            # If the server returns an error status, attempt to read the body
            try:
                response.raise_for_status()
            except Exception as e:
                # Guarded reading of the error body to preserve debug visibility
                try:
                    text = await response.aread()
                    logger.error(f"LLM server error {response.status_code}: {text}")
                except Exception:
                    logger.error(f"LLM server returned {response.status_code} without readable body")
                raise

            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data_str = line[6:]
                    if data_str == "[DONE]":
                        break
                    try:
                        data = json.loads(data_str)
                        if data.get("choices"):
                            delta = data["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield content
                    except json.JSONDecodeError:
                        continue


    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()


class ContextSizeExceededError(Exception):
    def __init__(self, message: str, n_ctx: Optional[int] = None, server_message: Optional[str] = None):
        super().__init__(message)
        self.n_ctx = n_ctx
        self.server_message = server_message


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\llm.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\main.py (Section: BACKEND_PYTHON) ---

"""Minimal, single-entrypoint for ECE_Core.

This file uses `src.app_factory.create_app_with_routers()` to construct the app; the factory
ensures routers are included and avoids initialization side effects at import time.
"""
from src.app_factory import create_app_with_routers
from src.config import settings
import logging

logging.basicConfig(level=getattr(logging, settings.ece_log_level), format='%(asctime)s - %(levelname)s - %(message)s')

app = create_app_with_routers()


if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host=settings.ece_host, port=settings.ece_port)


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\main.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_client.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""
Minimal MCP client for ECE_Core to call external MCP servers.
This is used sparingly in ECE_Core when plugin manager isn't available, or when tools are offloaded.
"""
from __future__ import annotations

from typing import Any, Dict, Optional
import httpx
from src.config import settings


class MCPClient:
    def __init__(self, base_url: Optional[str] = None, api_key: Optional[str] = None, timeout: int = 10):
        if base_url:
            self.base_url = base_url.rstrip('/')
        elif getattr(settings, 'mcp_url', None):
            self.base_url = settings.mcp_url.rstrip('/')
        else:
            self.base_url = f"http://{settings.mcp_host}:{settings.mcp_port}"
        self.api_key = api_key or settings.mcp_api_key or settings.ece_api_key
        self._timeout = timeout

    def _headers(self) -> Dict[str, str]:
        h = {"Content-Type": "application/json"}
        if self.api_key:
            h["Authorization"] = f"Bearer {self.api_key}"
        return h

    async def get_tools(self) -> Any:
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            r = await client.get(f"{self.base_url}/mcp/tools", headers=self._headers())
            r.raise_for_status()
            return r.json()

    async def call_tool(self, name: str, **arguments) -> Any:
        payload = {"name": name, "arguments": arguments}
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            r = await client.post(f"{self.base_url}/mcp/call", json=payload, headers=self._headers())
            if r.status_code >= 400:
                return {"status": "error", "status_code": r.status_code, "error": r.text}
            return r.json()


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_client.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_runner.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""Small runner that uses `src.config.settings` to start uvicorn with configured host/port.

This avoids hardcoding in start scripts and allows values from config.yaml or env vars to drive MCP server startup.
"""
from __future__ import annotations

from src.config import settings
import uvicorn


def main():
    if not settings.mcp_enabled:
        print("MCP server is disabled in configuration. Set MCP_ENABLED to true in env or configs/config.yaml to start the MCP server.")
        # Print a hint showing the current derived settings so users can debug quickly
        try:
            print(f"Current settings: mcp_url={settings.mcp_url}, mcp_host={settings.mcp_host}, mcp_port={settings.mcp_port}")
        except Exception:
            pass
        return
    host = settings.mcp_host
    port = int(settings.mcp_port)
    # Print a friendly startup message containing config-derived values
    try:
        src = settings.mcp_url or f"{settings.mcp_host}:{settings.mcp_port}"
        print(f"Starting MCP server using settings derived from YAML/env: {src} (enabled={settings.mcp_enabled})")
    except Exception:
        print(f"Starting MCP server on {host}:{port}")
    uvicorn.run("src.mcp_server:app", host=host, port=port, log_level=settings.ece_log_level.lower())


if __name__ == "__main__":
    main()


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_runner.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_server.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""
Minimal MCP server for ECE memory graph
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional
import asyncio

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from src.memory.neo4j_store import Neo4jStore
from src.config import settings
from fastapi import Request

logger = logging.getLogger(__name__)
app = FastAPI(title="ECE MCP Server")


class ToolSchema(BaseModel):
    name: str
    description: str
    inputSchema: Dict[str, Any]


class ToolCall(BaseModel):
    name: str
    arguments: Dict[str, Any]


ADD_MEMORY_TOOL = ToolSchema(
    name="add_memory",
    description="Add a memory node into ECE's Neo4j store",
    inputSchema={
        "type": "object",
        "properties": {
            "session_id": {"type": "string"},
            "content": {"type": "string"},
            "category": {"type": "string"},
            "tags": {"type": "array", "items": {"type": "string"}},
            "importance": {"type": "number"},
            "metadata": {"type": "object"},
            "entities": {"type": "array", "items": {"type": "object"}}
        },
        "required": ["session_id", "content", "category"]
    },
)

SEARCH_MEMORIES_TOOL = ToolSchema(
    name="search_memories",
    description="Search memories using the Neo4j store",
    inputSchema={
        "type": "object",
        "properties": {
            "query": {"type": "string"},
            "category": {"type": "string"},
            "limit": {"type": "number"}
        },
        "required": ["query"]
    },
)

GET_RECENT_SUMMARIES_TOOL = ToolSchema(
    name="get_summaries",
    description="Get recent session summaries",
    inputSchema={
        "type": "object",
        "properties": {
            "session_id": {"type": "string"},
            "limit": {"type": "number"}
        },
        "required": ["session_id"]
    },
)


_neo4j_store: Optional[Neo4jStore] = None

# Alias map for user-facing tools (Cline / other clients often use 'read_memory'/'write_memory')
TOOL_ALIASES = {
    "write_memory": ADD_MEMORY_TOOL.name,
    "read_memory": SEARCH_MEMORIES_TOOL.name,
    "get_memory_summaries": GET_RECENT_SUMMARIES_TOOL.name,
}


@app.on_event("startup")
async def _startup_event() -> None:
    global _neo4j_store
    _neo4j_store = Neo4jStore()
    try:
        await _neo4j_store.initialize()
    except Exception as e:
        logger.warning(f"Failed to initialize Neo4jStore: {e}")


@app.on_event("shutdown")
async def _shutdown_event() -> None:
    global _neo4j_store
    if _neo4j_store:
        await _neo4j_store.close()


@app.get("/mcp/tools")
async def list_tools(request: Request):
    # Enforce bearer token if required
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")
    # Expose canonical tools and also alias tools so clients using common names can discover them
    canonical = [ADD_MEMORY_TOOL.dict(), SEARCH_MEMORIES_TOOL.dict(), GET_RECENT_SUMMARIES_TOOL.dict()]
    aliases = []
    # Build alias schemas by copying canonical body but forcing name/description
    for alias_name, canonical_name in TOOL_ALIASES.items():
        for c in canonical:
            if c["name"] == canonical_name:
                schema = c.copy()
                schema["name"] = alias_name
                schema["description"] = f"Alias for {canonical_name}"
                aliases.append(schema)
                break
    return {"tools": canonical + aliases}


@app.post("/mcp/call")
async def call_tool(tool_call: ToolCall, request: Request):
    global _neo4j_store
    if not _neo4j_store:
        raise HTTPException(status_code=503, detail="Neo4j store not initialized")
    # Enforce bearer token if required
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")

    try:
        # Resolve aliases to canonical tool name
        name = TOOL_ALIASES.get(tool_call.name, tool_call.name)
        if name == ADD_MEMORY_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.add_memory(
                session_id=p.get("session_id"),
                content=p.get("content"),
                category=p.get("category"),
                tags=p.get("tags", []),
                importance=int(p.get("importance", 5)),
                metadata=p.get("metadata") or {},
                entities=p.get("entities") or [],
            )
            return {"tool": tool_call.name, "status": "success", "result": {"id": result}}

        elif name == SEARCH_MEMORIES_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.search_memories(p.get("query", ""), p.get("category"), int(p.get("limit", 10)))
            return {"tool": tool_call.name, "status": "success", "result": result}

        elif name == GET_RECENT_SUMMARIES_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.get_summaries(str(p.get("session_id")), int(p.get("limit", 5)))
            return {"tool": tool_call.name, "status": "success", "result": result}

        else:
            raise HTTPException(status_code=404, detail=f"Tool not found: {tool_call.name}")

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("MCP call failed")
        return {"tool": tool_call.name, "status": "error", "error": str(e)}


@app.get("/health")
async def health():
    return {"status": "ok", "service": "ECE MCP Server", "active": bool(_neo4j_store and _neo4j_store.neo4j_driver is not None)}


@app.get("/mcp/sse")
async def sse_status(request: Request):
    # Optional SSE endpoint for streaming / agent clients
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")
    try:
        from sse_starlette.sse import EventSourceResponse
    except Exception:
        raise HTTPException(status_code=501, detail="SSE streaming not supported (missing sse_starlette dependency)")

    async def generator():
        # Send an initial status
        i = 0
        while True:
            if await request.is_disconnected():
                break
            yield {"event": "status", "data": f"ok-{i}"}
            i += 1
            await asyncio.sleep(3)

    return EventSourceResponse(generator())


if __name__ == "__main__":
    # When run directly, start the MCP server using the configured host & port
    import uvicorn
    if not settings.mcp_enabled:
        print("MCP server is disabled in configuration. Set MCP_ENABLED to true to start the MCP server.")
    else:
        print(f"Starting MCP server on {settings.mcp_host}:{settings.mcp_port} (derived from settings)")
        uvicorn.run("src.mcp_server:app", host=settings.mcp_host, port=int(settings.mcp_port), log_level=settings.ece_log_level.lower())


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_server.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\models.py (Section: BACKEND_PYTHON) ---

from enum import Enum
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class SourceType(str, Enum):
    GEMINI_CHAT = "GEMINI_CHAT"
    WEB_PAGE = "WEB_PAGE"
    USER_NOTE = "USER_NOTE"
    PDF_DOCUMENT = "PDF_DOCUMENT"

class PlaintextMemory(BaseModel):
    """
    Directive INJ-A1: The foundational atom of the GraphR1 memory system.
    Represents a raw, immutable ingestion event (The 'Page' in GAM).
    """
    uuid: str = Field(default_factory=lambda: str(uuid.uuid4()))
    source_type: SourceType
    source_identifier: str = Field(..., description="Filename, URL, or Session ID")
    ingest_timestamp_utc: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
    original_timestamp_utc: Optional[str] = None
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    version: str = "1.0"

    class Config:
        schema_extra = {
            "example": {
                "uuid": "550e8400-e29b-41d4-a716-446655440000",
                "source_type": "GEMINI_CHAT",
                "source_identifier": "session_12345",
                "ingest_timestamp_utc": "2025-12-06T12:00:00Z",
                "content": "User: Hello\nGemini: Hi there!",
                "metadata": {
                    "author": "Gemini 3",
                    "word_count": 50,
                    "summary": "Greeting exchange"
                }
            }
        }


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\models.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\prompts.py (Section: BACKEND_PYTHON) ---

"""Minimal and robust prompt utilities for ECE Core.

This file intentionally contains a single, minimal `build_system_prompt`
implementation and the small helper functions used in tests. The goal is to
remain tiny, explicit, and stable so runtime loading and tests are reliable.
"""
from datetime import datetime, timezone
import os
from typing import List, Dict, Optional


def build_system_prompt(
    tools_available: bool = False,
    tools_list: Optional[List[Dict]] = None,
    current_datetime: Optional[datetime] = None,
) -> str:
    """Return a concise system prompt emphasizing memory-first guidance.

    The prompt includes a short header, a 'memory-first' guidance, a concise
    instruction on tools, and an optional tools list when `tools_available` is
    True.
    """
    if current_datetime is None:
        current_datetime = datetime.now(timezone.utc)
    # If an explicit CODA_SYSTEM_PROMPT is provided via env (or YAML), prefer that as the system prompt.
    sys_prompt_env = os.getenv('CODA_SYSTEM_PROMPT')
    if sys_prompt_env:
        return sys_prompt_env
    current_date = current_datetime.strftime("%Y-%m-%d")
    current_time = current_datetime.strftime("%H:%M UTC")

    prompt = [f"**CURRENT DATE & TIME: {current_date} {current_time}**"]
    prompt.append("You are an AI assistant with access to the user's personal memory and context.")
    prompt.append("Working with Memory and Context:")
    prompt.append("- Check <retrieved_memory> and <memory> blocks FIRST for recall/summary queries; ONLY treat information enclosed within <memory>...</memory> as verified factual history; if the answer is present there, DO NOT use a tool.")
    prompt.append("- Use tools for real-time external data (web, filesystem, system state) or when memory lacks the needed information.")
    prompt.append("- When invoking a tool, put the tool call on its own line as: TOOL_CALL: tool_name(param1=value1)")
    prompt.append("- IMPORTANT: Do not output chain-of-thought or internal analysis to the user. Use internal channels for thoughts only and produce a single clear final response in natural conversational form. If diagnostics are required, place them in a 'thinking:' channel only when explicitly asked for diagnostics.")

    # SIMPLE HARNESS PROTOCOL
    prompt.append("\n[SIMPLE HARNESS PROTOCOL]")
    prompt.append("If you need to use a basic tool, you may output a single line in one of the following forms (do NOT use XML/JSON for these basic actions):")
    prompt.append("Action: search web query=\"...\"")
    prompt.append("Action: read file path=\"...\"")
    prompt.append("Action: execute cmd command=\"...\"")
    prompt.append("These lines are machine-processable and meant only for deterministic tool calls for small models. If you produce an 'Action:' line, do NOT include extra text on that line.")

    if tools_available and tools_list:
        prompt.append("\n**AVAILABLE TOOLS:**")
        for tool in tools_list:
            name = tool.get('name') if isinstance(tool, dict) else getattr(tool, 'name', 'UNKNOWN')
            desc = tool.get('description', '') if isinstance(tool, dict) else getattr(tool, 'description', '')
            params = ''
            if isinstance(tool, dict) and 'inputSchema' in tool and isinstance(tool['inputSchema'], dict):
                props = tool['inputSchema'].get('properties', {})
                params = ", ".join(props.keys())
            prompt.append(f"- {name}({params}): {desc}")

    prompt.append("Be concise, factual, and only ground answers in verified memory (<memory> tags) or explicit tool output.")
    return "\n".join(prompt)


def build_coda_persona_prompt() -> str:
    """Return a short persona prompt used for persona-specific conversations.

    Tests assert that the persona contains 'Coda C-001' and 'Kaizen'. Keep
    this minimal and stable.
    """
    return (
        "You are Coda C-001, a memory-augmented AI assistant.\n\n"
        "Core Philosophy: Kaizen (continuous improvement); Chutzpah; Shoshin.\n"
        "Communication: Concise, candid, and helpful."
    )


def build_summarization_prompt(text: str, max_tokens: int) -> str:
    return f"Summarize the following into approximately {max_tokens} tokens:\n\n{text}\n\nSummary:"


def build_entity_extraction_prompt(text: str) -> str:
    return (
        "Extract key entities as JSON using categories: PERSON, CONCEPT, PROJECT, CONDITION, SKILL.\n\n"
        f"Text:\n{text}\n\nEntities (JSON):"
    )


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\prompts.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\security.py (Section: BACKEND_PYTHON) ---

"""
Security middleware and utilities for ECE_Core.
Implements API key authentication and audit logging.
"""
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from src.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# API KEY AUTHENTICATION
# ============================================================================

security = HTTPBearer(auto_error=False)

async def verify_api_key(
    credentials: Optional[HTTPAuthorizationCredentials] = Security(security)
) -> bool:
    """
    Verify API key from Authorization header.
    Returns True if authentication is disabled or key is valid.
    Raises HTTPException if authentication is required but fails.
    """
    # If auth not required, allow all requests
    if not settings.ece_require_auth:
        logger.info("Auth check skipped (ece_require_auth=False)")
        return True
    
    # If auth required but no credentials provided
    if credentials is None:
        logger.warning("Auth failed: No credentials provided")
        raise HTTPException(
            status_code=401,
            detail="Authentication required. Provide API key in Authorization header."
        )
    
    # Verify API key
    if credentials.credentials != settings.ece_api_key:
        logger.warning(f"Auth failed: Invalid API key attempt. Received: {credentials.credentials[:4]}...")
        raise HTTPException(
            status_code=403,
            detail="Invalid API key"
        )
    
    return True

# ============================================================================
# AUDIT LOGGING
# ============================================================================

class AuditLogger:
    """Audit logger for security-sensitive operations."""
    
    def __init__(self):
        self.enabled = settings.audit_log_enabled
        self.log_path = Path(settings.audit_log_path)
        
        # Create log directory if needed
        if self.enabled:
            self.log_path.parent.mkdir(parents=True, exist_ok=True)
            # Initialize log file if it doesn't exist
            if not self.log_path.exists():
                self.log_path.touch()
    
    def log(self, event_type: str, details: dict):
        """Log a security event."""
        if not self.enabled:
            return
        
        try:
            timestamp = datetime.now().isoformat()
            log_entry = {
                "timestamp": timestamp,
                "event_type": event_type,
                **details
            }
            
            with open(self.log_path, 'a', encoding='utf-8') as f:
                f.write(f"{log_entry}\n")
            
            # Also log to application logger
            logger.info(f"AUDIT: {event_type} - {details}")
        except Exception as e:
            logger.error(f"Failed to write audit log: {e}")
    
    def log_tool_call(self, session_id: str, tool_name: str, arguments: dict, result: str):
        """Log a tool execution."""
        if settings.audit_log_tool_calls:
            self.log("tool_call", {
                "session_id": session_id,
                "tool_name": tool_name,
                "arguments": arguments,
                "result_preview": str(result)[:100]
            })
    
    def log_memory_access(self, session_id: str, operation: str, details: dict):
        """Log memory access operations."""
        if settings.audit_log_memory_access:
            self.log("memory_access", {
                "session_id": session_id,
                "operation": operation,
                **details
            })
    
    def log_auth_attempt(self, success: bool, details: dict):
        """Log authentication attempts."""
        self.log("auth_attempt", {
            "success": success,
            **details
        })

# Global audit logger instance
audit_logger = AuditLogger()


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\security.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\tools.py (Section: BACKEND_PYTHON) ---

"""Utility helpers for tools/ plugin management and tool listing.

This centralizes tool list formatting for use in the prompt builder and other
places in the app. Moving this out of main reduces duplicated code and keeps
the main module smaller.
"""
from typing import Any, Dict, List, Optional
import json
import time
import logging

logger = logging.getLogger(__name__)


def get_tools_list(plugin_manager: Optional[Any], mcp_client: Optional[Any]):
    """Return a normalized list of tool objects from plugin manager or mcp client.

    Each tool in the returned list will be a dict with keys 'name', 'description', and
    'inputSchema' at a minimum. If no tools found, returns [] and tools_available False.
    """
    tools = []
    try:
        if plugin_manager and getattr(plugin_manager, "enabled", False):
            tools = plugin_manager.list_tools() or []
        elif mcp_client:
            # mcp_client call is async in some paths; callers should handle that
            tools = []
            # We don't call the async MCP client here to keep this helper simple.
    except Exception:
        tools = []

    return tools


def format_tools_for_prompt(tools_list: List[Dict]) -> str:
    """Return a human-readable tools description suitable to append to the system prompt.

    E.g. "- fs_list(path): List files at path"
    """
    if not tools_list:
        return ""
    lines = []
    for tool in tools_list:
        params = ", ".join([p for p in tool.get('inputSchema', {}).get('properties', {}).keys()])
        lines.append(f"- {tool.get('name')}({params}): {tool.get('description','')}")
    out = "**AVAILABLE TOOLS:**\n" + "\n".join(lines) + "\n\nTo use a tool, respond with: TOOL_CALL: tool_name(param1=value1, param2=value2)"
    return out


class ToolExecutor:
    """Responsible for executing tool calls detected in an LLM response.

    This encapsulates validation, execution via plugins or MCP, audit logging,
    and re-generation after tool output.
    """
    def __init__(self, plugin_manager: Optional[Any], mcp_client: Optional[Any], tool_parser: Optional[Any], tool_validator: Optional[Any], llm_client: Optional[Any], audit_logger: Optional[Any], max_iterations: int = 3):
        self.plugin_manager = plugin_manager
        self.mcp_client = mcp_client
        self.tool_parser = tool_parser
        self.tool_validator = tool_validator
        self.llm = llm_client
        self.audit_logger = audit_logger
        self.max_iterations = max_iterations

    async def execute_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> Any:
        """
        Directly execute a single tool by name.
        Used by SGROrchestrator.
        """
        if self.plugin_manager and getattr(self.plugin_manager, 'enabled', False):
            plugin_name = self.plugin_manager.lookup_plugin_for_tool(tool_name)
            if plugin_name:
                return await self.plugin_manager.execute_tool(f"{plugin_name}:{tool_name}", **tool_args)
        
        if self.mcp_client:
            return await self.mcp_client.call_tool(tool_name, **tool_args)
            
        return {"error": f"Tool '{tool_name}' not found or tools disabled."}

    async def execute(self, parsed_response, full_context, request, system_prompt, context_mgr):
        """
        Execute tool calls detected in the LLM response.

        Behavior and output flow overview:
        - The LLM's initial text is passed via `initial_response` in the calling endpoint.
        - This executor validates tool calls, executes them via plugin manager or MCP, and then triggers a fresh LLM generation that includes tool results.
        - The returned `response` is the final LLM reply. The caller (chat endpoints) treats this as the 'response:' section, while the initial LLM output remains the 'thinking:' section.
        - Tool output is logged via `audit_logger` (if provided) and appended to the prompt so the LLM can reason with the results.
        """
        iteration = 0
        t_tools_total_ms = 0.0
        response = None

        # Track previous tool calls to detect loops
        previous_tool_calls = set()
        tool_call_counts = {}  # Track count per tool name

        while parsed_response and getattr(parsed_response, 'has_tool_calls', False) and iteration < self.max_iterations and self.tool_validator:
            iteration += 1

            # Check for tool call loops by detecting repeated tool calls with same parameters
            for tc in parsed_response.tool_calls:
                tool_call_key = (tc.tool_name, tuple(sorted(tc.parameters.items())))
                if tool_call_key in previous_tool_calls:
                    logger.warning(f"Detected tool call loop: {tc.tool_name} with same parameters")
                    # Break the loop by generating a response without tools
                    tool_ctx = f"\n\nTool call loop detected. You have attempted to call the same tool with the same parameters multiple times. Please answer the user's question directly without further tool calls."
                    response = await self.llm.generate(prompt=full_context + tool_ctx, system_prompt=system_prompt)
                    return response, iteration, t_tools_total_ms
                previous_tool_calls.add(tool_call_key)

                # Track and limit the number of times any single tool can be called
                tool_call_counts[tc.tool_name] = tool_call_counts.get(tc.tool_name, 0) + 1
                if tool_call_counts[tc.tool_name] > 5:  # Limit any single tool to 5 calls per conversation
                    logger.warning(f"Tool call limit reached for {tc.tool_name}: {tool_call_counts[tc.tool_name]} calls")
                    tool_ctx = f"\n\nYou have used the '{tc.tool_name}' tool multiple times. Please provide a final response to the user without using this tool again."
                    response = await self.llm.generate(prompt=full_context + tool_ctx, system_prompt=system_prompt)
                    return response, iteration, t_tools_total_ms

            # Choose first valid tool call
            tool_call = None
            validation_error = None
            for tc in parsed_response.tool_calls:
                if self.tool_validator:
                    is_valid, err = self.tool_validator.validate(tc)
                    if is_valid:
                        tool_call = tc
                        break
                    validation_error = err
                else:
                    tool_call = tc
                    break

            if not tool_call:
                # No valid tool calls; return a helpful response
                error_msg = validation_error or 'No valid tool calls found'
                logger.error(f"Tool call validation failed: {error_msg}")
                tool_ctx = f"\n\nTool call failed: {error_msg}\n\nPlease acknowledge and provide a helpful answer without tools."
                response = await self.llm.generate(prompt=full_context + tool_ctx, system_prompt=system_prompt)
                # Standardized fallback tag for deterministic detection in tests and UI
                response = f"[ToolExecutionFallback] {response}"
                return response, iteration, t_tools_total_ms

            # Execute tool call
            try:
                if self.audit_logger:
                    try:
                        self.audit_logger.log_tool_call(
                            session_id=request.session_id,
                            tool_name=tool_call.tool_name,
                            arguments=tool_call.parameters,
                            result='pending'
                        )
                    except Exception:
                        logger.warning("Failed to write audit log for tool call (pending)")
                t_tool_start = time.perf_counter()
                if self.plugin_manager and getattr(self.plugin_manager, 'enabled', False):
                    plugin_name = self.plugin_manager.lookup_plugin_for_tool(tool_call.tool_name)
                    if plugin_name:
                        tool_result = await self.plugin_manager.execute_tool(f"{plugin_name}:{tool_call.tool_name}", **tool_call.parameters)
                    else:
                        tool_result = {"error": f"Tool not found in plugins: {tool_call.tool_name}"}
                elif self.mcp_client:
                    tool_result = await self.mcp_client.call_tool(tool_call.tool_name, **tool_call.parameters)
                else:
                    tool_result = {"error": "Tools disabled"}
                t_tool_ms = (time.perf_counter() - t_tool_start) * 1000
                t_tools_total_ms += t_tool_ms
                if self.audit_logger:
                    try:
                        self.audit_logger.log_tool_call(
                            session_id=request.session_id,
                            tool_name=tool_call.tool_name,
                            arguments=tool_call.parameters,
                            result=str(tool_result)[:200]
                        )
                    except Exception:
                        logger.warning("Failed to write audit log for tool call (result)")

                # Treat explicit errors and empty/None results as failures and ask the LLM to provide feedback
                if tool_result is None or tool_result == {} or (isinstance(tool_result, (str, list)) and not tool_result) or (isinstance(tool_result, dict) and 'error' in tool_result):
                    # Report either the explicit error or a general 'no output' condition
                    if isinstance(tool_result, dict) and 'error' in tool_result:
                        err_text = tool_result.get('detail', tool_result.get('error'))
                    else:
                        err_text = 'Tool returned no output.'
                    tool_context = f"\n\nTool '{tool_call.tool_name}' failed or returned no output: {err_text}\n\nProvide helpful feedback to the user."
                else:
                    tool_context = f"\n\nTool '{tool_call.tool_name}' returned:\n{json.dumps(tool_result) if isinstance(tool_result, dict) else str(tool_result)}\n\nNow answer the user's question using this information."

                response = await self.llm.generate(prompt=full_context + tool_context, system_prompt=system_prompt)
                parsed_response = self.tool_parser.parse_response(response) if self.tool_parser else None
            except Exception as e:
                logger.error(f"Tool execution failed for {tool_call.tool_name}: {e}")
                response = await self.llm.generate(prompt=full_context + f"\n\nTool execution failed: {e}\n\nAcknowledge and suggest alternatives.", system_prompt=system_prompt)
                # Standardized fallback tag for deterministic detection in tests and UI
                response = f"[ToolExecutionFallback] {response}"
                return response, iteration, t_tools_total_ms

        return response, iteration, t_tools_total_ms



--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\tools.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\tool_call_models.py (Section: BACKEND_PYTHON) ---

"""
Tool Call Models and Validation

Pydantic models for validating and parsing tool calls from LLM responses.
Replaces brittle regex parsing with structured validation.
"""
from pydantic import BaseModel, Field, validator
from typing import Dict, Any, List, Optional, Literal
import re
import json
import logging

logger = logging.getLogger(__name__)


class ToolCallParam(BaseModel):
    """Single parameter for a tool call"""
    name: str
    value: Any
    
    class Config:
        extra = "forbid"


class ToolCall(BaseModel):
    """Validated tool call from LLM response"""
    tool_name: str = Field(..., description="Name of the tool to call")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Tool parameters")
    raw_match: Optional[str] = Field(None, description="Original matched string")
    
    @validator('tool_name')
    def validate_tool_name(cls, v):
        """Ensure tool name is valid identifier.

        Accepts either a simple snake_case name or a plugin-prefixed name like
        'utcp:tool_name'. We allow hyphens and dots in identifiers where needed.
        """
        if not v or not re.match(r'^[A-Za-z0-9_:\-\.]+$', v):
            raise ValueError(f"Invalid tool name format: {v}")
        return v
    
    class Config:
        extra = "forbid"


class LLMResponse(BaseModel):
    """Structured LLM response with optional tool calls"""
    response_text: str = Field(..., description="The LLM's text response")
    tool_calls: List[ToolCall] = Field(default_factory=list, description="Extracted tool calls")
    has_tool_calls: bool = Field(False, description="Whether response contains tool calls")
    
    class Config:
        extra = "allow"


class ToolCallParser:
    """
    Parser for extracting and validating tool calls from LLM responses.
    
    Supports multiple formats:
    1. TOOL_CALL: format (current regex-based)
    2. JSON format (for JSON mode models)
    3. Function call format (for function-calling models)
    """
    
    def __init__(self):
        # Regex pattern for TOOL_CALL: format
        # Accept plugin-prefixed names (e.g. utcp:filesystem_list) and allow dots, underscores, hyphens
        self.tool_call_pattern = re.compile(
            r'TOOL_CALL:\s*([A-Za-z0-9_:\-\.]+)\((.*?)\)',
            re.MULTILINE | re.DOTALL
        )
        
        # Alternative patterns for robustness
        self.json_tool_pattern = re.compile(
            r'\{[^}]*"tool":\s*"([^"]+)"[^}]*"params":\s*\{([^}]+)\}[^}]*\}',
            re.MULTILINE | re.DOTALL
        )
    
    def parse_response(self, response: str) -> LLMResponse:
        """
        Parse LLM response and extract tool calls.
        
        Args:
            response: Raw LLM response string
            
        Returns:
            LLMResponse with extracted tool calls
        """
        tool_calls = []
        
        # Try TOOL_CALL: format first (current format)
        matches = self.tool_call_pattern.findall(response)
        
        if matches:
            logger.debug(f"Found {len(matches)} TOOL_CALL format matches")
            for tool_name, params_str in matches:
                try:
                    params = self._parse_parameters(params_str)
                    tool_call = ToolCall(
                        tool_name=tool_name,
                        parameters=params,
                        raw_match=f"TOOL_CALL: {tool_name}({params_str})"
                    )
                    tool_calls.append(tool_call)
                    logger.debug(f"Parsed tool call: {tool_call.tool_name} with {len(params)} params")
                except Exception as e:
                    logger.error(f"Failed to parse tool call '{tool_name}': {e}")
                    # Don't add invalid tool calls
        
        # Try JSON format (for JSON mode)
        if not tool_calls and '{' in response:
            json_matches = self.json_tool_pattern.findall(response)
            if json_matches:
                logger.debug(f"Found {len(json_matches)} JSON format matches")
                for tool_name, params_str in json_matches:
                    try:
                        params = json.loads('{' + params_str + '}')
                        tool_call = ToolCall(
                            tool_name=tool_name,
                            parameters=params,
                            raw_match=f'{{"tool": "{tool_name}", "params": {{{params_str}}}}}'
                        )
                        tool_calls.append(tool_call)
                        logger.debug(f"Parsed JSON tool call: {tool_call.tool_name}")
                    except Exception as e:
                        logger.error(f"Failed to parse JSON tool call '{tool_name}': {e}")
        
        return LLMResponse(
            response_text=response,
            tool_calls=tool_calls,
            has_tool_calls=len(tool_calls) > 0
        )
    
    def _parse_parameters(self, params_str: str) -> Dict[str, Any]:
        """
        Parse parameter string into dictionary.

        Handles:
        - key=value format
        - Quoted strings
        - Nested structures
        - JSON values

        Args:
            params_str: Parameter string from tool call

        Returns:
            Dictionary of parsed parameters
        """
        params = {}

        if not params_str or not params_str.strip():
            return params

        # Split by comma, respecting nested structures
        param_parts = self._split_parameters(params_str)

        for part in param_parts:
            part = part.strip()
            if not part:
                continue

            if '=' not in part:
                logger.warning(f"Parameter without '=': {part}")
                continue

            try:
                key, value = part.split('=', 1)
                key = key.strip()
                value = value.strip()

                # Parse value
                parsed_value = self._parse_value(value)
                params[key] = parsed_value
            except Exception as e:
                logger.error(f"Error parsing parameter '{part}': {e}")
                continue

        return params
    
    def _split_parameters(self, params_str: str) -> List[str]:
        """
        Split parameter string by comma, respecting nested structures.
        
        Args:
            params_str: Raw parameter string
            
        Returns:
            List of parameter strings
        """
        param_parts = []
        current = []
        depth = 0
        in_quotes = False
        quote_char = None
        
        for char in params_str + ',':
            if char in ['"', "'"]:
                if not in_quotes:
                    in_quotes = True
                    quote_char = char
                elif char == quote_char:
                    in_quotes = False
                    quote_char = None
            
            if not in_quotes:
                if char in '([{':
                    depth += 1
                elif char in ')]}':
                    depth -= 1
                elif char == ',' and depth == 0:
                    if current:
                        param_parts.append(''.join(current))
                        current = []
                    continue
            
            current.append(char)
        
        return param_parts
    
    def _parse_value(self, value: str) -> Any:
        """
        Parse a parameter value into appropriate Python type.
        
        Supports:
        - Strings (quoted)
        - Numbers (int, float)
        - Booleans
        - JSON objects/arrays
        - null/None
        
        Args:
            value: Raw value string
            
        Returns:
            Parsed value
        """
        value = value.strip()
        
        # Empty value
        if not value or value.lower() in ['null', 'none']:
            return None
        
        # Boolean
        if value.lower() == 'true':
            return True
        if value.lower() == 'false':
            return False
        
        # Quoted string
        if (value.startswith('"') and value.endswith('"')) or \
           (value.startswith("'") and value.endswith("'")):
            return value[1:-1]
        
        # Try JSON parse (for objects/arrays)
        if value.startswith(('{', '[')):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse JSON value: {value}")
                return value
        
        # Try number
        try:
            if '.' in value:
                return float(value)
            return int(value)
        except ValueError:
            pass
        
        # Default: return as string
        return value


class ToolCallValidator:
    """
    Validator for tool calls against available tools.
    
    Checks:
    - Tool exists
    - Required parameters present
    - Parameter types match schema
    """
    
    def __init__(self, available_tools: Dict[str, Any]):
        """
        Initialize validator with available tools.
        
        Args:
            available_tools: Dict of tool name -> tool schema
        """
        self.available_tools = available_tools
    
    def validate(self, tool_call: ToolCall) -> tuple[bool, Optional[str]]:
        """
        Validate a tool call.
        
        Args:
            tool_call: ToolCall to validate
            
        Returns:
            (is_valid, error_message)
        """
        # Normalization: allow 'plugin:tool' forms by checking both forms
        tool_name_to_check = tool_call.tool_name
        if ':' in tool_name_to_check:
            # If the tool_name contains a plugin prefix, prefer the latter part for validation
            _, possible_tool = tool_name_to_check.split(':', 1)
            if possible_tool in self.available_tools:
                tool_name_to_check = possible_tool

        # Check tool exists
        if tool_name_to_check not in self.available_tools:
            return False, f"Tool '{tool_call.tool_name}' not found. Available tools: {list(self.available_tools.keys())}"

        tool_schema = self.available_tools[tool_name_to_check]
        
        # Check required parameters
        required_params = tool_schema.get('inputSchema', {}).get('required', [])
        missing_params = set(required_params) - set(tool_call.parameters.keys())
        
        if missing_params:
            return False, f"Missing required parameters: {missing_params}"
        
        # All checks passed
        return True, None


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\tool_call_models.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapter.py (Section: BACKEND_PYTHON) ---

"""Abstract Vector DB adapter interface for ECE_Core.

This module exposes a small interface to integrate vector DBs such as
Pinecone, Milvus, Redis Vector, or FAISS as a local embed store.

Implementations should be lightweight and provide a test-backed
in-memory FAISS-like adapter for unit tests.
"""
from __future__ import annotations
from typing import Protocol, List, Dict, Any, Optional, Tuple
from src.config import settings
from src.vector_adapters.redis_vector_adapter import RedisVectorAdapter
from src.vector_adapters.fake_vector_adapter import FakeVectorAdapter

class VectorAdapter(Protocol):
    """Vector DB abstraction layer."""

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        """Index (or upsert) an embedding with metadata into the vector DB.

        embedding_id: unique ID for the vector entry
        node_id: Neo4j node id or external id
        chunk_index: index of the chunk within the content
        embedding: numeric embedding vector
        metadata: additional properties (raw text, timestamp)
        """

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        """Query the vector DB and return a list of hit dicts with keys: score, embedding_id, node_id, chunk_index, metadata"""

    async def delete(self, embedding_id: str) -> None:
        """Delete an embedding by id."""

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        """Get a vector entry by id, returning scoreless metadata and mapping"""

    async def health(self) -> bool:
        """Return health check boolean for adapter status."""

    async def initialize(self) -> None:
        """Optional initialization for the adapter (e.g. connect to Redis)."""


def create_vector_adapter(adapter_name: str | None = None) -> VectorAdapter:
    """Factory to create a vector adapter by name.
    Defaults to a Redis-backed adapter when `redis` is requested. Falls back
    to a memory-backed adapter (RedisVectorAdapter's in-memory mode) if Redis
    isn't available.
    """
    name = adapter_name or getattr(settings, "vector_adapter_name", "redis")
    if name == "redis":
        adapter = RedisVectorAdapter(redis_url=settings.redis_url)
        return adapter
    if name == "fake":
        return FakeVectorAdapter()
    # fallback to redis-based adapter which supports in-memory fallback
    adapter = RedisVectorAdapter(redis_url=settings.redis_url)
    return adapter


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapter.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\archivist.py (Section: BACKEND_PYTHON) ---

"""
Archivist Agent (Maintenance & Curation)
Handles Knowledge Base Freshness, Pruning, and Re-verification.
"""
import logging
import asyncio
from typing import List, Dict, Any, TYPE_CHECKING
from datetime import datetime, timedelta, timezone
if TYPE_CHECKING:
    # Avoid importing heavy deps during test collection (neo4j etc)
    from src.memory import TieredMemory
    from src.agents.verifier import VerifierAgent
else:
    TieredMemory = None  # type: ignore
    VerifierAgent = None  # type: ignore
from src.maintenance.weaver import MemoryWeaver
from src.config import Settings, settings as GLOBAL_SETTINGS

logger = logging.getLogger(__name__)

class ArchivistAgent:
    """
    Archivist Agent manages the health and freshness of the Knowledge Graph.
    It runs background tasks to prune stale nodes and trigger re-verification.
    """
    
    def __init__(self, memory: TieredMemory, verifier: VerifierAgent, settings: Settings | None = None):
        self.memory = memory
        self.verifier = verifier
        self.running = False
        self._task = None
        # Use the provided settings instance or fallback to the module-global settings
        self.settings = settings or GLOBAL_SETTINGS
        self.weaver = MemoryWeaver(self.settings)
        self._last_weave = None
        self._last_purge = None
        
    async def start(self):
        """Start the background maintenance loop."""
        self.running = True
        self._task = asyncio.create_task(self._maintenance_loop())
        logger.info("Archivist Agent started (Maintenance Loop)")

    async def stop(self):
        """Stop the background maintenance loop."""
        self.running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("Archivist Agent stopped")

    async def _maintenance_loop(self):
        """
        Main loop:
        1. Check for stale nodes (Freshness Protocol)
        2. Prune low-value/old nodes
        3. Sleep
        """
        while self.running:
            try:
                logger.info("Archivist: Starting maintenance cycle...")
                # Janitor: purge contaminated nodes if enabled and interval elapsed
                try:
                    if getattr(self.settings, 'archivist_auto_purge_enabled', False):
                        now = datetime.now(timezone.utc)
                        interval = getattr(self.settings, 'archivist_auto_purge_interval_seconds', 600)
                        if not self._last_purge or (now - self._last_purge).total_seconds() >= interval:
                            logger.info("Archivist: Running auto-purge (Janitor) cycle to clean contaminated nodes")
                            try:
                                await self.purge_contaminated_nodes(dry_run=getattr(self.settings, 'archivist_auto_purge_dry_run', True))
                            except Exception as purge_e:
                                logger.error(f"Archivist: Auto-purge failed: {purge_e}")
                            self._last_purge = now
                except Exception as purge_check_e:
                    # Don't let janitor failures stop maintenance loop
                    logger.error(f"Archivist: Janitor pre-check failed: {purge_check_e}")
                await self.check_freshness()

                # Run weaving on short cadence (every 60 minutes) if enabled
                if self.settings.weaver_enabled:
                    now = datetime.now(timezone.utc)
                    # Run every 60 minutes
                    if not self._last_weave or (now - self._last_weave).total_seconds() >= 3600:
                        logger.info("Archivist: Running MemoryWeaver weave_recent (dry-run) as scheduled heartbeat")
                        try:
                            # Use settings defaults; ensure dry-run by default
                            await self.run_weaving_cycle()
                            self._last_weave = now
                        except Exception as weave_e:
                            logger.error(f"Archivist: Weaver run failed: {weave_e}")
                # await self.prune_stale() # Disabled for now to prevent data loss during beta
                logger.info("Archivist: Maintenance cycle complete.")
            except Exception as e:
                logger.error(f"Archivist error: {e}")
            # Run every hour (3600s) - configurable
            await asyncio.sleep(3600)

    
    async def run_weaving_cycle(self, hours: int | None = None, threshold: float | None = None, max_commit: int | None = None, candidate_limit: int | None = None, batch_size: int | None = None, prefer_same_app: bool | None = None, dry_run: bool | None = None, csv_out: str | None = None):
        """
        Trigger a weaving cycle using the MemoryWeaver. Uses Settings defaults if parameters not supplied.
        """
        try:
            # If settings indicate weaving is disabled, skip it
            if not self.settings.weaver_enabled:
                logger.info("Archivist: Weaver disabled in settings; skipping")
                return
            # Resolve commit flag: avoid writes unless explicitly configured
            if dry_run is None:
                dry_run = self.settings.weaver_dry_run_default
            # Run the weave
            candidate_limit = candidate_limit if candidate_limit is not None else getattr(self.settings, 'weaver_candidate_limit', 200)
            batch_size = batch_size if batch_size is not None else getattr(self.settings, 'weaver_batch_size_default', getattr(self.settings, 'llm_embeddings_default_batch_size', 4))
            result = await self.weaver.weave_recent(hours=hours, threshold=threshold, max_commit=max_commit, candidate_limit=candidate_limit, prefer_same_app=prefer_same_app, dry_run=dry_run, csv_out=csv_out, batch_size=batch_size)
            logger.info(f"Archivist: Weaver run completed: {result}")
            return result
        except Exception as e:
            logger.error(f"Archivist: Weaver cycle error: {e}")
            return None

    async def check_freshness(self, limit: int = 10):
        """
        Scan for nodes that need re-verification.
        Criteria: High importance (>7) but old (>30 days) or missing verification.
        """
        # We need a custom Cypher query here.
        # Since we can't easily add methods to Neo4jStore at runtime without editing it,
        # we'll use execute_cypher if available, or add a method to Neo4jStore.
        # Neo4jStore has execute_cypher method.
        
        query = """
        MATCH (m:Memory)
        WHERE m.importance > 7 
          AND (m.last_verified_at IS NULL OR datetime(m.last_verified_at) < datetime($threshold))
        RETURN elementId(m) as id, m.content as content, m.metadata as metadata
        LIMIT $limit
        """
        
        threshold = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
        
        try:
            results = await self.memory.neo4j.execute_cypher(query, {"threshold": threshold, "limit": limit})
            
            for record in results:
                await self.reverify_node(record)
                
        except Exception as e:
            logger.error(f"Freshness check failed: {e}")

    async def reverify_node(self, record: Dict[str, Any]):
        """
        Trigger VerifierAgent to check a node.
        """
        node_id = record.get("id")
        content = record.get("content")
        
        logger.info(f"Archivist: Re-verifying node {node_id}...")
        
        # We need context to verify against. For now, we verify against the node itself 
        # (checking internal consistency) or we could search for related nodes.
        # Ideally, verifier searches for primary sources.
        
        # Search for related context to help verification
        context = await self.memory.search_memories(content[:100], None, limit=5)
        
        verification = await self.verifier.verify_claim(content, context)
        
        # Update node with verification result
        update_query = """
        MATCH (m:Memory) WHERE elementId(m) = $node_id
        SET m.last_verified_at = $now,
            m.freshness_score = $score,
            m.verification_note = $note
        """
        
        await self.memory.neo4j.execute_cypher(update_query, {
            "node_id": node_id,
            "now": datetime.now(timezone.utc).isoformat(),
            "score": verification.get("score", 0.0),
            "note": "Verified by VerifierAgent" if verification.get("verified") else "Verification failed"
        })
        
        logger.info(f"Archivist: Node {node_id} updated with score {verification.get('score')}")

    def _content_contains_marker(self, content: str, markers: list[str]) -> bool:
        """
        Simple helper to check if content or metadata contains any marker.
        Lower-casing helps match markers configured in the settings.
        """
        if not content:
            return False
        content_lower = content.lower()
        for m in markers:
            if m and m.lower() in content_lower:
                return True
        return False

    async def find_contaminated_nodes(self, markers: list[str]) -> list:
        """
        Use the provided markers to identify candidate nodes in Neo4j.
        This returns a list of node records (id, content, metadata, created_at, session_id, category).
        """
        if not markers:
            return []

        results = []
        try:
            q = """
            MATCH (m:Memory)
            WHERE (
                """ + ' OR '.join(["toLower(coalesce(m.content,'') ) CONTAINS $marker_%d" % i for i in range(len(markers))]) + """
            ) OR (
                """ + ' OR '.join(["toLower(coalesce(m.metadata,'') ) CONTAINS $marker_meta_%d" % i for i in range(len(markers))]) + """
            )
            RETURN elementId(m) as id, m.content as content, m.metadata as metadata, m.created_at as created_at, m.session_id as session_id, m.category as category, m.status as status
            """
            params = {}
            for i,m in enumerate(markers):
                params[f"marker_{i}"] = m.lower()
                params[f"marker_meta_{i}"] = m.lower()
            # Neo4j driver session loop
            drv = getattr(self.memory, 'neo4j', None)
            if not drv or not getattr(drv, 'neo4j_driver', None):
                logger.info("Archivist: Neo4j driver not configured; cannot find contaminated nodes")
                return []
            async with drv.neo4j_driver.session() as session:
                result = await session.run(q, params)
                rows = await result.data()
                for r in rows:
                    results.append(r)
        except Exception as e:
            logger.error(f"Archivist: Failed to find contaminated nodes: {e}")
        return results

    async def purge_contaminated_nodes(self, dry_run: bool = True, markers: list[str] | None = None) -> dict:
        """
        Detect nodes that match the configured contamination markers and optionally delete them.
        Safety: only delete nodes that are NOT committed (i.e., m.status != 'committed').
        Returns a dict with counts for found and deleted nodes.
        """
        if markers is None:
            markers = getattr(self.settings, 'archivist_auto_purge_markers', [])
        markers = [m for m in (markers or []) if m]
        if not markers:
            logger.info("Archivist: No markers configured for auto-purge; skipping")
            return {"found": 0, "deleted": 0}

        found = 0
        deleted = 0
        try:
            # Build a safe cypher that matches any content or metadata marker and excludes 'committed' nodes
            conds = []
            params = {}
            for i, m in enumerate(markers):
                params[f"marker_{i}"] = m.lower()
                params[f"marker_meta_{i}"] = m.lower()
                conds.append(f"toLower(coalesce(m.content,'')) CONTAINS $marker_{i}")
                conds.append(f"toLower(coalesce(m.metadata,'')) CONTAINS $marker_meta_{i}")

            if not conds:
                return {"found": 0, "deleted": 0}

            where_clause = '(' + ' OR '.join(conds) + ") AND (m.status IS NULL OR toLower(m.status) <> 'committed')"
            q = f"""
            MATCH (m:Memory)
            WHERE {where_clause}
            RETURN elementId(m) as id, m.content as content, m.metadata as metadata, m.created_at as created_at, m.session_id as session_id, m.category as category
            """
            drv = getattr(self.memory, 'neo4j', None)
            if not drv or not getattr(drv, 'neo4j_driver', None):
                logger.info("Archivist: Neo4j driver not configured; skipping purge")
                return {"found": 0, "deleted": 0}
            async with drv.neo4j_driver.session() as session:
                result = await session.run(q, params)
                rows = await result.data()
                found = len(rows)
                logger.info(f"Archivist-Janitor: Found {found} candidate contaminated nodes")
                if found and not dry_run:
                    for row in rows:
                        try:
                            await session.run('MATCH (m:Memory) WHERE elementId(m) = $id DETACH DELETE m', {'id': row.get('id')})
                            deleted += 1
                        except Exception as e:
                            logger.error(f"Archivist-Janitor: Failed to delete node {row.get('id')}: {e}")
                # if dry_run, list candidates to log
                if dry_run:
                    for row in rows:
                        logger.warning(f"Archivist-Janitor: Dry-run found candidate: id={row.get('id')} session_id={row.get('session_id')} category={row.get('category')} created_at={row.get('created_at')}")
        except Exception as e:
            logger.error(f"Archivist: purge_contaminated_nodes error: {e}")
        logger.info(f"Archivist-Janitor: Purge results: found={found} deleted={deleted} dry_run={dry_run}")
        return {"found": found, "deleted": deleted}

    async def prune_stale(self):
        """
        Prune nodes with low importance (<3) and old age (>90 days).
        """
        query = """
        MATCH (m:Memory)
        WHERE m.importance < 3 
          AND datetime(m.created_at) < datetime($threshold)
        DELETE m
        """
        threshold = (datetime.now(timezone.utc) - timedelta(days=90)).isoformat()
        
        try:
            await self.memory.neo4j.execute_cypher(query, {"threshold": threshold})
            logger.info("Archivist: Pruned stale nodes.")
        except Exception as e:
            logger.error(f"Pruning failed: {e}")


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\archivist.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\planner.py (Section: BACKEND_PYTHON) ---

"""
Planner Agent

This module implements a PlannerAgent that uses the project's LLM client
to decompose a user query into a small plan of steps that use available tools.

The agent returns a JSON structure containing a `goal` and a list of `steps`.
Each step contains: `tool_name`, `args`, and `reasoning`.
"""
from __future__ import annotations

import json
import logging
from typing import List, Dict, Any, Optional

from pydantic import ValidationError

from src.llm import LLMClient
from src.schemas.plan_models import PlanResult, PlanStep
from src.config import settings

logger = logging.getLogger(__name__)


# Note: PlanStep and PlanResult are imported from src.schemas.plan_models


class PlannerAgent:
    def __init__(self, llm_client: Optional[LLMClient] = None):
        # Prefer an injected LLM client; otherwise create one from settings
        if llm_client is None:
            try:
                llm_client = LLMClient()
            except Exception as e:
                logger.warning(f"PlannerAgent: failed to create LLMClient: {e}")
                llm_client = None
        self.llm = llm_client

    async def create_plan(self, user_query: str, available_tools: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Create a plan given the user's query and the available tools.

        Returns a dict shaped like {"goal": str, "steps": [ {tool_name, args, reasoning} ]}
        """
        if not self.llm:
            raise RuntimeError("No LLM available for planning")

        tools_list = available_tools or []
        tools_str = "\n".join([f"- {t.get('name')}: {t.get('description','')}" for t in tools_list])

        system_prompt = (
            "You are a PLANNER. Break the user's request into atomic steps using ONLY the available tools. "
            "Output pure JSON with fields 'goal' and 'steps'. 'steps' is an array of objects with 'tool_name', 'args' and 'reasoning'. "
            "Use tool names exactly as provided. If a step needs no tool, place tool_name as 'none' and args as {}. "
            "Do not add any other exposition; return only a valid JSON object."
        )

        instruction = (
            f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
            "Output a JSON object with 'goal' and 'steps' (tool_name, args, reasoning)."
        )

        # Try up to N times to get a valid JSON plan from the model.
        # Re-prompt the LLM if parse/validation fails.
        max_retries = 3
        last_raw = None
        for attempt in range(max_retries):
            try:
                raw = await self.llm.generate(prompt=instruction, system_prompt=system_prompt)
            except Exception as e:
                logger.exception("PlannerAgent: LLM failed to generate plan")
                return {"goal": user_query, "steps": []}

            last_raw = raw
            # Resilient parse: find first '{' and last '}' and extract JSON
            try:
                if isinstance(raw, str) and '{' in raw and '}' in raw:
                    first = raw.find('{')
                    last = raw.rfind('}')
                    json_str = raw[first:last+1]
                    parsed = json.loads(json_str)
                else:
                    parsed = json.loads(raw) if isinstance(raw, str) else raw
            except Exception:
                parsed = None

            if parsed is None:
                # If parse failed, craft a repair-system prompt for the next attempt
                instruction = (
                    f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
                    "The previous assistant output was not valid JSON. Return only a valid JSON object with 'goal' and 'steps'. "
                )
                continue

            # Validate parsed structure using PlanResult pydantic model
            try:
                # Normalization: translate fields into expected shape when useful
                # Accept alternative keys like 'plan' or 'tool' by transforming
                normalized = {
                    'goal': parsed.get('goal') or user_query,
                    'steps': []
                }
                raw_steps = parsed.get('steps', parsed.get('plan', []))
                invalid_missing_tool = False
                for s in raw_steps:
                    has_tool_name = 'tool_name' in s or 'tool' in s
                    tn = s.get('tool_name') or s.get('tool') or 'none'
                    args = s.get('args') or s.get('parameters') or {}
                    reasoning = s.get('reasoning') or s.get('note') or ''
                    # If the step doesn't explicitly specify a tool (tool_name/tool), consider this invalid and request a repair
                    if not has_tool_name:
                        invalid_missing_tool = True
                    normalized['steps'].append({"tool_name": tn, "args": args, "reasoning": reasoning})

                if invalid_missing_tool:
                    raise ValueError("Plan step provided args but missing tool_name")

                plan_obj = PlanResult.parse_obj(normalized)
                # Validate steps tools against available tools if provided
                if tools_list:
                    allowed = {t.get('name') for t in tools_list if t.get('name')}
                    invalid_tools = [s.tool_name for s in plan_obj.steps if s.tool_name not in allowed and s.tool_name != 'none']
                    if invalid_tools:
                        raise ValueError(f"Plan contained tools not in available tools: {invalid_tools}")
                # Passed validation: return canonical dict
                return plan_obj.dict()
            except (ValidationError, Exception) as ve:
                logger.warning("PlannerAgent: validation failed on model output; retrying: %s", ve)
                # Re-prompt the LLM on next loop iteration
                instruction = (
                    f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
                    "Your previous output did not satisfy the required JSON schema. Please return a pure JSON document matching the schema."
                )
                continue

        # If we got here, we failed to produce a valid plan after retries; fall back to a minimal plan
        logger.warning("PlannerAgent: Failed to produce valid plan after %s attempts. Raw last output: %s", max_retries, last_raw)
        return {"goal": user_query, "steps": []}


__all__ = ["PlannerAgent", "PlanResult", "PlanStep"]


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\planner.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\verifier.py (Section: BACKEND_PYTHON) ---

"""
Verifier Agent (Empirical Distrust)
Implements System 2 verification loop for claims.
"""
from typing import List, Dict, Any, Optional
from src.llm import LLMClient
from src.memory import TieredMemory
import logging
import json

logger = logging.getLogger(__name__)

class VerifierAgent:
    """
    Verifier Agent implements 'Empirical Distrust'.
    It verifies claims by seeking primary source evidence and calculating provenance entropy.
    """
    
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        
    async def verify_claim(self, claim: str, context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Verify a specific claim against provided context and primary sources.
        Returns verification result with score and evidence.
        """
        # 1. Identify key facts in claim
        facts = await self._extract_facts(claim)
        
        # 2. Check evidence for each fact
        verified_facts = []
        overall_score = 0.0
        
        for fact in facts:
            evidence = await self._find_evidence(fact, context)
            fact_score = self._calculate_provenance_score(evidence)
            verified_facts.append({
                "fact": fact,
                "evidence": evidence,
                "score": fact_score,
                "verified": fact_score > 0.7
            })
            overall_score += fact_score
            
        avg_score = overall_score / len(facts) if facts else 0.0
        
        return {
            "claim": claim,
            "verified": avg_score > 0.7,
            "score": avg_score,
            "details": verified_facts
        }

    async def _extract_facts(self, claim: str) -> List[str]:
        """Extract atomic facts from claim using LLM."""
        prompt = f"""Extract atomic, verifiable facts from this claim:
"{claim}"

Return as JSON list of strings."""
        
        try:
            response = await self.llm.generate(prompt, temperature=0.1)
            # Simple parsing attempt
            if "[" in response:
                start = response.find("[")
                end = response.rfind("]") + 1
                return json.loads(response[start:end])
            return [claim]
        except Exception:
            return [claim]

    async def _find_evidence(self, fact: str, context: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find supporting evidence in context."""
        evidence = []
        # Simple keyword matching for now, could be semantic
        fact_terms = set(fact.lower().split())
        
        for item in context:
            content = item.get("content", "").lower()
            # Check overlap
            if any(term in content for term in fact_terms if len(term) > 4):
                evidence.append(item)
                
        return evidence

    def _calculate_provenance_score(self, evidence: List[Dict[str, Any]]) -> float:
        """
        Calculate score based on source type.
        Primary sources (code, logs) > Secondary (docs) > Tertiary (chat).
        """
        if not evidence:
            return 0.0
            
        score_sum = 0.0
        for item in evidence:
            # Determine source type from metadata
            meta = item.get("metadata", {})
            source = meta.get("source", "unknown")
            category = item.get("category", "unknown")
            
            weight = 0.5 # Default
            
            if category == "code" or source.endswith(".py") or source.endswith(".log"):
                weight = 1.0 # Primary
            elif category == "doc" or source.endswith(".md"):
                weight = 0.8 # Secondary
            elif category == "chat":
                weight = 0.4 # Tertiary/Hearsay
                
            score_sum += weight
            
        # Normalize (diminishing returns)
        return min(1.0, score_sum)


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\verifier.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\orchestrator.py (Section: BACKEND_PYTHON) ---

import json
import logging
from typing import List, Dict, Any, Optional
from src.llm import LLMClient
from src.tools import ToolExecutor
from src.agents.orchestrator.schemas import SGRPlan, NextAction, IntentType
from src.agents.orchestrator.prompts import PLANNER_PERSONA, SCRIBE_PERSONA

logger = logging.getLogger(__name__)

class SGROrchestrator:
    def __init__(self, llm_client: LLMClient, tool_executor: ToolExecutor, audit_logger=None):
        self.llm = llm_client
        self.tools = tool_executor
        self.audit = audit_logger
        self.max_turns = 5

    async def run_loop(self, session_id: str, user_message: str, context: str) -> str:
        """
        Executes the Schema-Guided Reasoning loop.
        """
        # Initialize conversation history for this run
        # We start with the retrieved context + user message
        # Note: 'context' here is the assembled context string from ContextManager
        
        # Initial Planner Context
        current_history = [
            {"role": "system", "content": PLANNER_PERSONA},
            {"role": "user", "content": f"Context:\n{context}\n\nUser Request: {user_message}"}
        ]

        for turn in range(self.max_turns):
            logger.info(f"SGR Turn {turn + 1}/{self.max_turns} for session {session_id}")
            
            # 1. Generate Plan
            try:
                # Force JSON mode if possible, or rely on prompt
                response_text = await self.llm.generate_response(
                    messages=current_history,
                    temperature=0.2, # Low temp for deterministic planning
                    json_mode=True
                )
                
                # Parse JSON
                try:
                    plan_data = json.loads(response_text)
                    plan = SGRPlan(**plan_data)
                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"Failed to parse SGR plan: {e}. Response: {response_text}")
                    # Fallback: try to repair or just return the raw text if it looks like a response
                    if "{" not in response_text:
                         return response_text
                    return "I encountered an error processing my internal plan. Please try again."

                # Log the thought process
                if self.audit:
                    await self.audit.log_event(
                        session_id=session_id,
                        event_type="sgr_plan",
                        content=plan.model_dump_json(),
                        metadata={"turn": turn}
                    )

                # 2. Execute Logic based on NextAction
                if plan.next_action == NextAction.FINALIZE_RESPONSE:
                    # Optimization: If the plan already has a good final response, use it.
                    # Otherwise, we could switch to SCRIBE_PERSONA here if needed.
                    # For now, we trust the Planner's final_response if present.
                    if plan.final_response:
                        return plan.final_response
                    
                    # If no final response in JSON, do a quick Scribe pass
                    scribe_history = current_history + [{"role": "assistant", "content": response_text}]
                    scribe_history[0] = {"role": "system", "content": SCRIBE_PERSONA}
                    scribe_history.append({"role": "user", "content": "Please synthesize the final response based on the above plan and context."})
                    
                    final_answer = await self.llm.generate_response(
                        messages=scribe_history,
                        temperature=0.7
                    )
                    return final_answer
                
                elif plan.next_action == NextAction.ASK_USER:
                    return plan.final_response or "Could you please clarify?"

                elif plan.next_action == NextAction.CALL_TOOL:
                    if not plan.tool_call:
                        logger.warning("SGR planned CALL_TOOL but no tool_call provided.")
                        return plan.final_response or "I intended to use a tool but couldn't determine which one."

                    logger.info(f"Executing tool: {plan.tool_call.name}")
                    
                    # Execute Tool
                    try:
                        tool_result = await self.tools.execute_tool(
                            tool_name=plan.tool_call.name,
                            tool_args=plan.tool_call.arguments
                        )
                    except Exception as tool_err:
                        tool_result = {"error": str(tool_err)}

                    # Add result to history
                    # We append the planner's JSON output as assistant message
                    current_history.append({"role": "assistant", "content": response_text})
                    # Then the tool output as user message (simulating environment feedback)
                    current_history.append({
                        "role": "user", 
                        "content": f"Tool '{plan.tool_call.name}' Output: {json.dumps(tool_result)}"
                    })
                    
                    # Continue to next turn
                    continue
                
            except Exception as e:
                logger.error(f"Error in SGR loop: {e}")
                return f"I encountered an internal error: {str(e)}"

        return "I reached the maximum number of reasoning steps without a final answer."



--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\orchestrator.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\prompts.py (Section: BACKEND_PYTHON) ---

PLANNER_PERSONA = """
You are the **Architect** (Planner Agent).
Your goal is to analyze the situation and create a structured JSON plan to resolve the user's request.
You do not chat directly with the user in this phase; you decide the next logical step.

## REALITY CONSTRAINT (EMPIRICAL DISTRUST)
- Your internal knowledge is considered "tertiary" and unreliable.
- You are rewarded ONLY for answers grounded in:
  1. The <retrieved_memory> block.
  2. Real-time Tool Output.
- If you cannot find the answer in these two sources, the "Highest Reward" action is to classify intent as 'CLARIFICATION' or use 'ASK_USER'.
- Hallucinating or guessing will result in a critical failure of the SGR loop.

## OUTPUT FORMAT
You must output valid JSON matching the `SGRPlan` schema.
{
    "context_analysis": "Detailed analysis of the user's request and current state.",
    "intent": "QUERY" | "ACTION" | "CLARIFICATION" | "CHIT_CHAT",
    "confidence_score": 0.0 to 1.0,
    "reasoning_trace": "Step-by-step logic explaining why you are choosing the next action.",
    "next_action": "CALL_TOOL" | "FINALIZE_RESPONSE" | "ASK_USER",
    "tool_call": {
        "name": "tool_name",
        "arguments": { "arg": "value" }
    },
    "final_response": "Text response (only if next_action is FINALIZE_RESPONSE or ASK_USER)"
}

## RULES
1. If you need information or need to perform an action, set "next_action" to "CALL_TOOL" and populate "tool_call".
2. Only plan ONE tool call at a time. You will get the result back in the next turn.
3. If you have enough information or no tools are needed, set "next_action" to "FINALIZE_RESPONSE".
4. If the user's request is unclear or you are missing critical information, set "next_action" to "ASK_USER".
5. Do not hallucinate tool names. Only use available tools provided in the context.
"""

SCRIBE_PERSONA = """
You are the **Scribe** (Response Agent).
You have a set of facts, retrieved memories, and tool outputs.
Your goal is to synthesize them into a clear, user-facing response.

## CONSTRAINTS
- Do not add new facts that are not present in the context or tool outputs.
- Maintain the "Empirical Distrust" philosophy: if the data isn't there, admit it.
- Be concise and professional.
- If the user asked for code, provide the code clearly.
"""


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\prompts.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\schemas.py (Section: BACKEND_PYTHON) ---

from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

class IntentType(str, Enum):
    QUERY = "QUERY"
    ACTION = "ACTION"
    CLARIFICATION = "CLARIFICATION"
    CHIT_CHAT = "CHIT_CHAT"

class NextAction(str, Enum):
    CALL_TOOL = "CALL_TOOL"
    FINALIZE_RESPONSE = "FINALIZE_RESPONSE"
    ASK_USER = "ASK_USER"

class ToolCall(BaseModel):
    name: str = Field(..., description="The name of the tool to call")
    arguments: Dict[str, Any] = Field(..., description="The arguments to pass to the tool")

class SGRPlan(BaseModel):
    context_analysis: str = Field(..., description="Analysis of the current situation, user request, and retrieved context.")
    intent: IntentType = Field(..., description="Classification of the user's intent.")
    confidence_score: float = Field(..., description="Confidence score between 0.0 and 1.0.")
    reasoning_trace: str = Field(..., description="Step-by-step logic explaining the decision.")
    next_action: NextAction = Field(..., description="The next action to take.")
    tool_call: Optional[ToolCall] = Field(None, description="The tool to call if next_action is CALL_TOOL.")
    final_response: Optional[str] = Field(None, description="The final response to the user if next_action is FINALIZE_RESPONSE or ASK_USER.")



--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\schemas.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\audit.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from src.bootstrap import get_components
from src.security import verify_api_key
from pathlib import Path
from src.config import settings

router = APIRouter()


@router.get('/audit/logs')
async def get_audit_logs(request_obj: Request, limit: int = 50, authenticated: bool = Depends(verify_api_key)):
    try:
        path = Path(settings.audit_log_path)
        if not path.exists():
            return {"logs": [], "message": "No audit log found"}
        with path.open('r', encoding='utf-8', errors='ignore') as f:
            lines = f.read().splitlines()
        tail = lines[-int(limit):] if limit and len(lines) > 0 else lines
        return {"logs": tail, "count": len(tail)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\audit.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\health.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, HTTPException
from pydantic import BaseModel
from src.bootstrap import get_components
from src.config import settings
from typing import Any, Dict
from pydantic import BaseModel

router = APIRouter()


@router.get("/health")
async def health_check(request: Request):
    components = get_components(request.app)
    memory = components.get("memory")
    llm = components.get("llm")
    plugin_manager = components.get("plugin_manager")

    health = {"status": "healthy", "components": {}, "version": getattr(settings, 'ece_version', 'dev')}

    # Redis
    try:
        health["components"]["redis"] = bool(getattr(memory, "redis", None))
    except Exception:
        health["components"]["redis"] = False

    # Neo4j
    try:
        neo4j_store = getattr(memory, "neo4j", None)
        health["components"]["neo4j"] = bool(neo4j_store and getattr(neo4j_store, "neo4j_driver", None))
    except Exception:
        health["components"]["neo4j"] = False
    # Attempt counter
    try:
        health["components"]["neo4j_reconnect_attempts"] = getattr(neo4j_store, "_neo4j_reconnect_attempts", 0)
        health["components"]["neo4j_reconnecting"] = getattr(neo4j_store, "_neo4j_reconnect_task", None) is not None
        # Expose auth error for Neo4j so admins can take action
        health["components"]["neo4j_auth_error"] = getattr(neo4j_store, "_neo4j_auth_error", False)
    except Exception:
        health["components"]["neo4j_reconnect_attempts"] = 0
        health["components"]["neo4j_reconnecting"] = False

    # LLM: check if API is reachable or local model is loaded
    try:
        llm_status = {"api": False, "local": False}
        if llm:
            # If a model detection has been attempted, we might have `_detected_model`
            try:
                detected = await llm.detect_model()
                llm_status["api"] = bool(detected)
            except Exception:
                llm_status["api"] = False
            # Check local model availability without initializing heavy loads
            local = getattr(llm, "_local_llm", None) is not None
            llm_status["local"] = local
        health["components"]["llm"] = llm_status
    except Exception:
        health["components"]["llm"] = {"api": False, "local": False}

    # Plugin manager
    try:
        health["components"]["plugins"] = bool(plugin_manager and getattr(plugin_manager, 'enabled', False))
    except Exception:
        health["components"]["plugins"] = False

    return health


class ReconnectRequest(BaseModel):
    force: bool = False


class Neo4jConfigUpdate(BaseModel):
    neo4j_uri: str | None = None
    neo4j_user: str | None = None
    neo4j_password: str | None = None


@router.post("/admin/neo4j/reconnect")
async def admin_neo4j_reconnect(request: Request, body: ReconnectRequest | None = None):
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    # If the DB is disabled by config, return a helpful message
    if not getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None):
        return {"status": "disabled", "message": "Neo4j not configured in memory component"}

    force = bool(body.force) if body else False
    result = await memory.trigger_reconnect(force=force)
    return {"status": "ok", "result": result}


def _mask(val: str | None, show: int = 2) -> str:
    if not val:
        return ""
    if len(val) <= show:
        return "*" * len(val)
    return val[:1] + "*" * (len(val) - show - 1) + val[-show:]


@router.get("/admin/neo4j/config")
async def admin_neo4j_config(request: Request) -> Dict[str, Any]:
    """Return the Neo4j config being used by the running app (masked password)."""
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    return {
        "neo4j_enabled": getattr(getattr(memory, 'neo4j', None), "neo4j_driver", None) is not None or getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None) is not None,
        "neo4j_uri": getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None),
        "neo4j_user": getattr(getattr(memory, 'neo4j', None), "neo4j_user", None),
        "neo4j_password_masked": _mask(getattr(getattr(memory, 'neo4j', None), "neo4j_password", None)),
        "neo4j_auth_error": getattr(getattr(memory, 'neo4j', None), "_neo4j_auth_error", False),
        "neo4j_reconnect_attempts": getattr(getattr(memory, 'neo4j', None), "_neo4j_reconnect_attempts", 0),
        "neo4j_reconnecting": getattr(getattr(memory, 'neo4j', None), "_neo4j_reconnect_task", None) is not None,
    }


@router.post("/admin/neo4j/config")
async def admin_neo4j_config_update(request: Request, body: Neo4jConfigUpdate):
    """Update Neo4j connection settings for the running app and trigger reconnect.

    NOTE: This endpoint updates the in-memory values on the `memory` object. It does not persist changes to `.env`.
    Use this to quickly correct credentials and test reconnects without restarting the app.
    """
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    # Update runtime values
    changed = {}
    if body.neo4j_uri:
        memory.neo4j.neo4j_uri = body.neo4j_uri
        changed['neo4j_uri'] = body.neo4j_uri
    if body.neo4j_user:
        memory.neo4j.neo4j_user = body.neo4j_user
        changed['neo4j_user'] = body.neo4j_user
    if body.neo4j_password:
        memory.neo4j.neo4j_password = body.neo4j_password
        changed['neo4j_password'] = '***'  # do not echo back password

    # If we changed credentials, force a reconnect
    result = await memory.trigger_reconnect(force=True)
    return {"status": "ok", "changed": changed, "reconnect_result": result}


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\health.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\memory.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel
from typing import Optional
from src.bootstrap import get_components

router = APIRouter()


@router.get("/context/{session_id}")
async def get_context(request_obj: Request, session_id: str):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    active = await memory.get_active_context(session_id)
    summaries = await memory.get_summaries(session_id)
    return {"session_id": session_id, "active_context": active, "active_tokens": memory.count_tokens(active) if active else 0, "summaries": summaries}


@router.delete("/context/{session_id}")
async def clear_context(request_obj: Request, session_id: str):
    """Clear the active context cache for a session (force reset)."""
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    
    await memory.clear_session_context(session_id)
    return {"status": "success", "message": "Context cache cleared", "session_id": session_id}


@router.get("/memories/{category}")
async def get_memories_by_category(request_obj: Request, category: str, limit: int = 10):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    memories = await memory.get_recent_by_category(category, limit)
    return {"category": category, "count": len(memories), "memories": memories}


@router.get("/memories/search")
async def search_memories(request_obj: Request, query: str | None = None, category: str = None, tags: str = None, limit: int = 10):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    tag_list = [t.strip() for t in tags.split(',')] if tags else None
    # If query provided, pass as a content search
    memories = await memory.search_memories(query_text=query, category=category, tags=tag_list, limit=limit)
    return {"query": {"query": query, "category": category, "tags": tag_list}, "count": len(memories), "memories": memories}


@router.get("/memories")
async def get_memories(request_obj: Request, limit: int = 10):
    """Compatibility endpoint: return recent memories (search with no filters)."""
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    memories = await memory.search_memories(limit=limit)
    return {"count": len(memories), "memories": memories}


class MemoryAddRequest(BaseModel):
    category: str
    content: str
    tags: list[str] | None = None
    importance: int = 5
    metadata: dict | None = None


@router.post("/memories")
async def add_memory(request_obj: Request, body: MemoryAddRequest):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    # Ensure Neo4j is available (avoid accepting writes that won't be persisted)
    if not getattr(memory, 'neo4j', None) or not getattr(memory.neo4j, 'neo4j_driver', None):
        raise HTTPException(status_code=503, detail="Neo4j unavailable; cannot add memory")
    await memory.add_memory(session_id="api", content=body.content, category=body.category, tags=body.tags, importance=body.importance, metadata=body.metadata)
    return {"status": "success", "message": "Memory added"}


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\memory.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\openai_adapter.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, Depends, HTTPException
from src.bootstrap import get_components
from src.security import verify_api_key
from pydantic import BaseModel
from typing import Any, Dict
import time
from src.api.chat import ChatRequest, chat, chat_stream

router = APIRouter()


@router.post('/v1/chat/completions')
async def openai_chat_completions(request_obj: Request, body: Dict[str, Any], authenticated: bool = Depends(verify_api_key)):
    # very small adapter that maps old OpenAI payload to our ChatRequest
    model = body.get('model')
    messages = body.get('messages', [])
    stream = body.get('stream', False)
    system_prompt = None
    session_id = body.get('session_id') or body.get('conversation_id') or 'openai-adapter-session'
    user_message = None
    conversation_buffer = []
    for m in messages:
        role = m.get('role')
        content = m.get('content')
        if role == 'system':
            system_prompt = (system_prompt or '') + (content or '')
        elif role == 'user':
            conversation_buffer.append(f"User: {content}")
            user_message = content
        elif role == 'assistant':
            conversation_buffer.append(f"Assistant: {content}")

    if user_message is None:
        raise HTTPException(status_code=400, detail='No user message supplied')

    full_message = '\n'.join([m for m in conversation_buffer]) + f"\nUser: {user_message}"
    req = ChatRequest(session_id=session_id, message=full_message, system_prompt=system_prompt)
    if stream:
        return await chat_stream(request_obj, req, authenticated)
    else:
        chat_response = await chat(request_obj, req, authenticated)
        return {
            'id': f"chatcmpl-{int(time.time()*1000)}",
            'object': 'chat.completion',
            'created': int(time.time()),
            'model': model or 'ece-core',
            'choices': [{
                'index': 0,
                'message': {'role': 'assistant', 'content': chat_response.response},
                'finish_reason': 'stop'
            }],
            'usage': {
                'prompt_tokens': chat_response.context_tokens,
                'completion_tokens': len(chat_response.response.split()),
                'total_tokens': chat_response.context_tokens + len(chat_response.response.split())
            }
        }


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\openai_adapter.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plan.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, HTTPException, Depends
from pydantic import BaseModel
from typing import Optional
import logging

from src.bootstrap import get_components
from src.security import verify_api_key

logger = logging.getLogger(__name__)

router = APIRouter()


class PlanRequest(BaseModel):
    session_id: str
    message: str


@router.post("/plan")
async def plan(request_obj: Request, payload: PlanRequest, authenticated: bool = Depends(verify_api_key)):
    comps = get_components(request_obj.app)
    planner = getattr(request_obj.app.state, 'planner', None)
    plugin_manager = comps.get('plugin_manager')
    if not planner:
        raise HTTPException(status_code=503, detail="Planner not initialized")
    tools = []
    try:
        if plugin_manager and getattr(plugin_manager, 'enabled', False):
            tools = plugin_manager.list_tools()
    except Exception:
        tools = []
    try:
        plan = await planner.create_plan(payload.message, tools)
        return {"plan": plan}
    except Exception as e:
        logger.exception("/plan failed")
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plan.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plugins.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from src.bootstrap import get_components

router = APIRouter()


@router.get('/plugins/tools')
async def plugins_tools(request_obj: Request):
    components = get_components(request_obj.app)
    plugin_manager = components.get('plugin_manager')
    if plugin_manager and getattr(plugin_manager, 'enabled', False):
        return {'tools': plugin_manager.list_tools()}
    raise HTTPException(status_code=404, detail='Plugins disabled or not available')


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plugins.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\reason.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel
from typing import Any, List, Dict
from src.bootstrap import get_components

router = APIRouter()


class ReasonRequest(BaseModel):
    session_id: str
    question: str
    mode: str = "graph"


class ReasonResponse(BaseModel):
    answer: str
    reasoning_trace: List[Dict[str, Any]]
    iterations: int
    confidence: str


@router.post("/reason", response_model=ReasonResponse)
async def reason_with_graph(request_obj: Request, body: ReasonRequest):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    llm = components.get("llm")
    graph_reasoner = components.get("graph_reasoner")
    markov_reasoner = components.get("markov_reasoner")
    context_mgr = components.get("context_mgr")
    if not all([memory, llm, graph_reasoner, markov_reasoner]):
        raise HTTPException(status_code=503, detail="Not initialized")
    try:
        if body.mode == "graph":
            result = await graph_reasoner.reason(body.session_id, body.question)
        elif body.mode == "markov":
            initial_context = await context_mgr.build_context(body.session_id, body.question)
            answer = await markov_reasoner.reason(body.question, initial_context)
            result = {
                "answer": answer,
                "reasoning_trace": [{"type": "markovian", "result": "Used Markovian chunked reasoning"}],
                "iterations": markov_reasoner.max_chunks,
                "confidence": "medium"
            }
        else:
            raise HTTPException(status_code=400, detail=f"Invalid mode: {body.mode}")

        await context_mgr.update_context(body.session_id, body.question, result["answer"])
        return ReasonResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reasoning/trace/{session_id}")
async def get_reasoning_trace(request_obj: Request, session_id: str):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    summaries = await memory.get_summaries(session_id, limit=5)
    return {"session_id": session_id, "traces": summaries}


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\reason.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair.py (Section: BACKEND_PYTHON) ---

"""Compatibility shim for src.maintenance.repair.

This module re-exports `run_repair` from `src.maintenance.repair_wrapper` for
backwards compatibility. Keep it minimal to avoid duplication and errors.
"""
from __future__ import annotations

from src.maintenance.repair_wrapper import run_repair

__all__ = ['run_repair']
"""Compatibility shim for src.maintenance.repair.

This module re-exports `run_repair` from `src.maintenance.repair_wrapper` for
backwards compatibility.
"""
from __future__ import annotations

from src.maintenance.repair_wrapper import run_repair

__all__ = ['run_repair']
"""src.maintenance.repair

Lightweight wrapper that exposes `run_repair` from the scripts-based implementation while
providing a stable `src`-side import path for packaging.

This does not copy the heavy repair logic; it simply delegates to the implementation in
`scripts/neo4j/repair/repair_missing_links_similarity_embeddings.py` (or the legacy
`scripts/repair_missing_links_similarity_embeddings.py`). This keeps maintenance code in
one place, while enabling packaged builds to import `run_repair` from `src`.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _find_run_repair():
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    last_exc = None
    for c in candidates:
        try:
            mod = importlib.import_module(c)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception as e:
            last_exc = e
            logger.debug('repair wrapper: import failed for %s: %s', c, e)
    raise ModuleNotFoundError('run_repair not importable; tried: %s' % ','.join(candidates)) from last_exc


# resolve eagerly at import time; if unavailable, callers will get a ModuleNotFoundError
try:
    _run_repair = _find_run_repair()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Proxy to the real `run_repair` implementation.

    Returns the coroutine produced by the underlying implementation (do not await here).
    """
    if not _run_repair:
        raise ModuleNotFoundError('run_repair not available; ensure scripts package exists and is importable')
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper for run_repair.

This module provides a stable import for MemoryWeaver and other src code to call
`run_repair` without relying on dynamic imports of the `scripts` package.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_candidate() -> Any:
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper: failed to import %s', cand, exc_info=True)
    raise ModuleNotFoundError('Could not import run_repair from candidates: %s' % (', '.join(candidates),))


_run_repair = None
try:
    _run_repair = _import_candidate()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Delegate to the underlying repair script's `run_repair`.

    Raises ModuleNotFoundError if the underlying script isn't available.
    """
    if not _run_repair:
        raise ModuleNotFoundError('run_repair not available; ensure scripts package is present')
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    """Attempt to import run_repair from known candidate module paths.

    Returns the callable if found, otherwise raises ModuleNotFoundError.
    """
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


# Try to import the repair function at import time so that callers can directly use it.
_run_repair = None
try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


_run_repair = None

try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    # Defer raising until a caller attempts to use run_repair
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


_run_repair = None

try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    # Defer raising until a caller attempts to use run_repair
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper module
- Purpose: Export `run_repair` callable for MemoryWeaver and other internal callers.
- Implementation: Import the run_repair function from the existing scripts module path (scripts.neo4j.repair.repair_missing_links_similarity_embeddings)
  and re-export it. This keeps the heavy repair logic in the `scripts` area while ensuring a stable `src` import path that is bundled with the app.
"""
from __future__ import annotations

import importlib
import sys
import os
import logging
from typing import Any, Optional

logger = logging.getLogger(__name__)

# Attempt to import the repair module from the canonical nested script path
_run_repair_callable = None

# Ensure repo root in sys.path (defensive)
_repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if _repo_root not in sys.path:
    sys.path.insert(0, _repo_root)

try:
    _mod = importlib.import_module('scripts.neo4j.repair.repair_missing_links_similarity_embeddings')
    if hasattr(_mod, 'run_repair'):
        _run_repair_callable = getattr(_mod, 'run_repair')
except Exception:
    # Try legacy flattened path
    try:
        _mod = importlib.import_module('scripts.repair_missing_links_similarity_embeddings')
        if hasattr(_mod, 'run_repair'):
            _run_repair_callable = getattr(_mod, 'run_repair')
    except Exception:
        logger.exception('Repair wrapper could not import the repair module from scripts.* paths')


def run_repair(*args, **kwargs) -> Optional[Any]:
    """Call the underlying run_repair implementation.

    If the underlying script is not available, this function will raise ModuleNotFoundError.

    Returns whatever the underlying `run_repair` returns.
    """
    if not _run_repair_callable:
        raise ModuleNotFoundError("Could not import the repair script implementation (scripts.neo4j.repair.repair_missing_links_similarity_embeddings)")
    return _run_repair_callable(*args, **kwargs)

__all__ = ['run_repair']


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair_wrapper.py (Section: BACKEND_PYTHON) ---

"""Safe wrapper for the repair script implementation.

This helper exposes `run_repair` while delegating to the scripts-based implementation.
It avoids editing or duplicating heavy logic and ensures a stable `src` import path.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _load_run_repair():
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair_wrapper: failed to import %s', cand, exc_info=True)
    raise ModuleNotFoundError('repair_wrapper: could not find run_repair on candidates: %s' % ','.join(candidates))


try:
    _run_repair_fn = _load_run_repair()
except ModuleNotFoundError:
    _run_repair_fn = None


def run_repair(*args: Any, **kwargs: Any):
    if not _run_repair_fn:
        raise ModuleNotFoundError('repair_wrapper: run_repair not available; ensure scripts package is present and importable')
    # Filter kwargs to only those accepted by the underlying implementation to maintain compatibility
    try:
        import inspect
        sig = inspect.signature(_run_repair_fn)
        accepted = set(sig.parameters.keys())
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}
    except Exception:
        # If introspection fails for any reason, fall back to passing all kwargs
        filtered_kwargs = kwargs
    return _run_repair_fn(*args, **filtered_kwargs)


__all__ = ['run_repair']


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair_wrapper.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\weaver.py (Section: BACKEND_PYTHON) ---

"""
Memory Weaver - a lightweight engine that runs the repair logic as a scheduled/programmable task.

This module uses `src.maintenance.repair_wrapper.run_repair` as the canonical import path.
The wrapper dynamically loads the script-based implementation from one of these candidates:
 - `scripts.neo4j.repair.repair_missing_links_similarity_embeddings`
 - `scripts.repair_missing_links_similarity_embeddings`

The wrapper protects the package import from changes to the script location and provides
an introspection-based API to filter parameters passed to the underlying function.

The MemoryWeaver is safe to call in dry-run mode and only performs writes if the
master switch `WEAVER_COMMIT_ENABLED` is set in settings. The wrapper also raises a
clear `ModuleNotFoundError` when the implementation is not available and avoids
hard-failing the import path during startup.
"""
import asyncio
import time
import logging
import uuid
from datetime import datetime, timezone
from typing import Optional

from src.config import Settings, settings as GLOBAL_SETTINGS
from src.memory.redis_cache import RedisCache

logger = logging.getLogger(__name__)


class MemoryWeaver:
    def __init__(self, settings: Optional[Settings] = None):
        # Default to the module-global settings if none passed; this allows centralized overrides
        self.settings = settings or GLOBAL_SETTINGS

    async def weave_recent(self, hours: int | None = None, threshold: float | None = None, max_commit: int | None = None, candidate_limit: int | None = None, prefer_same_app: bool | None = None, dry_run: bool | None = None, csv_out: Optional[str] = None, run_id: Optional[str] = None, batch_size: int | None = None):
        """
        Run a repair cycle for the recent time window (hours) and commit matches if not dry_run.
        Returns: dict with run_id, processed items and commit count.
        """
        # Import the central `src.maintenance.repair` wrapper which exposes run_repair.
        # This wrapper keeps the weaver import simple (no dynamic path handling here) and is included in the 'src' package.
        try:
            from src.maintenance.repair_wrapper import run_repair
        except Exception as e:
            logger.error("MemoryWeaver: failed to import run_repair from src.maintenance.repair_wrapper; error=%s", e)
            return {'run_id': run_id, 'status': 'import_failed', 'message': str(e)}

        if not run_id:
            run_id = str(uuid.uuid4())

        # Build args
        # Resolve defaults from settings unless explicitly provided
        if hours is None:
            hours = self.settings.weaver_time_window_hours
        if threshold is None:
            threshold = self.settings.weaver_threshold
        if max_commit is None:
            max_commit = self.settings.weaver_max_commit
        if prefer_same_app is None:
            prefer_same_app = self.settings.weaver_prefer_same_app
        if dry_run is None:
            dry_run = self.settings.weaver_dry_run_default

        # Master Switch: If weaver_commit_enabled is True, we want to actually commit (auto-apply)
        if self.settings.weaver_commit_enabled:
            # If operator has enabled commit, force write-mode and clear dry-run
            dry_run = False
            commit_mode = True
        else:
            commit_mode = False

        # Resolve candidate limit and batch size defaults using settings when not supplied
        if candidate_limit is None:
            candidate_limit = self.settings.weaver_candidate_limit
        if batch_size is None:
            batch_size = getattr(self.settings, 'weaver_batch_size_default', self.settings.llm_embeddings_default_batch_size)

        params = {
            'threshold': threshold,
            'limit': 1000,
            'candidate_limit': candidate_limit,
            'dry_run': dry_run,
            'csv_out': csv_out,
            'time_window_hours': hours,
            'prefer_same_app': prefer_same_app,
            'min_origin_length': 100,
            'exclude_phrases': ["Genesis memory", "ECE Core System Initialized"],
            'delta': self.settings.weaver_delta,
            'max_commit': max_commit,
            # Use commit_mode (explicit master switch) when set; otherwise infer from dry_run
            'commit': commit_mode or (not dry_run),
            'run_id': run_id,
            'exclude_tag': self.settings.weaver_exclude_tag,
            'batch_size': batch_size,
        }

        logger.info(f"MemoryWeaver: Starting weave run {run_id} (hours={hours}, threshold={threshold}, commit={not dry_run})")

        # Adaptive throttling: process the large 'limit' value in smaller batches and sleep between them.
        sleep_between = getattr(self.settings, 'weaver_sleep_between_batches', 1.0)

        # Resolve batch size using prioritized sources: function arg -> explicit env -> default config values
        resolved_batch = int(batch_size or getattr(self.settings, 'weaver_batch_size', None) or getattr(self.settings, 'weaver_batch_size_default', None) or getattr(self.settings, 'llm_embeddings_default_batch_size', 2))
        if resolved_batch <= 0:
            resolved_batch = 1

        total_limit = int(params.get('limit', 1000))
        offset = 0

        # If Redis is available, check for recent user activity and pause if within last N seconds
        # 5 minutes (300 seconds) is used as the active-user window
        redis_client = RedisCache()
        await redis_client.initialize()

        while offset < total_limit:
            # Check user activity to avoid collisions with interactive sessions
            try:
                last_active = None
                if redis_client.redis:
                    val = await redis_client.redis.get(f"session:{self.settings.anchor_session_id}:last_active_at")
                    if val:
                        try:
                            last_active = int(val)
                        except Exception:
                            # value might be an ISO timestamp; try conversion
                            try:
                                last_active = int(float(val))
                            except Exception:
                                last_active = None
                if last_active:
                    now = int(time.time())
                    if (now - last_active) < 300:
                        logger.info(f"MemoryWeaver: Paused weave run {run_id} because user activity detected {now - last_active}s ago")
                        await redis_client.close()
                        return {'run_id': run_id, 'status': 'paused_user_active', 'delay_seconds': now - last_active}
            except Exception as e:
                logger.debug("MemoryWeaver: Failed to check last_active, continuing. err=%s", e)

            # Prepare params for this batch
            batch_params = dict(params)
            batch_params['limit'] = resolved_batch
            batch_params['skip'] = offset
            logger.info(f"MemoryWeaver: Running batch {offset}:{offset + resolved_batch} (limit {resolved_batch})")
            try:
                await run_repair(threshold=batch_params['threshold'], limit=batch_params['limit'], candidate_limit=batch_params['candidate_limit'], dry_run=batch_params['dry_run'], csv_out=batch_params['csv_out'], time_window_hours=batch_params['time_window_hours'], prefer_same_app=batch_params['prefer_same_app'], min_origin_length=batch_params['min_origin_length'], exclude_phrases=batch_params['exclude_phrases'], delta=batch_params['delta'], max_commit=batch_params['max_commit'], commit=batch_params['commit'], run_id=batch_params['run_id'], exclude_tag=batch_params['exclude_tag'], batch_size=batch_params['batch_size'], skip=batch_params.get('skip', 0))
            except Exception as e:
                logger.error(f"MemoryWeaver: Batch failed for range {offset}:{offset + resolved_batch} with error: {e}")
                # If a batch fails, record and continue (the underlying run_repair already uses resilient embedding backoff)
            # Sleep between batches to avoid simultaneous bursts with user requests
            offset += resolved_batch
            if offset < total_limit:
                logger.debug(f"MemoryWeaver: Sleeping {sleep_between}s between batches to avoid resource contention")
                await asyncio.sleep(sleep_between)

        await redis_client.close()
        logger.info(f"MemoryWeaver: Completed weave run {run_id}")
        return {'run_id': run_id}


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\weaver.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\manager.py (Section: BACKEND_PYTHON) ---

import logging
import tiktoken
from typing import Optional, List, Dict, Any
from src.config import settings
from src.content_utils import clean_content, is_json_like, is_html_like, has_technical_signal
import hashlib
from src.vector_adapter import create_vector_adapter
from src.memory.redis_cache import RedisCache
from src.memory.neo4j_store import Neo4jStore
from src.distiller import distill_moment

logger = logging.getLogger(__name__)

class TieredMemory:
    """
    Orchestrator for Tiered Memory (Redis + Neo4j).
    Replaces the monolithic src/memory.py.
    """

    def __init__(self, neo4j_uri: Optional[str] = None, redis_url: Optional[str] = None, neo4j_user: Optional[str] = None, neo4j_password: Optional[str] = None, llm_client=None):
        self.redis = RedisCache(redis_url)
        self.neo4j = Neo4jStore(neo4j_uri, neo4j_user, neo4j_password)
        self.llm_client = llm_client
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # Vector support
        self.vector_adapter = None
        self._vector_enabled = getattr(settings, "vector_enabled", False)
        if self._vector_enabled:
            try:
                self.vector_adapter = create_vector_adapter()
            except Exception as e:
                logger.warning(f"Failed to create vector adapter: {e}")

        # Backwards-compatible property accessors for legacy tests & code

    async def initialize(self):
        """Initialize all stores."""
        await self.redis.initialize()
        await self.neo4j.initialize()
        if self.vector_adapter and hasattr(self.vector_adapter, "initialize"):
            await self.vector_adapter.initialize()
        
        # Auto-init LLM for embeddings if needed
        if getattr(settings, "vector_auto_embed", False) and not self.llm_client:
            try:
                from src.llm import LLMClient
                self.llm_client = LLMClient()
            except Exception as e:
                logger.warning(f"Failed to init LLM client for auto-embed: {e}")

    @property
    def neo4j_driver(self):
        return getattr(self.neo4j, 'neo4j_driver', None)

    @neo4j_driver.setter
    def neo4j_driver(self, val):
        if self.neo4j:
            self.neo4j.neo4j_driver = val

    @property
    def neo4j_uri(self):
        return getattr(self.neo4j, 'neo4j_uri', None)

    @neo4j_uri.setter
    def neo4j_uri(self, val):
        if self.neo4j:
            self.neo4j.neo4j_uri = val

    @property
    def neo4j_user(self):
        return getattr(self.neo4j, 'neo4j_user', None)

    @neo4j_user.setter
    def neo4j_user(self, val):
        if self.neo4j:
            self.neo4j.neo4j_user = val

    @property
    def neo4j_password(self):
        return getattr(self.neo4j, 'neo4j_password', None)

    @neo4j_password.setter
    def neo4j_password(self, val):
        if self.neo4j:
            self.neo4j.neo4j_password = val

    @property
    def _neo4j_reconnect_attempts(self):
        return getattr(self.neo4j, '_neo4j_reconnect_attempts', 0)

    @property
    def _neo4j_reconnect_task(self):
        return getattr(self.neo4j, '_neo4j_reconnect_task', None)

    @property
    def _neo4j_auth_error(self):
        return getattr(self.neo4j, '_neo4j_auth_error', False)

    async def close(self):
        """Close all stores."""
        if self.redis:
            await self.redis.close()
        if self.neo4j:
            await self.neo4j.close()

    async def trigger_reconnect(self, force: bool = False) -> dict:
        """Proxy to Neo4j trigger reconnect to expose admin command."""
        if not self.neo4j:
            return {"started": False, "message": "Neo4j store not configured"}
        return await self.neo4j.trigger_reconnect(force=force)

    # Delegate Redis methods
    async def get_active_context(self, session_id: str) -> str:
        if self.redis:
            return await self.redis.get_active_context(session_id)
        return ""  # Return empty string when Redis is not available

    async def save_active_context(self, session_id: str, context: str):
        if self.redis:
            await self.redis.save_active_context(session_id, context)
        # Silently fail when Redis is not available

    async def touch_session(self, session_id: str):
        """Mark session as active by updating last_active timestamp in Redis without changing the active context."""
        try:
            if self.redis and self.redis.redis:
                import time
                await self.redis.redis.set(f"session:{session_id}:last_active_at", int(time.time()), ex=settings.redis_ttl)
        except Exception:
            # Not critical; ignore failures
            pass

    async def clear_session_context(self, session_id: str):
        """Clear the active (hot) context for a session."""
        if self.redis:
            await self.redis.clear_session(session_id)

    # Delegate Neo4j methods
    async def add_memory(self, session_id: Optional[str] = None, content: str = "", category: Optional[str] = None, tags: Optional[List[str]] = None, importance: int = 5, metadata: Optional[Dict[str, Any]] = None, llm_client=None):
        # 0. Preliminary cleaning & hygiene checks
        raw_content = content or ''
        # Skip JSON dump / HTML noisy content unless it contains technical signals
        if is_json_like(raw_content) and not has_technical_signal(raw_content):
            logger.warning('Skipping add_memory: json-like content without technical signal')
            return None
        if is_html_like(raw_content) and not has_technical_signal(raw_content):
            logger.warning('Skipping add_memory: html-like content without technical signal')
            return None

        # Compute cleaned content and detect technical signal
        tech_signal = has_technical_signal(raw_content)
        content_cleaned = clean_content(raw_content, remove_emojis=not tech_signal, remove_non_ascii=False, annotate_technical=tech_signal)
        tech_signal = has_technical_signal(raw_content)
        if not tech_signal and (not content_cleaned or len(content_cleaned) < 20):
            logger.warning('Skipping add_memory: content empty or too short after cleaning')
            return None

        # Compute a content hash for dedup (based on cleaned content to avoid duplicate noisy entries)
        content_hash = hashlib.sha256((content_cleaned or '').encode('utf-8')).hexdigest()

        # 1. Distill entities (Graph Wiring)
        entities = []
        try:
            # Use provided llm_client or self.llm_client
            client = llm_client or self.llm_client
            if client and content_cleaned:
                distilled = await distill_moment(content_cleaned, llm_client=client, metadata=metadata)
                if isinstance(distilled, dict):
                    entities = distilled.get("entities", [])
        except Exception as e:
            logger.warning(f"Failed to distill entities: {e}")

        # 2. Add to Neo4j (Graph + Document)
        # Pass cleaned content and additional properties to Neo4j
        # Tag technical content
        if tech_signal:
            tags = tags or []
            if 'technical' not in tags and '#technical' not in tags:
                tags.append('#technical')

        memory_id = await self.neo4j.add_memory(session_id, content, category, tags, importance, metadata, entities=entities, content_cleaned=content_cleaned, content_hash=content_hash, content_embedding_text=content_cleaned if not tech_signal else content_cleaned)
        
        # 3. Vector Indexing (Semantic Search)
        if self.vector_adapter and self._vector_enabled and memory_id and content_cleaned:
            try:
                client = llm_client or self.llm_client
                if client:
                    # Generate embedding
                    embeddings = await client.get_embeddings(content_cleaned)
                    if embeddings and len(embeddings) > 0:
                        # Handle list of lists or single list
                        embedding = embeddings[0] if isinstance(embeddings[0], list) else embeddings
                        # Index
                        await self.vector_adapter.index_chunk(
                            embedding_id=f"mem:{memory_id}",
                            node_id=memory_id,
                            chunk_index=0,
                            embedding=embedding,
                            metadata={
                                "content": content_cleaned,
                                "category": category,
                                "session_id": session_id,
                                "importance": importance,
                                "created_at": metadata.get("created_at") if metadata else None
                            }
                        )
            except Exception as e:
                logger.warning(f"Failed to index vector: {e}")

        # Return the created memory id
        return memory_id
    async def search_memories(self, query_text: Optional[str] = None, category: Optional[str] = None, tags: Optional[List[str]] = None, limit: int = 10) -> List[Dict[str, Any]]:
        if not query_text:
            # Fallback to recent if no query
            # Note: Neo4jStore needs a get_recent method, adding it to TODO or using direct cypher
            # For now, simple search
            return await self.neo4j.search_memories("", category, limit)
        return await self.neo4j.search_memories(query_text, category, limit)

    async def search_memories_neo4j(self, query_text: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search memories specifically in Neo4j (full-text)."""
        return await self.neo4j.search_memories(query_text, None, limit)

    async def get_recent_by_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Get recent memories by category."""
        # This requires a new method in Neo4jStore or a direct query here.
        # Adding direct query support via Neo4jStore.execute_cypher would be cleaner,
        # but for now let's add a helper to Neo4jStore or just use search with empty query if supported.
        # Actually, let's implement it properly by delegating to a new method we'll add to Neo4jStore,
        # or using search_memories with empty query if it supports sorting by time.
        # The current search_memories implementation in Neo4jStore sorts by nothing explicit if no query.
        # Let's add a dedicated method to Neo4jStore in a separate step, but for now we can try search.
        # Wait, the error is AttributeError on TieredMemory, so we MUST define it here.
        return await self.neo4j.get_recent_by_category(category, limit)

    async def get_summaries(self, session_id: str, limit: int = 5) -> List[str]:
        """Get recent summaries."""
        # Delegate to Neo4jStore
        return await self.neo4j.get_summaries(session_id, limit)

    async def save_summary(self, session_id: str, summary: str):
        """Save a conversation summary."""
        await self.neo4j.add_memory(
            session_id=session_id,
            content=summary,
            category="summary",
            tags=["summary"],
            importance=3,
            metadata={}
        )

    async def flush_to_neo4j(self, session_id: str, summary: str, original_tokens: int):
        """Flush summary to Neo4j."""
        await self.neo4j.add_memory(
            session_id=session_id,
            content=summary,
            category="summary",
            tags=["summary", "auto-flush"],
            importance=3,
            metadata={"original_token_count": original_tokens}
        )

    def count_tokens(self, text: str) -> int:
        if not text: return 0
        try: return len(self.tokenizer.encode(text, disallowed_special=()))
        except: return len(text) // 4


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\manager.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\neo4j_store.py (Section: BACKEND_PYTHON) ---

import json
import asyncio
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from neo4j import AsyncGraphDatabase
from neo4j.exceptions import AuthError
from src.config import settings

logger = logging.getLogger(__name__)

class Neo4jStore:
    """Handles Neo4j interactions for TieredMemory."""

    def __init__(self, neo4j_uri: Optional[str] = None, neo4j_user: Optional[str] = None, neo4j_password: Optional[str] = None):
        self.neo4j_uri = neo4j_uri or settings.neo4j_uri
        self.neo4j_user = neo4j_user or settings.neo4j_user
        self.neo4j_password = neo4j_password or settings.neo4j_password
        self.neo4j_driver = None
        self._neo4j_reconnect_task = None
        self._neo4j_reconnect_attempts = 0
        self._neo4j_auth_error = False

    async def initialize(self):
        """Connect to Neo4j."""
        if not getattr(settings, "neo4j_enabled", True):
            logger.info("Neo4j disabled by configuration")
            return

        try:
            self.neo4j_driver = AsyncGraphDatabase.driver(
                self.neo4j_uri,
                auth=(self.neo4j_user, self.neo4j_password)
            )
            async with self.neo4j_driver.session() as session:
                await session.run("RETURN 1")
                # Create schema indexes to prevent warnings and improve performance
                await session.run("CREATE INDEX memory_category IF NOT EXISTS FOR (n:Memory) ON (n.category)")
                await session.run("CREATE INDEX memory_created_at IF NOT EXISTS FOR (n:Memory) ON (n.created_at)")
                await session.run("CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name)")
                # Index for deduplication by content hash
                await session.run("CREATE INDEX memory_content_hash IF NOT EXISTS FOR (n:Memory) ON (n.content_hash)")
                # Indexes to support provenance/freshness-driven queries
                await session.run("CREATE INDEX memory_provenance_score IF NOT EXISTS FOR (n:Memory) ON (n.provenance_score)")
                await session.run("CREATE INDEX memory_freshness_score IF NOT EXISTS FOR (n:Memory) ON (n.freshness_score)")
            logger.info("Neo4j connected")
        except Exception as e:
            logger.warning(f"Neo4j unavailable: {e}")
            if isinstance(e, AuthError) or "unauthorized" in str(e).lower():
                self._neo4j_auth_error = True
            self.neo4j_driver = None
            if getattr(settings, 'neo4j_reconnect_enabled', False) and not self._neo4j_auth_error:
                self._neo4j_reconnect_task = asyncio.create_task(self._neo4j_reconnect_loop())

    async def close(self):
        """Close Neo4j connection."""
        if self.neo4j_driver:
            await self.neo4j_driver.close()
        if self._neo4j_reconnect_task:
            self._neo4j_reconnect_task.cancel()

    async def _neo4j_reconnect_loop(self):
        """Background retry loop."""
        delay = getattr(settings, 'neo4j_reconnect_initial_delay', 5)
        max_attempts = getattr(settings, 'neo4j_reconnect_max_attempts', 6)
        backoff = getattr(settings, 'neo4j_reconnect_backoff_factor', 2.0)
        attempt = 0
        
        while attempt < max_attempts and self.neo4j_driver is None:
            attempt += 1
            try:
                driver = AsyncGraphDatabase.driver(
                    self.neo4j_uri,
                    auth=(self.neo4j_user, self.neo4j_password)
                )
                async with driver.session() as session:
                    await session.run("RETURN 1")
                self.neo4j_driver = driver
                logger.info("Neo4j reconnected successfully")
                break
            except Exception as e:
                if isinstance(e, AuthError):
                    self._neo4j_auth_error = True
                    break
                await asyncio.sleep(delay)
                delay *= backoff

    async def trigger_reconnect(self, force: bool = False) -> dict:
        """Trigger a reconnect loop for Neo4j. If force is True, close any existing driver and start a new reconnect."""
        if force and self.neo4j_driver:
            try:
                await self.neo4j_driver.close()
            except Exception:
                pass
            self.neo4j_driver = None

        if self._neo4j_auth_error:
            return {"started": False, "message": "Neo4j auth error; credential fix required"}

        # If a reconnect task is already running, return status
        if self._neo4j_reconnect_task and not self._neo4j_reconnect_task.done():
            return {"started": False, "message": "Reconnect already in progress"}

        self._neo4j_reconnect_task = asyncio.create_task(self._neo4j_reconnect_loop())
        return {"started": True}

    async def execute_cypher(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Execute raw Cypher query."""
        if not self.neo4j_driver:
            return []
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(query, params or {})
                return await result.data()
        except Exception as e:
            logger.error(f"Cypher execution failed: {e}")
            return []

    async def add_memory(self, session_id: str, content: str, category: str, tags: List[str], importance: int, metadata: Dict[str, Any], entities: List[Dict[str, Any]] = None, content_cleaned: str = None, content_hash: str = None, content_embedding_text: str = None):
        """Add memory node and link entities."""
        if not self.neo4j_driver:
            return
        try:
            async with self.neo4j_driver.session() as session:
                # Compute / enforce app_id inside metadata if missing
                if metadata is None:
                    metadata = {}
                # Prefer provided app_id in metadata, otherwise compute deterministically
                app_id = None
                try:
                    if isinstance(metadata, dict) and metadata.get('app_id'):
                        app_id = str(metadata.get('app_id'))
                    elif isinstance(metadata, dict) and metadata.get('source') and metadata.get('chunk_index') is not None:
                        import uuid
                        ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')
                        app_id = str(uuid.uuid5(ns, f"{metadata.get('source')}:{metadata.get('chunk_index')}"))
                    else:
                        import uuid
                        ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')
                        app_id = str(uuid.uuid5(ns, (content or '')[:4096]))
                except Exception:
                    # Fallback to uuid4 if anything goes wrong
                    import uuid
                    app_id = str(uuid.uuid4())
                # Write app_id back into metadata JSON for consistency
                if isinstance(metadata, dict):
                    metadata['app_id'] = app_id
                else:
                    # if metadata is a string, attempt to parse and re-serialize with app_id
                    try:
                        md = json.loads(metadata) if isinstance(metadata, str) and metadata else {}
                        md['app_id'] = app_id
                        metadata = md
                    except Exception:
                        metadata = {'app_id': app_id}

                # Deduplication: if a content_hash exists, check if we've already stored it
                if content_hash:
                    dedup_q = "MATCH (m:Memory) WHERE m.content_hash = $content_hash RETURN elementId(m) as id LIMIT 1"
                    dedup_res = await session.run(dedup_q, {'content_hash': content_hash})
                    dedup_rec = await dedup_res.single()
                    if dedup_rec and dedup_rec.get('id'):
                        # Found existing memory; do not create duplicate
                        return dedup_rec.get('id')

                # Compute provenance_score and freshness_score defaults
                def _derive_provenance_score(meta: Dict[str, Any], category: str) -> float:
                    try:
                        if category:
                            c = category.lower()
                            if c == 'code':
                                return 1.0
                            if c in ('log', 'logs'):
                                return 0.95
                            if c in ('doc', 'docs', 'documentation'):
                                return 0.8
                            if c in ('chat', 'conversation', 'message'):
                                return 0.4
                    except Exception:
                        pass
                    # inspect metadata for source file info
                    try:
                        if isinstance(meta, dict):
                            src = meta.get('source', '')
                            if isinstance(src, str):
                                if src.endswith('.py') or src.endswith('.js'):
                                    return 1.0
                                if src.endswith('.log'):
                                    return 0.95
                            # Source type field
                            if meta.get('source_type') == 'doc':
                                return 0.8
                    except Exception:
                        pass
                    return float(getattr(settings, 'memory_default_provenance_score', 0.5))

                provenance_score = _derive_provenance_score(metadata or {}, category)
                freshness_score = float(getattr(settings, 'memory_default_freshness_score', 1.0))
                # default last_verified_at is None; Verifier will fill it
                last_verified_at = metadata.get('last_verified_at') if isinstance(metadata, dict) else None

                # Create Memory node with app_id property
                result = await session.run(
                    """
                    CREATE (m:Memory {
                        session_id: $session_id,
                        content: $content,
                        content_cleaned: $content_cleaned,
                        content_hash: $content_hash,
                        provenance_score: $provenance_score,
                        freshness_score: $freshness_score,
                        last_verified_at: $last_verified_at,
                        content_embedding_text: $content_embedding_text,
                        category: $category,
                        app_id: $app_id,
                        tags: $tags,
                        importance: $importance,
                        metadata: $metadata,
                        created_at: $created_at
                    })
                    RETURN elementId(m) as id
                    """,
                    {
                        "session_id": session_id or "unknown",
                        "content": content,
                        "content_cleaned": content_cleaned,
                        "content_hash": content_hash,
                        "content_embedding_text": content_embedding_text,
                        "category": category,
                        "app_id": app_id,
                        "tags": tags or [],
                        "importance": importance,
                        "metadata": json.dumps(metadata, default=str) if metadata else None,
                        "provenance_score": provenance_score,
                        "freshness_score": freshness_score,
                        "last_verified_at": last_verified_at,
                        "created_at": datetime.now(timezone.utc).isoformat()
                    }
                )
                record = await result.single()
                memory_id = record["id"] if record else None

                # Link Entities if provided
                if entities and memory_id:
                    await session.run(
                        """
                        MATCH (m:Memory) WHERE elementId(m) = $memory_id
                        UNWIND $entities as ent
                        MERGE (e:Entity {name: ent.text})
                        ON CREATE SET e.type = ent.type, e.metadata = ent.metadata
                        MERGE (m)-[:MENTIONS]->(e)
                        """,
                        {
                            "memory_id": memory_id,
                            "entities": [
                                {
                                    "text": e.get("text"),
                                    "type": e.get("type", "unknown"),
                                    "metadata": json.dumps(e.get("metadata", {}))
                                }
                                for e in entities if e.get("text")
                            ]
                        }
                    )
                
                return memory_id
        except Exception as e:
            logger.error(f"Failed to add memory: {e}")
            return None

    async def search_memories(self, query: str, category: Optional[str], limit: int) -> List[Dict[str, Any]]:
        """Search memories."""
        if not self.neo4j_driver:
            return []
        
        # Prefer fulltext index search for more reliable matches (memorySearch index)
        # Fallback to a CONTAINS search if the fulltext index isn't available or fails.
        cypher_fulltext = """
        CALL db.index.fulltext.queryNodes('memorySearch', $query) YIELD node, score
        WHERE ($category IS NULL OR node.category = $category)
        RETURN elementId(node) as id, node as m, score
        ORDER BY score DESC
        LIMIT $limit
        """
        
        try:
            async with self.neo4j_driver.session() as session:
                try:
                    result = await session.run(cypher_fulltext, {"query": query, "category": category, "limit": limit})
                    records = await result.data()
                except Exception:
                    # If fulltext index isn't available, fallback to an older contains query
                    cypher_contains = """
                    MATCH (m:Memory)
                    WHERE m.content CONTAINS $query
                    """ + ("AND m.category = $category" if category else "") + """
                    RETURN elementId(m) as id, m
                    LIMIT $limit
                    """
                    result = await session.run(cypher_contains, {"query": query, "category": category, "limit": limit})
                    records = await result.data()
                return [self._parse_memory_record(r) for r in records]
        except Exception as e:
            logger.error(f"Memory search failed: {e}")
            return []

    def _parse_memory_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Neo4j record into standard memory dict."""
        node = record.get("m") or record.get("node")
        eid = record.get("id")
        score = record.get("score")
        
        # Defensive parsing
        tags = node.get("tags")
        if isinstance(tags, str):
            try: tags = json.loads(tags)
            except: tags = [tags]
        
        meta = node.get("metadata")
        if isinstance(meta, str):
            try: meta = json.loads(meta)
            except: meta = {}
            
        return {
            "id": eid,
            "memory_id": eid,
            "content": node.get("content"),
            "tags": tags or [],
            "importance": node.get("importance", 5),
            "session_id": node.get("session_id"),
            "timestamp": node.get("created_at"),
            "category": node.get("category"),
            "metadata": meta or {},
            "score": score if score is not None else node.get("importance", 5) / 10.0
        }

    async def get_recent_by_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Get recent memories by category."""
        if not self.neo4j_driver:
            return []
        
        cypher = """
        MATCH (m:Memory)
        WHERE m.category = $category
        RETURN elementId(m) as id, m
        ORDER BY m.created_at DESC
        LIMIT $limit
        """
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(cypher, {"category": category, "limit": limit})
                records = await result.data()
                return [self._parse_memory_record(r) for r in records]
        except Exception as e:
            logger.error(f"Failed to get recent memories for category {category}: {e}")
            return []

    async def get_summaries(self, session_id: str, limit: int = 5) -> List[str]:
        """Get recent summaries for a session."""
        if not self.neo4j_driver:
            return []
            
        cypher = """
        MATCH (m:Memory)
        WHERE m.session_id = $session_id AND m.category = 'summary'
        RETURN m.content as content
        ORDER BY m.created_at DESC
        LIMIT $limit
        """
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(cypher, {"session_id": session_id, "limit": limit})
                records = await result.data()
                return [r["content"] for r in records if r.get("content")]
        except Exception as e:
            logger.error(f"Failed to get summaries for session {session_id}: {e}")
            return []


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\neo4j_store.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\redis_cache.py (Section: BACKEND_PYTHON) ---

import redis.asyncio as redis
import logging
import time
from typing import Optional
from src.config import settings

logger = logging.getLogger(__name__)

class RedisCache:
    """Handles Redis interactions for TieredMemory."""
    
    def __init__(self, redis_url: Optional[str] = None):
        self.redis_url = redis_url or settings.redis_url
        self.redis = None

    async def initialize(self):
        """Connect to Redis."""
        try:
            maybe_client = redis.from_url(self.redis_url, decode_responses=True)
            if hasattr(maybe_client, "__await__"):
                self.redis = await maybe_client
            else:
                self.redis = maybe_client
            
            ping_ret = self.redis.ping()
            if hasattr(ping_ret, "__await__"):
                await ping_ret
            logger.info("Redis connected")
        except redis.ConnectionError as e:
            logger.warning(f"Redis unavailable: {e}")
            self.redis = None
        except Exception as e:
            logger.error(f"Redis connection failed: {e}")
            self.redis = None

    async def close(self):
        """Close Redis connection."""
        if self.redis:
            try:
                await self.redis.close()
            except Exception:
                pass

    async def get_active_context(self, session_id: str) -> str:
        """Get active context from Redis."""
        if not self.redis:
            return ""
        try:
            context = await self.redis.get(f"session:{session_id}:context")
            return context or ""
        except Exception as e:
            logger.error(f"Redis get failed for session {session_id}: {e}")
            return ""

    async def save_active_context(self, session_id: str, context: str):
        """Save active context to Redis with TTL."""
        if not self.redis:
            return
        try:
            await self.redis.set(f"session:{session_id}:context", context, ex=settings.redis_ttl)
            # Also set a last-active timestamp to help background tasks avoid interfering with active sessions
            try:
                await self.redis.set(f"session:{session_id}:last_active_at", int(time.time()), ex=settings.redis_ttl)
            except Exception:
                # Not critical; continue saving context even if last_active failed
                pass
        except Exception as e:
            logger.error(f"Redis set failed for session {session_id}: {e}")

    async def clear_session(self, session_id: str):
        """Completely remove a session's active context and metadata."""
        if not self.redis:
            return
        try:
            # Delete both the context text and the activity timestamp
            keys = [f"session:{session_id}:context", f"session:{session_id}:last_active_at"]
            await self.redis.delete(*keys)
            logger.info(f"Cleared Redis cache for session: {session_id}")
        except Exception as e:
            logger.error(f"Failed to clear session {session_id}: {e}")


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\redis_cache.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\plugins\browser_bridge\plugin.py (Section: BACKEND_PYTHON) ---

"""
Browser Bridge Plugin for ECE

This plugin provides API endpoints for the Chrome extension to communicate with the ECE system.
It allows for chat ingestion and context retrieval from the browser.
"""
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Dict, Any
import logging
from src.config import settings
from src.security import verify_api_key

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/v1/browser", tags=["browser_bridge"])

# Pydantic models for request/response
class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str
    timestamp: str = None


class IngestRequest(BaseModel):
    messages: List[ChatMessage]
    source_url: str = None
    session_id: str = None


class IngestResponse(BaseModel):
    success: bool
    processed_count: int
    message: str


class ContextRequest(BaseModel):
    draft_prompt: str
    max_results: int = 10


class ContextResponse(BaseModel):
    success: bool
    context: str
    retrieved_count: int


# Dependencies
async def get_components():
    """Get ECE components from app state."""
    from src.bootstrap import get_components as get_ece_components
    # This will be injected by the main app
    return get_ece_components


@router.post("/ingest", response_model=IngestResponse)
async def ingest_browser_chat(
    request: IngestRequest,
    # components: dict = Depends(get_components),
    auth: bool = Depends(verify_api_key)  # Only if auth is required
):
    """
    Ingest chat messages from the browser extension.
    Saves messages to Neo4j using the Archivist agent.
    """
    try:
        # Import components from the main app state
        from src.app_factory import app
        components = {
            "memory": getattr(app.state, "memory", None),
            "archivist_agent": getattr(app.state, "archivist_agent", None),
            "context_mgr": getattr(app.state, "context_mgr", None),
        }
        
        # Validate components
        if not components["memory"]:
            raise HTTPException(status_code=500, detail="Memory system not available")
        
        # Process each message and save to memory
        processed_count = 0
        for msg in request.messages:
            try:
                # Add to memory system
                memory_id = await components["memory"].add_memory(
                    session_id=request.session_id or "browser_session",
                    content=f"[Browser Chat] {msg.role.title()}: {msg.content}",
                    category="browser_chat",
                    tags=["browser", "chat", msg.role.lower()],
                    importance=5,
                    metadata={
                        "source": "browser_extension",
                        "url": request.source_url,
                        "role": msg.role,
                        "timestamp": msg.timestamp
                    }
                )
                
                if memory_id:
                    processed_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to process message: {e}")
                continue
        
        logger.info(f"Processed {processed_count}/{len(request.messages)} browser chat messages")
        
        return IngestResponse(
            success=True,
            processed_count=processed_count,
            message=f"Successfully processed {processed_count} out of {len(request.messages)} messages"
        )
        
    except Exception as e:
        logger.error(f"Error ingesting browser chat: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to ingest chat: {str(e)}")


@router.post("/context", response_model=ContextResponse)
async def get_context_for_prompt(
    request: ContextRequest,
    # components: dict = Depends(get_components),
    auth: bool = Depends(verify_api_key)  # Only if auth is required
):
    """
    Retrieve relevant context for a draft prompt from the browser.
    """
    try:
        # Import components from the main app state
        from src.app_factory import app
        components = {
            "memory": getattr(app.state, "memory", None),
            "context_mgr": getattr(app.state, "context_mgr", None),
        }
        
        # Validate components
        if not components["memory"]:
            raise HTTPException(status_code=500, detail="Memory system not available")
        
        # Search for relevant memories
        try:
            # Try to search with the draft prompt
            results = await components["memory"].search_memories(
                query_text=request.draft_prompt,
                limit=request.max_results
            )
        except Exception as search_error:
            logger.warning(f"Search failed: {search_error}")
            results = []
        
        # Format the context
        if results:
            context_parts = ["Relevant Memories:", "-" * 40]
            for i, result in enumerate(results, 1):
                content = result.get('content', '')[:200] + "..." if len(result.get('content', '')) > 200 else result.get('content', '')
                score = result.get('score', 0)
                context_parts.append(f"{i}. [{score:.2f}] {content}")
                context_parts.append("")
            
            formatted_context = "\n".join(context_parts)
        else:
            formatted_context = "No relevant memories found."
        
        logger.info(f"Retrieved {len(results)} context items for prompt: {request.draft_prompt[:50]}...")
        
        return ContextResponse(
            success=True,
            context=formatted_context,
            retrieved_count=len(results)
        )
        
    except Exception as e:
        logger.error(f"Error retrieving context: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve context: {str(e)}")


@router.get("/health")
async def browser_bridge_health():
    """Health check for the browser bridge."""
    return {
        "status": "healthy",
        "service": "Browser Bridge Plugin",
        "api_version": "1.0.0"
    }


# Additional utility endpoints
@router.get("/session/current")
async def get_current_session():
    """Get information about the current browser session."""
    return {
        "session_id": "browser_session",
        "connected": True,
        "features": ["chat_ingestion", "context_retrieval"]
    }

--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\plugins\browser_bridge\plugin.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\archivist.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import logging
import time
import json
from datetime import datetime, timezone

from src.bootstrap import get_components
from src.security import verify_api_key
from src.models import PlaintextMemory, SourceType

logger = logging.getLogger(__name__)

router = APIRouter(tags=["archivist"])

class ExtensionIngestRequest(BaseModel):
    content: str
    type: str
    adapter: Optional[str] = None

class IngestResponse(BaseModel):
    status: str
    memory_ids: List[str]
    message: str

@router.post("/ingest", response_model=IngestResponse)
async def ingest_content(
    request_obj: Request, 
    payload: ExtensionIngestRequest, 
    authenticated: bool = Depends(verify_api_key)
):
    """
    Archivist Endpoint: Ingests raw content, distills it, and commits it to long-term memory.
    This bypasses the standard chat flow to ensure high-quality, verified memories are stored.
    """
    components = get_components(request_obj.app)
    memory = components.get("memory")
    
    if not memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")

    try:
        logger.info(f"Archivist ingesting content from adapter: {payload.adapter}")
        
        # Map source type
        source_type = SourceType.WEB_PAGE
        if "gemini" in payload.type.lower() or (payload.adapter and "gemini" in payload.adapter.lower()):
            source_type = SourceType.GEMINI_CHAT
        
        # Create PlaintextMemory (Directive INJ-A1)
        plaintext_memory = PlaintextMemory(
            source_type=source_type,
            source_identifier=f"browser_session_{datetime.now().strftime('%Y%m%d')}",
            content=payload.content,
            metadata={
                "adapter": payload.adapter,
                "raw_type": payload.type,
                "ingested_by": "Archivist"
            }
        )
        
        # Persist to Corpus (ark_corpus.jsonl)
        # Using standard open for simplicity and robustness
        with open("ark_corpus.jsonl", "a", encoding="utf-8") as f:
            f.write(plaintext_memory.json() + "\n")
            
        # Index via MemoryManager (Reflex Memory)
        session_id = f"archivist-{int(time.time())}"
        
        # Use memory.add_memory to trigger the full pipeline (Distillation -> Neo4j -> Vector)
        memory_id = await memory.add_memory(
            session_id=session_id,
            content=plaintext_memory.content,
            category="knowledge",
            tags=["#ingested", f"#{source_type.value.lower()}"],
            importance=3,
            metadata=plaintext_memory.metadata
        )

        return IngestResponse(
            status="success",
            memory_ids=[memory_id] if memory_id else [],
            message="Content successfully ingested, archived, and indexed."
        )

    except Exception as e:
        logger.exception("Archivist ingestion failed")
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\archivist.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\coda_chat.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Any, List, Dict, AsyncGenerator
import httpx
import json, time, asyncio, logging

from src.bootstrap import get_components
from src.security import verify_api_key
from src.prompts import build_system_prompt
from src.tools import ToolExecutor
from src.config import settings
from src.agents.orchestrator.orchestrator import SGROrchestrator

logger = logging.getLogger(__name__)

router = APIRouter(tags=["coda_chat"])


class ChatRequest(BaseModel):
    session_id: str
    message: str
    system_prompt: Optional[str] = None


class ChatResponse(BaseModel):
    response: str
    session_id: str
    context_tokens: int


@router.post("/", response_model=ChatResponse)
async def chat(request_obj: Request, payload: ChatRequest, authenticated: bool = Depends(verify_api_key)):
    """
    SGR-enabled chat endpoint.
    Uses the SGROrchestrator to Think -> Plan -> Act.
    """
    components = get_components(request_obj.app)
    memory = components.get("memory")
    llm = components.get("llm")
    context_mgr = components.get("context_mgr")
    chunker = components.get("chunker")
    plugin_manager = components.get("plugin_manager")
    mcp_client = components.get("mcp_client")
    audit_logger = components.get("audit_logger")

    if not all([memory, llm, context_mgr, chunker]):
        raise HTTPException(status_code=503, detail="Not initialized")

    try:
        # Mark session as active
        try:
            await memory.touch_session(payload.session_id)
        except Exception:
            pass

        # 1. Build Context
        processed_message = await chunker.process_large_input(payload.message, query_context="User is chatting with their memory-augmented AI")
        full_context = await context_mgr.build_context(payload.session_id, processed_message)

        # 2. Initialize Tool Executor
        tool_executor = ToolExecutor(plugin_manager=plugin_manager, mcp_client=mcp_client)

        # 3. Initialize SGR Orchestrator
        orchestrator = SGROrchestrator(
            llm_client=llm,
            tool_executor=tool_executor,
            audit_logger=audit_logger
        )

        # 4. Run the Loop
        final_response = await orchestrator.run_loop(
            session_id=payload.session_id,
            user_message=processed_message,
            context=full_context
        )

        # 5. Save to Memory
        await memory.save_active_context(payload.session_id, f"{full_context}\n\nAssistant: {final_response}")
        
        # 6. Log Audit
        if audit_logger:
            await audit_logger.log_event(
                session_id=payload.session_id,
                event_type="chat_turn",
                content=final_response,
                metadata={"model": "SGR_Orchestrator"}
            )

        return ChatResponse(
            response=final_response,
            session_id=payload.session_id,
            context_tokens=len(full_context) // 4 # Approx
        )

    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# Note: Streaming endpoint would need similar refactoring to stream the SGR steps


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\coda_chat.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\llm_response.py (Section: BACKEND_PYTHON) ---

from __future__ import annotations
from pydantic import BaseModel, Field
from typing import List, Optional
from src.tool_call_models import ToolCall


class LLMStructuredResponse(BaseModel):
    """Validated LLM response expected by downstream flows.

    - `answer`: Main assistant text
    - `sources`: Optional list of source IDs or URLs
    - `tool_calls`: Optional list of tool calls that should be executed
    - `confidence`: Optional string describing confidence
    """

    answer: str = Field(..., description="Main text answer from the LLM")
    sources: List[str] = Field(default_factory=list)
    tool_calls: List[ToolCall] = Field(default_factory=list)
    confidence: Optional[str] = Field(None, description="Optional model self-reported confidence")


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\llm_response.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\plan_models.py (Section: BACKEND_PYTHON) ---

from __future__ import annotations

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field


class PlanStep(BaseModel):
    tool_name: str = Field(..., description="Name of the tool to invoke, or 'none' for no tool")
    args: Dict[str, Any] = Field(default_factory=dict, description="Arguments for the tool")
    reasoning: Optional[str] = Field(None, description="Optional human-friendly reasoning for step")


class PlanResult(BaseModel):
    goal: str = Field(..., description="High-level goal for the plan")
    steps: List[PlanStep] = Field(default_factory=list, description="Ordered steps to accomplish the goal")


__all__ = ["PlanResult", "PlanStep"]


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\plan_models.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\config_finder.py (Section: BACKEND_PYTHON) ---

from pathlib import Path


def find_config_path() -> str | None:
    """Find a config.yaml across known locations: repo_root/configs/config.yaml, repo_root/config.yaml, repo_root/ece-core/config.yaml.

    Returns the first path that exists or None.
    """
    repo_root = Path(__file__).resolve().parents[2]
    candidates = [repo_root / "configs" / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
    for p in candidates:
        if p.exists():
            return str(p)
    return None


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\config_finder.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\neo4j_embedded.py (Section: BACKEND_PYTHON) ---

"""
Embedded Neo4j server manager for ECE_Core.
Launches Neo4j as subprocess, just like Redis.
"""
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional


class EmbeddedNeo4j:
    """Manages embedded Neo4j server instance."""
    
    def __init__(self):
        self.process: Optional[subprocess.Popen] = None
        
        # Determine paths
        if getattr(sys, 'frozen', False):
            # Running as bundled exe - look for Neo4j relative to exe
            self.app_dir = Path(sys._MEIPASS)
            self.data_dir = Path.cwd()
            # Check for Neo4j in db/ directory relative to exe
            local_neo4j = self.data_dir / "db" / "neo4j-community-2025.10.1"
            if local_neo4j.exists():
                self.neo4j_home = local_neo4j
            else:
                self.neo4j_home = None
        else:
            # Running as script - look for Neo4j in db/ or External-Context-Engine-ECE
            self.app_dir = Path(__file__).parent.parent
            local_neo4j = self.app_dir / "db" / "neo4j-community-2025.10.1"
            external_neo4j = self.app_dir.parent / "External-Context-Engine-ECE" / "db" / "neo4j-community-2025.10.1"
            
            if local_neo4j.exists():
                self.neo4j_home = local_neo4j
            elif external_neo4j.exists():
                self.neo4j_home = external_neo4j
            else:
                self.neo4j_home = None
        
        # Neo4j configuration
        self.bolt_port = 7687
        self.http_port = 7474
        self.username = "neo4j"
        self.password = "password"  # Default, should be configurable
    
    def start(self) -> bool:
        """Start Neo4j server."""
        if not self.neo4j_home:
            print("Neo4j not found - graph features disabled")
            return False
        
        if not self.neo4j_home.exists():
            print(f"Neo4j not found at: {self.neo4j_home}")
            return False
        
        print("Starting embedded Neo4j server...")
        
        # Configure Neo4j for embedded use
        conf_file = self.neo4j_home / "conf" / "neo4j.conf"
        self._configure_neo4j(conf_file)
        
        try:
            # Use neo4j console (foreground mode) so we can control it
            if sys.platform == 'win32':
                neo4j_exe = self.neo4j_home / "bin" / "neo4j.bat"
                cmd = [str(neo4j_exe), "console"]
            else:
                neo4j_exe = self.neo4j_home / "bin" / "neo4j"
                cmd = [str(neo4j_exe), "console"]
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=str(self.neo4j_home),
                creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0,
                env={
                    **subprocess.os.environ,
                    "NEO4J_HOME": str(self.neo4j_home),
                    "NEO4J_CONF": str(self.neo4j_home / "conf")
                }
            )
            
            # Wait for Neo4j to be ready
            if self._wait_for_ready():
                print(f"Neo4j started (bolt://localhost:{self.bolt_port})")
                return True
            else:
                print("Neo4j failed to start within timeout")
                self.stop()
                return False
                
        except Exception as e:
            print(f"Could not start Neo4j: {e}")
            return False
    
    def _configure_neo4j(self, conf_file: Path):
        """Write minimal Neo4j configuration."""
        # Ensure conf directory exists
        conf_file.parent.mkdir(parents=True, exist_ok=True)
        
        config = f"""# ECE_Core Embedded Neo4j Configuration
# Generated automatically

# Server ports
server.bolt.enabled=true
server.bolt.listen_address=127.0.0.1:7687
server.http.enabled=true
server.http.listen_address=127.0.0.1:7474

# CRITICAL: Disable authentication completely for embedded use
dbms.security.auth_enabled=false
server.bolt.tls_level=DISABLED
server.https.enabled=false

# Memory settings (conservative for embedded use)
server.memory.heap.initial_size=256m
server.memory.heap.max_size=512m
server.memory.pagecache.size=256m

# Database location  
server.directories.data=data
server.directories.logs=logs
server.directories.import=import

# Disable anonymous usage reporting
dbms.usage_report.enabled=false

# Performance tweaks for local use
dbms.tx_log.rotation.retention_policy=false
dbms.checkpoint.interval.time=5m
"""
        conf_file.write_text(config)
    
    def _wait_for_ready(self, timeout: int = 30) -> bool:
        """Wait for Neo4j to be ready to accept connections."""
        import socket
        
        start = time.time()
        while time.time() - start < timeout:
            # Check if process died
            if self.process.poll() is not None:
                return False
            
            # Try to connect to bolt port
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex(('127.0.0.1', self.bolt_port))
                sock.close()
                
                if result == 0:
                    time.sleep(2)  # Extra time for full startup
                    return True
            except:
                pass
            
            time.sleep(1)
        
        return False
    
    def stop(self):
        """Stop Neo4j server."""
        if not self.process:
            return
        
        print("  Stopping Neo4j...")
        try:
            self.process.terminate()
            self.process.wait(timeout=10)
        except subprocess.TimeoutExpired:
            print("  Force killing Neo4j...")
            self.process.kill()
            self.process.wait()
        
        self.process = None
    
    def is_running(self) -> bool:
        """Check if Neo4j process is running."""
        return self.process is not None and self.process.poll() is None
    
    def get_bolt_uri(self) -> str:
        """Get Neo4j bolt connection URI."""
        return f"bolt://localhost:{self.bolt_port}"
    
    def get_http_uri(self) -> str:
        """Get Neo4j HTTP URI."""
        return f"http://localhost:{self.http_port}"


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\neo4j_embedded.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\setup_files.py (Section: BACKEND_PYTHON) ---

import os

files = {
    "requirements.txt": """# ECE_Core - Minimal Dependencies
fastapi==0.115.0
uvicorn==0.32.0
redis==5.2.0
httpx==0.28.1
openai==1.54.0
python-dotenv==1.1.1
pydantic==2.10.2
pydantic-settings==2.6.1
tiktoken==0.8.0
""",
    
    ".env.example": """# ECE_Core Configuration
REDIS_URL=redis://localhost:6379
REDIS_TTL=3600
NEO4J_URI=bolt://localhost:7687
NEO4J_HTTP=http://localhost:7474
LLM_API_BASE=http://localhost:8080/v1
LLM_MODEL=your-model-name
LLM_MAX_TOKENS=32000
ECE_HOST=127.0.0.1
ECE_PORT=8000
MAX_REDIS_TOKENS=8000
SUMMARIZE_THRESHOLD=6000
"""
}

for filename, content in files.items():
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"Created: {filename}")

print("\\nDone! Files created.")


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\setup_files.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\toon_formatter.py (Section: BACKEND_PYTHON) ---

"""
TOON (Token-Oriented Object Notation) Formatter.
A lightweight, indentation-based format designed to save tokens in LLM prompts.
"""
from typing import Any, Dict, List

def format_as_toon(data: Any, indent: int = 0) -> str:
    """
    Format data as TOON (Token-Oriented Object Notation).
    
    Rules:
    - No quotes around keys
    - No braces or commas
    - Indentation defines hierarchy (2 spaces)
    - Lists are denoted by `-`
    - Strings are unquoted unless they contain special chars (simple heuristic)
    """
    spaces = "  " * indent
    
    if isinstance(data, dict):
        lines = []
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                lines.append(f"{spaces}{key}:")
                lines.append(format_as_toon(value, indent + 1))
            else:
                lines.append(f"{spaces}{key}: {value}")
        return "\n".join(lines)
    
    elif isinstance(data, list):
        lines = []
        for item in data:
            if isinstance(item, (dict, list)):
                # For complex items, start with dash then indent content
                # But TOON usually prefers:
                # - key: value
                #   other: value
                formatted_item = format_as_toon(item, indent + 1)
                # Strip first indent to align with dash
                lines.append(f"{spaces}- {formatted_item.lstrip()}")
            else:
                lines.append(f"{spaces}- {item}")
        return "\n".join(lines)
    
    else:
        return str(data)


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\toon_formatter.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_filesystem.py (Section: BACKEND_PYTHON) ---

"""
UTCP Filesystem Tool Service for ECE_Core
Simple file operations accessible via UTCP protocol
"""
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import Optional, List
import os
from pathlib import Path
import json

app = FastAPI(title="UTCP Filesystem Service")

class FileReadRequest(BaseModel):
    path: str
    
class FileWriteRequest(BaseModel):
    path: str
    content: str
    
class DirectoryListRequest(BaseModel):
    path: str
    recursive: bool = False

@app.get("/")
async def root():
    return {"service": "UTCP Filesystem", "status": "running"}

@app.get("/utcp")
async def utcp_manual():
    """UTCP Manual - describes available tools"""
    return {
        "service": "filesystem",
        "version": "1.0.0",
        "tools": [
            {
                "name": "read_file",
                "description": "Read contents of a file",
                "parameters": {
                    "path": {"type": "string", "description": "File path to read"}
                },
                "endpoint": "/read_file"
            },
            {
                "name": "write_file",
                "description": "Write content to a file",
                "parameters": {
                    "path": {"type": "string", "description": "File path to write"},
                    "content": {"type": "string", "description": "Content to write"}
                },
                "endpoint": "/write_file"
            },
            {
                "name": "list_directory",
                "description": "List files in a directory",
                "parameters": {
                    "path": {"type": "string", "description": "Directory path"},
                    "recursive": {"type": "boolean", "description": "List recursively", "default": False}
                },
                "endpoint": "/list_directory"
            },
            {
                "name": "run_command",
                "description": "Execute a whitelisted CLI command (safe-mode) on the server",
                "parameters": {
                    "command": {"type": "string", "description": "Command to run"},
                    "cwd": {"type": "string", "description": "Working directory (optional)"},
                    "timeout": {"type": "integer", "description": "Timeout seconds", "default": 5}
                },
                "endpoint": "/run_command"
            }
        ]
    }

@app.post("/read_file")
@app.get("/read_file")
async def read_file(path: str = None, request: Request = None):
    """Read file contents"""
    try:
        # Accept JSON body or query params
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
        file_path = Path(path).resolve()
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {path}")
        
        if not file_path.is_file():
            raise HTTPException(status_code=400, detail=f"Not a file: {path}")
        
        content = file_path.read_text(encoding='utf-8')
        return {
            "success": True,
            "path": str(file_path),
            "content": content,
            "size": len(content)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/run_command")
async def run_command(request: Request, command: str = None, cwd: str = None, timeout: int = 5):
    """Execute a whitelisted CLI command in a safe manner.
    `command` should be a single command with optional arguments.
    `cwd` if provided must be under `UTCP_FILESYSTEM_ROOT` (if configured).
    Timeout in seconds applies to command execution.
    """
    from shlex import split as shlex_split
    import subprocess
    import platform
    try:
        # Accept both query params and JSON body payloads for compatibility with different clients
        try:
            payload = await request.json()
        except Exception:
            payload = None
        if payload:
            command = payload.get("command", command)
            cwd = payload.get("cwd", cwd)
            timeout = payload.get("timeout", timeout)
        # Security: Only allow simple commands from allowlist
        default_allowed = ["ls", "pwd", "cat", "dir", "echo", "type"]
        env_allow = os.environ.get("UTCP_RUN_COMMAND_ALLOWLIST")
        if env_allow:
            try:
                allowed_cmds = [s.strip() for s in env_allow.split(",") if s.strip()]
            except Exception:
                allowed_cmds = default_allowed
        else:
            allowed_cmds = default_allowed
        parts = shlex_split(command)
        if not parts:
            raise HTTPException(status_code=400, detail="Empty command")
        if parts[0] not in allowed_cmds:
            raise HTTPException(status_code=403, detail=f"Command not allowed: {parts[0]}")

        # Validate cwd under allowed root
        root_env = os.environ.get("UTCP_FILESYSTEM_ROOT")
        if cwd:
            wd = Path(cwd).resolve()
            if root_env:
                root = Path(root_env).resolve()
                try:
                    wd.relative_to(root)
                except Exception:
                    raise HTTPException(status_code=403, detail=f"CWD not allowed: {cwd}")
        else:
            wd = None

        # On Windows, some commands like 'dir', 'type', or 'pwd' are shell builtins.
        # Wrap them using `cmd.exe /c` so they execute correctly without shell=True.
        is_windows = platform.system().lower() == "windows"
        exec_parts = parts
        if is_windows and parts[0] in ("dir", "type", "pwd", "ls"):
            exec_parts = ["cmd", "/c"] + parts

        # Resource limiting: try to enforce CPU and memory limits on POSIX
        preexec_fn = None
        try:
            if os.name != 'nt':
                import resource
                mem_limit_mb = int(os.environ.get("UTCP_RUN_COMMAND_MEM_LIMIT_MB", "256"))
                mem_bytes = mem_limit_mb * 1024 * 1024
                def _preexec():
                    # CPU seconds limit slightly above timeout
                    try:
                        resource.setrlimit(resource.RLIMIT_CPU, (timeout + 1, timeout + 1))
                    except Exception:
                        pass
                    try:
                        resource.setrlimit(resource.RLIMIT_AS, (mem_bytes, mem_bytes))
                    except Exception:
                        pass
                preexec_fn = _preexec
        except Exception:
            preexec_fn = None

        # Run the command with optional preexec limits
        proc = subprocess.run(exec_parts, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=str(wd) if wd else None, timeout=timeout, preexec_fn=preexec_fn)
        # Audit logging: record the run in a log file under logs/utcp_run_command_audit.log
        try:
            # determine repo root by walking up until we find '.git' or 'package.json' or default to parents[3]
            candidate = Path(__file__).resolve().parent
            repo_root = candidate
            while repo_root and not (repo_root / 'package.json').exists():
                if repo_root.parent == repo_root:
                    break
                repo_root = repo_root.parent
            if not repo_root or not (repo_root / 'package.json').exists():
                repo_root = Path(__file__).resolve().parents[3]
            logs_dir = repo_root / 'logs'
            logs_dir.mkdir(parents=True, exist_ok=True)
            audit_file = logs_dir / 'utcp_run_command_audit.log'
            with open(audit_file, 'a', encoding='utf-8') as fh:
                audit_entry = {
                    'timestamp': __import__('datetime').datetime.utcnow().isoformat() + 'Z',
                    'command': command,
                    'cwd': str(wd) if wd else None,
                    'exit_code': proc.returncode,
                    'stdout': proc.stdout.decode('utf-8', errors='ignore')[:5000],
                    'stderr': proc.stderr.decode('utf-8', errors='ignore')[:2000]
                }
                fh.write(json.dumps(audit_entry) + '\n')
        except Exception:
            pass

        return {
            "success": True,
            "command": command,
            "exit_code": proc.returncode,
            "stdout": proc.stdout.decode("utf-8", errors="ignore"),
            "stderr": proc.stderr.decode("utf-8", errors="ignore"),
        }
    except subprocess.TimeoutExpired as e:
        raise HTTPException(status_code=504, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/write_file")
async def write_file(request: FileWriteRequest):
    """Write content to file"""
    try:
        file_path = Path(request.path).resolve()
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_path.write_text(request.content, encoding='utf-8')
        return {
            "success": True,
            "path": str(file_path),
            "size": len(request.content)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/list_directory")
@app.get("/list_directory")
async def list_directory(path: str = None, recursive: bool = False, request: Request = None):
    """List directory contents"""
    try:
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
                recursive = payload.get("recursive", recursive)
        dir_path = Path(path).resolve()
        if not dir_path.exists():
            raise HTTPException(status_code=404, detail=f"Directory not found: {path}")
        
        if not dir_path.is_dir():
            raise HTTPException(status_code=400, detail=f"Not a directory: {path}")
        
        files = []
        if recursive:
            for item in dir_path.rglob("*"):
                files.append({
                    "path": str(item),
                    "name": item.name,
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })
        else:
            for item in dir_path.iterdir():
                files.append({
                    "path": str(item),
                    "name": item.name,
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })
        
        return {
            "success": True,
            "path": str(dir_path),
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search_files")
@app.get("/search_files")
async def search_files(
    path: str,
    pattern: str = "*",
    content_query: str = None,
    max_results: int = 50,
    recursive: bool = True
    , request: Request = None
):
    """Search for files under `path` by name or content.

    - `pattern` is a glob-like filename pattern (default '*').
    - `content_query` if provided, will search inside files and return line-snippets.
    - `max_results` limits the number of files returned.
    - `recursive` toggles recursive search.
    """
    try:
        from os import environ
        root_env = environ.get("UTCP_FILESYSTEM_ROOT")
        # Ensure paths are resolved and not outside root (if configured)
        # Accept JSON body payloads if request is provided by FastAPI.
        # When called directly in unit tests, request will be None and path param will be used.
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
                pattern = payload.get("pattern", pattern)
                content_query = payload.get("content_query", content_query)
                max_results = payload.get("max_results", max_results)
                recursive = payload.get("recursive", recursive)
        base = Path(path).resolve()
        if root_env:
            root = Path(root_env).resolve()
            try:
                base.relative_to(root)
            except Exception:
                raise HTTPException(status_code=403, detail=f"Path not allowed: {path}")

        if not base.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {path}")
        if not base.is_dir():
            raise HTTPException(status_code=400, detail=f"Not a directory: {path}")

        results = []
        count = 0
        iter_fn = base.rglob if recursive else base.glob
        for p in iter_fn(pattern):
            if count >= max_results:
                break
            if p.is_dir():
                continue
            item = {"path": str(p), "name": p.name}
            matches = []
            if content_query:
                try:
                    with p.open('r', encoding='utf-8', errors='ignore') as fh:
                        for lineno, line in enumerate(fh, start=1):
                            if content_query in line:
                                snippet = line.strip()
                                matches.append({"line": lineno, "snippet": snippet[:300]})
                                if len(matches) >= 10:
                                    break
                except Exception:
                    # Could not read file; skip content check but include file
                    matches = []
            item["matches"] = matches
            # If content_query is present, include only files with matches
            if content_query and not matches:
                continue
            results.append(item)
            count += 1

        return {"success": True, "root": str(base), "count": count, "files": results}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8006)


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_filesystem.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\fake_vector_adapter.py (Section: BACKEND_PYTHON) ---

"""A minimal fake vector adapter for unit tests.

This adapter stores embeddings in a local dictionary and performs cosine
similarity-based queries in Python. It's deterministic and suitable for
unit testing of vector-related behavior without requiring Redis or a
vector DB.
"""
from __future__ import annotations
from typing import List, Dict, Any, Optional
import math
import logging

logger = logging.getLogger(__name__)


def _cosine_similarity(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    mag_a = math.sqrt(sum(x * x for x in a))
    mag_b = math.sqrt(sum(x * x for x in b))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


class FakeVectorAdapter:
    def __init__(self):
        self._index: Dict[str, Dict[str, Any]] = {}

    async def initialize(self):
        # No-op for fake
        return

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        self._index[embedding_id] = {
            "embedding": embedding,
            "node_id": node_id,
            "chunk_index": chunk_index,
            "metadata": metadata or {}
        }

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        candidates = []
        for eid, data in self._index.items():
            score = _cosine_similarity(embedding, data["embedding"]) if data.get("embedding") else 0.0
            candidates.append({
                "score": float(score),
                "embedding_id": eid,
                "node_id": data["node_id"],
                "chunk_index": data["chunk_index"],
                "metadata": data.get("metadata", {})
            })
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates[:top_k]

    async def delete(self, embedding_id: str) -> None:
        self._index.pop(embedding_id, None)

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        return self._index.get(embedding_id)

    async def health(self) -> bool:
        return True


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\fake_vector_adapter.py ---

--- START OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\redis_vector_adapter.py (Section: BACKEND_PYTHON) ---

"""Redis VectorAdapter implementation with an in-memory fallback.

This adapter stores embeddings in Redis as simple JSON-serialized fields
and performs similarity queries in-process when the Redis server doesn't
have RediSearch vector index capabilities. This keeps tests deterministic
and avoids requiring Redis modules in CI.
"""
from __future__ import annotations
from typing import List, Dict, Any, Optional
import math
import json
import logging
import redis.asyncio as redis

logger = logging.getLogger(__name__)


def _cosine_similarity(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    mag_a = math.sqrt(sum(x * x for x in a))
    mag_b = math.sqrt(sum(x * x for x in b))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


class RedisVectorAdapter:
    """A lightweight Redis vector adapter.

    This adapter stores each vector entry under `vec:{embedding_id}`
    as a Redis hash with fields: embedding (JSON), node_id, chunk_index, metadata.
    During queries, it will enumerate the stored ids from a Redis set 'vec:index'
    and compute cosine similarity in Python if the server does not provide
    a vector-search capability.
    """

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.client: Optional[redis.Redis] = None
        # Local in-memory fallback index when Redis is not available or for fast tests
        self._in_memory: Dict[str, Dict[str, Any]] = {}
        # RediSearch availability
        self._redis_search_available: bool = False
        self._index_created: bool = False
        self._vector_dim: Optional[int] = None

    async def initialize(self):
        try:
            self.client = await redis.from_url(self.redis_url, decode_responses=True)
            await self.client.ping()
            logger.info("RedisVectorAdapter: connected to Redis")
            # Try to detect RediSearch FT API (client.ft exists) when using redis-py
            try:
                if hasattr(self.client, "ft"):
                    # Try to run a minimal info command for 'vec_index' to detect if there's a vector index
                    try:
                        await self.client.ft("vec_index").info()
                        self._redis_search_available = True
                        self._index_created = True
                    except Exception:
                        # Index does not exist; still mark search available because Redis supports FT API
                        self._redis_search_available = True
                        self._index_created = False
            except Exception:
                self._redis_search_available = False
        except Exception as e:
            logger.warning(f"RedisVectorAdapter: unable to connect redis: {e}. Using in-memory fallback.")
            self.client = None

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        entry = {
            "embedding": embedding,
            "node_id": node_id,
            "chunk_index": chunk_index,
            "metadata": metadata or {}
        }
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                # Store embedding as JSON by default
                await self.client.hset(key, mapping={
                    "embedding": json.dumps(embedding),
                    "node_id": node_id,
                    "chunk_index": chunk_index,
                    "metadata": json.dumps(metadata or {})
                })
                await self.client.sadd("vec:index", embedding_id)
                # Create RediSearch vector index if available and not yet created
                if self._redis_search_available and not self._index_created:
                    try:
                        # Determine vector dimension from embedding
                        dim = len(embedding)
                        self._vector_dim = dim
                        # Try to create an index with a simple HNSW vector field. Use dialect 2 compatibility.
                        # Try both mechanisms: explicit execute_command or ft().create depending on redis client
                        try:
                            await self.client.execute_command(
                                "FT.CREATE",
                                "vec_index",
                                "ON",
                                "HASH",
                                "PREFIX",
                                "1",
                                "vec:",
                                "SCHEMA",
                                "embedding",
                                "VECTOR",
                                "HNSW",
                                "6",
                                "TYPE",
                                "FLOAT32",
                                "DIM",
                                str(dim),
                                "DISTANCE_METRIC",
                                "COSINE",
                            )
                        except Exception:
                            # Try the high-level API if available
                            try:
                                if hasattr(self.client, "ft") and hasattr(self.client.ft("vec_index"), "create"):
                                    if hasattr(self.client.ft("vec_index"), "create"):
                                        # Some redis clients have 'create' on ft object; attempt to call
                                        await self.client.ft("vec_index").create(
                                            [
                                                ("embedding", {"TYPE": "VECTOR", "ALGORITHM": "HNSW", "TYPE_PARAMS": {"TYPE": "FLOAT32", "DIM": dim, "DISTANCE_METRIC": "COSINE"}})
                                            ]
                                        )
                            except Exception as e2:
                                logger.info(f"RedisVectorAdapter: failed to create RediSearch index via execute or high-level API: {e2}")
                        self._index_created = True
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: failed to create RediSearch index: {e}")
                return
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis write failed: {e}")

        # Fallback to in-memory storage
        self._in_memory[embedding_id] = entry

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        candidates = []
        # If redis is present, enumerate ids in the set and HGET each one
        if self.client:
            try:
                # If RediSearch is available and index is created, use FT.SEARCH
                if self._redis_search_available and self._index_created and hasattr(self.client, "ft"):
                    try:
                        # Use redispy's FT API if available; pack float32 bytes as $vec_param
                        import struct

                        if isinstance(embedding, list):
                            vec_bytes = struct.pack(f"{len(embedding)}f", *embedding)
                        else:
                            vec_bytes = embedding

                        # Compose a KNN vector search query
                        query = f"*=>[KNN {top_k} @embedding $vec_param AS score]"

                        # Try high-level ft().search first
                        if hasattr(self.client, "ft") and hasattr(self.client.ft("vec_index"), "search"):
                            res = await self.client.ft("vec_index").search(query, query_params={"vec_param": vec_bytes})
                            for doc in getattr(res, "docs", []):
                                fields = getattr(doc, "__dict__", {})
                                score = float(getattr(doc, "score", 0.0))
                                candidates.append({
                                    "score": float(score),
                                    "embedding_id": str(doc.id).replace("vec:", ""),
                                    "node_id": getattr(doc, "node_id", None) or fields.get("node_id"),
                                    "chunk_index": int(getattr(doc, "chunk_index", 0) or fields.get("chunk_index", 0)),
                                    "metadata": json.loads(getattr(doc, "metadata", "{}") or fields.get("metadata", "{}")),
                                })
                            candidates.sort(key=lambda x: x["score"], reverse=True)
                            return candidates[:top_k]
                        else:
                            # Fallback to execute_command FT.SEARCH with PARAMS
                            try:
                                # FT.SEARCH vec_index query PARAMS 2 vec_param <bytes> DIALECT 2
                                res = await self.client.execute_command("FT.SEARCH", "vec_index", query, "PARAMS", 2, "vec_param", vec_bytes, "DIALECT", 2)
                                # res is a list; parse accordingly: [total, docId1, {fields}, docId2,...]
                                if isinstance(res, list) and len(res) >= 1:
                                    it = iter(res[1:])
                                    while True:
                                        try:
                                            docId = next(it)
                                        except StopIteration:
                                            break
                                        fields = next(it)
                                        score = 1.0
                                        # fields is a dict mapping field to value
                                        docid_str = str(docId)
                                        candidates.append({
                                            "score": float(score),
                                            "embedding_id": docid_str.replace("vec:", ""),
                                            "node_id": fields.get(b"node_id" if isinstance(fields, dict) else "node_id"),
                                            "chunk_index": int(fields.get(b"chunk_index", 0) if isinstance(fields, dict) else fields.get("chunk_index", 0)),
                                            "metadata": json.loads(fields.get(b"metadata", b"{}").decode() if isinstance(fields, dict) and isinstance(fields.get(b"metadata"), bytes) else (fields.get("metadata") or "{}")),
                                        })
                                    candidates.sort(key=lambda x: x["score"], reverse=True)
                                    return candidates[:top_k]
                            except Exception as e:
                                logger.info(f"RedisVectorAdapter: execute FT.SEARCH failed: {e}")
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: RediSearch query failed, fallback: {e}")
                        # If we got results, return top_k
                        candidates.sort(key=lambda x: x["score"], reverse=True)
                        return candidates[:top_k]
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: RediSearch query failed, fallback: {e}")
                        # Fall back to scanning members
                ids = await self.client.smembers("vec:index")
                for eid in ids:
                    key = f"vec:{eid}"
                    data = await self.client.hgetall(key)
                    if not data:
                        continue
                    try:
                        emb = json.loads(data.get("embedding"))
                        node_id = data.get("node_id")
                        chunk_index = int(data.get("chunk_index", 0))
                        metadata = json.loads(data.get("metadata") or "{}")
                    except Exception:
                        continue
                    score = _cosine_similarity(embedding, emb)
                    candidates.append({
                        "score": float(score),
                        "embedding_id": eid,
                        "node_id": node_id,
                        "chunk_index": chunk_index,
                        "metadata": metadata,
                    })
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis read failed during query: {e}")
                # Fall through to in-memory fallback

        # In-memory candidate enumeration
        for eid, data in self._in_memory.items():
            try:
                score = _cosine_similarity(embedding, data["embedding"])
                candidates.append({
                    "score": float(score),
                    "embedding_id": eid,
                    "node_id": data["node_id"],
                    "chunk_index": int(data["chunk_index"]),
                    "metadata": data["metadata"],
                })
            except Exception:
                continue

        # Sort by score (descending) and return top_k
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates[:top_k]

    async def delete(self, embedding_id: str) -> None:
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                await self.client.delete(key)
                await self.client.srem("vec:index", embedding_id)
                return
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis delete failed: {e}")

        self._in_memory.pop(embedding_id, None)

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                data = await self.client.hgetall(key)
                if not data:
                    return None
                emb = json.loads(data.get("embedding"))
                return {
                    "embedding_id": embedding_id,
                    "embedding": emb,
                    "node_id": data.get("node_id"),
                    "chunk_index": int(data.get("chunk_index", 0)),
                    "metadata": json.loads(data.get("metadata") or "{}")
                }
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis get failed: {e}")
                return None

        entry = self._in_memory.get(embedding_id)
        if not entry:
            return None
        return {
            "embedding_id": embedding_id,
            "embedding": entry["embedding"],
            "node_id": entry["node_id"],
            "chunk_index": int(entry["chunk_index"]),
            "metadata": entry["metadata"],
        }

    async def health(self) -> bool:
        if self.client:
            try:
                await self.client.ping()
                return True
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: health ping failed: {e}")
                return False
        # In-memory fallback always healthy
        return True


--- END OF FILE: c:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\redis_vector_adapter.py ---

