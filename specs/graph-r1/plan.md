# Implementation Plan for Graph R1 Module

## 1. Tech Stack and Architecture

- **Programming Language**: Python
- **Framework**: FastAPI (consistent with the main application)
- **Database**: Neo4j (for the knowledge graph)
- **Libraries**: 
  - neo4j-driver for interacting with the Neo4j database
  - numpy for numerical computations (Q-Table)
  - pydantic for data validation
  - asyncio for asynchronous operations
- **Architecture**: 
  - The module will be implemented as a standalone component within the `src/external_context_engine/memory_management/q_learning/` directory.
  - It will expose methods for path finding, Q-Table management, and training.
  - The module will integrate with the existing Neo4jManager for database operations.

## 2. Data Models

### 2.1 GraphState Model
```python
@dataclass
class GraphState:
    """Represents a state in the knowledge graph"""
    node_id: str
    node_name: str
    node_type: str = "Concept"
    features: Dict[str, Any] = None
```

### 2.2 Action Model
```python
@dataclass
class Action:
    """Represents an action (edge traversal) in the graph"""
    from_node: str
    to_node: str
    relationship_type: str
    weight: float = 1.0
```

### 2.3 MemoryPath Model
```python
class MemoryPath(BaseModel):
    """Represents a path through the knowledge graph"""
    nodes: List[str] = Field(default_factory=list, description="Node names in the path")
    relationships: List[Dict[str, Any]] = Field(default_factory=list, description="Relationships in the path")
    score: float = Field(default=0.0, description="Path relevance score")
    length: int = Field(default=0, description="Path length (number of hops)")
```

## 3. API Contracts

### 3.1 Find Paths Endpoint
- **Endpoint**: `/find_paths`
- **Method**: POST
- **Request Body**: `MemoryPathRequest`
- **Response**: `List[MemoryPath]` (paths ranked by Q-values)

### 3.2 Update Q-Values Endpoint
- **Endpoint**: `/update_q_values`
- **Method**: POST
- **Request Body**: `Dict[str, Any]` (containing path and reward)
- **Response**: `Dict[str, Any]` (confirmation of update)

### 3.3 Train Endpoint
- **Endpoint**: `/train`
- **Method**: POST
- **Request Body**: `List[Tuple[str, str, float]]` (training data)
- **Response**: `Dict[str, Any]` (training completion status)

### 3.4 Get Convergence Metrics Endpoint
- **Endpoint**: `/convergence_metrics`
- **Method**: GET
- **Response**: `Dict[str, Any]` (convergence metrics)

## 4. Research and Implementation Details

- Investigate and implement the Q-Learning algorithm for graph navigation.
- Design the Q-Table implementation with persistence support.
- Implement path finding algorithms using Q-values for guidance.
- Implement the epsilon-greedy strategy for balancing exploration and exploitation.
- Implement methods for updating Q-values based on path success.
- Implement training methods with historical path data.
- Implement metrics for tracking Q-Learning convergence.
- Optimize algorithms for performance with large graphs.
- Implement logging for monitoring and debugging purposes.

## 5. Quickstart Guide

1. Install required dependencies:
   ```
   pip install neo4j numpy
   ```

2. Ensure Neo4j database is running and accessible.

3. Place the module files in `src/external_context_engine/memory_management/q_learning/`.

4. Configure the database connection settings in the module.

5. Use the module in the application:
   ```python
   from src.external_context_engine.memory_management.q_learning.q_learning_agent import QLearningGraphAgent
   
   # Initialize the agent
   agent = QLearningGraphAgent(graph_manager, config)
   
   # Find paths
   paths = await agent.find_paths(start_nodes, end_nodes)
   
   # Update Q-values
   await agent.update_q_values(path, reward)
   
   # Train the agent
   await agent.train(training_data)
   ```

6. The module will handle graph navigation using reinforcement learning.