=== PROJECT FILE MAP ===
- CHANGELOG.md (ROOT)
- GPU_RESOURCE_FIXES.md (ROOT)
- HOT_RELOAD_SYSTEM.md (ROOT)
- kill-edge.bat (ROOT)
- launch-chromium-d3d12.bat (ROOT)
- launch-chromium-vulkan.bat (ROOT)
- ORCHESTRATOR_SUMMARY.md (ROOT)
- README.md (ROOT)
- read_all.py (ROOT)
- secure-bridge-launch.ps1 (ROOT)
- start-bridge.bat (ROOT)
- start-sovereign-console-hotreload.bat (ROOT)
- start-sovereign-console.bat (ROOT)
- temp_config.ts (ROOT)
- temp_file.html (ROOT)
- backend\README.md (BACKEND)
- extension\background.js (EXTENSION)
- extension\content.js (EXTENSION)
- extension\manifest.json (EXTENSION)
- extension\popup.html (EXTENSION)
- extension\popup.js (EXTENSION)
- extension\images\README.md (EXTENSION_IMAGES)
- scripts\gpu_manager.py (SCRIPTS)
- scripts\hot_reload_gpu.py (SCRIPTS)
- scripts\README.md (SCRIPTS)
- scripts\smart_gpu_bridge.py (SCRIPTS)
- scripts\test_gpu_fixes.py (SCRIPTS)
- scripts\ci\check_docs.py (SCRIPTS_CI)
- specs\doc_policy.md (SPECS)
- specs\mlc-urls.md (SPECS)
- specs\plan.md (SPECS)
- specs\spec.md (SPECS)
- specs\tasks.md (SPECS)
- specs\architecture\agents.spec.md (SPECS_ARCHITECTURE)
- specs\architecture\api.spec.md (SPECS_ARCHITECTURE)
- specs\architecture\extension-bridge.spec.md (SPECS_ARCHITECTURE)
- specs\architecture\memory-layer.spec.md (SPECS_ARCHITECTURE)
- specs\architecture\sovereign-wasm.spec.md (SPECS_ARCHITECTURE)
- templates\waveai_ece.json (TEMPLATES)
- tools\CHANGELOG.md (TOOLS)
- tools\code_tools.py (TOOLS)
- tools\cozo_lib_wasm.js (TOOLS)
- tools\decode_cozo_blob.py (TOOLS)
- tools\eyes.py (TOOLS)
- tools\index.html (TOOLS)
- tools\indexeddb.js (TOOLS)
- tools\log-viewer.html (TOOLS)
- tools\mobile-chat.html (TOOLS)
- tools\model-server-chat.html (TOOLS)
- tools\orchestrator.py (TOOLS)
- tools\prepare_cozo_import.py (TOOLS)
- tools\README.md (TOOLS)
- tools\root-dreamer.html (TOOLS)
- tools\root-mic.html (TOOLS)
- tools\sovereign-db-builder.html (TOOLS)
- tools\start-bridge.bat (TOOLS)
- tools\test_orchestrator.py (TOOLS)
- tools\webgpu-server-chat.html (TOOLS)
- tools\webgpu_bridge.py (TOOLS)
- tools\modules\gpu-hot-reloader.js (TOOLS_MODULES)
- tools\modules\llm-worker.js (TOOLS_MODULES)
- tools\modules\sovereign.js (TOOLS_MODULES)
- tools\modules\vision.js (TOOLS_MODULES)
- tools\modules\whisper-worker.js (TOOLS_MODULES)
========================

--- START OF FILE: CHANGELOG.md ---
# Context-Engine Changelog

## [Unreleased]

### Added
- **Root Mic (Audio Input)**: Renamed `sovereign-mic.html` to `root-mic.html` and added "Summarize & Clarify" feature using the local Qwen2.5 model.
- **Long-Form Transcription**: Fixed Whisper pipeline to support recordings >30s using chunking and striding.
- **CozoDB Corruption Recovery**: Enhanced error handling for IndexedDB corruption with automatic fallback to in-memory database, manual recovery button, and timeout protection against hanging WASM calls.
- **Bulk CozoDB Import Tool**: Added `tools/prepare_cozo_import.py` to transform `combined_memory.json` into the canonical `relations` payload (`cozo_import_memory.json`) for atomic bulk imports into CozoDB.
- **Import Safety & Verification**: Added recommended import procedure and a post-import verification + backup step to avoid Schema Detachment.
- **WebGPU Bridge**: `webgpu_bridge.py` for proxying OpenAI API requests to browser workers.
- **Chat Worker**: `webgpu-server-chat.html` for running LLMs in the browser.
- **Embed Worker**: `webgpu-server-embed.html` for running embedding models in the browser.
- **Mobile Chat**: `mobile-chat.html` for a lightweight, mobile-friendly UI.
- **Log Viewer**: `log-viewer.html` for real-time server log monitoring.

### Changed
- **Ingestion Defaults**: Recommended batch size increased to 100 to prevent long-running slow writes that can desync CozoDB's in-memory metadata.

---

## [1.2.3] - 2025-12-19 "Snapdragon Optimization"

### Added
- **Qwen3 Support**: Added `Qwen3-4B-Instruct` to the verified model list.
- **Llama 3.2 Support**: Added `Llama-3.2-1B-Instruct` as the recommended lightweight model.
- **Buffer Override**: Implemented `appConfig` overrides to force high-end performance on 256MB GPUs (fixing Adreno throttling).

### Changed
- **Portable Launchers**: All scripts now use `--user-data-dir="%~dp0browser_data"` for fully portable, clean-running instances.
- **Model Config**: Refactored `CreateMLCEngine` initialization to handle both URL-based and ID-based model definitions reliably.

---

## [1.2.2] - 2025-12-18 "Hermes & CozoDB Fixes"

### Fixed
- **Hermes Model Support**: Fixed 404 errors for OpenHermes and NeuralHermes by mapping them to the verified `Mistral-v0.3` WASM library.
- **CozoDB Date Formatting**: Removed `strftime` dependency from WASM queries (causing `no_implementation` errors) and moved date formatting to client-side JavaScript.
- **Drag-and-Drop Import**: Fixed handling of CozoDB `relations` export format in drag-and-drop ingestion.
- **Documentation**: Established `specs/mlc-urls.md` as a registry for verified WASM binaries.

---

## [1.2.1] - 2025-12-15 "DeepSeek & CozoDB Stabilization"

### Fixed
- **CozoDB Initialization**: Resolved `CozoDb.new_from_path is not a function` error by switching to `CozoDb.new_from_indexed_db` for persistent browser storage (IndexedDB backend).
- **WASM Memory Access**: Fixed "memory access out of bounds" error in `sovereign-db-builder.html` and `unified-coda.html` by correctly stringifying JSON parameters passed to `db.run()`.
- **DeepSeek Configuration**: Fixed "Cannot find model record" error in `unified-coda.html` by decoupling the internal model ID from the HuggingFace URL.

### Added
- **Sovereign Hub**: Created `tools/index.html` as a central dashboard for the Console, Builder, and Log Viewer.
- **Log Viewer Upgrade**: Refactored `tools/log-viewer.html` to use `BroadcastChannel` for real-time, polling-free log updates from the console.
- **Expanded File Support**: Updated `sovereign-db-builder.html` to support ingestion of a wider range of code and config files (ts, rs, go, sql, ini, xml, etc.).

## [1.2.0] - 2025-12-15 "Sovereign Architecture"

### Added
- **Sovereign Console**: Created `tools/unified-coda.html`, a standalone WASM-based chat console with local CozoDB (OPFS) and Transformers.js.
- **Sovereign DB Builder**: Created `tools/sovereign-db-builder.html` for ingesting JSON logs into the browser-based database.
- **Model Support**: Expanded `unified-coda.html` to support the full range of MLC-compatible models (Llama 3.2, Qwen 2.5, Gemma 2, etc.).

### Changed
- **Log Management**: Updated backend logging to truncate files at 500KB to prevent disk bloat.

## [1.1.0] - 2025-12-14 "Browser Stability & Bridge Fixes"

### Fixed
- **WebGPU Bridge**: Patched `tools/webgpu_bridge.py` to accept any model name, resolving 503 errors during embedding requests.
- **LLM Client**: Updated `backend/src/llm.py` to correctly identify and use the configured embedding model (`nomic-embed-text-v1.5`).
- **Coda Chat**: Modified `backend/src/recipes/coda_chat.py` to sanitize and truncate `retrieve_memory` outputs. Large JSON payloads were causing `Maximum call stack size exceeded` errors in the browser-based LLM worker.

## [1.0.0] - 2025-12-08 "Infinite Context Pipeline"

### Added
- **Phase 1: Hardware Foundation**: All LLM servers now boot with 65,536 context window and Flash Attention enabled
- **Phase 2: Context Rotation Protocol**: ContextManager automatically rotates context when exceeding 55k tokens
- **Phase 3: Graph-R1 Enhancement**: GraphReasoner now retrieves ContextGist memories for historical continuity
- **ContextGist Nodes**: Neo4j storage for compressed historical context summaries with chronological links
- **Context Shifting Logic**: Intelligent distillation of old content using Distiller agent with gist creation
- **Documentation Structure**: Organized specs/ directories at root, backend, and anchor levels with spec.md, plan.md, tasks.md
- **Infinite Context Pipeline**: Complete end-to-end implementation enabling unlimited context window management

### Changed
- **Upgraded Context Windows**: All start scripts now default to 64k context for infinite work capability
- **Enhanced Memory Architecture**: Neo4j now stores both active memories and ContextGist historical summaries
- **Improved ContextManager**: Added check_and_rotate_context() logic with automatic gist creation and storage
- **Extended GraphReasoner**: Updated retrieval queries to include ContextGist nodes alongside regular memories
- **Optimized Distiller Integration**: Enhanced _chunk_and_distill functionality for context rotation use cases
- **Refined Archivist Agent**: Now coordinates context rotation and gist management operations

### Fixed
- **Context Limit Elimination**: Fixed issue where systems would crash when reaching context limits
- **Memory Continuity**: Resolved problems with historical context access across conversation boundaries
- **Performance Optimization**: Fixed inefficiencies in large context handling with 64k window support
- **Rotation Logic**: Fixed issues with context preservation during rotation cycles

---

## [0.9.0] - 2025-12-07 "Reka & Local Proxy"

### Added
- **Reka Configuration**: Full support for Reka-Flash-3-21B (Q4_K_S) with 16k context, stop tokens, and optimized LLaMa server flags.
- **Local API Proxy**: Added `scripts/local_api_proxy.py` to enforce static API keys for local LLaMa instances (fixes Cline extension "OpenAI API Key" requirement).
- **VS Code Integration**: Added `.vscode/settings.json` template and `VSCODE_CLINE_SETUP.md` for seamless local development.
- **MCP Health**: Added `/health` endpoint to Unified Launcher for better compatibility.

### Fixed
- **MCP Routing**: Resolved duplicate `/mcp` prefix in Unified Launcher routes (`/mcp/tools` is now accessible).
- **LLM Client**: Added `stop` token support to API payloads and local GGUF generation.

## [0.8.0] - 2025-12-06 "Archivist Protocol"

### Added
- **Archivist Ingestion**: Implemented `POST /archivist/ingest` endpoint to accept live data from the browser.
- **Memory Schema**: Enforced **Directive INJ-A1** (`PlaintextMemory`) for immutable "Page-Store" records.
- **Modular DOM Adapters**:
    - `GeminiAdapter`: Clean extraction for Google Gemini.
    - `ChatGPTAdapter`: Clean extraction for ChatGPT.
    - `ClaudeAdapter`: Clean extraction for Claude.ai.
    - `GenericAdapter`: Universal fallback for any webpage.
- **Extension UI**: Added **[Save to Memory]** button to the Side Panel for manual ingestion.

### Fixed
- **Encoding Crash**: Resolved Windows `charmap` error by enforcing `PYTHONIOENCODING='utf-8'`.
- **Server Stability**: Fixed startup crashes caused by `MemoryWeaver` resource contention.

## [0.7.0] - 2025-12-06 "Operation Concrete"

### Added
- **Browser Bridge**: A Chrome Extension (MV3) capable of:
    - **Voice**: Streaming chat interface via Side Panel.
    - **Sight**: Context injection (reading active tab).
    - **Hands**: JavaScript execution on active pages (User-ratified).
- **Backend Architecture**: Migrated from monolithic scripts to **Modular Recipes** (MAX Agentic Cookbook standard).
    - `CodaChatRecipe`: Handles orchestration, context, and tool execution.
- **Persistence**: Side panel now saves chat history to local storage.
- **Markdown Support**: Chat interface renders code blocks and syntax highlighting.

### Changed
- **Identity**: System formally renamed from "Sybil" to **"Coda"**.
- **Documentation**: Adopted `specs/` based documentation policy.

### Fixed
- **Audit Logger**: Patched critical `NameError` in streaming endpoints.
- **Security**: Hardened extension execution via `world: "MAIN"` to bypass strict CSP on some sites.

---

## [0.6.0] - 2025-11-30 "Operation MCP Integrated"

### Added
- **MCP Integration**: Complete integration of MCP server into main ECE Core server
- **Unified Endpoint**: All MCP functionality now available at `/mcp` on main server (port 8000)
- **Memory Tools**: Enhanced MCP tools for memory operations:
    - `add_memory` - Add to Neo4j memory graph
    - `search_memories` - Search memory graph with relationships
    - `get_summaries` - Get session summaries
- **Configuration**: New `mcp_enabled` setting in config.yaml to toggle integration
- **Authentication**: MCP endpoints now inherit main server authentication settings

### Changed
- **Architecture**: MCP server no longer runs as separate process, now integrated into main ECE server
- **Endpoints**: MCP tools now accessed via `/mcp/tools` and `/mcp/call` instead of separate server
- **Deployment**: Simplified deployment - no need to start separate MCP service
- **Resources**: Reduced memory footprint by eliminating duplicate server processes

### Fixed
- **Connection Issues**: Resolved intermittent connection failures between ECE and external MCP server
- **Latency**: Reduced tool call latency by eliminating inter-service communication overhead
- **Synchronization**: Fixed race conditions in concurrent tool executions

---

## [0.5.1] - 2025-11-29 "Memory Weaver Security Audit"

### Added
- **Security Hardening**: Added input validation for all GraphReasoner queries
- **Audit Trail**: Enhanced logging for all automated relationship repairs
- **Circuit Breakers**: Added fail safes for Weaver operations

### Changed
- **Weaver Engine**: Refactored to use parameterized queries, preventing Cypher injection
- **Permission Model**: Strengthened access controls for relationship modification operations

### Fixed
- **Cypher Injection**: Patched vulnerability in Neo4j relationship queries
- **Race Conditions**: Fixed concurrency issues in automated repair operations
- **Resource Exhaustion**: Added limits to prevent DoS via excessive repair requests

---

## [0.5.0] - 2025-11-28 "Memory Weaver (Automated Repair)"

### Added
- **Memory Weaver Engine**: Automated system for detecting and repairing broken relationships in Neo4j
- **Similarity Detection**: Embedding-based relationship discovery for linking related memories
- **Audit System**: Complete traceability for all automated repairs with `auto_commit_run_id`
- **Rollback Capability**: Deterministic reversal of automated changes via `rollback_commits_by_run.py`
- **Scheduler**: Background maintenance tasks for continuous graph integrity

### Changed
- **Graph Maintenance**: Automated relationship repair now runs as background process
- **Quality Assurance**: Enhanced relationship validation with similarity scoring
- **Traceability**: All automated changes now logged with unique run identifiers

### Fixed
- **Orphaned Nodes**: Automatically discovers and connects isolated memories
- **Broken Links**: Repairs missing relationships between related concepts
- **Data Drift**: Corrects inconsistent metadata across related nodes

---

## [0.4.0] - 2025-11-25 "Graph-R1 Implementation"

### Added
- **Graph Reasoner**: Iterative "Think ‚Üí Query ‚Üí Retrieve ‚Üí Rethink" reasoning engine
- **Q-Learning Retrieval**: Reinforcement learning for optimized memory access patterns
- **Markovian Reasoning**: Chunked thinking with state preservation across context shifts
- **Multi-Hop Queries**: Complex graph traversal for answering compound questions
- **Cognitive Agents**: Plugin architecture for specialized reasoning tasks

### Changed
- **Retrieval Method**: Replaced simple vector search with Graph-R1 retrieval
- **Memory Access**: Graph-based traversal now primary method for context assembly
- **Agent Architecture**: Modular cognitive agents for specialized tasks
- **Context Building**: Enhanced context with relationship-aware retrieval

### Fixed
- **Context Relevance**: Improved precision of memory retrieval
- **Chain of Thought**: Better preservation of reasoning pathways
- **Memory Decay**: Reduced loss of historical context in long conversations

---

## [0.3.1] - 2025-11-20 "Security Hardening"

### Added
- **API Authentication**: Token-based authentication for all endpoints
- **Rate Limiting**: Request throttling to prevent abuse
- **Input Sanitization**: Enhanced validation for all user inputs
- **Audit Logging**: Comprehensive logging of all sensitive operations
- **Secure Defaults**: Safe configuration presets for common deployment scenarios

### Changed
- **Security Model**: Implemented zero-trust architecture
- **Credential Handling**: Secure storage and transmission of API keys
- **Access Controls**: Granular permissions for different API endpoints

### Fixed
- **Authentication Bypass**: Patched critical vulnerability in API access
- **Data Exposure**: Resolved information disclosure in error messages
- **Injection Attacks**: Fixed potential SQL injection in Neo4j queries

---

## [0.3.0] - 2025-11-15 "Neo4j Migration Complete"

### Added
- **Neo4j Integration**: Complete migration from SQLite to Neo4j graph database
- **Redis Cache**: Hot cache layer for active session management
- **Graph Schema**: Formal schema definition for memory relationships
- **Migration Tools**: Scripts to migrate existing SQLite data to Neo4j
- **Backup System**: Automated graph backup and restoration procedures

### Changed
- **Storage Architecture**: Tiered storage (Redis hot cache + Neo4j persistent)
- **Query Language**: Cypher queries for graph operations
- **Relationship Modeling**: Graph-based connections between memories
- **Indexing Strategy**: Graph-based indices for faster retrieval

### Fixed
- **Performance**: Significantly improved query performance for complex relationships
- **Scalability**: Better handling of large-scale memory graphs
- **Consistency**: Stronger data integrity with ACID-compliant transactions

---

## [0.2.0] - 2025-10-30 "Cognitive Agents"

### Added
- **Verifier Agent**: Fact-checking via empirical distrust protocol
- **Archivist Agent**: Memory maintenance and staleness detection
- **Distiller Agent**: Content summarization and extraction
- **Agent Framework**: Plugin system for extensible cognitive capabilities
- **Truth Scoring**: Provenance-aware fact-checking with primary source priority

### Changed
- **Memory Hygiene**: Automated maintenance of memory quality
- **Verification Process**: Evidence-based fact-checking system
- **Quality Assurance**: Continuous assessment of memory reliability
- **Maintenance Schedule**: Regular memory grooming operations

### Fixed
- **Hallucinations**: Reduced false information in responses
- **Stale Information**: Automatic detection and updating of outdated memories
- **Data Quality**: Improved content validation and cleaning procedures

---

## [0.1.0] - 2025-09-15 "Initial Architecture"

### Added
- **Core Backend**: Initial ECE_Core with SQLite memory system
- **Anchor Interface**: Terminal interface for user interaction
- **Basic Memory**: Text-based memory storage and retrieval
- **LLM Integration**: Support for various local LLM servers
- **Plugin System**: Extensible tool architecture (UTCP)

### Changed
- **Foundation**: Established core architecture patterns
- **API Design**: Defined RESTful API structure for components

### Fixed
- **Basic Functionality**: Initial implementation of core features
--- END OF FILE: CHANGELOG.md ---

--- START OF FILE: GPU_RESOURCE_FIXES.md ---
# GPU Resource Management Fixes

## Overview
This document outlines the fixes implemented to resolve GPU resource contention and timeout issues in the ECE_Core system.

## Issues Identified
1. **GPU Lock Timeouts**: Multiple processes (Root Console, Root Mic, Root Dreamer) competing for GPU access
2. **Inadequate Timeout Handling**: 60-second timeouts too short for model loading
3. **No Status Monitoring**: Difficult to diagnose GPU lock issues
4. **No Emergency Release**: Stuck locks could not be released without restarting services

## Changes Made

### 1. Enhanced GPU Controller (`tools/modules/sovereign.js`)
- Increased default lock timeout from 60s to 120s (2 minutes)
- Added retry logic with better error handling
- Implemented fallback to direct WebGPU access when bridge is unavailable
- Added GPU status checking functionality
- **NEW: Model Loading Serialization**: Added `withModelLoadLock()` to ensure models load sequentially, preventing GPU overload during initial loading

### 2. Improved GPU Bridge (`tools/webgpu_bridge.py`)
- Increased timeout from 60s to 120s for lock acquisition
- Added request tracking to prevent starvation
- Implemented emergency force-release-all endpoint
- Enhanced logging for better debugging

### 3. Updated Components
- **Root Console (`model-server-chat.html`)**: Added 2-minute timeout for model loading, improved error handling, now uses model loading lock, fixed model URL
- **Root Mic (`root-mic.html`)**: Enhanced GPU lock acquisition with status checking, now uses model loading lock
- **Root Dreamer (`root-dreamer.html`)**: Added retry logic for model loading, now uses model loading lock, uses more reliable model configuration

### 4. New Utilities
- **GPU Manager Script** (`scripts/gpu_manager.py`): Command-line tool to monitor and manage GPU resources
- **Test Script** (`scripts/test_gpu_fixes.py`): Comprehensive testing of GPU resource management

## Usage

### Monitoring GPU Status
```bash
python scripts/gpu_manager.py --status
```

### Monitoring Continuously
```bash
python scripts/gpu_manager.py --monitor --interval 10
```

### Force Releasing GPU Locks
```bash
python scripts/gpu_manager.py --force-release
```

### Testing the Fixes
```bash
python scripts/test_gpu_fixes.py
```

## Emergency Procedures

If GPU locks become stuck:

1. **Check Status**: `python scripts/gpu_manager.py --status`
2. **Standard Reset**: `python scripts/gpu_manager.py --reset`
3. **Emergency Release**: `python scripts/gpu_manager.py --force-release`

## Bridge Endpoints

- `GET /v1/gpu/status` - Get current GPU lock status
- `POST /v1/gpu/lock` - Acquire GPU lock (with 2-minute timeout)
- `POST /v1/gpu/unlock` - Release GPU lock
- `POST /v1/gpu/reset` - Standard reset
- `POST /v1/gpu/force-release-all` - Emergency release all locks

## Priority System

The system uses a priority-based queue:
- **Priority 0**: Microphone (voice input) - highest priority
- **Priority 10**: Console (chat) - medium priority
- **Priority 15**: Default
- **Priority 20**: Dreamer (background processing) - lowest priority

## Best Practices

1. Always use the enhanced `GPUController.withLock()` method
2. Implement proper error handling around GPU operations
3. Monitor GPU status when experiencing issues
4. Use appropriate timeouts for different operations:
   - Model loading: 2 minutes (120s)
   - Inference: 1 minute (60s)
   - Status checks: 10 seconds (10s)
--- END OF FILE: GPU_RESOURCE_FIXES.md ---

--- START OF FILE: HOT_RELOAD_SYSTEM.md ---
# Hot Reload System for GPU Management

## Overview
The hot reload system allows for real-time updates to GPU management logic without requiring service restarts. This significantly improves development workflow and system maintainability.

## Components

### 1. Python Bridge Hot Reload (`scripts/smart_gpu_bridge.py`)
- Monitors GPU-related Python files for changes
- Automatically reloads bridge logic when changes are detected
- Maintains all existing functionality while updating code
- Includes emergency release mechanism to prevent stale locks

### 2. JavaScript Hot Reload (`tools/modules/gpu-hot-reloader.js`)
- Browser-side monitoring for HTML/JS changes
- Provides manual reload triggers for development
- Integrates with existing GPU management system
- Auto-detects development environment (localhost/127.0.0.1)

### 3. Enhanced GPU Manager (`scripts/gpu_manager.py`)
- Includes hot reload status checking
- Provides manual reload triggers
- Enhanced monitoring capabilities

## Usage

### Development Mode
The hot reload system automatically activates when running on localhost:
- Files are monitored every 2 seconds for changes
- Changes to GPU-related files trigger automatic reloads
- Existing GPU locks are safely released during reload

### Manual Triggers
- **Python Bridge**: Send POST to `/v1/hot-reload` endpoint
- **Browser Console**: Call `window.triggerGPUHotReload()`
- **GPU Manager Script**: `python scripts/gpu_manager.py --hot-reload`

### Enable/Disable
- **Browser**: `window.setGPUHotReloadEnabled(true/false)`
- **Environment**: Only active on localhost/127.0.0.1 by default

## Files Monitored
- `tools/webgpu_bridge.py` - Backend bridge logic
- `tools/modules/sovereign.js` - Frontend GPU controller
- `tools/model-server-chat.html` - Main console interface
- `tools/root-mic.html` - Voice input interface
- `tools/root-dreamer.html` - Background processing

## Startup Scripts
- `start-sovereign-console-hotreload.bat` - Launches system with hot reload
- Uses the smart GPU bridge with built-in monitoring

## Benefits
1. **Faster Development**: Changes take effect immediately
2. **No Service Interruption**: Updates occur without restarting services
3. **Stale Lock Prevention**: Automatic cleanup during reloads
4. **Development Convenience**: Built-in triggers for manual reloads

## Production Considerations
- Hot reload is disabled by default in production environments
- Only activates on localhost/127.0.0.1
- Can be manually enabled/disabled as needed
- Includes safety mechanisms to prevent conflicts

## Troubleshooting
- If hot reload isn't working, check file permissions
- Ensure the bridge is running on the expected port (8080)
- Check browser console for hot reload messages
- Use the emergency release endpoint if locks become stuck
--- END OF FILE: HOT_RELOAD_SYSTEM.md ---

--- START OF FILE: kill-edge.bat ---
@echo off
echo üî™ Killing all Microsoft Edge processes...
taskkill /F /IM msedge.exe /T
echo.
echo ‚úÖ Edge terminated. You can now run 'launch-edge-unsafe.bat' cleanly.
pause
--- END OF FILE: kill-edge.bat ---

--- START OF FILE: launch-chromium-d3d12.bat ---
@echo off
setlocal EnableDelayedExpansion

:: Define the User Data Directory (Project Relative)
set "USER_DATA=%~dp0browser_data"
if not exist "%USER_DATA%" mkdir "%USER_DATA%"

:: Define Flags for D3D12 (Default for Windows)
set "FLAGS=--user-data-dir="%USER_DATA%" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --disable-gpu-watchdog"
set "URL=http://localhost:8000/model-server-chat.html"

echo ---------------------------------------------------
echo üîç Detecting Browsers...
echo ---------------------------------------------------

set "count=0"

:: 1. Check Microsoft Edge
if exist "C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe" (
    set /a count+=1
    set "name[!count!]=Microsoft Edge"
    set "path[!count!]=C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe"
)

:: 2. Check Google Chrome
if exist "C:\Program Files\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome"
    set "path[!count!]=C:\Program Files\Google\Chrome\Application\chrome.exe"
) else if exist "C:\Program Files (x86)\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome (x86)"
    set "path[!count!]=C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
) else if exist "%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome (User)"
    set "path[!count!]=%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe"
)

:: 3. Check Brave
if exist "C:\Program Files\BraveSoftware\Brave-Browser\Application\brave.exe" (
    set /a count+=1
    set "name[!count!]=Brave Browser"
    set "path[!count!]=C:\Program Files\BraveSoftware\Brave-Browser\Application\brave.exe"
)

:: Check if any found
if %count%==0 (
    echo ‚ùå No compatible Chromium browser found.
    pause
    exit /b
)

:: Display Menu
echo Select a browser to launch:
for /L %%i in (1,1,%count%) do (
    echo [%%i] !name[%%i]!
)
echo.

:prompt
set /p "choice=Enter number (1-%count%): "

:: Validate Input
if "%choice%"=="" goto prompt
if %choice% LSS 1 goto prompt
if %choice% GTR %count% goto prompt

set "BROWSER=!path[%choice%]!"
set "BROWSER_NAME=!name[%choice%]!"

echo.
echo üöÄ Launching %BROWSER_NAME% with D3D12 (Default) backend...
echo Path: "%BROWSER%"
echo Data: "%USER_DATA%"
echo URL: "%URL%"
echo.
echo Executing: "%BROWSER%" %FLAGS% %URL%
echo.

"%BROWSER%" %FLAGS% %URL%
pause
--- END OF FILE: launch-chromium-d3d12.bat ---

--- START OF FILE: launch-chromium-vulkan.bat ---
@echo off
setlocal EnableDelayedExpansion

:: Define the User Data Directory (Project Relative)
set "USER_DATA=%~dp0browser_data"
if not exist "%USER_DATA%" mkdir "%USER_DATA%"

:: Define Flags (Critical for Snapdragon)
set "FLAGS=--user-data-dir="%USER_DATA%" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --enable-features=Vulkan --use-angle=vulkan --disable-gpu-watchdog"
set "URL=http://localhost:8000/tools/model-server-chat.html"

echo ---------------------------------------------------
echo üîç Detecting Browsers (Vulkan Mode)...
echo ---------------------------------------------------

set "count=0"

:: 1. Check Microsoft Edge
if exist "C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe" (
    set /a count+=1
    set "name[!count!]=Microsoft Edge"
    set "path[!count!]=C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe"
)

:: 2. Check Google Chrome
if exist "C:\Program Files\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome"
    set "path[!count!]=C:\Program Files\Google\Chrome\Application\chrome.exe"
) else if exist "C:\Program Files (x86)\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome (x86)"
    set "path[!count!]=C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
) else if exist "%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe" (
    set /a count+=1
    set "name[!count!]=Google Chrome (User)"
    set "path[!count!]=%LOCALAPPDATA%\Google\Chrome\Application\chrome.exe"
)

:: 3. Check Brave
if exist "C:\Program Files\BraveSoftware\Brave-Browser\Application\brave.exe" (
    set /a count+=1
    set "name[!count!]=Brave Browser"
    set "path[!count!]=C:\Program Files\BraveSoftware\Brave-Browser\Application\brave.exe"
)

:: Check if any found
if %count%==0 (
    echo ‚ùå No compatible Chromium browser found.
    pause
    exit /b
)

:: Display Menu
echo Select a browser to launch:
for /L %%i in (1,1,%count%) do (
    echo [%%i] !name[%%i]!
)
echo.

:prompt
set /p "choice=Enter number (1-%count%): "

:: Validate Input
if "%choice%"=="" goto prompt
if %choice% LSS 1 goto prompt
if %choice% GTR %count% goto prompt

set "BROWSER=!path[%choice%]!"
set "BROWSER_NAME=!name[%choice%]!"

echo.
echo üöÄ Launching %BROWSER_NAME% with VULKAN backend...
echo Path: "%BROWSER%"
echo Data: "%USER_DATA%"
echo.

"%BROWSER%" %FLAGS% %URL%
pause
--- END OF FILE: launch-chromium-vulkan.bat ---

--- START OF FILE: ORCHESTRATOR_SUMMARY.md ---
# ECE_Core GPU Management System - Orchestrator Summary

## Current Status
- **Issue**: GPU resource contention and deadlock in the WebGPU bridge system
- **Root Cause**: Insufficient timeout values (60s) and poor queue management
- **Components Affected**: Root Console, Root Mic, Root Dreamer
- **Symptoms**: "GPU Locked. Please close other AI tools." errors, timeout drops, queue deadlocks

## Implemented Fixes

### 1. Backend Bridge (`tools/webgpu_bridge.py`)
- **Timeout Increase**: From 60s to 120s for lock acquisition
- **Queue Tracking**: Added request_start_times to prevent starvation
- **Emergency Release**: Implemented `force_release_all()` method
- **Enhanced Status**: Added detailed queue information to status endpoint

### 2. Frontend Kernel (`tools/modules/sovereign.js`)
- **Enhanced GPU Controller**: Improved timeout handling with 120s default
- **Retry Logic**: Added retry mechanism with proper error handling
- **Fallback Mechanism**: Direct WebGPU access when bridge unavailable
- **Status Checking**: Added GPU status check functionality

### 3. Component Updates
- **Root Console**: 2-minute timeout for model loading operations
- **Root Mic**: Enhanced error handling with status checking
- **Root Dreamer**: Retry logic for model loading failures

### 4. New Utilities
- **GPU Manager**: `scripts/gpu_manager.py` for monitoring and management
- **Test Suite**: `scripts/test_gpu_fixes.py` for verification
- **Hot Reload**: `scripts/smart_gpu_bridge.py` with file monitoring
- **Documentation**: Comprehensive documentation files

### 5. Hot Reload System
- **Automatic Reload**: Python bridge monitors file changes
- **Browser Integration**: JS hot reload for development
- **Enhanced Startup**: `start-sovereign-console-hotreload.bat`

## System Architecture Changes

### Before
- 60-second timeouts causing frequent failures
- No queue starvation prevention
- No emergency release mechanism
- Manual restart required for fixes

### After
- 120-second timeouts for better resource allocation
- Queue tracking to prevent indefinite waits
- Emergency release endpoints for stuck locks
- Hot reload capability for development

## Next Steps for Orchestrator

### 1. Immediate Actions
1. **Restart Services**: Ensure the new bridge with fixes is running
2. **Verify Fixes**: Run `python scripts/test_gpu_fixes.py` to confirm functionality
3. **Monitor Performance**: Use `python scripts/gpu_manager.py --monitor` to observe queue behavior

### 2. Validation Steps
1. Launch the system using `start-sovereign-console-hotreload.bat`
2. Test concurrent access with multiple components
3. Verify timeout behavior is now 120s instead of 60s
4. Test emergency release functionality

### 3. Advanced Monitoring
1. Monitor queue depths and wait times
2. Check for any remaining deadlock patterns
3. Verify priority system works correctly
4. Test under high load conditions

### 4. Potential Issues to Watch
1. Memory leaks in the new queue tracking system
2. Performance impact of extended timeouts
3. Compatibility with existing workflows
4. Edge cases in the force release mechanism

## Expected Outcomes
- Reduced timeout errors by 80-90%
- Elimination of queue deadlocks
- Improved concurrent access performance
- Better development workflow with hot reload

## Rollback Plan
If issues arise, revert to the previous bridge by:
1. Restoring the original `webgpu_bridge.py` from backup
2. Restarting services
3. Removing hot reload components if needed

## Success Metrics
- Zero "GPU Queue Timeout" errors during normal operation
- Proper priority-based queue processing
- Sub-10s response times for GPU lock acquisition under normal load
- Successful handling of concurrent requests without deadlocks
--- END OF FILE: ORCHESTRATOR_SUMMARY.md ---

--- START OF FILE: README.md ---
# Context Engine (Sovereign Edition)

> **Philosophy:** Your mind, augmented. Your data, sovereign. Your tools, open.

A **Browser-Native** cognitive extraction system. No servers. No cloud. No installation.
Just you, your browser, and your infinite context.

---

## ‚ö° Quick Start

1.  **Download** this repository.
2.  **Open** `tools/index.html` in Chrome or Edge.
3.  **Click** "Double Click to Launch" on the Console.

*That's it. You are running a local LLM with persistent Graph Memory.*

---

## üèóÔ∏è Architecture

The system runs entirely in `tools/` using WebAssembly (WASM).

### 1. The Sovereign Loop
```mermaid
graph TD
    User -->|Input| HTML[model-server-chat.html]

    subgraph Browser_Memory ["Two Birds, One Stone"]
        HTML -->|Store/Retrieve| Cozo["CozoDB WASM"]
        Cozo -->|Persist| IDB["IndexedDB/OPFS"]
    end

    subgraph Cognitive_Engine
        HTML -->|Context + Prompt| WebLLM["DeepSeek-R1 (WASM)"]
        WebLLM -->|Reasoning Trace| HTML
    end
```

### 2. Core Components
*   **Brain**: `model-server-chat.html` - Runs the Graph-R1 Reasoning Loop. Now uses **Hybrid Search** (Vector + Lexical) and supports SOTA models (Qwen 3, Gemma 3).
*   **Memory**: `CozoDB (WASM)` - Stores relations (`*memory`) and vectors. Persists to browser IndexedDB.
*   **Stomach**: `sovereign-db-builder.html` - Ingests files into the graph. Now "Multisensory-Ready" (Phase A): accepts images/audio as references.

---

## üî• Hot Reload System

The system includes a comprehensive hot reload mechanism for GPU management and development:

*   **Automatic Reload**: Changes to GPU-related files trigger automatic reloads
*   **Browser Integration**: Hot reload functionality built into all components
*   **No Service Restart**: Updates occur without restarting services
*   **Stale Lock Prevention**: Automatic cleanup during reloads

### Getting Started with Hot Reload
1. Use the enhanced startup script: `start-sovereign-console-hotreload.bat`
2. Monitor changes in real-time with the GPU manager: `python scripts/gpu_manager.py`
3. Manual reload triggers available in browser console: `window.triggerGPUHotReload()`

## üîÑ Model Loading Serialization

The system now includes model loading serialization to prevent GPU overload:

*   **Sequential Loading**: Models load one at a time to prevent GPU resource contention
*   **Queue Management**: Proper queuing of model loading requests
*   **Resource Protection**: Prevents multiple models from loading simultaneously
*   **Improved Stability**: Reduces GPU memory allocation conflicts during startup
*   **Model URL Fixes**: Corrected model URLs to use reliable endpoints

---

## üìö Documentation

*   **Architecture**: [specs/spec.md](specs/spec.md)
*   **Roadmap**: [specs/plan.md](specs/plan.md)
*   **Memory Schema**: [specs/architecture/memory-layer.spec.md](specs/architecture/memory-layer.spec.md)
*   **WASM Layer**: [specs/architecture/sovereign-wasm.spec.md](specs/architecture/sovereign-wasm.spec.md)
*   **Hot Reload System**: [HOT_RELOAD_SYSTEM.md](HOT_RELOAD_SYSTEM.md)

---

## üßπ Legacy Support
The old Python/Neo4j backend has been **archived**.
*   Legacy README: [archive/v1_python_backend/README_LEGACY.md](archive/v1_python_backend/README_LEGACY.md)
*   Legacy Code: `archive/v1_python_backend/`
--- END OF FILE: README.md ---

--- START OF FILE: read_all.py ---
#!/usr/bin/env python3
"""
Root Reader: Aggregates content from relevant project files for orchestration.

This script scans the project directory and combines the content of source code,
configuration, and documentation files into a single text file (combined_text.txt).

It respects the current project structure:
- Root level scripts and docs
- tools/: Sovereign Core (JS/HTML/CSS)
- specs/: System Specifications
- scripts/: CI/Utility scripts
"""
import argparse
import json
import os
from typing import List, Tuple, Any

def find_project_root(start_path: str | None = None) -> str:
    """
    Locate project root by looking for indicators like .git, package.json, README.md
    """
    if start_path is None:
        start_path = os.path.abspath(__file__)

    path = os.path.abspath(start_path)
    if os.path.isfile(path):
        path = os.path.dirname(path)

    root_indicators = (".git", "package.json", "README.md")
    while True:
        if any(os.path.exists(os.path.join(path, ind)) for ind in root_indicators):
            return path
        parent = os.path.dirname(path)
        if parent == path:
            return os.getcwd()
        path = parent

def get_allowed_files(project_root: str) -> List[Tuple[str, str]]:
    """
    Returns list of (file_path, section_name) for all relevant project files.
    """
    allowed_files = []
    
    # Extensions we care about
    code_exts = {'.py', '.js', '.ts', '.html', '.css', '.json', '.md', '.bat', '.ps1', '.sh', '.yaml', '.yml'}
    
    # Directories to completely ignore
    ignored_dirs = {'.git', '.venv', 'browser_data', 'archive', '__pycache__', 'node_modules', '.github'}
    
    # Files to ignore
    ignored_files = {
        'package-lock.json', 
        'combined_text.txt', 
        'cozo_lib_wasm_bg.wasm',
        'combined_memory.json',
        'cozo_import_memory.json'
    }

    for root, dirs, files in os.walk(project_root):
        # Filter directories in-place to avoid walking into ignored ones
        dirs[:] = [d for d in dirs if d not in ignored_dirs and not d.startswith('.')]
        
        rel_root = os.path.relpath(root, project_root)
        section = "ROOT" if rel_root == "." else rel_root.replace(os.sep, "_").upper()

        for f in files:
            if f in ignored_files:
                continue
            
            ext = os.path.splitext(f)[1].lower()
            if ext in code_exts:
                full_path = os.path.join(root, f)
                allowed_files.append((full_path, section))
                
    return allowed_files

def to_yaml_style(obj: Any, indent: int = 0) -> str:
    """
    Recursively converts a JSON-compatible object to a YAML-like string.
    """
    lines = []
    prefix = "  " * indent
    
    if isinstance(obj, dict):
        for k, v in obj.items():
            if isinstance(v, (dict, list)):
                lines.append(f"{prefix}{k}:")
                lines.append(to_yaml_style(v, indent + 1))
            else:
                # Handle multiline strings safely
                v_str = str(v)
                if '\n' in v_str:
                     lines.append(f"{prefix}{k}: |")
                     for line in v_str.split('\n'):
                         lines.append(f"{prefix}  {line}")
                else:
                    lines.append(f"{prefix}{k}: {v}")
    elif isinstance(obj, list):
        for item in obj:
            if isinstance(item, (dict, list)):
                lines.append(f"{prefix}-")
                # For list items that are objects, we want the properties to align slightly differently
                # But for simplicity in this custom dumper:
                sub = to_yaml_style(item, indent + 1)
                lines.append(sub)
            else:
                lines.append(f"{prefix}- {item}")
    else:
        return f"{prefix}{obj}"

    return "\n".join(lines)

def create_project_corpus(
    output_file: str | None = None,
    dry_run: bool = False,
):
    """
    Aggregates content from project files into a single corpus.
    """
    project_root = find_project_root()
    output_file = output_file or os.path.join(project_root, "combined_text.txt")

    print(f"Project Root Detected: {project_root}")
    allowed_files = get_allowed_files(project_root)

    if not allowed_files:
        print(f"No relevant files found in '{project_root}'.")
        return

    print(f"Found {len(allowed_files)} files to process.")

    if dry_run:
        print(f"Dry run enabled ‚Äî would process {len(allowed_files)} files:")
        for file_path, section in allowed_files:
            print(f"  - {os.path.relpath(file_path, project_root)} ({section})")
        return

    memory_records = []

    with open(output_file, "w", encoding="utf-8") as outfile:
        # Add a file map at the very top for the orchestrator
        outfile.write("=== PROJECT FILE MAP ===\n")
        for file_path, section in allowed_files:
            rel_path = os.path.relpath(file_path, project_root)
            outfile.write(f"- {rel_path} ({section})\n")
        outfile.write("========================\n\n")

        for file_path, section in allowed_files:
            rel_path = os.path.relpath(file_path, project_root)
            print(f"Processing '{rel_path}'...")
            try:
                with open(file_path, "rb") as raw_file:
                    raw_data = raw_file.read()
                if not raw_data:
                    continue
                
                try:
                    decoded_content = raw_data.decode("utf-8")
                except UnicodeDecodeError:
                    decoded_content = raw_data.decode("utf-8", errors="replace")

                ext = os.path.splitext(file_path)[1].lower()
                final_content = decoded_content
                
                # Upgrade: Convert JSON to YAML-like text
                if ext == '.json':
                    try:
                        json_obj = json.loads(decoded_content)
                        # Use pretty print json as a reliable fallback or strict yaml style
                        # The user asked for "YAML-like string (key: value) or pretty-printed JSON (indent=2)"
                        # Let's try our YAML converter first, it's cleaner for reading.
                        final_content = to_yaml_style(json_obj)
                    except Exception:
                        # Fallback to original content if parsing fails
                        pass

                outfile.write(f"--- START OF FILE: {rel_path} ---\n")
                outfile.write(final_content)
                if not final_content.endswith('\n'):
                    outfile.write('\n')
                outfile.write(f"--- END OF FILE: {rel_path} ---\n\n")

                # Store for JSON memory export (Node structure)
                memory_records.append({
                    "id": rel_path,
                    "timestamp": int(os.path.getmtime(file_path)),
                    "role": "file",
                    "content": final_content,
                    "source": rel_path
                })

            except Exception as e:
                print(f"Error processing '{rel_path}': {e}")

    # Save the combined memory records for Builder ingestion
    memory_file = os.path.join(project_root, "combined_memory.json")
    with open(memory_file, "w", encoding="utf-8") as f:
        json.dump(memory_records, f, indent=2, ensure_ascii=False)
    print(f"Memory records saved to '{memory_file}'.")

    print(f"\nAggregation complete. Corpus saved to '{output_file}'.")

def _parse_cli() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Aggregate project code and docs for orchestration.")
    p.add_argument(
        "--out",
        "-o",
        default=None,
        help="Output file path (defaults to combined_text.txt in project root)",
    )
    p.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be processed without writing the combined file",
    )
    return p.parse_args()

if __name__ == "__main__":
    args = _parse_cli()
    create_project_corpus(
        output_file=args.out,
        dry_run=args.dry_run,
    )
--- END OF FILE: read_all.py ---

--- START OF FILE: secure-bridge-launch.ps1 ---
<#
.SYNOPSIS
    Launches the WebGPU Bridge with an ephemeral Firewall rule.
.DESCRIPTION
    1. Picks a random port (or uses -Port).
    2. Adds a Windows Firewall 'Allow' rule for that port.
    3. Starts webgpu_bridge.py.
    4. Removes the Firewall rule immediately upon exit.
#>

param (
    [int]$Port = 0
)

# 1. Pick Port
if ($Port -eq 0) {
    $Port = Get-Random -Minimum 9000 -Maximum 9999
}

$RuleName = "SovereignCoda-Bridge-$Port"

try {
    Write-Host "üîí [Sovereign Coda] Configuring Secure Network..." -ForegroundColor Cyan

    # 2. Add Firewall Rule (Requires Admin usually, logic checks availability)
    # Check if running as Admin
    $isElevated = ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
    
    if (-not $isElevated) {
        Write-Warning "‚ö†Ô∏è  Not running as Administrator. Firewall rules might fail."
        Write-Warning "    Please right-click this script and 'Run as Administrator' if basic access fails."
        Write-Host "    Attempting to proceed (Localhost will work, LAN might not)..." -ForegroundColor DarkGray
    } else {
        Write-Host "    + Opening TCP Port $Port..." -ForegroundColor Green
        New-NetFirewallRule -DisplayName $RuleName -Direction Inbound -LocalPort $Port -Protocol TCP -Action Allow -Profile Private,Domain | Out-Null
    }

    # 3. Launch Bridge
    $env:BRIDGE_PORT = $Port
    $env:BRIDGE_HOST = "0.0.0.0"
    
    # Check if webgpu_bridge.py is in the same dir as script, or in tools/
    $BridgeScript = Join-Path $PSScriptRoot "webgpu_bridge.py"
    if (-not (Test-Path $BridgeScript)) {
        $BridgeScript = Join-Path $PSScriptRoot "tools\webgpu_bridge.py"
    }
    
    if (-not (Test-Path $BridgeScript)) {
        Write-Error "Could not find webgpu_bridge.py in $PSScriptRoot or $PSScriptRoot\tools"
        exit 1
    }

    Write-Host "üöÄ Launching Bridge ($BridgeScript) on Port $Port..." -ForegroundColor Cyan
    python $BridgeScript

} catch {
    Write-Error "An error occurred: $_"
} finally {
    # 4. Cleanup
    if ($isElevated) {
        Write-Host "`nüîí [Sovereign Coda] Cleaning up Firewall Rules..." -ForegroundColor Cyan
        try {
            Remove-NetFirewallRule -DisplayName $RuleName -ErrorAction SilentlyContinue
            Write-Host "    + Rule '$RuleName' Removed." -ForegroundColor Green
        } catch {
            Write-Warning "Failed to remove firewall rule. Please remove '$RuleName' manually."
        }
    }
}
--- END OF FILE: secure-bridge-launch.ps1 ---

--- START OF FILE: start-bridge.bat ---
@echo off
REM Start the WebGPU Bridge with proper configuration
echo Starting WebGPU Bridge...
cd /d "C:\Users\rsbii\Projects\ECE_Core\tools"
python webgpu_bridge.py
--- END OF FILE: start-bridge.bat ---

--- START OF FILE: start-sovereign-console-hotreload.bat ---
@echo off
REM Enhanced Sovereign Console Startup with Hot Reload Support
echo Starting Sovereign Console with Hot Reload Support...

echo.
echo ========================================
echo  ECE_Core Startup with Hot Reload
echo ========================================
echo.

REM Start the WebGPU Bridge with hot reload capability in a new window
echo Launching WebGPU Bridge with Hot Reload (API Backend)...
set BRIDGE_PORT=8080
set BRIDGE_TOKEN=sovereign-secret
start "WebGPU Bridge (Hot Reload)" cmd /k "cd /d "C:\Users\rsbii\Projects\ECE_Core" && python scripts/smart_gpu_bridge.py"

echo.
echo ========================================
echo  Available Services:
echo  - Bridge: http://localhost:8080
echo  - File Server: http://localhost:8000
echo  - Console: http://localhost:8000/model-server-chat.html
echo  - Mic: http://localhost:8000/root-mic.html
echo  - Dreamer: http://localhost:8000/root-dreamer.html
echo ========================================
echo.

REM Launch the file server
echo Launching File Server...
cd /d "C:\Users\rsbii\Projects\ECE_Core"
python -m http.server 8000

pause
--- END OF FILE: start-sovereign-console-hotreload.bat ---

--- START OF FILE: start-sovereign-console.bat ---
@echo off
echo Starting Sovereign Console Server...

echo.
echo Local Access: http://localhost:8000/

echo.
echo Network Access (for phone/other devices):
for /f "tokens=*" %%a in ('python -c "import socket; s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM); s.connect(('8.8.8.8', 80)); print(s.getsockname()[0]); s.close()"') do set IP=%%a
echo http://%IP%:8000/
echo.

cd tools

echo Launching WebGPU Bridge (API Backend)...
set BRIDGE_PORT=8080
set BRIDGE_TOKEN=sovereign-secret
start "WebGPU Bridge" cmd /k python webgpu_bridge.py

echo Launching File Server...
python -m http.server 8000
pause
--- END OF FILE: start-sovereign-console.bat ---

--- START OF FILE: temp_config.ts ---
ÔøΩÔøΩi m p o r t   l o g   f r o m   " l o g l e v e l " ;  
 i m p o r t   {   R e s p o n s e F o r m a t   }   f r o m   " . / o p e n a i _ a p i _ p r o t o c o l s " ;  
 i m p o r t   {   L o g i t P r o c e s s o r ,   I n i t P r o g r e s s C a l l b a c k ,   L o g L e v e l   }   f r o m   " . / t y p e s " ;  
 i m p o r t   {  
     D e p e n d e n c y E r r o r ,  
     I n v a l i d N u m b e r S t r i n g E r r o r ,  
     M i n V a l u e E r r o r ,  
     N o n N e g a t i v e E r r o r ,  
     R a n g e E r r o r ,  
 }   f r o m   " . / e r r o r " ;  
  
 / * *  
   *   C o n v e r s a t i o n   t e m p l a t e   c o n f i g  
   * /  
 e x p o r t   i n t e r f a c e   C o n v T e m p l a t e C o n f i g   {  
     s y s t e m _ t e m p l a t e :   s t r i n g ;  
     s y s t e m _ m e s s a g e :   s t r i n g ;  
     r o l e s :   R e c o r d < R o l e ,   s t r i n g > ;  
     r o l e _ t e m p l a t e s ? :   P a r t i a l < R e c o r d < R o l e ,   s t r i n g > > ;  
     s e p s :   A r r a y < s t r i n g > ;  
     r o l e _ c o n t e n t _ s e p ? :   s t r i n g ;  
     r o l e _ e m p t y _ s e p ? :   s t r i n g ;  
     s t o p _ s t r :   A r r a y < s t r i n g > ;  
     s y s t e m _ p r e f i x _ t o k e n _ i d s ? :   A r r a y < n u m b e r > ;  
     s t o p _ t o k e n _ i d s :   A r r a y < n u m b e r > ;  
     a d d _ r o l e _ a f t e r _ s y s t e m _ m e s s a g e ? :   b o o l e a n ;  
 }  
  
 e x p o r t   e n u m   R o l e   {  
     u s e r   =   " u s e r " ,  
     a s s i s t a n t   =   " a s s i s t a n t " ,  
     t o o l   =   " t o o l " ,  
 }  
  
 e x p o r t   c o n s t   D e f a u l t L o g L e v e l :   L o g L e v e l   =   " W A R N " ;  
  
 / * *  
   *   P l a c e   h o l d e r s   t h a t   c a n   b e   u s e d   i n   r o l e   t e m p l a t e s .  
   *   F o r   e x a m p l e ,   a   r o l e   t e m p l a t e   o f  
   *   ` < < q u e s t i o n > >   $ { M e s s a g e P l a c e h o l d e r s . U S E R }   < < f u n c t i o n > >   $ { M e s s a g e P l a c e h o l d e r s . F U N C T I O N } `  
   *   w i l l   i n s e r t   t h e   u s e r   m e s s a g e   t o   $ { M e s s a g e P l a c e h o l d e r s . U S E R }  
   *   a n d   i n s e r t   t h e   f u n c t i o n   m e s s a g e   t o   $ { M e s s a g e P l a c e h o l d e r s . F U N C T I O N }  
   *   a t   r u n   t i m e .  
   * /  
 e x p o r t   e n u m   M e s s a g e P l a c e h o l d e r s   {  
     s y s t e m   =   " { s y s t e m _ m e s s a g e } " ,  
     u s e r   =   " { u s e r _ m e s s a g e } " ,  
     a s s i s t a n t   =   " { a s s i s t a n t _ m e s s a g e } " ,  
     t o o l   =   " { t o o l _ m e s s a g e } " ,  
     f u n c t i o n   =   " { f u n c t i o n _ s t r i n g } " ,  
     h e r m e s _ t o o l s   =   " { h e r m e s _ t o o l s } " ,  
 }  
  
 / * *  
   *   I n f o r m a t i o n   a b o u t   t h e   t o k e n i z e r .   C u r r e n t l y ,   o n l y   ` t o k e n _ p o s t p r o c _ m e t h o d `   i s   u s e d   t o  
   *   p o s t   p r o c e s s   t h e   t o k e n   t a b l e   w h e n   u s i n g   g r a m m a r .  
   * /  
 e x p o r t   i n t e r f a c e   T o k e n i z e r I n f o   {  
     t o k e n _ p o s t p r o c _ m e t h o d :   s t r i n g ;  
     p r e p e n d _ s p a c e _ i n _ e n c o d e :   b o o l e a n ;  
     s t r i p _ s p a c e _ i n _ d e c o d e :   b o o l e a n ;  
 }  
  
 / * *  
   *   C o n f i g   o f   o n e   c h a t   m o d e l ,   a   d a t a   s t r u c t u r e   r e p r e s e n t i n g   ` m l c - c h a t - c o n f i g . j s o n ` .  
   *   T h i s   o n l y   c o r r e s p o n d s   t o   t h e   c h a t - r e l a t e d   f i e l d s   a n d   ` t o k e n i z e r _ f i l e s `   o f   ` m l c - c h a t - c o n f i g . j s o n ` .  
   *   O n l y   t h e s e   f i e l d s   a f f e c t   t h e   c o n v e r s a t i o n   i n   r u n t i m e .  
   *   i . e .   T h e   t h i r d   p a r t   i n   h t t p s : / / l l m . m l c . a i / d o c s / g e t _ s t a r t e d / m l c _ c h a t _ c o n f i g . h t m l .  
   *  
   *   T h i s   i s   i n i t i a l i z e d   i n   ` M L C E n g i n e . r e l o a d ( ) `   w i t h   t h e   m o d e l ' s   ` m l c - c h a t - c o n f i g . j s o n ` .  
   * /  
 e x p o r t   i n t e r f a c e   C h a t C o n f i g   {  
     / /   F i r s t   t h r e e   f i e l d s   a f f e c t   t h e   e n t i r e   c o n v e r s a t i o n ,   i . e .   u s e d   i n   ` M L C E n g i n e . r e l o a d ( ) `  
     t o k e n i z e r _ f i l e s :   A r r a y < s t r i n g > ;  
     t o k e n i z e r _ i n f o ? :   T o k e n i z e r I n f o ;  
     t o k e n _ t a b l e _ p o s t p r o c _ m e t h o d ? :   s t r i n g ;   / /   T O D O :   b a c k w a r d   c o m p a t i b i l i t y ,   r e m o v e   s o o n  
     v o c a b _ s i z e :   n u m b e r ;  
     c o n v _ c o n f i g ? :   P a r t i a l < C o n v T e m p l a t e C o n f i g > ;  
     c o n v _ t e m p l a t e :   C o n v T e m p l a t e C o n f i g ;  
     / /   K V C a c h e   s e t t i n g s  
     c o n t e x t _ w i n d o w _ s i z e :   n u m b e r ;  
     s l i d i n g _ w i n d o w _ s i z e :   n u m b e r ;  
     a t t e n t i o n _ s i n k _ s i z e :   n u m b e r ;  
     / /   F i e l d s   b e l o w   c a n   b e   s w a p p e d   p e r - g e n e r a t i o n   v i a   ` G e n e r a t i o n C o n f i g `  
     / /   F i e l d s   o n l y   u s e d   i n   M L C  
     r e p e t i t i o n _ p e n a l t y :   n u m b e r ;  
     / /   F i e l d s   s h a r e d   b y   M L C   a n d   O p e n A I   A P I s  
     f r e q u e n c y _ p e n a l t y :   n u m b e r ;  
     p r e s e n c e _ p e n a l t y :   n u m b e r ;  
     t o p _ p :   n u m b e r ;  
     t e m p e r a t u r e :   n u m b e r ;  
     b o s _ t o k e n _ i d ? :   n u m b e r ;  
 }  
  
 / * *  
   *   C u s t o m   o p t i o n s   t h a t   c a n   b e   u s e d   t o   o v e r r i d e   k n o w n   c o n f i g   v a l u e s .  
   * /  
 / /   e s l i n t - d i s a b l e - n e x t - l i n e   @ t y p e s c r i p t - e s l i n t / n o - e m p t y - o b j e c t - t y p e  
 e x p o r t   i n t e r f a c e   C h a t O p t i o n s   e x t e n d s   P a r t i a l < C h a t C o n f i g >   { }  
  
 / * *  
   *   O p t i o n a l   c o n f i g u r a t i o n s   f o r   ` C r e a t e M L C E n g i n e ( ) `   a n d   ` C r e a t e W e b W o r k e r M L C E n g i n e ( ) ` .  
   *  
   *   a p p C o n f i g :   C o n f i g u r e   t h e   a p p ,   i n c l u d i n g   t h e   l i s t   o f   m o d e l s   a n d   w h e t h e r   t o   u s e   I n d e x e d D B   c a c h e .  
   *   i n i t P r o g r e s s C a l l b a c k :   A   c a l l b a c k   f o r   s h o w i n g   t h e   p r o g r e s s   o f   l o a d i n g   t h e   m o d e l .  
   *   l o g i t P r o c e s s o r R e g i s t r y :   A   r e g i s t e r   f o r   s t a t e f u l   l o g i t   p r o c e s s o r s ,   s e e   ` w e b l l m . L o g i t P r o c e s s o r ` .  
   *  
   *   @ n o t e   A l l   f i e l d s   a r e   o p t i o n a l ,   a n d   ` l o g i t P r o c e s s o r R e g i s t r y `   i s   o n l y   u s e d   f o r   ` M L C E n g i n e `   a n d   n o t  
   *   o t h e r   ` M L C E n g i n e ` s .  
   * /  
 e x p o r t   i n t e r f a c e   M L C E n g i n e C o n f i g   {  
     a p p C o n f i g ? :   A p p C o n f i g ;  
     i n i t P r o g r e s s C a l l b a c k ? :   I n i t P r o g r e s s C a l l b a c k ;  
     l o g i t P r o c e s s o r R e g i s t r y ? :   M a p < s t r i n g ,   L o g i t P r o c e s s o r > ;  
     l o g L e v e l ? :   L o g L e v e l ;  
 }  
  
 / * *  
   *   C o n f i g   f o r   a   s i n g l e   g e n e r a t i o n .  
   *   E s s e n t i a l l y   ` C h a t C o n f i g `   w i t h o u t   ` t o k e n i z e r _ f i l e s ` ,   ` c o n v _ c o n f i g ` ,   o r   ` c o n v _ t e m p l a t e ` .  
   *   W e   a l s o   s u p p o r t   a d d i t i o n a l   f i e l d s   n o t   p r e s e n t   i n   ` m l c - c h a t - c o n f i g . j s o n `   d u e   t o   O p e n A I - l i k e   A P I s .  
   *  
   *   N o t e   t h a t   a l l   v a l u e s   a r e   o p t i o n a l .   I f   u n s p e c i f i e d ,   w e   u s e   w h a t e v e r   v a l u e s   i n   ` C h a t C o n f i g `  
   *   i n i t i a l i z e d   d u r i n g   ` M L C E n g i n e . r e l o a d ( ) ` .  
   * /  
 e x p o r t   i n t e r f a c e   G e n e r a t i o n C o n f i g   {  
     / /   O n l y   u s e d   i n   M L C  
     r e p e t i t i o n _ p e n a l t y ? :   n u m b e r   |   n u l l ;  
     i g n o r e _ e o s ? :   b o o l e a n ;  
     / /   S h a r e d   b y   M L C   a n d   O p e n A I   A P I s  
     t o p _ p ? :   n u m b e r   |   n u l l ;  
     t e m p e r a t u r e ? :   n u m b e r   |   n u l l ;  
     / /   O n l y   i n   O p e n A I   A P I s  
     m a x _ t o k e n s ? :   n u m b e r   |   n u l l ;  
     f r e q u e n c y _ p e n a l t y ? :   n u m b e r   |   n u l l ;  
     p r e s e n c e _ p e n a l t y ? :   n u m b e r   |   n u l l ;  
     s t o p ? :   s t r i n g   |   n u l l   |   A r r a y < s t r i n g > ;  
     n ? :   n u m b e r   |   n u l l ;  
     l o g i t _ b i a s ? :   R e c o r d < s t r i n g ,   n u m b e r >   |   n u l l ;  
     l o g p r o b s ? :   b o o l e a n   |   n u l l ;  
     t o p _ l o g p r o b s ? :   n u m b e r   |   n u l l ;  
     r e s p o n s e _ f o r m a t ? :   R e s p o n s e F o r m a t   |   n u l l ;  
     / /   e x t r a _ b o d y   i n   C h a t C o m p l e t i o n s R e q u e s t  
     e n a b l e _ t h i n k i n g ? :   b o o l e a n   |   n u l l ;  
     e n a b l e _ l a t e n c y _ b r e a k d o w n ? :   b o o l e a n   |   n u l l ;  
 }  
  
 e x p o r t   f u n c t i o n   p o s t I n i t A n d C h e c k G e n e r a t i o n C o n f i g V a l u e s (  
     c o n f i g :   G e n e r a t i o n C o n f i g ,  
 ) :   v o i d   {  
     f u n c t i o n   _ h a s V a l u e ( v a l u e :   a n y ) :   b o o l e a n   {  
         / /   i f   w e   u s e   ` i f   v a l u e `   d i r e c t l y ,   ` v a l u e `   b e i n g   0   e v a l u a t e s   t o   f a l s e ,   v i o l a t i n g   s e m a n t i c s  
         r e t u r n   v a l u e   ! = =   u n d e f i n e d   & &   v a l u e   ! = =   n u l l ;  
     }  
     i f   (  
         c o n f i g . f r e q u e n c y _ p e n a l t y   & &  
         ( c o n f i g . f r e q u e n c y _ p e n a l t y   <   - 2 . 0   | |   c o n f i g . f r e q u e n c y _ p e n a l t y   >   2 . 0 )  
     )   {  
         t h r o w   n e w   R a n g e E r r o r ( " f r e q u e n c y _ p e n a l t y " ,   - 2 . 0 ,   2 . 0 ) ;  
     }  
     i f   (  
         c o n f i g . p r e s e n c e _ p e n a l t y   & &  
         ( c o n f i g . p r e s e n c e _ p e n a l t y   <   - 2 . 0   | |   c o n f i g . p r e s e n c e _ p e n a l t y   >   2 . 0 )  
     )   {  
         t h r o w   n e w   R a n g e E r r o r ( " p r e s e n c e _ p e n a l t y " ,   - 2 . 0 ,   2 . 0 ) ;  
     }  
     i f   ( _ h a s V a l u e ( c o n f i g . r e p e t i t i o n _ p e n a l t y )   & &   c o n f i g . r e p e t i t i o n _ p e n a l t y !   < =   0 )   {  
         t h r o w   n e w   M i n V a l u e E r r o r ( " r e p e t i t i o n _ p e n a l t y " ,   0 ) ;  
     }  
     i f   ( _ h a s V a l u e ( c o n f i g . m a x _ t o k e n s )   & &   c o n f i g . m a x _ t o k e n s !   < =   0 )   {  
         t h r o w   n e w   M i n V a l u e E r r o r ( " m a x _ t o k e n s " ,   0 ) ;  
     }  
     i f   ( ( _ h a s V a l u e ( c o n f i g . t o p _ p )   & &   c o n f i g . t o p _ p !   < =   0 )   | |   c o n f i g . t o p _ p !   >   1 )   {  
         t h r o w   n e w   R a n g e E r r o r ( " t o p _ p " ,   0 ,   1 ) ;  
     }  
     i f   ( _ h a s V a l u e ( c o n f i g . t e m p e r a t u r e )   & &   c o n f i g . t e m p e r a t u r e !   <   0 )   {  
         t h r o w   n e w   N o n N e g a t i v e E r r o r ( " t e m p e r a t u r e " ) ;  
     }  
     / /   I f   o n l y   o n e   o f   f r e q u e n c y   o r   p r e s e n c e   p e n a t l y   i s   s e t ,   m a k e   t h e   o t h e r   o n e   0 . 0  
     i f   (  
         _ h a s V a l u e ( c o n f i g . f r e q u e n c y _ p e n a l t y )   & &  
         ! _ h a s V a l u e ( c o n f i g . p r e s e n c e _ p e n a l t y )  
     )   {  
         c o n f i g . p r e s e n c e _ p e n a l t y   =   0 . 0 ;  
         l o g . w a r n ( " O n l y   f r e q u e n c y _ p e n a l t y   i s   s e t ;   w e   d e f a u l t   p r e s e n c e _ p e n a t y   t o   0 . " ) ;  
     }  
     i f   (  
         _ h a s V a l u e ( c o n f i g . p r e s e n c e _ p e n a l t y )   & &  
         ! _ h a s V a l u e ( c o n f i g . f r e q u e n c y _ p e n a l t y )  
     )   {  
         c o n f i g . f r e q u e n c y _ p e n a l t y   =   0 . 0 ;  
         l o g . w a r n (  
             " O n l y   p r e s e n c e _ p e n a l t y   i s   s e t ;   w e   d e f a u l t   f r e q u e n c y _ p e n a l t y   t o   0 . " ,  
         ) ;  
     }  
     / /   C h e c k   l o g i t _ b i a s   r a n g e  
     i f   ( _ h a s V a l u e ( c o n f i g . l o g i t _ b i a s ) )   {  
         f o r   ( c o n s t   t o k e n I D   i n   c o n f i g . l o g i t _ b i a s )   {  
             c o n s t   b i a s   =   c o n f i g . l o g i t _ b i a s [ t o k e n I D ] ;  
             i f   ( b i a s   >   1 0 0   | |   b i a s   <   - 1 0 0 )   {  
                 t h r o w   n e w   R a n g e E r r o r (  
                     " l o g i t _ b i a s " ,  
                     - 1 0 0 ,  
                     1 0 0 ,  
                     " G o t   "   +   b i a s   +   "   f o r   t o k e n I D   "   +   t o k e n I D ,  
                 ) ;  
             }  
             i f   ( i s N a N ( p a r s e I n t ( t o k e n I D ) ) )   {  
                 t h r o w   n e w   I n v a l i d N u m b e r S t r i n g E r r o r ( " l o g i t _ b i a s ' s   k e y s " ,   t o k e n I D ) ;  
             }  
         }  
     }  
     / /   l o g p r o b s   a n d   t o p _ l o g p r o b s  
     i f   ( _ h a s V a l u e ( c o n f i g . t o p _ l o g p r o b s ) )   {  
         / /   I f   t o p _ l o g p r o b s   i s   n o n - n u l l ,   l o g p r o b s   m u s t   b e   t r u e  
         i f   ( ! c o n f i g . l o g p r o b s )   {  
             t h r o w   n e w   D e p e n d e n c y E r r o r ( " t o p _ l o g p r o b s " ,   " l o g p r o b s " ,   t r u e ) ;  
         }  
         / /   t o p _ l o g p r o b s   s h o u l d   b e   i n   r a n g e   [ 0 , 5 ]  
         i f   ( c o n f i g . t o p _ l o g p r o b s !   <   0   | |   c o n f i g . t o p _ l o g p r o b s !   >   5 )   {  
             t h r o w   n e w   R a n g e E r r o r ( " t o p _ l o g p r o b s " ,   0 ,   5 ,   " G o t   "   +   c o n f i g . t o p _ l o g p r o b s ) ;  
         }  
     }  
     / /   I f   d e f i n e d   l o g p r o b s   b u t   n o t   t o p _ l o g p r o b s ,   s i m p l y   m a k e   i t   0  
     i f   ( c o n f i g . l o g p r o b s )   {  
         i f   ( ! _ h a s V a l u e ( c o n f i g . t o p _ l o g p r o b s ) )   {  
             c o n f i g . t o p _ l o g p r o b s   =   0 ;  
         }  
     }  
 }  
  
 e x p o r t   e n u m   M o d e l T y p e   {  
     " L L M " ,  
     " e m b e d d i n g " ,  
     " V L M " ,   / /   v i s i o n - l a n g u a g e   m o d e l  
 }  
  
 / * *  
   *   I n f o r m a t i o n   f o r   a   m o d e l .  
   *   @ p a r a m   m o d e l :   t h e   h u g g i n g f a c e   l i n k   t o   d o w n l o a d   t h e   m o d e l   w e i g h t s ,   a c c e p t i n g   f o u r   f o r m a t s :  
   *         -   h t t p s : / / h u g g i n g f a c e . c o / { U S E R N A M E } / { M O D E L } ,   w h i c h   w e   a u t o m a t i c a l l y   u s e   t h e   m a i n   b r a n c h  
   *         -   h t t p s : / / h u g g i n g f a c e . c o / { U S E R N A M E } / { M O D E L } / ,   w h i c h   w e   a u t o m a t i c a l l y   u s e   t h e   m a i n   b r a n c h  
   *         -   h t t p s : / / h u g g i n g f a c e . c o / { U S E R N A M E } / { M O D E L } / r e s o l v e / { B R A N C H }  
   *         -   h t t p s : / / h u g g i n g f a c e . c o / { U S E R N A M E } / { M O D E L } / r e s o l v e / { B R A N C H } /  
   *   @ p a r a m   m o d e l _ i d :   w h a t   w e   c a l l   t h e   m o d e l .  
   *   @ p a r a m   m o d e l _ l i b :   l i n k   t o   t h e   m o d e l   l i b r a r y   ( w a s m   f i l e )   t h e   m o d e l   u s e s .  
   *   @ p a r a m   o v e r r i d e s :   p a r t i a l   C h a t C o n f i g   t o   o v e r r i d e   m l c - c h a t - c o n f i g . j s o n ;   c a n   b e   u s e d   t o   c h a n g e   K V C a c h e   s e t t i n g s .  
   *   @ p a r a m   v r a m _ r e q u i r e d _ M B :   a m o u n t   o f   v r a m   i n   M B   r e q u i r e d   t o   r u n   t h e   m o d e l   ( c a n   u s e  
   *         ` u t i l s / v r a m _ r e q u i r e m e n t s `   t o   c a l c u l a t e ) .  
   *   @ p a r a m   l o w _ r e s o u r c e _ r e q u i r e d :   w h e t h e r   t h e   m o d e l   c a n   r u n   o n   l i m i t e d   d e v i c e s   ( e . g .   A n d r o i d   p h o n e ) .  
   *   @ p a r a m   b u f f e r _ s i z e _ r e q u i r e d _ b y t e s :   r e q u i r e d   ` m a x S t o r a g e B u f f e r B i n d i n g S i z e ` ,   d i f f e r e n t   f o r   e a c h   d e v i c e .  
   *   @ p a r a m   r e q u i r e d _ f e a t u r e s :   f e a t u r e   n e e d e d   t o   r u n   t h i s   m o d e l   ( e . g .   s h a d e r - f 1 6 ) .  
   *   @ p a r a m   m o d e l _ t y p e :   t h e   i n t e n d e d   u s e c a s e   f o r   t h e   m o d e l ,   i f   u n s p e c i f i e d ,   d e f a u l t   t o   L L M .  
   * /  
 e x p o r t   i n t e r f a c e   M o d e l R e c o r d   {  
     m o d e l :   s t r i n g ;  
     m o d e l _ i d :   s t r i n g ;  
     m o d e l _ l i b :   s t r i n g ;  
     o v e r r i d e s ? :   C h a t O p t i o n s ;  
     v r a m _ r e q u i r e d _ M B ? :   n u m b e r ;  
     l o w _ r e s o u r c e _ r e q u i r e d ? :   b o o l e a n ;  
     b u f f e r _ s i z e _ r e q u i r e d _ b y t e s ? :   n u m b e r ;  
     r e q u i r e d _ f e a t u r e s ? :   A r r a y < s t r i n g > ;  
     m o d e l _ t y p e ? :   M o d e l T y p e ;  
 }  
  
 / * *  
   *   E x t r a   c o n f i g u r a t i o n   t h a t   c a n   b e  
   *   p a s s e d   t o   t h e   l o a d .  
   *  
   *   @ p a r a m   m o d e l _ l i s t :   m o d e l s   t o   b e   u s e d .  
   *   @ p a r a m   u s e I n d e x e d D B C a c h e :   i f   t r u e ,   w i l l   u s e   I n d e x e d D B C a c h e   t o   c a c h e   m o d e l s   a n d   o t h e r   a r t i f a c t s .  
   *   I f   f a l s e   o r   u n s p e c i f i e d ,   w i l l   u s e   t h e   C a c h e   A P I .   F o r   m o r e   i n f o r m a t i o n   o f   t h e   t w o ,   s e e :  
   *   h t t p s : / / d e v e l o p e r . m o z i l l a . o r g / e n - U S / d o c s / W e b / A P I / S t o r a g e _ A P I / S t o r a g e _ q u o t a s _ a n d _ e v i c t i o n _ c r i t e r i a # w h a t _ t e c h n o l o g i e s _ s t o r e _ d a t a _ i n _ t h e _ b r o w s e r  
   *  
   *   @ n o t e   N o t e   t h a t   t h e   C a c h e   A P I   i s   m o r e   w e l l - t e s t e d   i n   W e b L L M   a s   o f   n o w .  
   * /  
 e x p o r t   i n t e r f a c e   A p p C o n f i g   {  
     m o d e l _ l i s t :   A r r a y < M o d e l R e c o r d > ;  
     u s e I n d e x e d D B C a c h e ? :   b o o l e a n ;  
 }  
  
 / * *  
   *   m o d e l V e r s i o n :   t h e   p r e b u i l t   m o d e l   l i b r a r i e s   t h a t   t h e   c u r r e n t   n p m   i s   c o m p a t i b l e   w i t h ,   a f f e c t s   t h e  
   *   ` m o d e l _ l i b ` s   i n   ` p r e b u i l t A p p C o n f i g ` .  
   *  
   *   @ n o t e   T h e   m o d e l   v e r s i o n   d o e s   n o t   h a v e   t o   m a t c h   t h e   n p m   v e r s i o n ,   s i n c e   n o t   e a c h   n p m   u p d a t e  
   *   r e q u i r e s   a n   u p d a t e   o f   t h e   m o d e l   l i b r a r i e s .  
   * /  
 e x p o r t   c o n s t   m o d e l V e r s i o n   =   " v 0 _ 2 _ 8 0 " ;  
 e x p o r t   c o n s t   m o d e l L i b U R L P r e f i x   =  
     " h t t p s : / / r a w . g i t h u b u s e r c o n t e n t . c o m / m l c - a i / b i n a r y - m l c - l l m - l i b s / m a i n / w e b - l l m - m o d e l s / " ;  
  
 / * *  
   *   M o d e l s   t h a t   s u p p o r t   f u n c t i o n   c a l l i n g   ( i . e .   u s a g e   o f   ` C h a t C o m p l e t i o n R e q u e s t . t o o l s ` ) .   M o r e   t o   c o m e .  
   * /  
 e x p o r t   c o n s t   f u n c t i o n C a l l i n g M o d e l I d s   =   [  
     " H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
     " H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
     " H e r m e s - 2 - P r o - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
     " H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
     " H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
 ] ;  
  
 / * *  
   *   D e f a u l t   m o d e l s   a n d   m o d e l   l i b r a r y   m a p p i n g   t o   b e   u s e d   i f   u n s p e c i f i e d .  
   *  
   *   @ n o t e   T h i s   i s   t h e   o n l y   s o u r c e   o f   t r u t h   o f   w h i c h   p r e b u i l t   m o d e l   l i b r a r i e s   a r e   c o m p a t i b l e   w i t h   t h e  
   *   c u r r e n t   W e b L L M   n p m   v e r s i o n .  
   * /  
 e x p o r t   c o n s t   p r e b u i l t A p p C o n f i g :   A p p C o n f i g   =   {  
     u s e I n d e x e d D B C a c h e :   f a l s e ,  
     m o d e l _ l i s t :   [  
         / /   L l a m a - 3 . 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 1 2 8 . 8 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   8 7 9 . 0 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   T O D O :   t e m p o r a r i l y   c o m m e n t i n g   o u t   q 0 f 3 2   m o d e l s   d u e   t o   c o r r e c t n e s s   i s s u e s  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ i d :   " L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 2 6 ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 1 B - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 7 3 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 9 5 1 . 5 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 2 6 3 . 6 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   L l a m a - 3 . 1  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 2 9 5 . 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 9 8 . 3 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 1 0 1 . 0 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 0 0 1 . 0 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   D e e p S e e k - R 1 - D i s t i l l - Q w e n  
         / /   T O D O ( C h a r l i e ) :   Q w e n 2 - 1 . 5 B   i s   e x p e r i e n c i n g   c o r r e c t n e s s   i s s u e ,   h e n c e   c o m m e n t e d   f o r   n o w .  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - Q w e n - 1 . 5 B - q 4 f 1 6 _ 1 - M L C " ,  
         / /       m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - Q w e n - 1 . 5 B - q 4 f 1 6 _ 1 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - Q w e n - 1 . 5 B - q 4 f 3 2 _ 1 - M L C " ,  
         / /       m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - Q w e n - 1 . 5 B - q 4 f 3 2 _ 1 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - Q w e n - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - Q w e n - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 6 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - Q w e n - 7 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - Q w e n - 7 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 9 0 0 . 0 9 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   D e e p S e e k - R 1 - D i s t i l l - L l a m a  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - L l a m a - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - L l a m a - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 1 0 1 . 0 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / D e e p S e e k - R 1 - D i s t i l l - L l a m a - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " D e e p S e e k - R 1 - D i s t i l l - L l a m a - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 0 0 1 . 0 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   H e r m e s - 3   a n d   H e r m e s - 2  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 2 - T h e t a - L l a m a - 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 2 - T h e t a - L l a m a - 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 9 7 6 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 2 - T h e t a - L l a m a - 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 2 - T h e t a - L l a m a - 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 0 5 1 . 2 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 9 7 6 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 2 - P r o - L l a m a - 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 0 5 1 . 2 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 3 - L l a m a - 3 . 2 - 3 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 3 - L l a m a - 3 . 2 - 3 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 9 5 1 . 5 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 3 - L l a m a - 3 . 2 - 3 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 3 - L l a m a - 3 . 2 - 3 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 . 2 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 2 6 3 . 6 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 7 7 9 . 2 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 3 - L l a m a - 3 . 1 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 8 7 6 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / H e r m e s - 2 - P r o - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " H e r m e s - 2 - P r o - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 0 3 3 . 2 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         / /   P h i 3 . 5 - m i n i - i n s t r u c t  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 6 7 2 . 0 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 4 8 3 . 1 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 2 0 . 0 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - m i n i - i n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 1 7 9 . 1 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   P h i - 3 . 5 - v i s i o n - i n s t r u c t  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 2 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 9 5 2 . 1 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
             m o d e l _ t y p e :   M o d e l T y p e . V L M ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 . 5 - v i s i o n - i n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 2 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 8 7 9 . 8 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
             m o d e l _ t y p e :   M o d e l T y p e . V L M ,  
         } ,  
         / /   M i s t r a l   v a r i a n t s  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 7 3 . 3 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 6 1 9 . 2 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / M i s t r a l - 7 B - I n s t r u c t - v 0 . 2 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " M i s t r a l - 7 B - I n s t r u c t - v 0 . 2 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 7 3 . 3 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / O p e n H e r m e s - 2 . 5 - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " O p e n H e r m e s - 2 . 5 - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 7 3 . 3 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / N e u r a l H e r m e s - 2 . 5 - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " N e u r a l H e r m e s - 2 . 5 - M i s t r a l - 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 7 3 . 3 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / W i z a r d M a t h - 7 B - V 1 . 1 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " W i z a r d M a t h - 7 B - V 1 . 1 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / M i s t r a l - 7 B - I n s t r u c t - v 0 . 3 - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 7 3 . 3 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
                 s l i d i n g _ w i n d o w _ s i z e :   - 1 ,  
             } ,  
         } ,  
         / /   S m o l L M 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 7 7 4 . 1 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 1 . 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 6 9 2 . 3 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   8 7 1 . 9 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 3 2 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 7 4 3 . 9 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 7 6 . 0 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 3 6 0 M - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 7 9 . 6 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 5 9 . 6 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 3 2 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / S m o l L M 2 - 1 3 5 M - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   7 1 9 . 3 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   G e m m a 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 8 9 5 . 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 0 8 . 7 5 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 5 8 3 . 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 4 . 7 5 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 9 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 9 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 9 b - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 4 2 2 . 0 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 9 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 9 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 9 b - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   8 3 8 3 . 3 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   G e m m a 2 - 2 b - j p n  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - j p n - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - j p n - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - j p n - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 8 9 5 . 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 - 2 b - j p n - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 - 2 b - j p n - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 - 2 b - j p n - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 0 8 . 7 5 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n - 3  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 0 . 6 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 0 . 6 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 0 . 6 B - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 4 0 3 . 3 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 0 . 6 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 0 . 6 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 0 . 6 B - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 9 2 4 . 9 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 0 . 6 B - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 0 . 6 B - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 0 . 6 B - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 2 2 0 . 3 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   T O D O :   t e m p o r a r i l y   c o m m e n t i n g   o u t   q 0 f 3 2   m o d e l s   d u e   t o   c o r r e c t n e s s   i s s u e s  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 0 . 6 B - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ i d :   " Q w e n 3 - 0 . 6 B - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 3 - 0 . 6 B - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       v r a m _ r e q u i r e d _ M B :   3 8 4 3 . 2 5 ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 1 . 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 1 . 7 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 1 . 7 B - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 0 3 6 . 6 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 1 . 7 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 1 . 7 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 1 . 7 B - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 6 3 5 . 4 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 4 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 4 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 4 B - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 4 3 1 . 5 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 4 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 4 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 4 B - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 3 2 7 . 7 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 8 B - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 8 B - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 6 9 5 . 7 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 3 - 8 B - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 3 - 8 B - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 8 5 2 . 5 5 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n - 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   9 4 4 . 6 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 0 6 0 . 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 4 . 1 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   T O D O :   t e m p o r a r i l y   c o m m e n t i n g   o u t   q 0 f 3 2   m o d e l s   d u e   t o   c o r r e c t n e s s   i s s u e s  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ i d :   " Q w e n 2 . 5 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       v r a m _ r e q u i r e d _ M B :   2 6 5 4 . 7 5 ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   2 5 0 4 . 7 6 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   2 8 9 3 . 6 4 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 6 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 9 0 0 . 0 9 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n 2 . 5 - C o d e r  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   9 4 4 . 6 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 0 6 0 . 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 4 . 1 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   T O D O :   t e m p o r a r i l y   c o m m e n t i n g   o u t   q 0 f 3 2   m o d e l s   d u e   t o   c o r r e c t n e s s   i s s u e s  
         / /   {  
         / /       m o d e l :  
         / /           " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       v r a m _ r e q u i r e d _ M B :   2 6 5 4 . 7 5 ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   2 5 0 4 . 7 6 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 . 5 - 3 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   2 8 9 3 . 6 4 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 6 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - C o d e r - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - C o d e r - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 9 0 0 . 0 9 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n 2 . 5 - M a t h  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 . 5 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 . 5 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   S t a b l e L M - z e p h y r - 1 . 6 B  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 0 8 7 . 6 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 9 9 9 . 3 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 5 1 1 . 6 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s t a b l e l m - 2 - z e p h y r - 1 _ 6 b - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 8 4 7 . 3 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   R e d P a j a m a  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 9 7 2 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 9 2 8 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 0 4 1 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / R e d P a j a m a - I N C I T E - C h a t - 3 B - v 1 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 5 8 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   T i n y L l a m a   v 1 . 0  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 9 7 . 2 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   8 3 9 . 9 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 7 5 . 2 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 1 . 0 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   7 9 5 . 9 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   B E L O W   A R E   M O D E L S   O F   O L D E R   V E R S I O N S   O R   N O T   A S   P R A C T I C A L  
         / /   L l a m a - 3 . 1   7 0 B  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 . 1 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 . 1 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 _ 1 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 1 1 5 3 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n - 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   9 4 4 . 6 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 1 6 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 4 . 1 2 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   T O D O :   t e m p o r a r i l y   c o m m e n t i n g   o u t   q 0 f 3 2   m o d e l s   d u e   t o   c o r r e c t n e s s   i s s u e s  
         / /   {  
         / /       m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ i d :   " Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - M L C " ,  
         / /       m o d e l _ l i b :  
         / /           m o d e l L i b U R L P r e f i x   +  
         / /           m o d e l V e r s i o n   +  
         / /           " / Q w e n 2 - 0 . 5 B - I n s t r u c t - q 0 f 3 2 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
         / /       l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
         / /       v r a m _ r e q u i r e d _ M B :   2 6 5 4 . 7 5 ,  
         / /       o v e r r i d e s :   {  
         / /           c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
         / /       } ,  
         / /   } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 6 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 9 0 0 . 0 9 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   Q w e n 2 - M a t h  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 6 2 9 . 7 5 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - M a t h - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 1 . 5 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             v r a m _ r e q u i r e d _ M B :   1 8 8 8 . 9 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - M a t h - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - M a t h - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 1 0 6 . 6 7 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / Q w e n 2 - M a t h - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " Q w e n 2 - M a t h - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / Q w e n 2 - 7 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             v r a m _ r e q u i r e d _ M B :   5 9 0 0 . 0 9 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   L l a m a - 3  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 2 9 5 . 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 5 9 8 . 3 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 1 0 1 . 0 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 8 B - I n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 0 0 1 . 0 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 3 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 3 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 3 - 7 0 B - I n s t r u c t - q 3 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 1 1 5 3 . 1 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   P h i 3 - m i n i - i n s t r u c t  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 6 7 2 . 0 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 4 8 3 . 1 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 5 2 0 . 0 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / P h i - 3 - m i n i - 4 k - i n s t r u c t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 1 7 9 . 1 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   L l a m a - 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 2 8 4 . 0 1 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 6 1 8 . 5 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   9 1 0 9 . 0 3 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 2 - 7 b - c h a t - h f - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 7 4 9 . 0 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / L l a m a - 2 - 1 3 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " L l a m a - 2 - 1 3 b - c h a t - h f - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / L l a m a - 2 - 1 3 b - c h a t - h f - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 1 8 1 4 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         / /   G e m m a - 2 B  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 4 7 6 . 5 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             b u f f e r _ s i z e _ r e q u i r e d _ b y t e s :   2 6 2 1 4 4 0 0 0 ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 7 5 0 . 6 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             b u f f e r _ s i z e _ r e q u i r e d _ b y t e s :   2 6 2 1 4 4 0 0 0 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   4 0 9 6 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 b - i t - q 4 f 1 6 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 4 7 6 . 5 2 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             b u f f e r _ s i z e _ r e q u i r e d _ b y t e s :   2 6 2 1 4 4 0 0 0 ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / g e m m a - 2 b - i t - q 4 f 3 2 _ 1 - c t x 4 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 7 5 0 . 6 6 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             b u f f e r _ s i z e _ r e q u i r e d _ b y t e s :   2 6 2 1 4 4 0 0 0 ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   P h i - 2  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 2 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 2 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 2 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   3 0 5 3 . 9 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 2 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 2 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 2 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   4 0 3 2 . 4 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   f a l s e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 2 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 2 - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 2 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 1 3 1 . 9 7 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 2 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 2 - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 2 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 7 4 0 . 4 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   P h i - 1 . 5  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 1 _ 5 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 1 _ 5 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 1 _ 5 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 2 1 0 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 1 _ 5 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 1 _ 5 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 1 _ 5 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 6 8 2 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 1 _ 5 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 1 _ 5 - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 1 _ 5 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 2 1 0 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / p h i - 1 _ 5 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " p h i - 1 _ 5 - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / p h i - 1 _ 5 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 6 8 2 . 0 9 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   T i n y L l a m a   v 0 . 4  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 9 7 . 2 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   8 3 9 . 9 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   2 0 4 8 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 1 6 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   6 7 5 . 2 4 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             r e q u i r e d _ f e a t u r e s :   [ " s h a d e r - f 1 6 " ] ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         {  
             m o d e l :  
                 " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - M L C " ,  
             m o d e l _ i d :   " T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - M L C - 1 k " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / T i n y L l a m a - 1 . 1 B - C h a t - v 0 . 4 - q 4 f 3 2 _ 1 - c t x 2 k _ c s 1 k - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   7 9 5 . 9 8 ,  
             l o w _ r e s o u r c e _ r e q u i r e d :   t r u e ,  
             o v e r r i d e s :   {  
                 c o n t e x t _ w i n d o w _ s i z e :   1 0 2 4 ,  
             } ,  
         } ,  
         / /   E m b e d d i n g   m o d e l s  
         / /   - b   m e a n s   m a x _ b a t c h _ s i z e   t h i s   m o d e l   a l l o w s .   T h e   s m a l l e r   i t   i s ,   t h e   l e s s   m e m o r y   t h e   m o d e l   c o n s u m e s .  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - M L C - b 3 2 " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - c t x 5 1 2 _ c s 5 1 2 _ b a t c h 3 2 - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 4 0 7 . 5 1 ,  
             m o d e l _ t y p e :   M o d e l T y p e . e m b e d d i n g ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - M L C - b 4 " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s n o w f l a k e - a r c t i c - e m b e d - m - q 0 f 3 2 - c t x 5 1 2 _ c s 5 1 2 _ b a t c h 4 - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   5 3 9 . 4 ,  
             m o d e l _ t y p e :   M o d e l T y p e . e m b e d d i n g ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - M L C - b 3 2 " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - c t x 5 1 2 _ c s 5 1 2 _ b a t c h 3 2 - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   1 0 2 2 . 8 2 ,  
             m o d e l _ t y p e :   M o d e l T y p e . e m b e d d i n g ,  
         } ,  
         {  
             m o d e l :   " h t t p s : / / h u g g i n g f a c e . c o / m l c - a i / s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - M L C " ,  
             m o d e l _ i d :   " s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - M L C - b 4 " ,  
             m o d e l _ l i b :  
                 m o d e l L i b U R L P r e f i x   +  
                 m o d e l V e r s i o n   +  
                 " / s n o w f l a k e - a r c t i c - e m b e d - s - q 0 f 3 2 - c t x 5 1 2 _ c s 5 1 2 _ b a t c h 4 - w e b g p u . w a s m " ,  
             v r a m _ r e q u i r e d _ M B :   2 3 8 . 7 1 ,  
             m o d e l _ t y p e :   M o d e l T y p e . e m b e d d i n g ,  
         } ,  
     ] ,  
 } ;  
 
--- END OF FILE: temp_config.ts ---

--- START OF FILE: temp_file.html ---
Ôªø<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Root Coda Console</title>
    <style>
        * {
            box-sizing: border-box;
        }

        body {
            background: #0f0f11;
            color: #ccc;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            height: 100vh;
            margin: 0;
        }

        /* --- Resizable Split Layout --- */
        #container {
            display: flex;
            width: 100%;
            height: 100vh;
            overflow: hidden;
        }

        #sidebar {
            width: 320px;
            /* Default width */
            min-width: 200px;
            max-width: 80%;
            background: #151517;
            color: #d4d4d4;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #333;
            transition: width 0.1s ease;
        }

        /* Resize Handle */
        #resizer {
            width: 5px;
            cursor: col-resize;
            background: #333;
            transition: background 0.2s;
            z-index: 10;
        }

        #resizer:hover,
        #resizer.resizing {
            background: #00ff88;
        }

        #main {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: #1e1e1e;
            position: relative;
            min-width: 0;
        }

        /* --- Collapsible Details --- */
        details {
            margin-bottom: 10px;
            border-bottom: 1px solid #333;
        }

        summary {
            padding: 10px;
            cursor: pointer;
            font-weight: bold;
            user-select: none;
            background: #252526;
            list-style: none;
        }

        summary::-webkit-details-marker {
            display: none;
        }

        summary::after {
            content: '‚ñº';
            float: right;
            font-size: 0.8em;
            transition: transform 0.2s;
        }

        details[open] summary::after {
            transform: rotate(180deg);
        }

        .panel-content {
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        /* --- Chat Box --- */
        #chat-box {
            flex: 1;
            overflow-y: auto;
            margin-bottom: 20px;
            padding-right: 10px;
        }

        #chat-box::-webkit-scrollbar {
            width: 8px;
        }

        #chat-box::-webkit-scrollbar-track {
            background: #333;
        }

        #chat-box::-webkit-scrollbar-thumb {
            background: #555;
            border-radius: 4px;
        }

        .msg {
            padding: 12px;
            margin: 8px 0;
            border-radius: 6px;
            background: #333;
            max-width: 85%;
            word-wrap: break-word;
        }

        .user {
            background: #005f3b;
            /* Root Green-ish */
            color: white;
            align-self: flex-end;
            margin-left: auto;
        }

        .assistant {
            background: #2d2d2d;
            border-left: 3px solid #00ff88;
        }

        .msg details {
            margin-top: 10px;
            font-size: 0.85rem;
            opacity: 0.7;
            border: none;
        }

        .msg pre {
            background: #151515;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 0.8rem;
        }

        h3 {
            margin: 0 0 10px 0;
        }

        #status-text {
            font-size: 0.9rem;
            color: #888;
            margin-bottom: 8px;
        }

        #progress-bar {
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            margin: 8px 0;
        }

        #progress {
            height: 100%;
            background: #00ff88;
            width: 0%;
            transition: width 0.2s;
        }

        #status-log {
            flex: 1;
            overflow-y: auto;
            font-family: 'Consolas', monospace;
            font-size: 0.75rem;
            margin-top: 10px;
            padding: 8px;
            background: #0f0f0f;
            border-radius: 4px;
            border: 1px solid #333;
        }

        #status-log div {
            margin: 2px 0;
            padding: 2px 0;
        }

        .info {
            color: #888;
        }

        .warn {
            color: #ffc107;
        }

        .error {
            color: #ff4444;
        }

        .success {
            color: #00ff88;
        }

        #input-area {
            display: flex;
            gap: 10px;
        }

        textarea {
            flex: 1;
            height: 60px;
            background: #3c3c3c;
            border: 1px solid #555;
            color: white;
            padding: 10px;
            border-radius: 4px;
            resize: vertical;
            font-family: 'Segoe UI', sans-serif;
        }

        textarea:focus {
            outline: none;
            border-color: #00ff88;
        }

        button {
            padding: 8px 20px;
            background: #2d2d2d;
            color: white;
            border: 1px solid #444;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            align-self: flex-end;
            transition: all 0.2s;
        }

        button:hover {
            border-color: #00ff88;
            color: #00ff88;
        }

        button:disabled {
            background: #222;
            border-color: #333;
            color: #555;
            cursor: not-allowed;
        }

        .streaming {
            border-right: 2px solid #00ff88;
            animation: blink 0.7s infinite;
        }

        @keyframes blink {
            50% {
                border-color: transparent;
            }
        }

        /* --- Drag & Drop --- */
        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }

        #image-preview-container {
            display: none; /* Hidden by default */
            margin-bottom: 5px;
            padding: 8px;
            background: #252526;
            border-radius: 6px;
            border: 1px solid #444;
            position: relative;
            width: fit-content;
        }

        #image-preview-container .image-preview {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 5px;
        }

        #image-preview-container img {
            max-height: 100px;
            max-width: 200px;
            border-radius: 4px;
            display: block;
            object-fit: contain;
        }

        #image-preview-container div {
            font-size: small;
            margin-top: 5px;
        }

        #image-preview-container button {
            margin-top: 5px;
            padding: 4px 8px;
            background: #a43131;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
        }

        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }
    </style>
</head>

<body>
    <div id="container">
        <div id="sidebar">
            <div
                style="padding: 10px; font-weight: bold; font-size: 16px; border-bottom: 1px solid #444; color: #00ff88;">
                ‚ö° ROOT CODA
            </div>
            <small id="status-text" style="padding: 0 10px;">Initializing...</small>
            <div id="progress-bar" style="margin: 8px 10px 15px 10px;">
                <div id="progress"></div>
            </div>

            <!-- COLLAPSIBLE: Model Selection -->
            <details open>
                <summary>Model Selection</summary>
                <div class="panel-content">
                    <label for="hw-profile"
                        style="display:block; font-size: 11px; color: #ccc; margin-bottom: 4px;">Hardware
                        Profile:</label>
                    <select id="hw-profile" class="form-control"
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555; margin-bottom: 10px;">
                        <option value="lite">üîã Lite (Mobile / Snapdragon) - 2k Context</option>
                        <option value="mid" selected>üíª Mid (8GB VRAM) - 4k Context</option>
                        <option value="high">üöÄ High (16GB VRAM) - 16k Context</option>
                        <option value="ultra">‚ö° Ultra (24GB+ VRAM) - 32k Context</option>
                    </select>
                    <select id="model-select" disabled
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555;">
                        <option value="" disabled>-- Select a Model --</option>

                        <optgroup label="‚ú® SOTA (Latest)">
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 (7B Distill) [Verified]</option>
                            <option value="mlc-ai/Qwen3-4B-q4f16_1-MLC">Qwen 3 4B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen3-8B-q4f16_1-MLC">Qwen 3 8B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC" selected>Qwen 2.5 7B (Instruct) [Verified]</option>
                            <option value="mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC">Phi 3.5 Mini (3.8B)</option>
                        </optgroup>

                        <optgroup label="üëÅÔ∏è Vision Models">
                            <option value="mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC">Phi 3.5 Vision (4.2B) [Multimodal]</option>
                        </optgroup>

                        <optgroup label="üåü 14B Models">
                            <option value="mlc-ai/Qwen2.5-14B-Instruct-q4f16_1-MLC">Qwen 2.5 14B (Instruct)</option>
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC">DeepSeek R1 (14B Distill)</option>
                        </optgroup>

                        <optgroup label="üöÄ High Performance (Small)">

                        <optgroup label="üß™ Experimental / Other">
                            <option value="mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B</option>
                            <option value="mlc-ai/gemma-2-2b-it-q4f16_1-MLC">Gemma 2 2B</option>
                            <!-- <option value="mlc-ai/gemma-3-2b-it-q4f16_1-MLC">Gemma 3 2B</option> REMOVED: Model not available in current MLC library -->
                            <option value="mlc-ai/TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B</option>
                        </optgroup>

                        <option value="custom">-- Custom --</option>
                    </select>
                    <input id="custom-model-input" type="text" placeholder="Custom model ID"
                        style="width: 100%; padding: 5px; font-size: 11px;" disabled />
                    <button id="load-model-btn" disabled
                        style="width: 100%; padding: 6px; margin-top:5px; font-size: 11px;">Load Model</button>
                    <div style="font-size: 9px; color: #888; margin-top: 5px;">
                        * Models marked with * may require checking availability in MLC-AI repository
                    </div>
                </div>
            </details>

            <!-- COLLAPSIBLE: Controls -->
            <details>
                <summary>System Controls</summary>
                <div class="panel-content">
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px;">
                        <input type="checkbox" id="enable-bridge-toggle" onchange="toggleBridge(this.checked)">
                        <label for="enable-bridge-toggle" style="font-size: 11px; cursor: pointer;">Enable Wave Bridge
                            (ws:8080)</label>
                        <div id="bridge-status" style="font-size: 10px; color: #666; margin-left: 20px;">Disconnected
                        </div>
                    </div>
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px; display: flex; align-items: center; gap: 8px;">
                        <input type="checkbox" id="autosave-toggle" checked onchange="toggleAutoSave(this.checked)">
                        <label for="autosave-toggle" style="font-size: 11px; cursor: pointer;">üíæ Auto-Save Memories</label>
                    </div>
                    <button id="clear-cache-btn"
                        style="width: 100%; padding: 6px; background: #a43131; border:none; margin-top:5px; font-size: 11px;">‚ö†Ô∏è
                        Delete Model Cache</button>
                    <button id="debug-gpu-btn"
                        style="width: 100%; padding: 6px; background: #555; border:none; margin-top: 5px; font-size: 11px;">‚ùì
                        Debug GPU</button>
                </div>
            </details>

            <!-- COLLAPSIBLE: Logs -->
            <details open style="flex: 1; display: flex; flex-direction: column;">
                <summary>System Logs</summary>
                <div class="panel-content" style="flex: 1; display: flex; flex-direction: column; overflow: hidden;">
                    <button id="copy-logs-btn" style="width: 100%; padding: 4px; background: #333; font-size: 10px;">üìã
                        Copy Logs</button>
                    <div id="status-log"></div>
                </div>
            </details>
        </div>

        <div id="resizer"></div>

        <div id="main">
            <!-- Split view for Chat and Context -->
            <div style="flex: 1; display: flex; height: 100%; overflow: hidden;">
                <!-- Chat Column -->
                <div style="flex: 1; display: flex; flex-direction: column; border-right: 1px solid #333;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üí¨ Chat Stream</div>
                    <div id="chat-box"></div>
                    
                    <!-- Input Area with Drag & Drop -->
                    <div id="input-container" style="border-top: 1px solid #333; padding: 10px; display: flex; flex-direction: column;">
                        <div id="image-preview-container"></div>
                        <div id="input-area" style="display: flex; gap: 10px; padding: 5px; border: 2px dashed transparent; border-radius: 6px; transition: all 0.2s;">
                            <textarea id="input" disabled placeholder="Type or Drag & Drop Image..."></textarea>
                            <button id="send-btn" disabled>Send</button>
                        </div>
                    </div>
                </div>

                <!-- Context Column -->
                <div style="width: 40%; display: flex; flex-direction: column; background: #151515;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üß† Root Memory</div>
                    <div id="context-box"
                        style="flex: 1; overflow-y: auto; padding: 10px; font-family: monospace; font-size: 12px; white-space: pre-wrap; color: #aaa;">
                        <div style="text-align: center; margin-top: 50px; color: #555;">(Active retrievals will appear
                            here)</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // --- IMPORTS ---
        import { SovereignLogger, createStore, getWebGPUConfig, initCozo } from './modules/sovereign.js';
        import { CozoDb } from './cozo_lib_wasm.js';
        import { loadAllFromIndexedDb, writeToIndexedDb } from './indexeddb.js';
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';
        import { CreateWebWorkerMLCEngine } from "https://esm.run/@mlc-ai/web-llm";
        import { marked } from "https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js";
        import { VisionController } from './modules/vision.js';

        env.allowLocalModels = false;

        // --- ROOT KERNEL SETUP ---
        const logger = new SovereignLogger('Root-Console');

        const { state, subscribe } = createStore({
            status: "Initializing...",
            progress: 0,
            activeModel: null,
            autoSave: true
        });

        // --- UTILS: The Archivist ---
        async function saveTurn(role, content) {
            if (!state.autoSave || !window.db) return;
            try {
                const ts = Date.now();
                const id = 'msg_' + ts + '_' + role;

                // Write to Memory (Embedding null for Dreamer to fix later)
                const query = ":put memory { id: $id, timestamp: $ts, role: $role, content: $content, source: 'root_console', embedding: null }";
                await window.db.run(query, JSON.stringify({ id, ts, role, content }));
                ui.log(`üíæ Memory Saved (${role})`, "debug");
            } catch (e) {
                ui.log(`Save Failed: ${e.message}`, "error");
            }
        }

        // Expose toggle handler globally
        window.toggleAutoSave = (checked) => { 
            state.autoSave = checked; 
            ui.log(`Auto-Save: ${checked ? 'ON' : 'OFF'}`, 'info');
        };

        // Bridge legacy UI logging to Sovereign Logger
        const ui = {
            log: (msg, type = 'info') => {
                // Bridge to Kernel Logger
                if (type === 'debug') type = 'info';
                if (logger[type]) logger[type](msg); else logger.info(msg);

                // Update on-screen log
                const el = document.getElementById('status-log');
                if (el) {
                    const div = document.createElement('div');
                    div.className = type;
                    div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
                    div.style.fontSize = '11px';
                    div.style.padding = '2px';
                    div.style.borderBottom = '1px solid #222';
                    el.insertBefore(div, el.firstChild);
                    if (el.children.length > 100) el.removeChild(el.lastChild);
                }
            },
            updateProgress: (pct, text) => {
                const bar = document.getElementById('progress');
                if (bar) bar.style.width = (pct * 100) + "%";
                if (text) {
                    const el = document.getElementById('status-text');
                    if (el) el.innerText = text;
                }
            },
            append: (role, text, meta = null) => {
                const box = document.getElementById('chat-box');
                const div = document.createElement('div');
                div.className = `msg ${role}`;
                div.innerHTML = text ? marked.parse(text) : "";

                if (meta && meta.trace && meta.trace.length > 0) {
                    const details = document.createElement('details');
                    const summary = document.createElement('summary');
                    summary.textContent = 'üìã Reasoning Trace';
                    const pre = document.createElement('pre');
                    pre.textContent = JSON.stringify(meta.trace, null, 2);
                    details.appendChild(summary);
                    details.appendChild(pre);
                    div.appendChild(details);
                }

                box.appendChild(div);
                box.scrollTop = box.scrollHeight;

                return {
                    div,
                    update: (newText, isMarkdown = true) => {
                        div.innerHTML = isMarkdown ? marked.parse(newText) : newText;
                        box.scrollTop = box.scrollHeight;
                    },
                    appendText: (chunk) => {
                        if (div.innerText.endsWith("...")) div.innerText = div.innerText.slice(0, -3);
                        div.innerText += chunk;
                        box.scrollTop = box.scrollHeight;
                    }
                };
            },
            appendContext: (title, details) => {
                const box = document.getElementById('context-box');
                const div = document.createElement('div');
                div.style.borderBottom = '1px solid #333';
                div.style.marginBottom = '10px';
                div.style.paddingBottom = '10px';

                const h4 = document.createElement('div');
                h4.style.fontWeight = 'bold';
                h4.style.color = '#00ff88';
                h4.style.marginBottom = '5px';
                h4.textContent = `[${new Date().toLocaleTimeString()}] ${title}`;

                const p = document.createElement('div');
                p.style.whiteSpace = 'pre-wrap';
                p.textContent = details;

                div.appendChild(h4);
                div.appendChild(p);
                box.appendChild(div);
                box.scrollTop = box.scrollHeight;
            }
        };

        // --- GLOBAL STATE ---
        let db;
        let embedder;
        let engine;
        let contextManager;
        let selectedModelId = null;

        // VisionController will handle all image-related functionality

        // --- UTILS: Response Pattern Matcher ---
        class ResponsePattern {
            static match(response) {
                let data = response;
                if (typeof response === 'string') {
                    try {
                        const clean = response.replace(/```json/g, '').replace(/```/g, '').trim();
                        if (clean.startsWith('{')) data = JSON.parse(clean);
                    } catch (e) { }
                }
                if (data?.rows && Array.isArray(data.rows)) return { type: 'DB_RESULT', rows: data.rows, count: data.rows.length };
                if (data?.error || (typeof response === 'string' && response.toLowerCase().includes('error:'))) return { type: 'ERROR', error: data?.error || response };
                if (data?.ok === false && data?.message) return { type: 'ERROR', error: data.message };
                return { type: 'RAW_TEXT', text: typeof response === 'string' ? response : JSON.stringify(response) };
            }
        }

        // --- LOGIC: Context Manager (SFS) ---
        class ContextManager {
            constructor(engine, db) {
                this.engine = engine;
                this.db = db;
                this.maxIterations = 3;
            }

            async retrieveInitialContext(userText) {
                ui.log("üîç Reflex: Initiating Hybrid Retrieval...", "info");

                // 1. GENERATE QUERY VECTOR (Semantic)
                let vectorResults = [];
                if (embedder) {
                    try {
                        const output = await embedder(userText, { pooling: 'mean', normalize: true });
                        const queryVec = Array.from(output.data);

                        // CozoDB Vector Search (Brute Force L2 for accuracy on local sets)
                        // Query: Find top 10 nearest neighbors where embedding exists
                        const vecQuery = `
                            ?[id, content, dist, timestamp] := *memory{id, content, embedding, timestamp},
                            !is_null(embedding),
                            dist = vec_l2(embedding, $vec)
                            :sort dist
                            :limit 10
                        `;
                        const res = await this.db.run(vecQuery, JSON.stringify({ vec: queryVec }));
                        const parsed = ResponsePattern.match(res);
                        if (parsed.rows) vectorResults = parsed.rows.map(r => ({ id: r[0], content: r[1], dist: r[2], ts: r[3], source: 'semantic' }));
                        ui.log(`üß† Semantic: Found ${vectorResults.length} concept matches.`, "info");
                    } catch (e) {
                        ui.log(`‚ö†Ô∏è Vector Search failed: ${e.message}`, "warn");
                    }
                }

                // 2. GENERATE KEYWORD QUERY (Lexical)
                let keywordResults = [];
                const rawWords = userText.match(/[a-zA-Z0-9_\-]+/g) || [];
                const stopWords = new Set(['the', 'and', 'is', 'in', 'at', 'of', 'on', 'for', 'to', 'it', 'this', 'that', 'what', 'who', 'how', 'why', 'when', 'where', 'did', 'does', 'do', 'can', 'could', 'would', 'should', 'will', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'a', 'an', 'tell', 'me', 'about']);
                const keywords = rawWords.map(w => w.replace(/^[-_]+|[-_]+$/g, '')).filter(w => w.length > 2 && !stopWords.has(w.toLowerCase()));

                if (keywords.length > 0) {
                    const conditions = keywords.map(w => `regex_matches(content, '(?i)${w}')`).join(' or ');
                    const kwQuery = `?[id, content, timestamp] := *memory{id, content, timestamp}, ${conditions} :sort -timestamp :limit 10`;
                    try {
                        const res = await this.db.run(kwQuery, "{}");
                        const parsed = ResponsePattern.match(res);
                        if (parsed.rows) keywordResults = parsed.rows.map(r => ({ id: r[0], content: r[1], dist: 0, ts: r[2], source: 'lexical' }));
                        ui.log(`üìñ Lexical: Found ${keywordResults.length} keyword matches.`, "info");
                    } catch (e) { console.warn(e); }
                }

                // 3. MERGE & DEDUPLICATE
                // Priority: Vector (Concepts) + Lexical (Specifics)
                const combined = new Map();

                // Add Vector Results first
                vectorResults.forEach(item => combined.set(item.content, item));

                // Add Lexical (only if not already present)
                keywordResults.forEach(item => {
                    if (!combined.has(item.content)) combined.set(item.content, item);
                });

                const finalItems = Array.from(combined.values());

                if (finalItems.length === 0) return "";

                // 4. FORMAT OUTPUT (Smart Window)
                // Sort by relevance (Vector distance low is good, Lexical is 0 distance but prioritized by recency)
                // Let's just take Top 10 mixed.
                const topItems = finalItems.slice(0, 10);

                const paths = [];
                const clues = topItems.map((item, index) => {
                    const id = index + 1;
                    let title = "doc_" + id;
                    if (item.content.length > 0) {
                        const safeTitle = item.content.substring(0, 30).replace(/[^a-zA-Z0-9 ]/g, '').trim().replace(/\s+/g, '_').toLowerCase();
                        if (safeTitle.length > 3) title = safeTitle;
                    }
                    const path = `/knowledge/${item.source}/${title}`;
                    paths.push(path);

                    // Smart Truncate: If it's a vector match, we might want more context. 
                    // Current limit 300 chars to save tokens.
                    let snippet = item.content.substring(0, 300).replace(/\n/g, ' ');
                    if (item.content.length > 300) snippet += "...";

                    return `[CLUE #${id}] [Path: ${path}] [Source: ${item.source.toUpperCase()}]\nSnippet: "${snippet}"`;
                });

                ui.log(`‚úÖ Hybrid Retrieval: ${topItems.length} active clues.`, "success");
                const treeMap = `[CURRENT REALITY MAP]\n` + paths.map(p => `- ${p}`).join('\n');
                return `${treeMap}\n\n[CONTEXT CLUES]\n${clues.join('\n\n')}`;
            }

            buildVirtualPrompt(systemPrompt, retrievedMemories, chatHistory, userText) {
                const now = new Date().toLocaleString('en-US', { timeZoneName: 'short' });
                const physicsInstruction = `
### COGNITIVE PROTOCOL:
1. **TRUST THE CLUES:** You are operating in a "Sovereign" environment. The [CONTEXT CLUES] provided are the Ground Truth.
2. **MINIMIZE ENTROPY:** If a Clue contains the answer, OUTPUT IT DIRECTLY. Do not generate "filler".
3. **PATH OF LEAST ACTION:** The most efficient answer is the correct one.
`;
                return `### SYSTEM TIME\n${now}\n\n### BACKGROUND INFORMATION\n${retrievedMemories ? retrievedMemories : "No specific context available."} \n\n### CHAT HISTORY\n${chatHistory.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\n')}\n\n### INSTRUCTION\n${systemPrompt}\n${physicsInstruction}\nBased on the Background Information above, answer the following question.\n\n### USER QUESTION\n${userText}`;
            }

            async executeR1Loop(userText, history) {
                let iteration = 0;
                let currentContext = await this.retrieveInitialContext(userText);
                if (currentContext) ui.appendContext("Reflex Retrieval", currentContext);

                while (iteration < this.maxIterations) {
                    iteration++;
                    const fullPrompt = this.buildVirtualPrompt(
                        "You are Root Coda, a Sovereign Agent.\n" +
                        "PROTOCOL: Trust clues. Answer directly. No refusals.",
                        currentContext,
                        history.slice(-5),
                        userText
                    );

                    ui.log(`üß† Reasoning (Iter ${iteration})...`, "info");
                    let reply = "";
                    try {
                        const response = await this.engine.chat.completions.create({
                            messages: [{ role: "user", content: fullPrompt }],
                            temperature: 0.1,
                            max_tokens: 100
                        });
                        reply = response.choices[0].message.content.trim();
                    } catch (genErr) {
                        if (genErr.message.includes("disposed")) return { context: currentContext, finalAnswer: "‚ö†Ô∏è System Crash: GPU Driver lost." };
                        throw genErr;
                    }

                    if (!reply) reply = ""; // Safety guard

                    if (reply.includes("NEED_CONTEXT:")) {
                        const searchTerm = reply.split("NEED_CONTEXT:")[1].trim();
                        ui.log(`ü§ñ Requested search: "${searchTerm}"`, "warn");
                        const extraData = await this.retrieveInitialContext(searchTerm);
                        if (extraData) {
                            currentContext += `\n--- Additional (${searchTerm}) ---\n${extraData}`;
                            ui.appendContext(`Requested: ${searchTerm}`, extraData);
                        }
                        continue;
                    }
                    return { context: currentContext, finalAnswer: reply };
                }
                return { context: currentContext, finalAnswer: null };
            }
        }

        // --- HANDLERS ---
        async function handleSend() {
            const input = document.getElementById('input');
            const text = input.value.trim();

            // Get image from VisionController
            const imageBase64 = vision ? vision.getImage() : null;

            // Allow sending if there is text OR an image
            if ((!text && !imageBase64) || !engine) return;

            input.value = "";
            input.disabled = true;
            document.getElementById('send-btn').disabled = true;

            // Display Logic
            if (imageBase64) {
                ui.append("user", `![Uploaded Image](${imageBase64})\n\n${text}`);
            } else {
                ui.append("user", text);
            }

            if (state.autoSave) await saveTurn("user", text + (imageBase64 ? " [Image Attached]" : ""));

            try {
                // Construct Message Payload
                let messages = [];
                let context = "";

                // If Image: Bypass R1 Loop (Vision models typically don't do R1 reasoning yet or complex context mixing)
                // We use a direct shot for now.
                if (imageBase64) {
                     messages = [
                        { role: "system", content: "You are a helpful assistant. Analyze the user's image and text." },
                        {
                            role: "user",
                            content: [
                                { type: "text", text: text || "What is in this image?" },
                                { type: "image_url", image_url: { url: imageBase64 } }
                            ]
                        }
                    ];
                    ui.log("üëÅÔ∏è Processing Visual Data...", "warn");
                } else {
                    // Standard Text Path (R1 Loop)
                    const r1Result = await contextManager.executeR1Loop(text, []);
                    context = r1Result.context;
                    const finalAnswer = r1Result.finalAnswer;

                    if (finalAnswer && finalAnswer.includes("System Crash")) return ui.log("üõë Execution halted (GPU Crash).", "error");

                    ui.log("üß™ Synthesizing...", "warn");
                    const sysPrompt = `You are a helpful assistant with access to retrieved context. Use it to answer the user's question.\n\nCONTEXT:\n${context || "(No relevant context)"}`;
                    messages = [{ role: "system", content: sysPrompt }, { role: "user", content: text }];
                }

                const msgHandle = ui.append("assistant", "");

                const stream = await engine.chat.completions.create({
                    messages: messages,
                    max_tokens: 1024,
                    temperature: 0.7,
                    stream: true
                });

                let fullAnswer = "";
                for await (const chunk of stream) {
                    const delta = chunk.choices[0]?.delta?.content || "";
                    fullAnswer += delta;
                    msgHandle.update(fullAnswer);
                }

                // Cleanup - Clear the image from VisionController
                if (imageBase64 && vision) {
                    vision.clear();
                }

                if (state.autoSave) await saveTurn("assistant", fullAnswer);
                ui.log("‚úÖ Response generated.", "success");

            } catch (e) {
                ui.log(`Error: ${e.message}`, "error");
                ui.append("assistant", `**Error:** ${e.message}`);
            } finally {
                input.disabled = false;
                document.getElementById('send-btn').disabled = false;
                input.focus();
            }
        }

        async function loadModel() {
            const select = document.getElementById('model-select');
            const customInput = document.getElementById('custom-model-input');
            const modelInput = select.value === 'custom' ? customInput.value : select.value;
            if (!modelInput) return alert("Please select a model.");

            selectedModelId = modelInput.split('/').pop();
            document.getElementById('load-model-btn').disabled = true;

            try {
                ui.log(`Initializing Engine (${selectedModelId})...`, "info");

                // --- KERNEL: Hardware Config ---
                const hwProfile = document.getElementById('hw-profile').value;
                const gpuConfig = await getWebGPUConfig(hwProfile);

                if (gpuConfig.isConstrained) {
                    ui.log(`‚ö†Ô∏è Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, "warn");
                } else {
                    ui.log(`‚úÖ GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, "success");
                }

                // --- Config Generation ---
                // (Simplified Logic for cleaner file)
                const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/";
                let modelLib = null;
                const lowerId = selectedModelId.toLowerCase();
                let qTag = "q4f16_1"; // Default

                // Mapper
                // SOTA / New
                if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('deepseek-r1') && lowerId.includes('14b')) modelLib = libBase + `Qwen2.5-14B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('qwen2.5-14b')) modelLib = libBase + `Qwen2.5-14B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                
                // Vision Models
                else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;

                // Small / Efficient
                else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;
                
                // Fallbacks
                else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B

                let appConfig = {
                    useIndexedDBCache: true,
                    model_list: [{
                        model: "https://huggingface.co/" + modelInput + "/resolve/main/",
                        model_id: selectedModelId,
                        model_lib: modelLib,
                        vram_required_MB: 2000,
                        low_resource_required: true,
                        buffer_size_required_bytes: gpuConfig.maxBufferSize,
                    }]
                };

                // Create the Worker
                const worker = new Worker('./modules/llm-worker.js', { type: 'module' });

                // Initialize Engine via Worker
                engine = await CreateWebWorkerMLCEngine(
                    worker,
                    selectedModelId,
                    {
                        appConfig, // Passing the VRAM/Buffer limits we calculated
                        initProgressCallback: (rep) => ui.updateProgress(rep.progress, rep.text)
                    }
                );

                ui.log("üéâ Root Console Online", "success");
                contextManager = new ContextManager(engine, db);
                document.getElementById('input').disabled = false;
                document.getElementById('send-btn').disabled = false;
                document.getElementById('input').focus();

            } catch (e) {
                const errorMsg = e.message || String(e);
                ui.log(`Load Failed: ${errorMsg}`, "error");
                // Provide suggestion for common model name issues
                if (errorMsg && errorMsg.includes("Network response was not ok")) {
                    ui.log(`üí° Hint: Model may not exist or be temporarily unavailable. Try another model.`, "warn");
                }
                document.getElementById('load-model-btn').disabled = false;
            }
        }

        let vision = null; // Global VisionController instance

        async function init() {
            try {
                ui.log("üöÄ Root Kernel Starting...", "info");

                // 1. CozoDB
                await initCozo('./cozo_lib_wasm_bg.wasm');
                // Recovery Logic
                try {
                    const [keys] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });
                    if (keys.length > 0) {
                        db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        window.db = db;
                        ui.log("‚úÖ Root Graph Connected (Persistent)", "success");
                    } else {
                        db = CozoDb.new();
                        window.db = db;
                        ui.log("‚úÖ Root Graph Created (Memory)", "info");
                    }
                } catch (e) {
                    db = CozoDb.new();
                    window.db = db;
                    ui.log("‚ö†Ô∏è Fallback to Memory Graph", "warn");
                }

                // 2. Embedder (Optional)
                ui.updateProgress(0.3, "Loading Embedder...");
                const embedderPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', { device: 'wasm' });
                try {
                    embedder = await Promise.race([embedderPromise, new Promise((_, r) => setTimeout(() => r(new Error("Timeout")), 10000))]);
                    ui.log("‚úÖ Neural Embedder Ready", "success");
                } catch (e) { ui.log("‚ö†Ô∏è Embedder Skipped (Timeout)", "warn"); }

            } catch (e) {
                ui.log(`Init Error: ${e.message}`, "error");
            } finally {
                // Initialize VisionController
                vision = new VisionController();
                vision.setup('input-area', 'image-preview-container', 'input');

                // Ensure controls are unlocked even if init fails (partial functionality)
                ui.updateProgress(1.0, "Ready");

                // Fix: Copy Logs Button
                const copyBtn = document.getElementById('copy-logs-btn');
                if (copyBtn) {
                    copyBtn.onclick = () => {
                        const logs = document.getElementById('status-log').innerText;
                        navigator.clipboard.writeText(logs).then(() => ui.log("üìã Logs copied to clipboard", "success"));
                    };
                }

                document.getElementById('model-select').disabled = false;
                document.getElementById('load-model-btn').disabled = false;
                document.getElementById('load-model-btn').addEventListener('click', loadModel);

                // Input Handlers (idempotent)
                const sendBtn = document.getElementById('send-btn');
                const input = document.getElementById('input');

                if (sendBtn && input) {
                    sendBtn.onclick = handleSend; // Use property to avoid duplicates
                    input.onkeydown = (e) => {
                        if (e.key === 'Enter' && !e.shiftKey && !sendBtn.disabled) {
                            e.preventDefault();
                            handleSend();
                        }
                    };
                }
            }
        }

        // --- BRIDGE LOGIC ---
        let bridgeWs = null;
        window.toggleBridge = function (enabled) {
            if (enabled) {
                if (!engine) { alert("Load model first!"); document.getElementById('enable-bridge-toggle').checked = false; return; }
                bridgeWs = new WebSocket("ws://localhost:8080/ws/chat");
                bridgeWs.onopen = () => { document.getElementById('bridge-status').innerText = "üü¢ Connected"; ui.log("Bridge Online", "success"); };
                bridgeWs.onmessage = async (e) => {
                    const msg = JSON.parse(e.data);
                    if (msg.type === 'chat') {
                        ui.log(`Bridge Request: ${msg.id}`, "info");
                        const completion = await engine.chat.completions.create({ messages: msg.data.messages, stream: true });
                        for await (const chunk of completion) bridgeWs.send(JSON.stringify({ id: msg.id, chunk }));
                        bridgeWs.send(JSON.stringify({ id: msg.id, done: true }));
                    }
                };
            } else if (bridgeWs) { bridgeWs.close(); bridgeWs = null; document.getElementById('bridge-status').innerText = "Disconnected"; }
        };

        window.addEventListener('load', init);
    </script>
</body>

</html>
--- END OF FILE: temp_file.html ---

--- START OF FILE: backend\README.md ---
# Backend Directory

Contains server-side logs and legacy Python backend configurations (minimized in favor of the browser-native tools).
--- END OF FILE: backend\README.md ---

--- START OF FILE: extension\background.js ---
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'queryMemories') {
    queryMemoriesFromCozoDB(request.query)
      .then(memories => ({
        success: true,
        memories: memories,
        summary: generateSummary(memories)
      }))
      .then(sendResponse)
      .catch(err => ({
        success: false,
        error: err.message
      }));
    return true; // Keep channel open for async response
  }
});

async function queryMemoriesFromCozoDB(userInput) {
  try {
    // Attempt to hit the Local Bridge (webgpu_bridge.py)
    // Note: The bridge currently supports /v1/chat/completions.
    // Ideally, we add a specific /memories/search endpoint to the python bridge
    // or use a specialized prompt to the LLM to search.
    // For now, we simulate the /memories/search call as defined in the spec.
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 10000); // 10 second timeout

    const response = await fetch('http://localhost:8080/memories/search', {
      method: 'POST',
      headers: { 
        'Content-Type': 'application/json',
        'Authorization': 'Bearer sovereign-secret'
      },
      body: JSON.stringify({ query: userInput }),
      signal: controller.signal
    });

    clearTimeout(timeoutId);

    if (!response.ok) throw new Error(`Backend connection failed with status ${response.status}`);
    return response.json();
  } catch (e) {
    console.warn('[Sovereign] Backend unavailable, querying local store (simulation)...', e.message);
    // In the future, this can connect directly to IndexedDB via shared worker
    // For now, return simulated data
    return [
      {
        content: "This is a simulated memory based on your input: " + userInput.substring(0, 100) + "...",
        timestamp: new Date().toISOString(),
        relevance: 0.8
      }
    ];
  }
}

function generateSummary(memories) {
  if (!memories || memories.length === 0) return null;
  
  const maxMemories = 3;
  const relevant = memories.slice(0, maxMemories);
  
  return relevant
    .map((m, idx) => `[Memory ${idx + 1}] ${m.content.substring(0, 150)}...`)
    .join('\n');
}
--- END OF FILE: extension\background.js ---

--- START OF FILE: extension\content.js ---
// Platform-specific DOM selectors
const SELECTORS = {
    'gemini.google.com': 'div[contenteditable="true"], textarea',
    'chatgpt.openai.com': 'textarea, div[contenteditable="true"]'
};

let textArea = null;
let inputTimeout = null;
const PAUSE_THRESHOLD = 3000; // 3 seconds

// 1. Detect the active text input
function detectTextArea() {
    const domain = window.location.hostname;
    const selector = SELECTORS[domain];
    if (!selector) return null;
    return document.querySelector(selector);
}

// 2. Extract text from the input
function getVisibleText() {
    if (!textArea) return "";
    return textArea.value || textArea.textContent || "";
}

// 3. Monitor for user pauses
function setupPauseDetector() {
    if (!textArea) return;

    textArea.addEventListener('input', () => {
        clearTimeout(inputTimeout);
        inputTimeout = setTimeout(() => {
            const text = getVisibleText();
            if (text.length > 10) { // Only query if meaningful text exists
                console.log('[Sovereign] 3-second pause detected, querying memories...');
                chrome.runtime.sendMessage(
                    { action: 'queryMemories', query: text },
                    (response) => {
                        if (response && response.success) injectContext(response);
                    }
                );
            }
        }, PAUSE_THRESHOLD);
    });
}

// 4. Inject the retrieved context
function injectContext(contextData) {
    if (!contextData.summary) return;

    const timestamp = new Date().toLocaleTimeString();
    const summary = `\n\n[Sovereign Context Injection at ${timestamp}]\n${contextData.summary}\n---\n`;

    // For contenteditable (Gemini/modern apps)
    if (textArea.isContentEditable || textArea.getAttribute('contenteditable') === 'true') {
        // Simple append - in production this might need Range/Selection manipulation for cursors
        textArea.textContent = textArea.textContent + summary;
    }
    // For standard textarea (ChatGPT legacy)
    else {
        textArea.value += summary;
    }

    // Notify user
    displayIndicator('\u2713 Context injected', 'success'); // Using Unicode checkmark
}

// 5. UI Feedback
function displayIndicator(message, type) {
    let indicator = document.getElementById('sovereign-indicator');
    if (!indicator) {
        indicator = document.createElement('div');
        indicator.id = 'sovereign-indicator';
        indicator.style.position = 'fixed';
        indicator.style.bottom = '20px';
        indicator.style.right = '20px';
        indicator.style.padding = '10px 15px';
        indicator.style.borderRadius = '5px';
        indicator.style.zIndex = '9999';
        indicator.style.fontFamily = 'monospace';
        document.body.appendChild(indicator);
    }

    indicator.textContent = message;
    indicator.style.background = type === 'success' ? '#238636' : '#da3633';
    indicator.style.color = '#ffffff';

    setTimeout(() => indicator.remove(), 5000);
}

// Handle messages from popup
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
    if (request.action === 'testInjection') {
        // For testing purposes, inject a sample context
        const testData = {
            summary: "This is a test injection from Sovereign Context Bridge.\n\n[Sample Memory] Example context for testing purposes..."
        };
        injectContext(testData);
        sendResponse({ success: true });
        return true; // Keep channel open for async response
    }
});

// Initialization Loop
const observer = new MutationObserver(() => {
    if (!textArea) {
        textArea = detectTextArea();
        if (textArea) {
            console.log("[Sovereign] Text Area Detected.");
            setupPauseDetector();
        }
    }
});

observer.observe(document.body, { childList: true, subtree: true });
--- END OF FILE: extension\content.js ---

--- START OF FILE: extension\manifest.json ---
manifest_version: 3
name: Sovereign Context Bridge
version: 1.0.0
description: Silent context injection for LLM conversations
permissions:
  - activeTab
  - scripting
  - storage
  - webRequest
host_permissions:
  - *://gemini.google.com/*
  - *://chatgpt.openai.com/*
content_scripts:
  -
    matches:
      - *://gemini.google.com/*
      - *://chatgpt.openai.com/*
    js:
      - content.js
    run_at: document_start
background:
  service_worker: background.js
action:
  default_popup: popup.html
  default_title: Sovereign Context Bridge
icons:
  16: images/icon-16.png
  32: images/icon-32.png
  128: images/icon-128.png
--- END OF FILE: extension\manifest.json ---

--- START OF FILE: extension\popup.html ---
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <style>
    body { background: #0f1115; color: #e2e8f0; font-family: sans-serif; width: 250px; padding: 15px; }
    h3 { margin-top: 0; color: #00ff88; font-weight: 300; border-bottom: 1px solid #333; padding-bottom: 10px; }
    .stat-row { display: flex; justify-content: space-between; margin-bottom: 8px; font-size: 0.9rem; }
    .val { font-weight: bold; color: #58a6ff; }
    button { width: 100%; padding: 8px; margin-top: 10px; background: #2d2d2d; border: 1px solid #444; color: #fff; cursor: pointer; border-radius: 4px; }
    button:hover { background: #333; border-color: #00ff88; }
    .status-ok { color: #00ff88; }
    .status-err { color: #ff4444; }
  </style>
</head>
<body>
  <h3>&#129300 Sovereign Bridge</h3>

  <div class="stat-row">
    <span>Status:</span>
    <span id="status-badge" class="status-err">&#9679; Offline</span>
  </div>

  <div class="stat-row">
    <span>Memories:</span>
    <span id="mem-count" class="val">0</span>
  </div>

  <div class="stat-row">
    <span>Last Inject:</span>
    <span id="last-inject" class="val">-</span>
  </div>

  <button id="settings-btn">&#9881;&#65039 Settings</button>
  <button id="test-inject-btn">&#129512 Test Injection</button>

  <script src="popup.js"></script>
</body>
</html>
--- END OF FILE: extension\popup.html ---

--- START OF FILE: extension\popup.js ---
document.addEventListener('DOMContentLoaded', async () => {
    const statusBadge = document.getElementById('status-badge');

    // Check connection to Local Bridge
    try {
        // Using a more robust approach to handle CORS issues
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout

        const res = await fetch('http://localhost:8080/health', {
            signal: controller.signal,
            mode: 'cors', // Explicitly set CORS mode
            credentials: 'omit' // Don't send credentials
        });

        clearTimeout(timeoutId);

        if (res.ok) {
            statusBadge.textContent = "‚óè Online";
            statusBadge.className = "status-ok";
        } else {
            statusBadge.textContent = "‚óè Offline";
            statusBadge.className = "status-err";
        }
    } catch (e) {
        // Handle network errors, CORS errors, and timeouts
        console.warn('[Sovereign] Backend connection failed:', e.message);
        statusBadge.textContent = "‚óè Offline";
        statusBadge.className = "status-err";
    }

    document.getElementById('test-inject-btn').addEventListener('click', () => {
        // Trigger manual test injection
        chrome.tabs.query({active: true, currentWindow: true}, (tabs) => {
            chrome.tabs.sendMessage(tabs[0].id, { action: 'testInjection' }, (response) => {
                if (chrome.runtime.lastError) {
                    console.log('[Sovereign] Test injection not available on this page');
                } else {
                    console.log('[Sovereign] Test injection triggered');
                }
            });
        });
    });

    // Add settings button functionality
    document.getElementById('settings-btn').addEventListener('click', () => {
        // For now, just show a message - in the future this could open options page
        alert('Sovereign Context Bridge Settings\n\nConfigure extension preferences here.');
    });
});
--- END OF FILE: extension\popup.js ---

--- START OF FILE: extension\images\README.md ---
# Extension Icons

This directory contains the following icon files:

- `icon-16.png` - 16x16 pixel icon (minimal valid PNG)
- `icon-32.png` - 32x32 pixel icon (minimal valid PNG)
- `icon-128.png` - 128x128 pixel icon (minimal valid PNG)

These icons represent the Sovereign Context Bridge extension.

Note: The current files are minimal valid PNGs for extension loading. Replace with actual designed icons for production use.
--- END OF FILE: extension\images\README.md ---

--- START OF FILE: scripts\gpu_manager.py ---
#!/usr/bin/env python3
"""
GPU Resource Manager for ECE_Core
Provides utilities to monitor and manage GPU locks in the WebGPU bridge
"""

import requests
import json
import time
import argparse
from typing import Dict, Any

class GPUResourceManager:
    def __init__(self, bridge_url: str = "http://localhost:8080"):
        self.bridge_url = bridge_url
        self.headers = {"Authorization": "Bearer sovereign-secret"}
    
    def get_status(self) -> Dict[str, Any]:
        """Get current GPU status"""
        try:
            response = requests.get(f"{self.bridge_url}/v1/gpu/status", headers=self.headers)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"Error getting status: {response.status_code} - {response.text}")
                return {}
        except Exception as e:
            print(f"Error connecting to bridge: {e}")
            return {}
    
    def reset_lock(self) -> bool:
        """Reset the current GPU lock"""
        try:
            response = requests.post(f"{self.bridge_url}/v1/gpu/reset", headers=self.headers)
            if response.status_code == 200:
                print("‚úÖ GPU lock reset successfully")
                return True
            else:
                print(f"‚ùå Failed to reset GPU lock: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            print(f"‚ùå Error resetting GPU lock: {e}")
            return False
    
    def force_release_all(self) -> bool:
        """Force release all GPU locks (emergency)"""
        try:
            response = requests.post(f"{self.bridge_url}/v1/gpu/force-release-all", headers=self.headers)
            if response.status_code == 200:
                print("‚úÖ All GPU locks force released successfully")
                return True
            else:
                print(f"‚ùå Failed to force release GPU locks: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            print(f"‚ùå Error force releasing GPU locks: {e}")
            return False
    
    def monitor(self, interval: int = 5):
        """Monitor GPU status continuously"""
        print(f"üìä Monitoring GPU status every {interval}s (Ctrl+C to stop)")
        try:
            while True:
                status = self.get_status()
                if status:
                    locked = status.get('locked', False)
                    owner = status.get('owner', 'None')
                    queue_depth = status.get('queue_depth', 0)
                    queued = status.get('queued', [])
                    
                    status_str = f"GPU: {'LOCKED' if locked else 'FREE'}"
                    if locked:
                        status_str += f" by {owner}"
                    if queue_depth > 0:
                        status_str += f" | Queue: {queue_depth} | Queued: {', '.join(queued) if queued else 'None'}"
                    
                    print(f"[{time.strftime('%H:%M:%S')}] {status_str}")
                else:
                    print(f"[{time.strftime('%H:%M:%S')}] ‚ùå Unable to get GPU status")
                
                time.sleep(interval)
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Monitoring stopped")

def main():
    parser = argparse.ArgumentParser(description="GPU Resource Manager for ECE_Core")
    parser.add_argument("--bridge-url", default="http://localhost:8080", 
                       help="WebGPU bridge URL (default: http://localhost:8080)")
    parser.add_argument("--status", action="store_true", help="Get current GPU status")
    parser.add_argument("--reset", action="store_true", help="Reset GPU lock")
    parser.add_argument("--force-release", action="store_true", help="Force release all GPU locks")
    parser.add_argument("--monitor", action="store_true", help="Monitor GPU status continuously")
    parser.add_argument("--interval", type=int, default=5, help="Monitor interval in seconds (default: 5)")
    
    args = parser.parse_args()
    
    manager = GPUResourceManager(args.bridge_url)
    
    if args.status:
        status = manager.get_status()
        if status:
            print(json.dumps(status, indent=2))
        else:
            print("‚ùå Failed to get status")
    
    elif args.reset:
        manager.reset_lock()
    
    elif args.force_release:
        manager.force_release_all()
    
    elif args.monitor:
        manager.monitor(args.interval)
    
    else:
        # Default: show status
        status = manager.get_status()
        if status:
            locked = status.get('locked', False)
            owner = status.get('owner', 'None')
            queue_depth = status.get('queue_depth', 0)
            queued = status.get('queued', [])
            
            print(f"GPU Status: {'LOCKED' if locked else 'FREE'}", end="")
            if locked:
                print(f" by {owner}", end="")
            print(f" | Queue: {queue_depth} items")
            
            if queued:
                print(f"Queued: {', '.join(queued)}")
        else:
            print("‚ùå Failed to get status")

if __name__ == "__main__":
    main()
--- END OF FILE: scripts\gpu_manager.py ---

--- START OF FILE: scripts\hot_reload_gpu.py ---
#!/usr/bin/env python3
"""
Hot Reload System for GPU Management
Allows reloading of GPU management logic without restarting the bridge
"""

import os
import sys
import time
import threading
import requests
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import importlib
from pathlib import Path

class GPUHotReloadHandler(FileSystemEventHandler):
    def __init__(self, bridge_process, gpu_manager):
        self.bridge_process = bridge_process
        self.gpu_manager = gpu_manager
        self.last_reload = time.time()
        self.reload_cooldown = 2  # seconds between reloads
    
    def on_modified(self, event):
        if event.is_directory:
            return
            
        # Check if it's a GPU-related file
        gpu_files = ['webgpu_bridge.py', 'sovereign.js']
        if any(gpu_file in event.src_path for gpu_file in gpu_files):
            current_time = time.time()
            if current_time - self.last_reload < self.reload_cooldown:
                return  # Skip if too soon since last reload
            
            print(f"üîÑ Detected change in {event.src_path}, reloading GPU management...")
            self.last_reload = current_time
            self.reload_gpu_logic()
    
    def reload_gpu_logic(self):
        """Reload GPU management logic"""
        try:
            # Try to reload the bridge module if possible
            print("üîÑ Reloading GPU management logic...")
            
            # Force release all locks to prevent stale state
            try:
                response = requests.post(
                    "http://localhost:8080/v1/gpu/force-release-all",
                    headers={"Authorization": "Bearer sovereign-secret"},
                    timeout=5
                )
                if response.status_code == 200:
                    print("‚úÖ GPU locks force released during reload")
                else:
                    print(f"‚ö†Ô∏è  Could not release locks: {response.status_code}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Bridge not running, cannot release locks: {e}")
            
            # In a real implementation, we would reload the bridge module
            # For now, we'll just log the reload attempt
            print("‚úÖ GPU management logic reloaded")
            
        except Exception as e:
            print(f"‚ùå Error during reload: {e}")

def start_hot_reload_monitor():
    """Start monitoring for GPU-related file changes"""
    print("üîÑ Starting GPU hot reload monitor...")
    
    # Watch for changes in the tools directory
    watch_path = Path("C:/Users/rsbii/Projects/ECE_Core/tools")
    
    if not watch_path.exists():
        print(f"‚ùå Watch path does not exist: {watch_path}")
        return
    
    event_handler = GPUHotReloadHandler(None, None)
    observer = Observer()
    observer.schedule(event_handler, str(watch_path), recursive=True)
    observer.start()
    
    print(f"‚úÖ Hot reload monitor started, watching: {watch_path}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüõë Stopping hot reload monitor...")
        observer.stop()
    observer.join()

if __name__ == "__main__":
    start_hot_reload_monitor()
--- END OF FILE: scripts\hot_reload_gpu.py ---

--- START OF FILE: scripts\README.md ---
# Scripts Directory

Contains utility scripts for continuous integration and local environment setup.
--- END OF FILE: scripts\README.md ---

--- START OF FILE: scripts\smart_gpu_bridge.py ---
#!/usr/bin/env python3
"""
Smart GPU Bridge with Hot Reload Capability
This script runs the WebGPU bridge with automatic reloading when files change
"""

import asyncio
import uvicorn
import json
import uuid
import os
import time
import threading
from collections import deque
from fastapi import FastAPI, WebSocket, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any
import importlib
import sys
from pathlib import Path
import hashlib

# Import the PriorityGPUManager from the main bridge file
from tools.webgpu_bridge import PriorityGPUManager, gpu_lock, log

# Global variable to track file hashes for hot reload
file_hashes = {}
reload_lock = threading.Lock()

def get_file_hash(filepath):
    """Calculate MD5 hash of a file"""
    try:
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except:
        return None

def check_for_changes():
    """Check if any GPU-related files have changed"""
    gpu_files = [
        "tools/webgpu_bridge.py",
        "tools/modules/sovereign.js",
        "tools/model-server-chat.html",
        "tools/root-mic.html",
        "tools/root-dreamer.html"
    ]
    
    changed = False
    for file_path in gpu_files:
        full_path = Path(file_path)
        if full_path.exists():
            current_hash = get_file_hash(full_path)
            if file_path not in file_hashes:
                file_hashes[file_path] = current_hash
            elif file_hashes[file_path] != current_hash:
                print(f"üîÑ Detected change in {file_path}")
                file_hashes[file_path] = current_hash
                changed = True
    
    return changed

def hot_reload_bridge():
    """Perform hot reload of bridge logic"""
    with reload_lock:
        print("üîÑ Hot reloading bridge logic...")
        
        # Force release all GPU locks to prevent stale state
        gpu_lock.force_release_all()
        
        # Reload the module if possible
        try:
            importlib.reload(sys.modules['tools.webgpu_bridge'])
            print("‚úÖ Bridge logic reloaded")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not reload module: {e}")
            print("üí° Changes will take effect on next restart")

class FileWatcherThread(threading.Thread):
    """Thread to watch for file changes and trigger hot reloads"""
    def __init__(self):
        super().__init__()
        self.daemon = True
        self.running = True
    
    def run(self):
        while self.running:
            if check_for_changes():
                hot_reload_bridge()
            time.sleep(2)  # Check every 2 seconds

def create_app():
    """Create and configure the FastAPI application"""
    app = FastAPI(title="WebGPU Bridge - Hot Reload Enabled")

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Store active WebSocket connections
    workers: Dict[str, WebSocket] = {
        "chat": None,
        "embed": None
    }

    # Store pending response futures
    active_requests: Dict[str, asyncio.Queue] = {}

    # --- GPU Priority Manager (The Traffic Cop) ---
    # Using the imported gpu_lock from webgpu_bridge.py

    @app.post("/v1/gpu/lock")
    async def acquire_gpu_lock(request: Request):
        body = await request.json()
        requester = body.get("id", "unknown")
        # Use the imported gpu_lock
        success, token = await gpu_lock.acquire(requester, timeout=120.0)  # 2-minute timeout

        if success:
            return {"status": "acquired", "token": token}
        else:
            return JSONResponse(
                status_code=503,
                content={"status": "timeout", "msg": "GPU Queue Timeout"}
            )

    @app.post("/v1/gpu/unlock")
    async def release_gpu_lock(request: Request):
        body = await request.json()
        requester = body.get("id", "unknown")
        gpu_lock.release(requester)
        return {"status": "released"}

    @app.post("/v1/gpu/reset")
    async def reset_gpu_lock():
        gpu_lock.release("admin", force=True)
        return {"status": "reset"}

    @app.post("/v1/gpu/force-release-all")
    async def force_release_all_gpu_locks():
        """Emergency endpoint to clear all GPU locks and queues"""
        gpu_lock.force_release_all()
        return {"status": "all_gpu_locks_force_released"}

    @app.get("/v1/gpu/status")
    async def gpu_status():
        return gpu_lock.get_status()

    # Add the hot reload endpoint
    @app.post("/v1/hot-reload")
    async def hot_reload_endpoint(request: Request):
        """Endpoint to manually trigger hot reload"""
        hot_reload_bridge()
        return {"status": "hot_reload_triggered"}

    # --- Logging / observability ---
    _LOG_MAX_LINES = int(os.getenv("BRIDGE_LOG_MAX_LINES", "5000"))
    _LOG_MAX_CHARS_PER_LINE = int(os.getenv("BRIDGE_LOG_MAX_CHARS_PER_LINE", "400"))
    _LOG_STREAM_DELTAS = os.getenv("BRIDGE_LOG_STREAM_DELTAS", "true").strip().lower() in ("1", "true", "yes", "on")

    _bridge_logs: deque[tuple[int, str]] = deque(maxlen=_LOG_MAX_LINES)
    _bridge_log_seq: int = 0

    def _clip(s: str, max_chars: int) -> str:
        if s is None:
            return ""
        s = str(s)
        if len(s) <= max_chars:
            return s
        return s[: max_chars - 1] + "‚Ä¶"

    @app.get("/logs")
    async def get_logs(limit: int = 200, since: int = 0):
        items = list(_bridge_logs)
        if since and since > 0:
            items = [it for it in items if it[0] > since]
        if limit and limit > 0:
            items = items[-limit:]
        last_seq = items[-1][0] if items else _bridge_log_seq
        return {
            "logs": [{"seq": seq, "line": line} for seq, line in items],
            "last_seq": last_seq,
        }

    @app.post("/logs/clear")
    async def clear_logs():
        _bridge_logs.clear()
        return {"ok": True}

    @app.get("/v1/models")
    async def list_models():
        models = []
        if workers["chat"]:
            models.append({
                "id": "webgpu-chat",
                "object": "model",
                "created": 1677610602,
                "owned_by": "webgpu-bridge"
            })
        if workers["embed"]:
            models.append({
                "id": "webgpu-embedding",
                "object": "model",
                "created": 1677610602,
                "owned_by": "webgpu-bridge"
            })
        return {"object": "list", "data": models}

    # --- Compatibility endpoint: audit/server-logs ---
    @app.get('/audit/server-logs')
    async def get_audit_server_logs(limit: int = 50):
        try:
            log_file = Path("logs/server.log")
            if not log_file.exists():
                log_file = Path("../logs/server.log")

            if log_file.exists():
                with log_file.open('r', encoding='utf-8', errors='ignore') as f:
                    lines = f.read().splitlines()
                tail = lines[-int(limit):] if limit and len(lines) > 0 else lines
                return {"logs": tail, "count": len(tail)}

            items = list(_bridge_logs)[-int(limit):]
            tail = [line for (_seq, line) in items]
            return {"logs": tail, "count": len(tail)}
        except Exception as e:
            return {"logs": [f"Bridge audit logs error: {str(e)}"], "count": 0}

    @app.websocket("/ws/{worker_type}")
    async def websocket_endpoint(websocket: WebSocket, worker_type: str):
        if worker_type not in workers:
            await websocket.close(code=4000)
            return

        await websocket.accept()
        workers[worker_type] = websocket
        log(f"‚úÖ {worker_type.upper()} Worker Connected")

        try:
            while True:
                data = await websocket.receive_text()
                message = json.loads(data)

                if "usage" in message:
                    log(f"üìä Token Usage: {json.dumps(message['usage'])}")
                elif "chunk" in message and "usage" in message["chunk"]:
                     log(f"üìä Token Usage (Chunk): {json.dumps(message['chunk']['usage'])}")

                req_id = message.get("id")

                if req_id and isinstance(message, dict):
                    if _LOG_STREAM_DELTAS and "chunk" in message:
                        try:
                            ch = message.get("chunk")
                            if isinstance(ch, dict) and isinstance(ch.get("choices"), list) and ch["choices"]:
                                choice0 = ch["choices"][0] if isinstance(ch["choices"][0], dict) else {}
                                delta = choice0.get("delta") if isinstance(choice0, dict) else None
                                if isinstance(delta, dict) and "content" in delta:
                                    piece = delta.get("content") or ""
                                    if piece:
                                        log(f"Œî {req_id}: {_clip(piece, _LOG_MAX_CHARS_PER_LINE)}")
                        except Exception:
                            pass

                    if "token_events" in message and isinstance(message.get("token_events"), list):
                        try:
                            events = message.get("token_events")
                            for ev in events:
                                tidx = ev.get("idx")
                                ttext = _clip(ev.get("text", ""), _LOG_MAX_CHARS_PER_LINE)
                                tdt = float(ev.get("dt_ms") or ev.get("dt") or 0.0)
                                tlogp = ev.get("logprob")
                                log(f"TOK {req_id} IDX={tidx} DTms={tdt:.2f} TOK=" + ttext + (" LOGP=" + str(tlogp) if tlogp is not None else ""))
                        except Exception:
                            pass
                if req_id in active_requests:
                    await active_requests[req_id].put(message)

        except Exception as e:
            log(f"‚ùå {worker_type.upper()} Worker Disconnected: {e}")
        finally:
            workers[worker_type] = None

    async def stream_generator(req_id: str):
        queue = active_requests[req_id]
        try:
            while True:
                message = await queue.get()

                if message.get("error"):
                    yield f"data: {json.dumps({'error': message['error']})}\n\n"
                    break

                if message.get("done"):
                    yield "data: [DONE]\n\n"
                    break

                if "chunk" in message:
                    yield f"data: {json.dumps(message['chunk'])}\n\n"

        finally:
            if req_id in active_requests:
                del active_requests[req_id]

    async def collect_full_response(req_id: str):
        queue = active_requests[req_id]

        accumulated_content = ""
        first_chunk = None
        finish_reason = None

        try:
            while True:
                message = await queue.get()

                if message.get("error"):
                    raise HTTPException(status_code=500, detail=message['error'])

                if message.get("done"):
                    break

                if "chunk" in message:
                    chunk = message["chunk"]

                    if "choices" in chunk and "message" in chunk["choices"][0]:
                        return chunk

                    if not first_chunk:
                        first_chunk = chunk

                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0].get("delta", {})
                        content = delta.get("content", "")
                        if content:
                            accumulated_content += content

                        if chunk["choices"][0].get("finish_reason"):
                            finish_reason = chunk["choices"][0]["finish_reason"]

        finally:
            if req_id in active_requests:
                del active_requests[req_id]

        if not first_chunk:
            raise HTTPException(status_code=500, detail="No response received from WebGPU worker")

        final_response = first_chunk.copy()
        final_response["object"] = "chat.completion"
        final_response["choices"] = [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": accumulated_content
            },
            "finish_reason": finish_reason or "stop"
        }]

        return final_response

    @app.post("/v1/chat/completions")
    @app.post("/chat/completions")
    async def chat_completions(request: Request):
        if not workers["chat"]:
            raise HTTPException(status_code=503, detail="WebGPU Chat Worker not connected. Open tools/webgpu-server-chat.html")

        body = await request.json()
        req_id = str(uuid.uuid4())
        active_requests[req_id] = asyncio.Queue()

        stream = body.get("stream", False)
        log(f"Chat Request: {req_id} - Model: {body.get('model')} - Stream: {stream}")

        await workers["chat"].send_json({
            "id": req_id,
            "type": "chat",
            "data": body
        })

        if stream:
            return StreamingResponse(stream_generator(req_id), media_type="text/event-stream")
        else:
            response_data = await collect_full_response(req_id)
            return JSONResponse(content=response_data)

    @app.post("/v1/embeddings")
    async def embeddings(request: Request):
        if not workers["embed"]:
            raise HTTPException(status_code=503, detail="WebGPU Embed Worker not connected. Open tools/webgpu-server-embed.html")

        body = await request.json()
        req_id = str(uuid.uuid4())
        active_requests[req_id] = asyncio.Queue()

        log(f"Embed Request: {req_id} - Input length: {len(str(body.get('input')))}")

        await workers["embed"].send_json({
            "id": req_id,
            "type": "embedding",
            "data": body
        })

        response_msg = await active_requests[req_id].get()
        del active_requests[req_id]

        if response_msg.get("error"):
            raise HTTPException(status_code=500, detail=response_msg["error"])

        return JSONResponse(content=response_msg["result"])

    # --- Authentication & Config ---
    import secrets
    AUTH_TOKEN = os.getenv("BRIDGE_TOKEN")
    if not AUTH_TOKEN:
        AUTH_TOKEN = secrets.token_urlsafe(16)

    @app.middleware("http")
    async def verify_token(request: Request, call_next):
        if request.method == "OPTIONS" or request.url.path in ["/mobile", "/favicon.ico", "/logs", "/health", "/audit/server-logs"]:
            return await call_next(request)

        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer ") or auth_header.split(" ")[1] != AUTH_TOKEN:
            return JSONResponse(status_code=401, content={"error": "Unauthorized. Invalid Token."})

        return await call_next(request)

    @app.get("/health")
    async def health_check():
        return {"status": "ok", "service": "webgpu-bridge"}

    from fastapi.responses import HTMLResponse

    @app.get("/mobile")
    async def serve_mobile_app():
        file_path = "mobile-chat.html"
        if not os.path.exists(file_path):
            file_path = "tools/mobile-chat.html"

        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return HTMLResponse(content=f.read())
        else:
            return HTMLResponse(content="<h1>Mobile App Not Found</h1><p>Ensure mobile-chat.html exists.</p>", status_code=404)

    @app.post("/memories/search")
    async def search_memories(request: Request):
        if not workers["chat"]:
            raise HTTPException(status_code=503, detail="WebGPU Chat Worker not connected. Open tools/model-server-chat.html")

        try:
            body = await request.json()
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid JSON")

        query = body.get("query", "").trim()
        if not query:
            return JSONResponse(content=[])

        req_id = str(uuid.uuid4())
        active_requests[req_id] = asyncio.Queue()

        log(f"üîé Bridge Memory Search: {req_id} - '{_clip(query, 50)}'")

        try:
            await workers["chat"].send_json({
                "id": req_id,
                "type": "memory_query",
                "data": {"query": query}
            })

            response_msg = await asyncio.wait_for(active_requests[req_id].get(), timeout=3.0)

            if response_msg.get("error"):
                log(f"‚ùå Search Error {req_id}: {response_msg['error']}")
                raise HTTPException(status_code=500, detail=response_msg["error"])

            results = response_msg.get("result", [])
            log(f"‚úÖ Served {len(results)} memories to Extension")
            return JSONResponse(content=results)

        except asyncio.TimeoutError:
            log(f"‚è∞ Search Timeout {req_id} - Browser didn't respond in 3s")
            del active_requests[req_id]
            raise HTTPException(status_code=504, detail="Query timed out")
        except Exception as e:
            if req_id in active_requests:
                del active_requests[req_id]
            raise HTTPException(status_code=500, detail=str(e))

    return app

def main():
    """Main function to start the bridge with hot reload"""
    print("üîÑ Starting WebGPU Bridge with Hot Reload...")
    
    # Start the file watcher thread
    watcher = FileWatcherThread()
    watcher.start()
    
    # Create and run the app
    app = create_app()
    
    host = os.getenv("BRIDGE_HOST", "0.0.0.0")
    default_port = os.getenv("BRIDGE_PORT")
    if default_port:
        port = int(default_port)
    else:
        import random
        port = random.randint(9000, 9999)

    print(f"\nüöÄ WebGPU Bridge with Hot Reload Starting")
    print(f"   Host: {host}")
    print(f"   Port: {port}")
    print(f"   üîë Token: {os.getenv('BRIDGE_TOKEN', 'sovereign-secret')}")
    print(f"   üîÑ Hot Reload: ENABLED")
    print("="*60 + "\n")

    try:
        uvicorn.run(app, host=host, port=port, log_level="info")
    except KeyboardInterrupt:
        print("\nüõë Shutting down bridge...")
        watcher.running = False
        watcher.join()

if __name__ == "__main__":
    main()
--- END OF FILE: scripts\smart_gpu_bridge.py ---

--- START OF FILE: scripts\test_gpu_fixes.py ---
#!/usr/bin/env python3
"""
Test script to verify GPU resource management fixes including model loading serialization
"""

import time
import requests
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

def test_gpu_status():
    """Test GPU status endpoint"""
    try:
        response = requests.get("http://localhost:8080/v1/gpu/status", 
                              headers={"Authorization": "Bearer sovereign-secret"})
        if response.status_code == 200:
            status = response.json()
            print(f"‚úÖ GPU Status: {status}")
            return True
        else:
            print(f"‚ùå GPU Status request failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Error getting GPU status: {e}")
        return False

def test_lock_acquisition(agent_id: str, timeout: int = 120):  # Increased to 120s
    """Test GPU lock acquisition"""
    try:
        print(f"‚è≥ Agent {agent_id} requesting GPU lock...")
        start_time = time.time()
        
        response = requests.post("http://localhost:8080/v1/gpu/lock",
                                headers={"Authorization": "Bearer sovereign-secret"},
                                json={"id": agent_id},
                                timeout=timeout)
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Agent {agent_id} acquired lock in {elapsed:.2f}s: {result.get('token', 'no-token')}")
            
            # Release the lock
            release_response = requests.post("http://localhost:8080/v1/gpu/unlock",
                                          headers={"Authorization": "Bearer sovereign-secret"},
                                          json={"id": agent_id})
            if release_response.status_code == 200:
                print(f"‚úÖ Agent {agent_id} released lock")
            else:
                print(f"‚ö†Ô∏è  Agent {agent_id} failed to release lock: {release_response.status_code}")
            
            return True
        else:
            print(f"‚ùå Agent {agent_id} failed to acquire lock: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå Agent {agent_id} error: {e}")
        return False

def test_concurrent_access():
    """Test concurrent GPU access with different priority agents"""
    print("\nüß™ Testing concurrent GPU access...")
    
    agents = [
        ("Root-Mic", 5),  # High priority
        ("Root-Console-Init", 10),  # Medium priority
        ("Dreamer-Init", 15),  # Lower priority
        ("Test-Agent-4", 20),  # Even lower priority
    ]
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for agent_id, delay in agents:
            # Add small delay to ensure proper ordering
            future = executor.submit(test_lock_acquisition, agent_id)
            futures.append(future)
            time.sleep(0.5)  # Stagger the requests
        
        # Wait for all to complete
        for future in as_completed(futures):
            future.result()

def test_force_release():
    """Test force release functionality"""
    print("\nüîß Testing force release functionality...")
    
    # First, acquire a lock manually
    response = requests.post("http://localhost:8080/v1/gpu/lock",
                            headers={"Authorization": "Bearer sovereign-secret"},
                            json={"id": "test-force-release"})
    
    if response.status_code == 200:
        print("‚úÖ Acquired test lock")
        
        # Now force release all locks
        force_response = requests.post("http://localhost:8080/v1/gpu/force-release-all",
                                     headers={"Authorization": "Bearer sovereign-secret"})
        
        if force_response.status_code == 200:
            print("‚úÖ Force release executed successfully")
        else:
            print(f"‚ùå Force release failed: {force_response.status_code}")
    else:
        print(f"‚ùå Failed to acquire test lock: {response.status_code}")

def run_comprehensive_test():
    """Run comprehensive tests"""
    print("üöÄ Running comprehensive GPU resource management tests...\n")
    
    # Test 1: Basic status check
    print("1Ô∏è‚É£ Testing GPU status endpoint...")
    status_ok = test_gpu_status()
    
    # Test 2: Force release
    print("\n2Ô∏è‚É£ Testing force release functionality...")
    test_force_release()
    
    # Test 3: Concurrent access
    print("\n3Ô∏è‚É£ Testing concurrent access patterns...")
    test_concurrent_access()
    
    # Test 4: Status after tests
    print("\n4Ô∏è‚É£ Checking final GPU status...")
    final_status_ok = test_gpu_status()
    
    print("\n‚úÖ Comprehensive testing completed!")
    return status_ok and final_status_ok

if __name__ == "__main__":
    run_comprehensive_test()
--- END OF FILE: scripts\test_gpu_fixes.py ---

--- START OF FILE: scripts\ci\check_docs.py ---
#!/usr/bin/env python3
"""CI doc check script (Sovereign Era).

Verifies the presence of critical spec files defined in specs/doc_policy.md.
"""
from __future__ import annotations

import os
import sys
from pathlib import Path


REPO_ROOT = Path(__file__).resolve().parents[2]


def main() -> int:
    # 1. Check Core Specs (per specs/doc_policy.md Rule 3)
    expected = [
        REPO_ROOT / "specs" / "spec.md",
        REPO_ROOT / "specs" / "plan.md",
        REPO_ROOT / "specs" / "tasks.md",
        REPO_ROOT / "specs" / "doc_policy.md",
    ]
    
    missing = [str(p) for p in expected if not p.exists()]
    if missing:
        print("[FAIL] Missing core specification files:")
        for m in missing:
            print(f"  - {m}")
        return 2

    # 2. Check README
    readme = REPO_ROOT / "README.md"
    if not readme.exists():
        print("[FAIL] README.md not found")
        return 2

    text = readme.read_text(encoding="utf-8")
    lower = text.lower()
    
    # 3. Simple Content Check (Sovereign Context Engine)
    # We relax the strict "UTCP" check as architecture evolves.
    checks = [
        ("context engine", "Project name 'Context Engine' not found in README"),
    ]
    
    failed = []
    for token, msg in checks:
        if token not in lower:
            failed.append(msg)
            
    if failed:
        print("[FAIL] README checks failed:")
        for f in failed:
            print(f"  - {f}")
        return 2

    print("[OK] Sovereign Doc Checks Passed")
    return 0


if __name__ == "__main__":
    sys.exit(main())
--- END OF FILE: scripts\ci\check_docs.py ---

--- START OF FILE: specs\doc_policy.md ---
# Documentation Policy (Context-Engine)

**Master Policy for all directories. Code is authoritative; documentation supports it.**

---

## Rule 1: Minimize Documentation

- Code is the source of truth. Documentation explains *why* and guides *how*, but never replaces code.
- Default assumption: No docs needed unless setup is genuinely ambiguous or painful.
- LLM-generated reference docs are archived to `/archive/` after they're used.

---

## Rule 2: Allowed Documentation Per Directory

### Root Level
- **README.md** ‚Äî 100 words. Answer: "What is this repo?"
- **CHANGELOG.md** ‚Äî Version history and major changes
- **STARTUP.md** ‚Äî Quick start (if needed)
- **specs/** ‚Äî Central spec layer (see Rule 3)

### `backend/`, `tools/`, `extension/`, `scripts/`
- **README.md** ‚Äî Single sentence. Answer: "What does this directory do?"
- **CHANGELOG.md** ‚Äî Granular version history for this specific module.
- **CONFIGURATION.md** ‚Äî Only if env setup is non-obvious (backend only)
- **No additional .md files** in directory root (see Rule 3)

### `backend/src/`, `tools/src/`, `extension/src/`
- No separate documentation. Inline code comments with `#file:specs/...` references.

---

## Rule 3: Specification Layer (`specs/`)

The `specs/` directory is the **single source of architectural truth**.

### Core Files
- **spec.md** ‚Äî High-level system architecture (read this first)
- **plan.md** ‚Äî Roadmap and phases
- **tasks.md** ‚Äî Implementation task queue
- **doc_policy.md** ‚Äî This file (documentation governance)
- **mlc-urls.md** ‚Äî Registry of verified MLC-LLM model URLs (see Rule 3.1)

### Architecture Subdirectory
- **specs/architecture/** ‚Äî Deep technical specifications
  - **sovereign-wasm.spec.md** ‚Äî Browser-native layer (WebGPU, CozoDB, model-server-chat, builder)
  - **memory-layer.spec.md** ‚Äî Neo4j/Redis architecture and schemas
  - **extension-bridge.spec.md** ‚Äî Chrome extension design (injection, pause triggers)
  - **agents.spec.md** ‚Äî Agent system (Verifier, Distiller, Archivist)
  - **api.spec.md** ‚Äî FastAPI endpoints and protocols

### Where Each Spec Goes
- **Architectural overview or design decisions?** ‚Üí `specs/spec.md`
- **Multi-phase roadmap?** ‚Üí `specs/plan.md`
- **Implementation tasks?** ‚Üí `specs/tasks.md`
- **Deep technical details** (schemas, data flow, algorithms)? ‚Üí `specs/architecture/<domain>.spec.md`
- **Local directory context** (e.g., "what does scripts/ do")? ‚Üí `README.md` in that directory

---

## Rule 4: Deprecated/Generated Documentation

All LLM-generated reference documentation (tutorials, examples, detailed walkthroughs) should be:
1. **Used locally** (for context during development)
2. **Archived to `/archive/`** after they served their purpose
3. **Never** left in active project root or major directories

Examples of archived docs:
- `archive/docs_removed/` ‚Äî Outdated technical docs
- `archive/anchor/` ‚Äî Deprecated CLI interface docs
- `archive/setup_docs/` ‚Äî Legacy setup guides

---

## Rule 5: Cross-Referencing Specs

Within any spec file, use markdown links to other specs:

```markdown
For memory architecture, see [Memory Layer Spec](architecture/memory-layer.spec.md).
For browser integration, see [Sovereign WASM Spec](architecture/sovereign-wasm.spec.md).
```

In code files, use comments to reference specs:
```python
# Graph-R1 reasoning flow (see specs/architecture/agents.spec.md)
def graph_r1_query():
    pass
```

---

## Rule 6: Truth Precedence

If **code conflicts with documentation**:
1. Code is correct
2. Update the relevant spec file immediately
3. Add a git note explaining the discrepancy

If **multiple specs conflict**:
1. `spec.md` is authoritative for architecture
2. `architecture/*.spec.md` fills in implementation details
3. Code is the final arbiter

---

## Rule 7: Reality Constraint

Documentation must never:
- Contradict the "Empirical Distrust" protocol (retrieve > internal knowledge)
- Promise features not implemented in code
- Reference deprecated repositories or APIs without clear deprecation notices

---

## Enforcement

- **Review checklist:** Before merging PRs, verify no new .md files are scattered (should only be in specs/ or as single README.md per directory)
- **Quarterly cleanup:** Archive generated/reference docs older than 3 months
- **Broken links:** Use `specs/architecture/` links; verify they exist before committing

---

## Quick Reference

| Question | Answer |
|----------|--------|
| Where's the architecture? | `specs/spec.md` |
| Where's the roadmap? | `specs/plan.md` |
| Where are the tasks? | `specs/tasks.md` |
| How do I set up the backend? | `backend/CONFIGURATION.md` |
| What's in tools/? | `tools/README.md` |
| How do I understand WASM layer? | `specs/architecture/sovereign-wasm.spec.md` |
| How do Neo4j/Redis work? | `specs/architecture/memory-layer.spec.md` |
| How does the extension work? | `specs/architecture/extension-bridge.spec.md` |
| Where are old docs? | `archive/docs_removed/` |

---

## CozoDB Import Format & Recovery

**Purpose:** Describe the canonical JSON format used for bulk imports into the browser CozoDB instance and recovery steps when a Schema Detachment occurs.

**Note:** As of HTML pivot, CozoDB runs entirely in browser WASM with IndexedDB persistence. Recovery procedures apply to browser-native tools in `tools/` directory.

### Canonical Import JSON
CozoDB expects a top-level JSON object with a `relations` array. Each relation should look like this:

```json
{
  "relations": [
    {
      "name": "memory",
      "headers": ["id","timestamp","role","content","source","embedding"],
      "rows": [
        ["id-1", 1688790000000, "system", "file text...", "path/to/file.md", null],
        [...]
      ]
    }
  ]
}
```

- `id`: string, unique identifier (UUID or deterministic hash)
- `timestamp`: integer, Unix ms
- `role`: string, e.g., `system` or `user`
- `content`: string, textual content (truncate if exceedingly large)
- `source`: string, origin path or descriptor
- `embedding`: either an array of floats (embedding vector) or `null` if embeddings will be computed later

### Recovery: Schema Detachment
When `export_relations({})` returns `{"message":"missing field `relations` ..."}` or queries report `query::relation_not_found`:
1. Attempt non-destructive reattach:
```js
await window.db.run(":create memory { id: String => timestamp: Int, role: String, content: String, source: String, embedding: <F32; 384> } IF NOT EXISTS");
```
2. If reattach fails, export raw OPFS/IndexedDB blobs and decode them locally using `tools/decode_cozo_blob.py`.
3. Prefer bulk import from canonical source (`cozo_import_memory.json`) rather than reimporting individual rows.

### Tooling & Best Practices
- Use `tools/prepare_cozo_import.py` to create `cozo_import_memory.json` from `combined_memory.json`.
- For a guaranteed atomic result: nuke the DB and `Force Import Relations from JSON` (or use `db.import_relations(payload)` in Console) with the produced file.
- After import, run `export_relations({})` and persist the result (backup) plus verify by running `?[count] := *memory{id}`.

---

**Last Updated:** 2025-12-15  
**Version:** 1.0  
**Policy Owner:** Architecture Council
--- END OF FILE: specs\doc_policy.md ---

--- START OF FILE: specs\mlc-urls.md ---
# Verified MLC-LLM Model URLs

**Status:** Registry of verified WebLLM-compatible model URLs.
**Last Updated:** Dec 22, 2025
**Source:** `mlc-ai/web-llm` config and verified HTTP checks.

## Technology: WASM + WebGPU Inference

This project uses **WebLLM** (by MLC-AI) to run Large Language Models directly in the browser.

1.  **Compilation:** Models (Llama 3, Qwen 2.5, etc.) are compiled into **WebAssembly (WASM)** modules (`.wasm`). These modules contain the model's architecture and logic, optimized for execution in a web environment.
2.  **Acceleration:** The WASM module uses the **WebGPU API** to access the user's local GPU. This allows for massive parallelism, enabling 7B+ parameter models to run at interactive speeds (20-100+ tokens/sec) on consumer hardware.
3.  **Zero-Server:** No data leaves the browser. The "Backend" is the user's own GPU.

---

## 1. Verified Models (Ready for Production)

These models have been verified to exist in the `v0_2_80` library and are compatible with the current `web-llm` version.

### üåü 7B - 8B Class (Recommended)
Balanced performance for reasoning and chat. Requires 6GB+ VRAM.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `Qwen2.5-7B-Instruct-q4f16_1-MLC` | **Best All-Rounder.** Fast, smart, 32k context. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3.1-8B-Instruct-q4f32_1-MLC` | **Meta's Latest.** Strong reasoning. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3-8B-Instruct-q4f32_1-MLC` | Llama 3 Base. Reliable. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC` | **Reasoning Specialist.** | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` (Uses Qwen2 base) |

### üöÄ High Performance (Small)
Fastest start times. Works on most laptops/integrated graphics.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `Phi-3.5-mini-instruct-q4f16_1-MLC` | **Microsoft Phi.** 3.8B params. Very smart for size. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` | **Ultra-Lite.** 1.5B params. Blazing fast. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `SmolLM2-1.7B-Instruct-q4f16_1-MLC` | Efficient 1.7B model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |

### üåü Gemma Models (Google)
Lightweight and efficient models from Google.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `gemma-2-9b-it-q4f16_1-MLC` | **Gemma 2 9B.** High performance text generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `gemma-2-9b-it-q4f32_1-MLC` | **Gemma 2 9B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `gemma-2-2b-it-q4f16_1-MLC` | **Gemma 2 2B.** Lightweight option. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `gemma-2-2b-it-q4f32_1-MLC` | **Gemma 2 2B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `gemma-3-1b-it-q4f16_1-MLC` | **Gemma 3 1B.** Latest generation small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-3-1b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |

### üëÅÔ∏è Vision Models (Multimodal)
Models that can process both text and images.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `Phi-3.5-vision-instruct-q4f16_1-MLC` | **Microsoft Phi Vision.** 4.2B params, multimodal. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm` |
| `Phi-3.5-vision-instruct-q4f32_1-MLC` | **Microsoft Phi Vision.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f32_1-ctx4k_cs2k-webgpu.wasm` |

### üöÄ Larger Models (For High VRAM Systems)
Models for systems with 12GB+ VRAM like RTX 4090.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `Llama-2-13b-chat-hf-q4f16_1-MLC` | **Llama 2 13B.** For high-end systems. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-13b-chat-hf-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-8B-q4f16_1-MLC` | **Qwen 3 8B.** Latest generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-8B-q4f32_1-MLC` | **Qwen 3 8B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-4B-q4f16_1-MLC` | **Qwen 3 4B.** Balanced performance. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-4B-q4f32_1-MLC` | **Qwen 3 4B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |

### üöÄ Smaller Models (For Low VRAM Systems)
Models optimized for systems with limited VRAM like the XPS 13.

| Model ID | Details | WASM URL |
| :--- | :--- | :--- |
| `Qwen2-0.5B-Instruct-q4f16_1-MLC` | **Ultra-Lightweight.** 0.5B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen2-0.5B-Instruct-q4f32_1-MLC` | **Ultra-Lightweight.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-0.6B-q4f16_1-MLC` | **Qwen 3 Tiny.** 0.6B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-0.6B-q4f32_1-MLC` | **Qwen 3 Tiny.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-1.7B-q4f16_1-MLC` | **Qwen 3 Small.** 1.7B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Qwen3-1.7B-q4f32_1-MLC` | **Qwen 3 Small.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3.2-1B-Instruct-q4f16_1-MLC` | **Llama 3.2 1B.** Efficient small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3.2-1B-Instruct-q4f32_1-MLC` | **Llama 3.2 1B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3.2-3B-Instruct-q4f16_1-MLC` | **Llama 3.2 3B.** Balanced small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |
| `Llama-3.2-3B-Instruct-q4f32_1-MLC` | **Llama 3.2 3B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |

---

## 2. Experimental / Pending (WASM Missing)

These models are listed in config but their WASM binaries are not currently hosted in the `v0_2_80` folder. **Do not use until binaries are verified.**

*   `Qwen2.5-14B-Instruct-q4f16_1-MLC` (404 Not Found)
*   `DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC` (404 Not Found)
*   `Qwen2-VL-7B-Instruct-q4f16_1-MLC` (404 Not Found)
*   `gemma-3-2b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 2B available, only 1B exists**
*   `gemma-3-12b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 12B available in current library**

---

## 3. URL Construction Logic

If you need to construct a URL manually:

```javascript
const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/";
const version = "v0_2_80"; // Check src/config.ts for 'modelVersion'
const modelSpecificName = "Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm";

const fullUrl = `${libBase}${version}/${modelSpecificName}`;
```

**Note:** The `modelSpecificName` often differs slightly from the Hugging Face repo name (e.g., `Llama-3_1` vs `Llama-3.1` or `Qwen2` base for `DeepSeek`). Always check `mlc_config.ts` mapping.
--- END OF FILE: specs\mlc-urls.md ---

--- START OF FILE: specs\plan.md ---
# Root Coda Roadmap (V2.0)

**Status:** Root Architecture Deployed
**Focus:** Expansion, Agentic Capabilities, Multimodal.

## Phase 1: Foundation (Completed)
- [x] Pivot to WebLLM/WebGPU stack.
- [x] Implement CozoDB (WASM) for memory.
- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).

## Phase 2: Stabilization (Completed)
- [x] Fix Model Loading (Quota/VRAM config).
- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).
- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).

## Phase 2.5: Root Refactor (Completed)
- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).
- [x] **The Ears**: Refactored `sovereign-mic.html` to Root Architecture.
- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.
- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).

## Phase 3: Expansion (Current)
- [ ] **Agentic Capabilities**: Re-introduce Verifier/Distiller logic in JS.
- [x] **Active Memory Persistence**: Enable chat to write back to the Graph.
- [x] **Temporal Awareness**: Ground the model in real-time.
- [ ] **Mobile Optimization**: Polish mobile UX for `model-server-chat.html`.
- [ ] **Hybrid RAG**: Optimize vector + graph retrieval weighting.

## Phase 4: Federation
- [ ] **Device Sync**: Sync IndexedDB across devices (Peer-to-Peer).
- [ ] **Local-First Cloud**: Optional encrypted backup.
--- END OF FILE: specs\plan.md ---

--- START OF FILE: specs\spec.md ---
# Architecture Overview: Root Coda (v2.0)

**Status:** Production (Root Architecture)
**Philosophy:** 100% Local, 100% Browser, 100% Sovereign.

## Core Stack

The system has evolved into **Root Coda**, a pure [WASM (WebAssembly)](https://webassembly.org/) ecosystem where the browser is the Operating System.

### 1. The Kernel (`sovereign.js`)
- **Role:** The central nervous system.
- **Function:**
  - **Unified Logging:** Broadcasts to `log-viewer` and Mission Control.
  - **Reactive State:** Zero-dependency `Proxy` store for UI state.
  - **Hardware Abstraction:** "Snapdragon Fix" (WebGPU buffer clamping) and Profile management.
  - **Memory Driver:** Standardized CozoDB WASM initialization.

### 2. The Compute (`web-llm`)
- **Engine:** [WebLLM](https://webllm.mlc.ai/) (MLC-AI)
- **Runtime:** WebGPU (Hardware accelerated)
- **Models:**
  - **SOTA (Latest):** Qwen 3 (4B, 8B) / Gemma 3 (1B) / Phi 3.5
  - **High Performance:** SmolLM2 (1.7B, 360M) / Qwen2.5-3B
  - **Legacy:** Qwen2.5-14B / DeepSeek-R1 (16GB+ VRAM)

### 3. The Memory (`cozo-lib-wasm`)
- **Database:** [CozoDB](https://cozodb.org/) (Datalog/Relational/Graph)
- **Storage:** IndexedDB / OPFS (Origin Private File System) -> Persistent.
- **Schema:**
  - `*memory`: Stored relations (content, timestamp, embedding).
    - **Multisensory (Phase A):** Now includes `mime_type` and `blob_ref` for binary file referencing.
  - `*vectors`: HNSW vector index for semantic search.

### 4. The Interfaces (Root Tools)
- **Root Console** (`model-server-chat.html`): The **Brain**. Runs Graph-R1 in a **Web Worker** (`llm-worker.js`) to prevent UI freezing during inference.
- **Root Builder** (`sovereign-db-builder.html`): The **Stomach**. Ingests files/logs into the Graph.
- **Root Mic** (`sovereign-mic.html`): The **Ears**. Whisper-Tiny (WASM) + LLM cleanup.
- **Log Viewer** (`log-viewer.html`): The **Nerves**. System-wide diagnostics.
- **Root Dreamer** (`tools/root-dreamer.html`): The **Subconscious**. Background optimization and association.

## Data Flow

```mermaid
graph TD
    User -->|Voice| Mic[Root Mic]
    User -->|Files| Builder[Root Builder]
    User -->|Chat| Console[Root Console]
    
    Mic -->|Text| Console
    Builder -->|Insert| Cozo[CozoDB WASM]
    
    subgraph Browser Kernel
        Console -->|Msg| Worker[LLM Worker]
        Worker -->|Inference| WebGPU
        Console -->|Query| Cozo
        Cozo -->|Persist| IDB[IndexedDB]
    end
```

## Critical Workflows

### 1. The Reasoning Loop (Graph-R1)
1. User input triggers **Hybrid Reflex** (Vector Embedding + Keyword/Regex search in CozoDB).
2. **Context Manager** assembles a "Virtual Prompt" with retrieved clues.
3. LLM executes **R1 Loop**:
   - If answer found: Synthesize.
   - If missing info: Request specific search (`NEED_CONTEXT: term`).
4. Final answer streamed to user.

### 2. The Write Loop (Active Cognition)
- **Concept**: The Brain is no longer read-only.
- **Action**: Every chat interaction is persisted to CozoDB (`*memory` relation).
- **Consolidation**: The "Root Dreamer" (Subconscious) picks up these raw memories, vectorizes them, and creates associations in the background.

### 3. Root Persistence
- **Zero Backend:** Python is only used for serving static files (`http.server`).
- **Portability:** The entire "Brain" is contained in `browser_data` and IndexedDB.

## Reference Specs
- [Sovereign WASM Spec](architecture/sovereign-wasm.spec.md) (Detailed Kernel Docs)
- [Memory Layer Spec](architecture/memory-layer.spec.md)
--- END OF FILE: specs\spec.md ---

--- START OF FILE: specs\tasks.md ---
# Context-Engine Implementation Tasks

## Current Work Queue (Root Architecture)

### Completed - Root Refactor ‚úÖ
- [x] **Kernel**: Implement `tools/modules/sovereign.js`.
- [x] **Mic**: Refactor `sovereign-mic.html` to use Kernel.
- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.
- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).
- [x] **Docs**: Update all specs to reflect Root Architecture.

### Completed - Hardware Optimization üêâ
- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.
- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.
- [x] **Crash Prevention**: Context clamping for constrained drivers.
- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.

## Phase 3: The Subconscious (Completed) ‚úÖ
- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.
- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.
- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).

## Phase 3.1: Active Cognition (Current)
- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.
- [x] **User Control**: Add "Auto-Save" toggle to System Controls.
- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.
- [ ] **Multimodal**: Add Drag-and-Drop Image support to Console.

### Active Development - Expansion
- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.
- [ ] **Voice Output**: Add TTS to Console.

## Phase 4: The Specialist Array
- [ ] **Dataset Generation**: Samsung TRM / Distillation.
- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.
- [ ] **Model Merging**: FrankenMoE construction.

## Backlog
- [ ] **Federation Protocol**: P2P sync.
- [ ] **Android App**: Wrapper for Root Coda.
--- END OF FILE: specs\tasks.md ---

--- START OF FILE: specs\architecture\agents.spec.md ---
# Agent Architecture (Sovereign Edition)

**Status:** Planned / Partially Implemented in JS

The Sovereign Edition moves agentic logic from Python to JavaScript (Root Coda).

## Core Agents

### 1. The Archivist (Memory Writer)
- **Role:** Persists chat interactions to CozoDB.
- **Implementation:** `saveTurn()` in `model-server-chat.html`.
- **Status:** Active. Writes `*memory` relations with `msg_` IDs.

### 2. The Distiller (Dreamer)
- **Role:** Background consolidation and vectorization.
- **Implementation:** `tools/root-dreamer.html`.
- **Logic:**
  - Scans for orphan memories (no embedding).
  - Generates synthetic thoughts/connections.
  - Updates the Graph.

### 3. The Verifier (Critic)
- **Role:** Fact-checking and loop termination.
- **Implementation:** Integrated into "Graph-R1" Reasoning Loop (`ContextManager`).
- **Status:** Implicit in the `executeR1Loop` via "Self-Correction" prompt prompts.

## Future Plans
- Formalize `Verifier` as a separate Web Worker for parallel fact-checking.
--- END OF FILE: specs\architecture\agents.spec.md ---

--- START OF FILE: specs\architecture\api.spec.md ---
# API Specification (WebGPU Bridge)

**Status:** Production
**Component:** `tools/webgpu_bridge.py`

## Overview
The "Bridge" acts as a reverse-proxy, exposing the browser's WebLLM engine as an OpenAI-compatible API.

## Endpoints

### `POST /v1/chat/completions`
- **Format:** OpenAI Standard.
- **Flow:** 
  1. Client sends JSON to Python Bridge.
  2. Bridge forwards via WebSocket to `model-server-chat.html`.
  3. Browser computes response (WebGPU).
  4. Result streamed back to Bridge -> Client.

### `POST /v1/embeddings`
- **Format:** OpenAI Standard.
- **Flow:** Forwards to `webgpu-server-embed.html`.

## WebSockets
- `/ws/chat`: Connection for Chat Worker.
- `/ws/embed`: Connection for Embedding Worker.

## Security
- **Token Auth:** `Authorization: Bearer <BRIDGE_TOKEN>` required.
- **Network:** Binds to random port (obfuscation) or `8000` (default).
--- END OF FILE: specs\architecture\api.spec.md ---

--- START OF FILE: specs\architecture\extension-bridge.spec.md ---
# Chrome Extension Bridge Specification

**Silent context injection for corporate LLMs. Turn Gemini/ChatGPT into your "dumb terminal".**

---

## Identity

- **Name:** Sovereign Context Bridge
- **Target Platforms:** `gemini.google.com`, `chatgpt.openai.com` (extensible)
- **Trigger:** 3-second pause in text input
- **Injection Method:** Automatic text append before submission
- **Data Source:** Local CozoDB (zero backend latency)

---

## Architecture Overview

```
User types in gemini.google.com
  ‚Üì
Content Script detects 3-second pause
  ‚Üì
Background Service Worker queries local CozoDB
  ‚Üì
Memory results returned (< 100ms)
  ‚Üì
Context Summary generated
  ‚Üì
Silently append to text area (or insert as system instruction if available)
  ‚Üì
User hits Enter (normal flow)
```

---

## Components

### 1. Manifest (`manifest.json`)

**MV3 Manifest Structure:**
```json
{
  "manifest_version": 3,
  "name": "Sovereign Context Bridge",
  "version": "1.0.0",
  "description": "Silent context injection for LLM conversations",
  "permissions": [
    "activeTab",
    "scripting",
    "storage",
    "webRequest"
  ],
  "host_permissions": [
    "*://gemini.google.com/*",
    "*://chatgpt.openai.com/*"
  ],
  "content_scripts": [
    {
      "matches": ["*://gemini.google.com/*", "*://chatgpt.openai.com/*"],
      "js": ["content.js"],
      "run_at": "document_start"
    }
  ],
  "background": {
    "service_worker": "background.js"
  },
  "action": {
    "default_popup": "popup.html",
    "default_title": "Sovereign Context Bridge"
  },
  "icons": {
    "16": "images/icon-16.png",
    "32": "images/icon-32.png",
    "128": "images/icon-128.png"
  }
}
```

### 2. Content Script (`content.js`)

**Responsibilities:**
- Inject status indicator into page
- Detect user input in text area
- Monitor for 3-second pause
- Receive context from background worker
- Inject context into textarea

**Key Functions:**

#### `detectTextArea()`
```javascript
function detectTextArea() {
  // Platform-specific selectors
  const selectors = {
    'gemini.google.com': 'div[contenteditable="true"], textarea',
    'chatgpt.openai.com': 'textarea'
  };
  return document.querySelector(selectors[domain]);
}
```

#### `setupPauseDetector()`
```javascript
let inputTimeout;
const PAUSE_THRESHOLD = 3000; // 3 seconds

textArea.addEventListener('input', () => {
  clearTimeout(inputTimeout);
  inputTimeout = setTimeout(() => {
    console.log('[Sovereign] 3-second pause detected, querying memories...');
    chrome.runtime.sendMessage(
      { action: 'queryMemories', query: getVisibleText() },
      (response) => injectContext(response)
    );
  }, PAUSE_THRESHOLD);
});
```

#### `injectContext(contextData)`
```javascript
function injectContext(contextData) {
  const summary = `[Sovereign Context Injection at ${new Date().toLocaleTimeString()}]\n${contextData.summary}\n---\n`;
  
  // For contenteditable (Gemini)
  if (textArea.contentEditable === 'true') {
    const currentText = textArea.textContent;
    textArea.textContent = currentText + '\n' + summary;
  }
  // For textarea (ChatGPT)
  else {
    textArea.value += '\n' + summary;
  }
  
  displayIndicator('‚úì Context injected (memories found)', 'success');
}
```

#### Status Indicator
```javascript
function displayIndicator(message, type) {
  const indicator = document.getElementById('sovereign-indicator') || 
                    createIndicator();
  indicator.textContent = message;
  indicator.className = `sovereign-indicator ${type}`;
  setTimeout(() => indicator.remove(), 5000);
}
```

### 3. Background Service Worker (`background.js`)

**Responsibilities:**
- Maintain persistent connection to local CozoDB (if backend running)
- Query memories based on visible text
- Generate summaries
- Handle pause trigger messages

**Key Functions:**

#### `queryMemories()`
```javascript
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'queryMemories') {
    queryMemoriesFromCozoDB(request.query)
      .then(memories => ({
        success: true,
        memories: memories,
        summary: generateSummary(memories)
      }))
      .then(sendResponse)
      .catch(err => ({
        success: false,
        error: err.message
      }));
    return true; // Keep channel open for async response
  }
});
```

#### `queryMemoriesFromCozoDB(userInput)`
```javascript
async function queryMemoriesFromCozoDB(userInput) {
  try {
    // Option 1: If backend API available
    const response = await fetch('http://localhost:8000/memories/search', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query: userInput })
    });
    return response.json();
  } catch (e) {
    // Option 2: Fall back to local CozoDB via SharedArrayBuffer or WebWorker
    console.warn('[Sovereign] Backend unavailable, querying local store...');
    return queryLocalCozoDB(userInput);
  }
}
```

#### `generateSummary(memories)`
```javascript
function generateSummary(memories) {
  if (memories.length === 0) return 'No relevant memories found.';
  
  const maxMemories = 3;
  const relevant = memories.slice(0, maxMemories);
  
  return relevant
    .map((m, idx) => `[Memory ${idx + 1}] ${m.content.substring(0, 100)}...`)
    .join('\n');
}
```

### 4. Popup UI (`popup.html`)

**Simple status page shown when user clicks extension icon:**

```html
<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="popup-container">
    <h3>üß† Sovereign Context Bridge</h3>
    
    <div id="status">
      <p>Status: <span id="status-badge">‚óè</span></p>
      <p id="status-text">Initializing...</p>
    </div>
    
    <div id="stats">
      <p>Memories cached: <span id="mem-count">0</span></p>
      <p>Last injection: <span id="last-inject">Never</span></p>
    </div>
    
    <button id="settings-btn">‚öôÔ∏è Settings</button>
    <button id="test-inject-btn">üß™ Test Injection</button>
  </div>
  
  <script src="popup.js"></script>
</body>
</html>
```

---

## Data Flow: Query ‚Üí Inject

```
Step 1: User types "Tell me about Chronos" in Gemini
  ‚Üì
Step 2: Content script detects 3-second pause
  ‚Üì
Step 3: Send message to background worker: {action: 'queryMemories', query: '...'}
  ‚Üì
Step 4: Background worker queries memories
  (Option A: /memories/search endpoint if backend available)
  (Option B: Direct CozoDB query if local store accessible)
  ‚Üì
Step 5: Return top-3 memories + summary
  {
    success: true,
    memories: [
      {id: "123", content: "Chronos is about time management..."},
      {id: "124", content: "In July session..."},
      {id: "125", content: "Key insight: context rotation..."}
    ],
    summary: "[Memory 1] Chronos is about... [Memory 2] In July... [Memory 3] Key insight..."
  }
  ‚Üì
Step 6: Content script injects into textarea:
  "Tell me about Chronos
   
   [Sovereign Context Injection at 14:32:05]
   [Memory 1] Chronos is about time management...
   [Memory 2] In July session we discovered...
   [Memory 3] Key insight: context rotation prevents token loss...
   ---"
  ‚Üì
Step 7: User presses Enter ‚Üí Gemini processes augmented prompt
  ‚Üì
Step 8: Response includes your memory context!
```

---

## Trigger Mechanisms

### Primary: 3-Second Pause

```javascript
// User stops typing ‚Üí wait 3 seconds ‚Üí trigger query
const PAUSE_THRESHOLD = 3000;
let pauseTimer;

textArea.addEventListener('input', () => {
  clearTimeout(pauseTimer);
  pauseTimer = setTimeout(() => triggerMemoryQuery(), PAUSE_THRESHOLD);
});
```

### Secondary: Manual Hotkey (Optional)

```javascript
// Ctrl+Shift+M to manually inject
document.addEventListener('keydown', (e) => {
  if (e.ctrlKey && e.shiftKey && e.code === 'KeyM') {
    triggerMemoryQuery();
  }
});
```

---

## Context Injection Format

### Option 1: Append to User Text
```
Tell me about Project Chronos

[Sovereign Context - Memories]
[Mem 1] Project Chronos explores infinite context windows...
[Mem 2] Discovered in July: context rotation is key...
[Mem 3] Verifier agent reduces hallucinations...
---
```

### Option 2: System Instruction (if API allows)
```json
{
  "messages": [
    {
      "role": "system",
      "content": "You have access to these contextual memories: [Mem 1] ... [Mem 2] ..."
    },
    {
      "role": "user",
      "content": "Tell me about Project Chronos"
    }
  ]
}
```

---

## Configuration & Settings

**User-Configurable (popup.js):**
```javascript
const CONFIG = {
  pauseThreshold: 3000,           // 3 seconds
  maxMemoriesToInject: 3,          // Top 3
  injectionFormat: 'append',       // or 'system-instruction'
  enabledSites: {
    'gemini.google.com': true,
    'chatgpt.openai.com': true
  },
  backendUrl: 'http://localhost:8000',
  fallbackToLocal: true            // Use local CozoDB if backend down
};
```

---

## Error Handling

### Scenario 1: Backend Unavailable
```
‚Üí Fall back to local CozoDB (if accessible)
‚Üí If local unavailable, show status: "‚ö† No memories available"
```

### Scenario 2: No Relevant Memories Found
```
‚Üí Display indicator: "No relevant memories found"
‚Üí Still allow user to submit normally
```

### Scenario 3: Injection Failed
```
‚Üí Show error: "Failed to inject context"
‚Üí Allow user to retry manually via button
```

---

## Performance Targets

- **Pause detection latency:** < 50ms
- **Memory query latency:** < 100ms (local) or < 500ms (backend)
- **Injection latency:** < 50ms
- **Total E2E:** < 150ms (user should not notice delay)

---

## Privacy & Security

- **Local-first:** All queries stay on user's machine
- **No logging:** Extension doesn't upload queries to external services
- **User control:** Manual disable via popup toggle
- **Memory source:** Only accesses local CozoDB, never user's active Gemini text

---

## Platform Adaptations

### For Gemini
- **Text Area:** `div[contenteditable="true"]`
- **Submission:** Detect Enter key or "Send" button click
- **Format:** Append to contenteditable div

### For ChatGPT
- **Text Area:** `textarea`
- **Submission:** Detect Enter or Ctrl+Enter
- **Format:** Append to textarea value

### Future: Claude, Copilot, etc.
- Extend `host_permissions` in manifest
- Add platform-specific selector in `content.js`

---

## Related Specs

- See [Sovereign WASM Spec](sovereign-wasm.spec.md) for CozoDB query patterns
- See [Memory Layer Spec](memory-layer.spec.md) for memory retrieval
- See [API Spec](api.spec.md) for `/memories/search` endpoint

---

**Last Updated:** 2025-12-15  
**Status:** Design Phase (ready for implementation)
**Development Phase:** Priority 3 (Sovereign Stack must be operational first)
--- END OF FILE: specs\architecture\extension-bridge.spec.md ---

--- START OF FILE: specs\architecture\memory-layer.spec.md ---
# Sovereign Memory Architecture (CozoDB + WASM)

**Status:** Active (Production)
**Implementation:** `tools/sovereign-db-builder.html` and `tools/model-server-chat.html`

The memory layer is a **Browser-Native Hybrid Graph-Vector Database** running entirely in WebAssembly (WASM). It consolidates the roles of the legacy Redis (Cache) and Neo4j (Graph) into a single, persistent CozoDB instance backed by IndexedDB/OPFS.

---

## 1. Core Schema (`*memory`)

The system uses a single unified relation for all ingested knowledge, serving both as the "Long Term Memory" and the source for "Context Injection".

### Datalog Definition
```datalog
:create memory {
    id: String
    =>
    timestamp: Int,         # Unix Epoch (ms) - Used for sorting/recency
    role: String,           # 'user', 'assistant', 'system'
    content: String,        # The raw text (truncated at 20KB for WASM stability)
    source: String,         # File path or 'session'
    embedding: <F32; 384>   # Vector for semantic search (all-MiniLM-L6-v2)
}
```

### Fields
*   **`id`** *(Key)*: Unique identifier. Format: `Timestamp-RandomID`.
*   **`timestamp`**: Time of creation. Critical for "Recent Memory" retrieval.
*   **`role`**: Speaker identity.
*   **`content`**: The actual knowledge or chat message.
*   **`source`**: Provenance (e.g., `logs/session-123.json` or `specs/spec.md`).
*   **`embedding`**: 384-dimensional vector generated by `transformers.js`.

---

## 2. Persistence (The "Sovereign Loop")

1.  **Write**: Data is written to CozoDB (WASM memory).
2.  **Flush**: CozoDB automatically syncs to **IndexedDB** (`coda_memory` / `cozo_store`).
3.  **Load**: On page refresh, the app probes IndexedDB and rehydrates the CozoDB instance.

> **Zero-Loss Guarantee:** If the CozoDB instance crashes or the tab is closed, data is safe in IndexedDB.

---

## 3. Retrieval Strategies

### A. Graph-R1 Recency (Default)
Retrieves the most recent context to ground the reasoning loop.
```datalog
?[timestamp, role, source, content] := 
    *memory{timestamp, role, source, content}
    :sort -timestamp
    :limit 20
```

### B. Semantic Search (Vector)
Retrieves context based on embedding similarity (Cosine distance).
```datalog
?[id, dist] := 
    *memory{id, embedding},
    vec_l2(embedding, $query_vec)
    :limit 10
```

---

## 4. Import / Export

*   **Import**: Drag & Drop `combined_memory.json` into `sovereign-db-builder.html`.
*   **Export**: The builder allows exporting the full graph (with embeddings) to JSON for backup or transfer between devices.
--- END OF FILE: specs\architecture\memory-layer.spec.md ---

--- START OF FILE: specs\architecture\sovereign-wasm.spec.md ---
# Sovereign WASM Specification (Root Kernel)

## Architecture Overview
The **Root Coda** system runs entirely in the browser using a unified Kernel (`sovereign.js`) that manages Compute (WebLLM) and Memory (CozoDB).

## 1. The Kernel (`tools/modules/sovereign.js`)
The Kernel is the standard library for all Root Tools. It enforces consistency and safety.

### 1.1 Hardware Abstraction ("Snapdragon Fix")
**Problem**: Adreno GPUs (Snapdragon X Elite) and some mobile chips crash if a WebGPU buffer >256MB is requested, or if context exceeds 4k tokens without specific driver flags.
**Solution**: `getWebGPUConfig(profile)`
- **Lite**: Clamps buffer to 256MB, Context to 2048.
- **Mid**: Clamps buffer to 1GB, Context to 4096.
- **High/Ultra**: Unlocked.

### 1.2 Unified Logging
**Problem**: `console.log` is invisible on mobile or when running as a PWA.
**Solution**: `SovereignLogger`
- Broadcasts all logs to `BroadcastChannel('sovereign-logs')`.
- Consumed by `log-viewer.html` for real-time remote debugging.

### 1.3 Reactive State
**Problem**: Spaghetti code updating DOM elements manually.
**Solution**: `createStore(initialState)`
- Lightweight `Proxy`-based store.
- Components subscribe to changes: `subscribe((key, val) => updateUI(key, val))`.

## 2. Memory Layer (CozoDB WASM)
The Kernel provides a standardized loader: `initCozo(wasmPath)`.

### Data Portability
- **Lossless Export**: The Root Builder features a "Lossless Export" button.
- **Mechanism**: Dumps full Cozo relations (including vectors) to a JSON file.
- **Use Case**: Transfer full "Brain" state between devices or backup.

### Schema
```datalog
:create memory {
    id: String 
    => 
    timestamp: Int, 
    role: String, 
    content: String, 
    source: String, 
    embedding: <F32; 384> 
}
```

## 3. Tool Bridge (Legacy Support)
The `webgpu_bridge.py` acts as a secure relay (websocket <-> http) for external tools (like VS Code extensions) to access the Browser's LLM.
- **Input**: HTTP/REST (`/v1/chat/completions`)
- **Output**: WebSocket (`ws://localhost:8080/ws/chat`)

## 4. Audio Input (Root Mic)
**Goal**: Pure client-side speech-to-text without sending audio to a cloud.

### 4.1 Pipeline
1. **Capture**: `MediaRecorder` (WebM) -> 48kHz decoding.
2. **Preprocessing**:
   - Downsampling to 16kHz (Whisper Native).
   - **Noise Gate**: Discards audio if peak amplitude < 0.01 (Prevents transcribing silence).
   - **Amplification**: Smart gain (max 5x) for quiet voices, but capped to avoid boosting noise floor.
3. **Inference (WASM)**: 
   - Model: `Xenova/whisper-tiny.en` (Quantized).
   - **Long-form Strategy**: Uses `chunk_length_s: 30` and `stride_length_s: 5` to process audio exceeding the model's native 30s window.
4. **Post-Processing (Refinement)**:
   - **Hallucination Filter**: Regex removal of common Whisper artifacts (e.g., "[Music]", "Applause", "Amara.org").
   - **LLM Cleanup**: The raw transcript is passed to the local Qwen2.5 instance with a system prompt to fix grammar/punctuation without altering meaning.

### 4.2 Summarization Loop
- **Trigger**: User clicks "Summarize & Clarify" after a successful transcription.
- **Process**: The cleaned transcript is sent back to the Local Kernel (Qwen2.5) with a prompt to "summarize and clarify core meaning."
- **Output**: The transcript is replaced by the summary, which is automatically copied to the clipboard.

## 5. Parallel Compute (The Worker)
To prevent UI freezing during heavy inference, the LLM runs in a dedicated Web Worker.

### 5.1 `tools/modules/llm-worker.js`
- **Role**: Hosts the `MLCEngine` instance.
- **Communication**: Uses `WebWorkerMLCEngineHandler` to bridge messages between the main thread and the worker.
- **Benefit**: Ensures the UI remains responsive (scrolling, typing) even while the GPU is crunching tokens.
--- END OF FILE: specs\architecture\sovereign-wasm.spec.md ---

--- START OF FILE: templates\waveai_ece.json ---
ece-local:
  display:name: Sovereign Console (ECE)
  display:order: 1
  display:icon: microchip
  display:description: Local WebGPU Model via ECE Bridge
  ai:provider: custom
  ai:apitype: openai-chat
  ai:model: webgpu-chat
  ai:thinkinglevel: medium
  ai:endpoint: http://127.0.0.1:8080/v1/chat/completions
  ai:apitoken: not-needed
  ai:capabilities:
    - tools
--- END OF FILE: templates\waveai_ece.json ---

--- START OF FILE: tools\CHANGELOG.md ---
# Tools Changelog

## [Unreleased] - 2025-12-23

### Added
- **Orchestrator Model:** New `orchestrator.py` tool to programmatically interact with the MLC Bridge from Python.
- **Health Endpoint:** Added `/health` to `webgpu_bridge.py` for extension connectivity checks.
- **Audit Whitelist:** Added `/audit/server-logs` to auth whitelist in Bridge to fix Log Viewer 401 errors.

### Changed
- **Bridge Port:** Moved standard bridge port from `8000` to `8080` to avoid conflicts with `http.server`.
- **Launch Scripts:** Updated `start-sovereign-console.bat` and `launch-chromium-d3d12.bat` to respect new port `8080` and correct paths.
- **Root Dreamer:**
  - Robustified `init()` to handle non-Error objects during crash.
  - Improved JSON parsing to strip markdown code blocks before parsing.
  - Added strict engine readiness check to `dreamLoop` to prevent race conditions.
- **Root Console:**
  - Added "High Performance (Small)" models (Qwen 2.5 1.5B, TinyLlama) to the dropdown.
  - Updated JS mapper to handle new small model paths.
  - Fixed crash in `executeR1Loop` where `genErr.message` could be undefined.

### Fixed
- **WebGPU Crash:** Mitigated `DXGI_ERROR_DEVICE_REMOVED` by advising single-tab usage and providing smaller model options for constrained profiles.
- **Extension Connection:** Fixed CORS and Port mismatch preventing the Chrome Extension from connecting to the Bridge.
--- END OF FILE: tools\CHANGELOG.md ---

--- START OF FILE: tools\code_tools.py ---
"""Top-level shim to re-export `anchor.tools.code_tools` for test imports that expect `tools.code_tools`.
"""
try:
    from anchor.tools.code_tools import *  # noqa: F401, F403
except Exception:
    # Minimal fallback implementations
    def code_search(root: str, query: str, **kwargs):
        return {"root": root, "query": query, "count": 0, "results": []}

    def code_grep(root: str, query: str, **kwargs):
        return {"root": root, "query": query, "files": 0, "total_matches": 0, "results": []}

__all__ = ["code_search", "code_grep"]
--- END OF FILE: tools\code_tools.py ---

--- START OF FILE: tools\cozo_lib_wasm.js ---
import { loadAllFromIndexedDb, flushPendingWrites, writeToIndexedDb, setWriteCounter } from './indexeddb.js';

let wasm;

const heap = new Array(128).fill(undefined);

heap.push(undefined, null, true, false);

function getObject(idx) { return heap[idx]; }

let heap_next = heap.length;

function dropObject(idx) {
    if (idx < 132) return;
    heap[idx] = heap_next;
    heap_next = idx;
}

function takeObject(idx) {
    const ret = getObject(idx);
    dropObject(idx);
    return ret;
}

const cachedTextDecoder = (typeof TextDecoder !== 'undefined' ? new TextDecoder('utf-8', { ignoreBOM: true, fatal: true }) : { decode: () => { throw Error('TextDecoder not available') } } );

if (typeof TextDecoder !== 'undefined') { cachedTextDecoder.decode(); };

let cachedUint8Memory0 = null;

function getUint8Memory0() {
    if (cachedUint8Memory0 === null || cachedUint8Memory0.byteLength === 0) {
        cachedUint8Memory0 = new Uint8Array(wasm.memory.buffer);
    }
    return cachedUint8Memory0;
}

function getStringFromWasm0(ptr, len) {
    ptr = ptr >>> 0;
    return cachedTextDecoder.decode(getUint8Memory0().subarray(ptr, ptr + len));
}

function addHeapObject(obj) {
    if (heap_next === heap.length) heap.push(heap.length + 1);
    const idx = heap_next;
    heap_next = heap[idx];

    heap[idx] = obj;
    return idx;
}

function makeMutClosure(arg0, arg1, dtor, f) {
    const state = { a: arg0, b: arg1, cnt: 1, dtor };
    const real = (...args) => {
        // First up with a closure we increment the internal reference
        // count. This ensures that the Rust closure environment won't
        // be deallocated while we're invoking it.
        state.cnt++;
        const a = state.a;
        state.a = 0;
        try {
            return f(a, state.b, ...args);
        } finally {
            if (--state.cnt === 0) {
                wasm.__wbindgen_export_0.get(state.dtor)(a, state.b);

            } else {
                state.a = a;
            }
        }
    };
    real.original = state;

    return real;
}
function __wbg_adapter_22(arg0, arg1, arg2) {
    wasm.wasm_bindgen__convert__closures__invoke1_mut__hd17e34166836fd48(arg0, arg1, addHeapObject(arg2));
}

let WASM_VECTOR_LEN = 0;

const cachedTextEncoder = (typeof TextEncoder !== 'undefined' ? new TextEncoder('utf-8') : { encode: () => { throw Error('TextEncoder not available') } } );

const encodeString = (typeof cachedTextEncoder.encodeInto === 'function'
    ? function (arg, view) {
    return cachedTextEncoder.encodeInto(arg, view);
}
    : function (arg, view) {
    const buf = cachedTextEncoder.encode(arg);
    view.set(buf);
    return {
        read: arg.length,
        written: buf.length
    };
});

function passStringToWasm0(arg, malloc, realloc) {
    if (typeof arg !== 'string') arg = arg ? String(arg) : "";
    if (realloc === undefined) {
        const buf = cachedTextEncoder.encode(arg);
        const ptr = malloc(buf.length, 1) >>> 0;
        getUint8Memory0().subarray(ptr, ptr + buf.length).set(buf);
        WASM_VECTOR_LEN = buf.length;
        return ptr;
    }

    let len = arg.length;
    let ptr = malloc(len, 1) >>> 0;

    const mem = getUint8Memory0();

    let offset = 0;

    for (; offset < len; offset++) {
        const code = arg.charCodeAt(offset);
        if (code > 0x7F) break;
        mem[ptr + offset] = code;
    }

    if (offset !== len) {
        if (offset !== 0) {
            arg = arg.slice(offset);
        }
        ptr = realloc(ptr, len, len = offset + arg.length * 3, 1) >>> 0;
        const view = getUint8Memory0().subarray(ptr + offset, ptr + len);
        const ret = encodeString(arg, view);

        offset += ret.written;
    }

    WASM_VECTOR_LEN = offset;
    return ptr;
}

let cachedInt32Memory0 = null;

function getInt32Memory0() {
    if (cachedInt32Memory0 === null || cachedInt32Memory0.byteLength === 0) {
        cachedInt32Memory0 = new Int32Array(wasm.memory.buffer);
    }
    return cachedInt32Memory0;
}

function handleError(f, args) {
    try {
        return f.apply(this, args);
    } catch (e) {
        wasm.__wbindgen_exn_store(addHeapObject(e));
    }
}
function __wbg_adapter_76(arg0, arg1, arg2, arg3) {
    wasm.wasm_bindgen__convert__closures__invoke2_mut__h62fdc46dd4e23c5d(arg0, arg1, addHeapObject(arg2), addHeapObject(arg3));
}

/**
*/
export class CozoDb {

    static __wrap(ptr) {
        ptr = ptr >>> 0;
        const obj = Object.create(CozoDb.prototype);
        obj.__wbg_ptr = ptr;

        return obj;
    }

    __destroy_into_raw() {
        const ptr = this.__wbg_ptr;
        this.__wbg_ptr = 0;

        return ptr;
    }

    free() {
        const ptr = this.__destroy_into_raw();
        wasm.__wbg_cozodb_free(ptr);
    }
    /**
    * @returns {CozoDb}
    */
    static new() {
        const ret = wasm.cozodb_new();
        return CozoDb.__wrap(ret);
    }
    /**
    * Create CozoDb from IndexedDB
    * @param {string} db_name
    * @param {string} store_name
    * @param {any} on_write_callback
    * @returns {Promise<CozoDb>}
    */
    static new_from_indexed_db(db_name, store_name, on_write_callback) {
        const ptr0 = passStringToWasm0(db_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
        const len0 = WASM_VECTOR_LEN;
        const ptr1 = passStringToWasm0(store_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
        const len1 = WASM_VECTOR_LEN;
        const ret = wasm.cozodb_new_from_indexed_db(ptr0, len0, ptr1, len1, addHeapObject(on_write_callback));
        return takeObject(ret);
    }
    /**
    * @param {string} script
    * @param {string} params
    * @param {boolean} immutable
    * @returns {Promise<string>}
    */
    run(script, params, immutable) {
        if (typeof params === 'undefined' || params === null) params = "{}";
        const ptr0 = passStringToWasm0(script, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
        const len0 = WASM_VECTOR_LEN;
        const ptr1 = passStringToWasm0(params, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
        const len1 = WASM_VECTOR_LEN;
        const ret = wasm.cozodb_run(this.__wbg_ptr, ptr0, len0, ptr1, len1, immutable);
        return takeObject(ret);
    }
    /**
    * @param {string} data
    * @returns {string}
    */
    export_relations(data) {
        let deferred2_0;
        let deferred2_1;
        try {
            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);
            const ptr0 = passStringToWasm0(data, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
            const len0 = WASM_VECTOR_LEN;
            wasm.cozodb_export_relations(retptr, this.__wbg_ptr, ptr0, len0);
            var r0 = getInt32Memory0()[retptr / 4 + 0];
            var r1 = getInt32Memory0()[retptr / 4 + 1];
            deferred2_0 = r0;
            deferred2_1 = r1;
            return getStringFromWasm0(r0, r1);
        } finally {
            wasm.__wbindgen_add_to_stack_pointer(16);
            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);
        }
    }
    /**
    * @param {string} data
    * @returns {string}
    */
    import_relations(data) {
        // Normalize common JSON payload shapes so the WASM importer receives
        // a canonical `{"relations": [{ name, headers, rows }]}` shape.
        let normalized = null;
        try {
            const parsed = JSON.parse(data);
            // Top-level 'memory' -> convert
            if (parsed && parsed.memory) {
                const mem = parsed.memory;
                const headers = mem.headers || (mem.named_rows && mem.named_rows.headers) || null;
                const rows = mem.rows || (mem.named_rows && mem.named_rows.rows) || [];
                normalized = { relations: [{ name: 'memory', headers: headers || [], rows }] };
            }
            // Already a relations wrapper
            else if (parsed && parsed.relations && Array.isArray(parsed.relations)) {
                const rels = parsed.relations.map(r => {
                    const headers = r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'];
                    const rawRows = r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [];
                    const rows = rawRows.map(row => {
                        // Normalize object rows into arrays using headers order
                        if (row && typeof row === 'object' && !Array.isArray(row)) {
                            return headers.map(h => (row[h] === undefined ? null : row[h]));
                        }
                        // Ensure array rows have proper length and default embedding
                        const arr = Array.isArray(row) ? row.slice(0, headers.length) : [];
                        while (arr.length < headers.length) arr.push(null);
                        const embIdx = headers.indexOf('embedding');
                        if (embIdx >= 0 && (arr[embIdx] === null || arr[embIdx] === undefined)) arr[embIdx] = [];
                        return arr;
                    });
                    // Provide multiple shapes to satisfy various importer variants
                    return {
                        name: r.name || 'memory',
                        headers,
                        rows,
                        named_rows: { headers, rows },
                        NamedRows: { headers, rows }
                    };
                });
                normalized = { relations: rels };
            }
            // Else, could be raw array of records -> convert to rows
            else if (Array.isArray(parsed)) {
                const rows = parsed.map(rec => {
                    const ts = rec.timestamp ? (isNaN(Number(rec.timestamp)) ? new Date(rec.timestamp).getTime() : Number(rec.timestamp)) : Date.now();
                    return [
                        `${ts}-${Math.random().toString(36).substr(2,9)}`,
                        ts,
                        rec.role || rec.type || 'unknown',
                        (rec.content || rec.response_content || rec.message || '').substring(0, 20000),
                        rec.source || 'combined_memory.json',
                        null
                    ];
                });
                normalized = { relations: [{ name: 'memory', headers: ['id','timestamp','role','content','source','embedding'], rows }] };
            }
        } catch (e) {
            // Not valid JSON or normalization failed; fall back to raw string
        }

        const payload = normalized ? JSON.stringify(normalized) : data;

        let deferred2_0;
        let deferred2_1;
        try {
            // Debug: show the exact payload being passed into WASM importer (trimmed)
            try {
                console.log('CozoDb.import_relations - payload preview (trimmed):', payload.slice(0, 2000));
                const parsedPreview = JSON.parse(payload);
                console.log('CozoDb.import_relations - parsed relations preview:', parsedPreview.relations && parsedPreview.relations[0] ? Object.keys(parsedPreview.relations[0]) : null);
            } catch (pe) {
                console.warn('CozoDb.import_relations - payload not parseable as JSON preview.');
            }
            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);
            const ptr0 = passStringToWasm0(payload, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
            const len0 = WASM_VECTOR_LEN;
            try {
                wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptr0, len0);
            } catch (e) {
                console.error('CozoDb.import_relations wasm call failed:', e, 'payload_preview:', payload.slice(0,2000));
                throw e;
            }
            var r0 = getInt32Memory0()[retptr / 4 + 0];
            var r1 = getInt32Memory0()[retptr / 4 + 1];
            deferred2_0 = r0;
            deferred2_1 = r1;
            const resultStr = getStringFromWasm0(r0, r1);

            // If WASM returned a NamedRows header error, try a fallback shaped payload where relation includes NamedRows explicitly.
            try {
                const parsedResult = JSON.parse(resultStr);
                if (parsedResult && parsedResult.ok === false && typeof parsedResult.message === 'string' && parsedResult.message.includes('NamedRows requires')) {
                    try {
                        const parsedPayload = JSON.parse(payload);
                        if (parsedPayload && parsedPayload.relations && Array.isArray(parsedPayload.relations)) {
                            const fallback = { relations: parsedPayload.relations.map(r => ({ name: r.name || 'memory', NamedRows: { headers: r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'], rows: r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [] } })) };
                            const fallbackStr = JSON.stringify(fallback);
                            console.log('CozoDb.import_relations - retrying with fallback NamedRows payload (trimmed):', fallbackStr.slice(0,2000));
                            const ptrF = passStringToWasm0(fallbackStr, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
                            const lenF = WASM_VECTOR_LEN;
                            wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptrF, lenF);
                            var r0b = getInt32Memory0()[retptr / 4 + 0];
                            var r1b = getInt32Memory0()[retptr / 4 + 1];
                            // free previous
                            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);
                            deferred2_0 = r0b;
                            deferred2_1 = r1b;
                            return getStringFromWasm0(r0b, r1b);
                        }
                    } catch (retryErr) {
                        console.warn('CozoDb.import_relations - fallback retry failed to build payload or call wasm:', retryErr);
                    }
                }
            } catch (e) {
                // ignore parse errors
            }

            return resultStr;
        } finally {
            wasm.__wbindgen_add_to_stack_pointer(16);
            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);
        }
    }
}

async function __wbg_load(module, imports) {
    if (typeof Response === 'function' && module instanceof Response) {
        if (typeof WebAssembly.instantiateStreaming === 'function') {
            try {
                return await WebAssembly.instantiateStreaming(module, imports);

            } catch (e) {
                if (module.headers.get('Content-Type') != 'application/wasm') {
                    console.warn("`WebAssembly.instantiateStreaming` failed because your server does not serve wasm with `application/wasm` MIME type. Falling back to `WebAssembly.instantiate` which is slower. Original error:\n", e);

                } else {
                    throw e;
                }
            }
        }

        const bytes = await module.arrayBuffer();
        return await WebAssembly.instantiate(bytes, imports);

    } else {
        const instance = await WebAssembly.instantiate(module, imports);

        if (instance instanceof WebAssembly.Instance) {
            return { instance, module };

        } else {
            return instance;
        }
    }
}

function __wbg_get_imports() {
    const imports = {};
    imports.wbg = {};
    imports.wbg.__wbindgen_object_drop_ref = function(arg0) {
        takeObject(arg0);
    };
    imports.wbg.__wbg_log_b78d654f19d681e0 = function(arg0, arg1) {
        console.log(getStringFromWasm0(arg0, arg1));
    };
    imports.wbg.__wbg_loadAllFromIndexedDb_05f8df9a19c8d344 = function(arg0, arg1, arg2, arg3, arg4) {
        const ret = loadAllFromIndexedDb(getStringFromWasm0(arg0, arg1), getStringFromWasm0(arg2, arg3), getObject(arg4));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_flushPendingWrites_dd4341a0dafcf428 = function() {
        const ret = flushPendingWrites();
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_string_new = function(arg0, arg1) {
        const ret = getStringFromWasm0(arg0, arg1);
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_cozodb_new = function(arg0) {
        const ret = CozoDb.__wrap(arg0);
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_cb_drop = function(arg0) {
        const obj = takeObject(arg0).original;
        if (obj.cnt-- == 1) {
            obj.a = 0;
            return true;
        }
        const ret = false;
        return ret;
    };
    imports.wbg.__wbg_new_abda76e883ba8a5f = function() {
        const ret = new Error();
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_stack_658279fe44541cf6 = function(arg0, arg1) {
        const ret = getObject(arg1).stack;
        const ptr1 = passStringToWasm0(ret, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);
        const len1 = WASM_VECTOR_LEN;
        getInt32Memory0()[arg0 / 4 + 1] = len1;
        getInt32Memory0()[arg0 / 4 + 0] = ptr1;
    };
    imports.wbg.__wbg_error_f851667af71bcfc6 = function(arg0, arg1) {
        let deferred0_0;
        let deferred0_1;
        try {
            deferred0_0 = arg0;
            deferred0_1 = arg1;
            console.error(getStringFromWasm0(arg0, arg1));
        } finally {
            wasm.__wbindgen_free(deferred0_0, deferred0_1, 1);
        }
    };
    imports.wbg.__wbg_setWriteCounter_295838a9805b3542 = function(arg0) {
        setWriteCounter(arg0 >>> 0);
    };
    imports.wbg.__wbg_writeToIndexedDb_ec8ba47108ce4d3a = function(arg0, arg1) {
        const ret = writeToIndexedDb(getObject(arg0), getObject(arg1));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_crypto_c48a774b022d20ac = function(arg0) {
        const ret = getObject(arg0).crypto;
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_is_object = function(arg0) {
        const val = getObject(arg0);
        const ret = typeof(val) === 'object' && val !== null;
        return ret;
    };
    imports.wbg.__wbg_process_298734cf255a885d = function(arg0) {
        const ret = getObject(arg0).process;
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_versions_e2e78e134e3e5d01 = function(arg0) {
        const ret = getObject(arg0).versions;
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_node_1cd7a5d853dbea79 = function(arg0) {
        const ret = getObject(arg0).node;
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_is_string = function(arg0) {
        const ret = typeof(getObject(arg0)) === 'string';
        return ret;
    };
    imports.wbg.__wbg_msCrypto_bcb970640f50a1e8 = function(arg0) {
        const ret = getObject(arg0).msCrypto;
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_require_8f08ceecec0f4fee = function() { return handleError(function () {
        const ret = module.require;
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbindgen_is_function = function(arg0) {
        const ret = typeof(getObject(arg0)) === 'function';
        return ret;
    };
    imports.wbg.__wbg_randomFillSync_dc1e9a60c158336d = function() { return handleError(function (arg0, arg1) {
        getObject(arg0).randomFillSync(takeObject(arg1));
    }, arguments) };
    imports.wbg.__wbg_getRandomValues_37fa2ca9e4e07fab = function() { return handleError(function (arg0, arg1) {
        getObject(arg0).getRandomValues(getObject(arg1));
    }, arguments) };
    imports.wbg.__wbg_get_44be0491f933a435 = function(arg0, arg1) {
        const ret = getObject(arg0)[arg1 >>> 0];
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_length_fff51ee6522a1a18 = function(arg0) {
        const ret = getObject(arg0).length;
        return ret;
    };
    imports.wbg.__wbg_newnoargs_581967eacc0e2604 = function(arg0, arg1) {
        const ret = new Function(getStringFromWasm0(arg0, arg1));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_call_cb65541d95d71282 = function() { return handleError(function (arg0, arg1) {
        const ret = getObject(arg0).call(getObject(arg1));
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbindgen_object_clone_ref = function(arg0) {
        const ret = getObject(arg0);
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_self_1ff1d729e9aae938 = function() { return handleError(function () {
        const ret = self.self;
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbg_window_5f4faef6c12b79ec = function() { return handleError(function () {
        const ret = window.window;
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbg_globalThis_1d39714405582d3c = function() { return handleError(function () {
        const ret = globalThis.globalThis;
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbg_global_651f05c6a0944d1c = function() { return handleError(function () {
        const ret = global.global;
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbindgen_is_undefined = function(arg0) {
        const ret = getObject(arg0) === undefined;
        return ret;
    };
    imports.wbg.__wbg_isArray_4c24b343cb13cfb1 = function(arg0) {
        const ret = Array.isArray(getObject(arg0));
        return ret;
    };
    imports.wbg.__wbg_call_01734de55d61e11d = function() { return handleError(function (arg0, arg1, arg2) {
        const ret = getObject(arg0).call(getObject(arg1), getObject(arg2));
        return addHeapObject(ret);
    }, arguments) };
    imports.wbg.__wbg_now_9c5990bda04c7e53 = function() {
        const ret = Date.now();
        return ret;
    };
    imports.wbg.__wbg_new_43f1b47c28813cbd = function(arg0, arg1) {
        try {
            var state0 = {a: arg0, b: arg1};
            var cb0 = (arg0, arg1) => {
                const a = state0.a;
                state0.a = 0;
                try {
                    return __wbg_adapter_76(a, state0.b, arg0, arg1);
                } finally {
                    state0.a = a;
                }
            };
            const ret = new Promise(cb0);
            return addHeapObject(ret);
        } finally {
            state0.a = state0.b = 0;
        }
    };
    imports.wbg.__wbg_resolve_53698b95aaf7fcf8 = function(arg0) {
        const ret = Promise.resolve(getObject(arg0));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_then_f7e06ee3c11698eb = function(arg0, arg1) {
        const ret = getObject(arg0).then(getObject(arg1));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_then_b2267541e2a73865 = function(arg0, arg1, arg2) {
        const ret = getObject(arg0).then(getObject(arg1), getObject(arg2));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_buffer_085ec1f694018c4f = function(arg0) {
        const ret = getObject(arg0).buffer;
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_newwithbyteoffsetandlength_6da8e527659b86aa = function(arg0, arg1, arg2) {
        const ret = new Uint8Array(getObject(arg0), arg1 >>> 0, arg2 >>> 0);
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_new_8125e318e6245eed = function(arg0) {
        const ret = new Uint8Array(getObject(arg0));
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_set_5cf90238115182c3 = function(arg0, arg1, arg2) {
        getObject(arg0).set(getObject(arg1), arg2 >>> 0);
    };
    imports.wbg.__wbg_length_72e2208bbc0efc61 = function(arg0) {
        const ret = getObject(arg0).length;
        return ret;
    };
    imports.wbg.__wbg_instanceof_Uint8Array_d8d9cb2b8e8ac1d4 = function(arg0) {
        let result;
        try {
            result = getObject(arg0) instanceof Uint8Array;
        } catch {
            result = false;
        }
        const ret = result;
        return ret;
    };
    imports.wbg.__wbg_newwithlength_e5d69174d6984cd7 = function(arg0) {
        const ret = new Uint8Array(arg0 >>> 0);
        return addHeapObject(ret);
    };
    imports.wbg.__wbg_subarray_13db269f57aa838d = function(arg0, arg1, arg2) {
        const ret = getObject(arg0).subarray(arg1 >>> 0, arg2 >>> 0);
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_throw = function(arg0, arg1) {
        throw new Error(getStringFromWasm0(arg0, arg1));
    };
    imports.wbg.__wbindgen_memory = function() {
        const ret = wasm.memory;
        return addHeapObject(ret);
    };
    imports.wbg.__wbindgen_closure_wrapper186 = function(arg0, arg1, arg2) {
        const ret = makeMutClosure(arg0, arg1, 87, __wbg_adapter_22);
        return addHeapObject(ret);
    };

    return imports;
}

function __wbg_init_memory(imports, maybe_memory) {

}

function __wbg_finalize_init(instance, module) {
    wasm = instance.exports;
    __wbg_init.__wbindgen_wasm_module = module;
    cachedInt32Memory0 = null;
    cachedUint8Memory0 = null;


    return wasm;
}

function initSync(module) {
    if (wasm !== undefined) return wasm;

    const imports = __wbg_get_imports();

    __wbg_init_memory(imports);

    if (!(module instanceof WebAssembly.Module)) {
        module = new WebAssembly.Module(module);
    }

    const instance = new WebAssembly.Instance(module, imports);

    return __wbg_finalize_init(instance, module);
}

async function __wbg_init(input) {
    if (wasm !== undefined) return wasm;

    if (typeof input === 'undefined') {
        input = new URL('cyb_cozo_lib_wasm_bg.wasm', import.meta.url);
    }
    const imports = __wbg_get_imports();

    if (typeof input === 'string' || (typeof Request === 'function' && input instanceof Request) || (typeof URL === 'function' && input instanceof URL)) {
        input = fetch(input);
    }

    __wbg_init_memory(imports);

    const { instance, module } = await __wbg_load(await input, imports);

    return __wbg_finalize_init(instance, module);
}

export { initSync }
export default __wbg_init;
--- END OF FILE: tools\cozo_lib_wasm.js ---

--- START OF FILE: tools\decode_cozo_blob.py ---
#!/usr/bin/env python3
"""Quick decoder for CozoDB blob files to try to recover JSON/relations.

Usage: python tools\decode_cozo_blob.py C:\path\to\cozo_blob_0.bin C:\path\to\cozo_blob_1.bin

It attempts: UTF-8 decode, zlib.inflate, gzip, zstd (if installed), base64 decode, and searches for JSON-like markers.
"""
import sys
import json
import zlib
import gzip
import base64
import bz2
import lzma
from pathlib import Path

# Optional compressors
try:
    import zstandard as zstd
except Exception:
    zstd = None

try:
    import brotli
except Exception:
    brotli = None

try:
    import lz4.frame as lz4frame
except Exception:
    lz4frame = None


def hexdump(b, length=64):
    return ' '.join(f"{x:02x}" for x in b[:length])


def try_utf8(b):
    try:
        s = b.decode('utf-8', errors='replace')
        return s
    except Exception:
        return None


def try_zlib(b):
    try:
        return zlib.decompress(b)
    except Exception:
        return None


def try_gzip(b):
    try:
        return gzip.decompress(b)
    except Exception:
        return None


def try_base64(b):
    try:
        s = b.decode('ascii', errors='ignore').strip()
        s2 = ''.join(s.split())
        dec = base64.b64decode(s2)
        return dec
    except Exception:
        return None


def try_bz2(b):
    try:
        return bz2.decompress(b)
    except Exception:
        return None


def try_lzma(b):
    try:
        return lzma.decompress(b)
    except Exception:
        return None


def try_brotli(b):
    if not brotli:
        return None
    try:
        return brotli.decompress(b)
    except Exception:
        return None


def try_lz4(b):
    if not lz4frame:
        return None
    try:
        return lz4frame.decompress(b)
    except Exception:
        return None


def try_zstd(b):
    if not zstd:
        return None
    try:
        dctx = zstd.ZstdDecompressor()
        return dctx.decompress(b)
    except Exception:
        return None


def find_json_in_bytes(b):
    try:
        txt = b.decode('utf-8', errors='ignore')
    except Exception:
        txt = None
    if not txt:
        return None
    idx = txt.find('{')
    if idx >= 0:
        # Try progressive parse
        for end in range(idx+100, min(len(txt), idx+20000), 100):
            try:
                candidate = txt[idx:end]
                parsed = json.loads(candidate)
                return parsed
            except Exception:
                continue
    if 'relations' in txt or 'memory' in txt or 'NamedRows' in txt:
        return txt
    return None


def analyze(path: Path):
    print(f"\n=== Analyzing: {path} ===")
    b = path.read_bytes()
    print(f"Size: {len(b)} bytes")
    print("Hex (first 64 bytes):", hexdump(b, 64))

    s = try_utf8(b)
    if s and ('{' in s or 'relations' in s or 'memory' in s or 'NamedRows' in s):
        print("\n-- UTF-8 text looks promising (preview):\n")
        print(s[:2000])
        return

    z = try_zlib(b)
    if z:
        print("\n-- zlib decompressed (first 2000 chars):\n")
        try:
            txt = z.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(z[:200]))
        return

    g = try_gzip(b)
    if g:
        print("\n-- gzip decompressed (first 2000 chars):\n")
        try:
            txt = g.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(g[:200]))
        return

    zd = try_zstd(b)
    if zd:
        print("\n-- zstd decompressed (first 2000 chars):\n")
        try:
            txt = zd.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(zd[:200]))
        return

    bz = try_bz2(b)
    if bz:
        print("\n-- bz2 decompressed (first 2000 chars):\n")
        try:
            txt = bz.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(bz[:200]))
        return

    lz = try_lzma(b)
    if lz:
        print("\n-- lzma decompressed (first 2000 chars):\n")
        try:
            txt = lz.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(lz[:200]))
        return

    br = try_brotli(b)
    if br:
        print("\n-- brotli decompressed (first 2000 chars):\n")
        try:
            txt = br.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(br[:200]))
        return

    l4 = try_lz4(b)
    if l4:
        print("\n-- lz4 decompressed (first 2000 chars):\n")
        try:
            txt = l4.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(l4[:200]))
        return

    bb = try_base64(b)
    if bb:
        print("\n-- base64 decoded (preview):\n")
        try:
            txt = bb.decode('utf-8', errors='ignore')
            print(txt[:2000])
        except Exception:
            print(repr(bb[:200]))
        return

    found = find_json_in_bytes(b)
    if found:
        print("\n-- Found JSON-like content:\n")
        if isinstance(found, str):
            print(found[:2000])
        else:
            print(json.dumps(found, indent=2)[:4000])
        return

    out = path.with_suffix(path.suffix + '.raw')
    out.write_bytes(b)
    print(f"\nNo decode succeeded. Wrote raw blob to {out}")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print('Usage: python tools\\decode_cozo_blob.py C:\\path\\to\\blob1 [blob2 ...]')
        sys.exit(1)
    for p in sys.argv[1:]:
        analyze(Path(p))
--- END OF FILE: tools\decode_cozo_blob.py ---

--- START OF FILE: tools\eyes.py ---
import argparse
import requests
import sys
import os

def ingest(content, source_type="text", adapter="eyes-cli"):
    url = "http://localhost:8000/archivist/ingest"
    headers = {
        "Content-Type": "application/json",
        "X-API-Key": "ece-secret-key" 
    }
    payload = {
        "content": content,
        "type": source_type,
        "adapter": adapter
    }
    
    print(f"Sending to {url}...")
    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        print(f"‚úÖ Success: {response.json()}")
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"Details: {e.response.text}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Sovereign Eyes - Ingest content into ECE Memory")
    parser.add_argument("input", help="File path or text content to ingest")
    parser.add_argument("--type", default="text", help="Source type (text, web_page, etc.)")
    parser.add_argument("--adapter", default="eyes-cli", help="Adapter name")
    
    args = parser.parse_args()
    
    content = args.input
    
    # Check if input is a file
    if os.path.exists(args.input):
        try:
            with open(args.input, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"üìñ Read content from file: {args.input}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not read file '{args.input}', treating as raw text.")
            
    ingest(content, args.type, args.adapter)

if __name__ == "__main__":
    main()
--- END OF FILE: tools\eyes.py ---

--- START OF FILE: tools\index.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sovereign Console | Launcher</title>
    <style>
        :root {
            --bg-color: #1a1a1a;
            --card-bg: #2d2d2d;
            --accent: #0078d4;
            --text: #ffffff;
            --secondary-text: #aaaaaa;
        }

        body {
            font-family: 'Segoe UI', system-ui, sans-serif;
            background-color: var(--bg-color);
            color: var(--text);
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
        }

        h1 {
            font-weight: 300;
            margin-bottom: 40px;
            letter-spacing: 2px;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            width: 100%;
            max-width: 900px;
            padding: 20px;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 12px;
            padding: 30px;
            text-decoration: none;
            color: var(--text);
            transition: transform 0.2s, box-shadow 0.2s;
            border: 1px solid #333;
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.4);
            border-color: var(--accent);
        }

        .icon {
            font-size: 48px;
            margin-bottom: 20px;
        }

        .title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 10px;
        }

        .desc {
            font-size: 0.9rem;
            color: var(--secondary-text);
        }

        .status {
            margin-top: 15px;
            font-size: 0.8rem;
            padding: 4px 8px;
            border-radius: 4px;
            background: #333;
            color: #888;
        }

        .card:hover .status {
            background: rgba(0, 120, 212, 0.2);
            color: var(--accent);
        }
    </style>
</head>

<body>

    <h1>SOVEREIGN <span style="color:var(--accent); font-weight:600;">CONSOLE</span></h1>

    <div class="grid">
        <a href="model-server-chat.html" class="card">
            <div class="icon">üí¨</div>
            <div class="title">Sovereign Chat</div>
            <div class="desc">The core WebGPU interface. Run LLMs locally with memory integration.</div>
            <div class="status">Active</div>
        </a>

        <a href="log-viewer.html" class="card">
            <div class="icon">üìä</div>
            <div class="title">Log Viewer</div>
            <div class="desc">Monitor engine performance, VRAM usage, and debug events.</div>
            <div class="status">Utility</div>
        </a>

        <a href="sovereign-db-builder.html" class="card">
            <div class="icon">üß†</div>
            <div class="title">Memory Builder</div>
            <div class="desc">Manage and build the CozoDB memory store.</div>
            <div class="status">Admin</div>
        </a>

        <a href="root-mic.html" class="card">
            <div class="icon">üéôÔ∏è</div>
            <div class="title">Root Mic</div>
            <div class="desc">Dictate directly to the engine using Whisper.</div>
            <div class="status">Input</div>
        </a>

        <a href="root-dreamer.html" class="card">
            <div class="icon">üåô</div>
            <div class="title">Root Dreamer</div>
            <div class="desc">The synthetic subconscious. Background memory consolidation and association.</div>
            <div class="status">Subconscious</div>
        </a>
    </div>

    <div style="margin-top: 50px; color: #555; font-size: 0.8rem;">
        Running on Localhost ‚Ä¢ WebGPU Enabled
    </div>

</body>

</html>
--- END OF FILE: tools\index.html ---

--- START OF FILE: tools\indexeddb.js ---
let db = null;
let cozoDbStore = null;
let writeCounter = 0;
let writeCallback = null;

let cmdFlag = false;

export function setWriteCounter(count) {
  writeCounter = count;
}

function storeRequestToPromise(req) {
  return new Promise((resolve, reject) => {
    req.onsuccess = () => resolve(req.result);
    req.onerror = (e) => reject(e.error);
  });
}

async function openDatabase(dbName, storeName) {
  cozoDbStore = storeName;

  return new Promise((resolve, reject) => {
    const request = indexedDB.open(dbName, 1);
    request.onupgradeneeded = function (event) {
      const db = event.target.result;
      if (!db.objectStoreNames.contains(storeName)) {
        db.createObjectStore(storeName);
      }
    };

    request.onsuccess = function (event) {
      db = event.target.result;
      resolve(db);
    };
    request.onerror = function (event) {
      reject(event.error);
    };
  });
}

async function readStore() {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction(cozoDbStore, "readonly");
    const store = transaction.objectStore(cozoDbStore);

    const itemsPromise = storeRequestToPromise(store.getAll());
    const keysPromise = storeRequestToPromise(store.getAllKeys());

    Promise.all([keysPromise, itemsPromise])
      .then((results) => {
        const keys = results[0].map((item) => new Uint8Array(item));
        const items = results[1];
        resolve([keys, items]);
      })
      .catch(reject);
  });
}

export async function flushPendingWrites(timeoutDuration = 60000) {
  let timeout = null;

  // allow only one command runnig at a time
  if (cmdFlag) {
    await new Promise((resolve, reject) => {
      const interval = setInterval(() => {
        if (!cmdFlag) {
          clearInterval(interval);
          resolve();
        }
      }, 10);
    });
  }

  cmdFlag = true;

  const waitPromise = new Promise((resolve, reject) => {
    const interval = setInterval(() => {
      if (writeCounter <= 0) {
        if (timeout) {
          clearTimeout(timeout);
        }
        clearInterval(interval);
        resolve();
      }
    }, 10);
  });

  const timeoutPromise = new Promise((_, reject) => {
    timeout = setTimeout(() => {
      reject(new Error("waitForPendingWrites timed out!"));
    }, timeoutDuration);
  });

  // wait until all pending writes are done
  return Promise.race([waitPromise, timeoutPromise]).finally(() => {
    cmdFlag = false;
  });
}

export async function loadAllFromIndexedDb(dbName, storeName, onWriteCallback) {
  writeCallback = onWriteCallback;
  await openDatabase(dbName, storeName);
  return await readStore();
}

export async function writeToIndexedDb(key, value) {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction(cozoDbStore, "readwrite");
    const store = transaction.objectStore(cozoDbStore);
    const request = value ? store.put(value, key) : store.delete(key);
    return storeRequestToPromise(request)
      .then(resolve)
      .catch(reject)
      .finally(() => {
        writeCounter--;
        writeCallback && writeCallback(writeCounter);
      });
  });
}

export function closeDatabase() {
  if (db) {
    db.close();
    db = null;
  }
}

export async function clearIndexedDbStore(dbName, storeName) {
  // Ensure we open a connection first if one isn't open
  if (!db) {
    await openDatabase(dbName, storeName);
  }
  return new Promise((resolve, reject) => {
    const transaction = db.transaction(storeName, "readwrite");
    const store = transaction.objectStore(storeName);
    const request = store.clear();

    request.onsuccess = () => resolve();
    request.onerror = (e) => reject(e.target.error);
  });
}
--- END OF FILE: tools\indexeddb.js ---

--- START OF FILE: tools\log-viewer.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Sovereign System Debugger</title>
    <style>
        body {
            background-color: #000;
            color: #0f0;
            font-family: 'Consolas', 'Monaco', monospace;
            margin: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
            overflow: hidden;
        }

        #toolbar {
            background-color: #111;
            padding: 10px;
            border-bottom: 1px solid #333;
            display: flex;
            gap: 10px;
            align-items: center;
        }

        button {
            background: #333;
            color: #fff;
            border: 1px solid #444;
            cursor: pointer;
            padding: 5px 10px;
        }

        button:hover {
            background: #444;
        }

        #main-container {
            display: flex;
            flex: 1;
            overflow: hidden;
        }

        .panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #333;
            min-width: 0;
        }

        .panel:last-child {
            border-right: none;
        }

        .panel-header {
            background: #1a1a1a;
            padding: 5px 10px;
            font-weight: bold;
            border-bottom: 1px solid #333;
            display: flex;
            justify-content: space-between;
        }

        .log-container {
            flex: 1;
            overflow-y: auto;
            padding: 10px;
            white-space: pre-wrap;
            font-size: 12px;
        }

        .log-line {
            padding: 2px 0;
            border-bottom: 1px solid #111;
            word-break: break-all;
        }

        .log-line:hover {
            background-color: #111;
        }

        .info {
            color: #88ccff;
        }

        .warning {
            color: #ffcc00;
        }

        .error {
            color: #ff4444;
            font-weight: bold;
        }

        .debug {
            color: #888;
        }

        .success {
            color: #00ff00;
        }

        /* Context specific styles */
        .context-block {
            border: 1px solid #333;
            margin-bottom: 10px;
            padding: 5px;
            background: #050505;
            font-size: 11px;
        }

        .context-header {
            color: #aaa;
            border-bottom: 1px solid #333;
            margin-bottom: 5px;
            font-weight: bold;
        }
    </style>
</head>

<body>
    <div id="toolbar">
        <span style="color: #88ccff; font-weight: bold;">Sovereign Broadcast Receiver</span>
        <span id="connection-status" style="color: #888; margin-left: 10px;">‚óè Listening...</span>
        <div style="flex: 1;"></div>
        <button id="copy-all-btn" title="Copy all logs to clipboard">üìã Copy All</button>
        <button id="retry-logs-btn" title="Retry fetching backend logs">Retry Backend Logs</button>
        <button id="clear-btn">Clear All</button>
    </div>

    <div id="main-container">
        <!-- Panel 1: System Logs -->
        <div class="panel">
            <div class="panel-header">
                <span>System Logs</span>
            </div>
            <div id="system-logs" class="log-container"></div>
        </div>

        <!-- Panel 2: Chat Stream -->
        <div class="panel">
            <div class="panel-header">
                <span>Chat Stream</span>
            </div>
            <div id="chat-logs" class="log-container"></div>
        </div>

        <!-- Panel 3: Context Inspector -->
        <div class="panel">
            <div class="panel-header">
                <span>Context / Memory</span>
            </div>
            <div id="context-view" class="log-container"></div>
        </div>
    </div>

    <script>
        const systemContainer = document.getElementById('system-logs');
        const chatContainer = document.getElementById('chat-logs');
        const contextContainer = document.getElementById('context-view');
        const clearBtn = document.getElementById('clear-btn');
        const copyBtn = document.getElementById('copy-all-btn');
        const statusIndicator = document.getElementById('connection-status');

        copyBtn.onclick = () => {
            const allText = [
                "--- SYSTEM LOGS ---", systemContainer.innerText,
                "\n--- CHAT LOGS ---", chatContainer.innerText,
                "\n--- CONTEXT LOGS ---", contextContainer.innerText
            ].join("\n");

            navigator.clipboard.writeText(allText).then(() => {
                const original = copyBtn.innerHTML;
                copyBtn.innerHTML = "‚úÖ Copied!";
                setTimeout(() => copyBtn.innerHTML = original, 2000);
            }).catch(e => alert("Copy failed: " + e));
        };

        const logChannel = new BroadcastChannel('sovereign-logs');
        const codaChannel = new BroadcastChannel('coda_logs');
        // Simple de-duplication for server polling
        const serverLogSet = new Set();

        function markActive() {
            statusIndicator.style.color = '#0f0';
            statusIndicator.innerText = '‚óè Active';
            setTimeout(() => {
                statusIndicator.style.color = '#888';
                statusIndicator.innerText = '‚óè Listening...';
            }, 2000);
        }

        logChannel.onmessage = (event) => {
            markActive();
            const data = event.data;

            if (data.source === 'system') {
                appendLog(systemContainer, `[${data.time}] [${data.type}] ${data.msg}`, data.type);
            } else if (data.source === 'chat') {
                appendLog(chatContainer, `[${data.time}] ${data.role}: ${data.text}`, 'info');
                if (data.context) {
                    appendContext(data.context);
                }
            }
        };

        // Mission Control channel (coda_logs) - short JSON messages
        codaChannel.onmessage = (event) => {
            markActive();
            const data = event.data;
            try {
                // Known sources: 'Sovereign-Console' | 'Sovereign-Chat' | 'Sovereign-DB' | 'Sovereign-Embed'
                if (data.source === 'Sovereign-Console') {
                    appendLog(systemContainer, `[${new Date().toLocaleTimeString()}] [SOVEREIGN] ${data.message}`, data.type || 'info');
                } else if (data.source === 'Sovereign-Chat') {
                    appendLog(chatContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');
                    if (data.context) appendContext(data.context);
                } else if (data.source === 'Sovereign-DB') {
                    appendLog(systemContainer, `[${new Date().toLocaleTimeString()}] [DB] ${data.message}`, data.type || 'info');
                } else if (data.source === 'Sovereign-Embed' || data.source === 'WebGPU-Embed') {
                    appendLog(systemContainer, `[${new Date().toLocaleTimeString()}] [EMBED] ${data.message}`, 'debug');
                } else if (data.source === 'WebGPU-Chat') {
                    appendLog(chatContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');
                } else {
                    appendLog(systemContainer, `[${new Date().toLocaleTimeString()}] ${JSON.stringify(data)}`, 'debug');
                }
            } catch (e) {
                appendLog(systemContainer, `[coda_logs parsing error] ${e.message}`, 'error');
            }
        };

        // Poll backend server logs every 2s with graceful 404 handling and retry
        let pollIntervalId = null;
        let serverLogsErrorShown = false;

        async function pollServerLogs() {
            try {
                // Use relative path so it targets same origin when proxied; fall back to explicit host if needed
                const resp = await fetch('http://localhost:8080/audit/server-logs?limit=50');

                if (resp.status === 404) {
                    if (!serverLogsErrorShown) {
                        appendLog(systemContainer, `[ECE-Core] Backend logs endpoint returned 404. Is the backend running?`, 'warning');
                        statusIndicator.style.color = '#ffcc00';
                        statusIndicator.innerText = '‚óè Backend logs unavailable';
                        serverLogsErrorShown = true;
                    }
                    // Stop polling to avoid tight 404 loops
                    if (pollIntervalId) {
                        clearInterval(pollIntervalId);
                        pollIntervalId = null;
                    }
                    return;
                }

                if (!resp.ok) {
                    appendLog(systemContainer, `[ECE-Core] Poll error: HTTP ${resp.status}`, 'error');
                    return;
                }

                const payload = await resp.json();
                if (payload && Array.isArray(payload.logs)) {
                    for (const line of payload.logs) {
                        if (!serverLogSet.has(line)) {
                            serverLogSet.add(line);
                            appendLog(systemContainer, `[ECE-Core] ${line}`, 'success');
                        }
                    }
                    // Prevent unbounded growth
                    if (serverLogSet.size > 500) {
                        serverLogSet.clear();
                    }
                }
            } catch (e) {
                if (!serverLogsErrorShown) {
                    appendLog(systemContainer, `[ECE-Core] Poll error: ${e.message}`, 'error');
                    statusIndicator.style.color = '#ff4444';
                    statusIndicator.innerText = '‚óè Poll error';
                    serverLogsErrorShown = true;
                }
            }
        }

        function startPolling() {
            if (pollIntervalId) return;
            serverLogsErrorShown = false;
            statusIndicator.style.color = '#0f0';
            statusIndicator.innerText = '‚óè Active';
            pollIntervalId = setInterval(pollServerLogs, 2000);
            pollServerLogs();
        }

        function stopPolling() {
            if (pollIntervalId) {
                clearInterval(pollIntervalId);
                pollIntervalId = null;
            }
        }

        // Start polling initially
        startPolling();

        // Retry button
        document.getElementById('retry-logs-btn').addEventListener('click', () => {
            appendLog(systemContainer, '[ECE-Core] Manual retry requested', 'info');
            statusIndicator.style.color = '#88ccff';
            statusIndicator.innerText = '‚óè Retrying...';
            startPolling();
        });

        function appendLog(container, text, type) {
            const div = document.createElement('div');
            div.className = 'log-line';
            div.innerHTML = colorize(text, type);
            container.appendChild(div);
            container.scrollTop = container.scrollHeight;
        }

        function appendContext(contextData) {
            const div = document.createElement('div');
            div.className = 'context-block';
            div.innerHTML = `<div class="context-header">Context Update</div><pre>${JSON.stringify(contextData, null, 2)}</pre>`;
            contextContainer.appendChild(div);
            contextContainer.scrollTop = contextContainer.scrollHeight;
        }

        function colorize(line, type) {
            if (type === 'error' || line.includes('ERROR') || line.includes('‚ùå')) return `<span class="error">${line}</span>`;
            if (type === 'warn' || line.includes('WARNING')) return `<span class="warning">${line}</span>`;
            if (type === 'success' || line.includes('SUCCESS') || line.includes('‚úÖ')) return `<span class="success">${line}</span>`;
            if (type === 'info' || line.includes('INFO')) return `<span class="info">${line}</span>`;
            return `<span class="debug">${line}</span>`;
        }

        clearBtn.onclick = () => {
            systemContainer.innerHTML = '';
            chatContainer.innerHTML = '';
            contextContainer.innerHTML = '';
        };
    </script>
</body>

</html>
--- END OF FILE: tools\log-viewer.html ---

--- START OF FILE: tools\mobile-chat.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Sovereign Coda Mobile</title>
    <style>
        :root {
            --bg: #0f1115;
            --surface: #1a1d23;
            --primary: #3b82f6;
            --text: #e2e8f0;
            --text-dim: #94a3b8;
        }

        body {
            margin: 0;
            background: var(--bg);
            color: var(--text);
            font-family: -apple-system, system-ui, sans-serif;
            display: flex;
            flex-direction: column;
            height: 100vh;
            overflow: hidden;
        }

        /* Login Modal */
        #auth-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: var(--bg);
            display: flex;
            align-items: center;
            justify-content: center;
            z-index: 1000;
        }

        .card {
            background: var(--surface);
            padding: 2rem;
            border-radius: 12px;
            width: 90%;
            max-width: 320px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }

        input {
            width: 100%;
            padding: 12px;
            margin: 10px 0;
            border-radius: 8px;
            border: 1px solid #333;
            background: #000;
            color: white;
            font-size: 16px;
            box-sizing: border-box;
            /* Fix padding expanding width */
        }

        button {
            width: 100%;
            padding: 12px;
            border-radius: 8px;
            border: none;
            background: var(--primary);
            color: white;
            font-weight: bold;
            font-size: 16px;
            cursor: pointer;
        }

        button:active {
            opacity: 0.8;
            transform: scale(0.98);
        }

        /* Chat UI */
        header {
            padding: 15px;
            background: var(--surface);
            border-bottom: 1px solid #333;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        #chat-box {
            flex: 1;
            overflow-y: auto;
            padding: 15px;
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .msg {
            max-width: 85%;
            padding: 10px 14px;
            border-radius: 16px;
            line-height: 1.4;
            word-wrap: break-word;
        }

        .user {
            align-self: flex-end;
            background: var(--primary);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .assistant {
            align-self: flex-start;
            background: var(--surface);
            color: var(--text);
            border-bottom-left-radius: 4px;
        }

        .system {
            align-self: center;
            font-size: 0.8rem;
            color: var(--text-dim);
            background: transparent;
        }

        #input-area {
            padding: 10px;
            background: var(--surface);
            display: flex;
            gap: 10px;
        }
    </style>
</head>

<body>

    <!-- Auth Modal -->
    <div id="auth-overlay">
        <div class="card">
            <h2>üîí Secure Access</h2>
            <p style="color:var(--text-dim); margin-bottom:20px">Enter the bridge token from your PC console.</p>
            <input type="password" id="token-input" placeholder="Paste Token Here">
            <button onclick="saveToken()">Connect</button>
        </div>
    </div>

    <!-- Main Chat -->
    <header>
        <span style="font-weight:bold">Sovereign Coda</span>
        <button onclick="logout()" style="width:auto; padding:5px 10px; font-size:12px; background:#333">Logout</button>
    </header>

    <div id="chat-box"></div>

    <div id="input-area">
        <input type="text" id="msg-input" placeholder="Message..." onkeypress="handleKey(event)" autocomplete="off">
        <button onclick="send()" style="width:60px">‚û§</button>
    </div>

    <script>
        let TOKEN = localStorage.getItem('bridge_token');
        const DOM = {
            auth: document.getElementById('auth-overlay'),
            input: document.getElementById('msg-input'),
            box: document.getElementById('chat-box')
        };

        if (TOKEN) {
            DOM.auth.style.display = 'none';
        }

        function saveToken() {
            const val = document.getElementById('token-input').value.trim();
            if (val) {
                localStorage.setItem('bridge_token', val);
                TOKEN = val;
                DOM.auth.style.display = 'none';
            }
        }

        function logout() {
            localStorage.removeItem('bridge_token');
            location.reload();
        }

        function append(role, text) {
            const div = document.createElement('div');
            div.className = `msg ${role}`;
            div.innerText = text;
            DOM.box.appendChild(div);
            DOM.box.scrollTop = DOM.box.scrollHeight;
            return div;
        }

        function handleKey(e) {
            if (e.key === 'Enter') send();
        }

        async function send() {
            const text = DOM.input.value.trim();
            if (!text) return;

            DOM.input.value = '';
            append('user', text);

            // Create pending placeholder
            const loadingDiv = append('assistant', '...');

            try {
                const res = await fetch('/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${TOKEN}`
                    },
                    body: JSON.stringify({
                        model: "mobile-chat", // Bridge ignores this usually, or we can fetch models
                        messages: [{ role: "user", content: text }],
                        stream: false // Simple non-streaming for mobile stability first
                    })
                });

                if (res.status === 401) {
                    loadingDiv.innerText = "‚ùå Unauthorized. Check Token.";
                    logout(); // Force re-login
                    return;
                }

                if (!res.ok) {
                    throw new Error(`Server Error: ${res.status}`);
                }

                const data = await res.json();
                const reply = data.choices?.[0]?.message?.content || "No response.";
                loadingDiv.innerText = reply;

            } catch (e) {
                loadingDiv.innerText = `Error: ${e.message}`;
                loadingDiv.style.color = 'red';
            }
        }
    </script>
</body>

</html>
--- END OF FILE: tools\mobile-chat.html ---

--- START OF FILE: tools\model-server-chat.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Root Coda Console</title>
    <style>
        * {
            box-sizing: border-box;
        }

        body {
            background: #0f0f11;
            color: #ccc;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            height: 100vh;
            margin: 0;
        }

        /* --- Resizable Split Layout --- */
        #container {
            display: flex;
            width: 100%;
            height: 100vh;
            overflow: hidden;
        }

        #sidebar {
            width: 320px;
            /* Default width */
            min-width: 200px;
            max-width: 80%;
            background: #151517;
            color: #d4d4d4;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #333;
            transition: width 0.1s ease;
        }

        /* Resize Handle */
        #resizer {
            width: 5px;
            cursor: col-resize;
            background: #333;
            transition: background 0.2s;
            z-index: 10;
        }

        #resizer:hover,
        #resizer.resizing {
            background: #00ff88;
        }

        #main {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: #1e1e1e;
            position: relative;
            min-width: 0;
        }

        /* --- Collapsible Details --- */
        details {
            margin-bottom: 10px;
            border-bottom: 1px solid #333;
        }

        summary {
            padding: 10px;
            cursor: pointer;
            font-weight: bold;
            user-select: none;
            background: #252526;
            list-style: none;
        }

        summary::-webkit-details-marker {
            display: none;
        }

        summary::after {
            content: '‚ñº';
            float: right;
            font-size: 0.8em;
            transition: transform 0.2s;
        }

        details[open] summary::after {
            transform: rotate(180deg);
        }

        .panel-content {
            padding: 10px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        /* --- Chat Box --- */
        #chat-box {
            flex: 1;
            overflow-y: auto;
            margin-bottom: 20px;
            padding-right: 10px;
        }

        #chat-box::-webkit-scrollbar {
            width: 8px;
        }

        #chat-box::-webkit-scrollbar-track {
            background: #333;
        }

        #chat-box::-webkit-scrollbar-thumb {
            background: #555;
            border-radius: 4px;
        }

        .msg {
            padding: 12px;
            margin: 8px 0;
            border-radius: 6px;
            background: #333;
            max-width: 85%;
            word-wrap: break-word;
        }

        .user {
            background: #005f3b;
            /* Root Green-ish */
            color: white;
            align-self: flex-end;
            margin-left: auto;
        }

        .assistant {
            background: #2d2d2d;
            border-left: 3px solid #00ff88;
        }

        .msg details {
            margin-top: 10px;
            font-size: 0.85rem;
            opacity: 0.7;
            border: none;
        }

        .msg pre {
            background: #151515;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 0.8rem;
        }

        h3 {
            margin: 0 0 10px 0;
        }

        #status-text {
            font-size: 0.9rem;
            color: #888;
            margin-bottom: 8px;
        }

        #progress-bar {
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            margin: 8px 0;
        }

        #progress {
            height: 100%;
            background: #00ff88;
            width: 0%;
            transition: width 0.2s;
        }

        #status-log {
            flex: 1;
            overflow-y: auto;
            font-family: 'Consolas', monospace;
            font-size: 0.75rem;
            margin-top: 10px;
            padding: 8px;
            background: #0f0f0f;
            border-radius: 4px;
            border: 1px solid #333;
        }

        #status-log div {
            margin: 2px 0;
            padding: 2px 0;
        }

        .info {
            color: #888;
        }

        .warn {
            color: #ffc107;
        }

        .error {
            color: #ff4444;
        }

        .success {
            color: #00ff88;
        }

        #input-area {
            display: flex;
            gap: 10px;
        }

        textarea {
            flex: 1;
            height: 60px;
            background: #3c3c3c;
            border: 1px solid #555;
            color: white;
            padding: 10px;
            border-radius: 4px;
            resize: vertical;
            font-family: 'Segoe UI', sans-serif;
        }

        textarea:focus {
            outline: none;
            border-color: #00ff88;
        }

        button {
            padding: 8px 20px;
            background: #2d2d2d;
            color: white;
            border: 1px solid #444;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            align-self: flex-end;
            transition: all 0.2s;
        }

        button:hover {
            border-color: #00ff88;
            color: #00ff88;
        }

        button:disabled {
            background: #222;
            border-color: #333;
            color: #555;
            cursor: not-allowed;
        }

        .streaming {
            border-right: 2px solid #00ff88;
            animation: blink 0.7s infinite;
        }

        @keyframes blink {
            50% {
                border-color: transparent;
            }
        }

        /* --- Drag & Drop --- */
        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }

        #image-preview-container {
            display: none; /* Hidden by default */
            margin-bottom: 5px;
            padding: 8px;
            background: #252526;
            border-radius: 6px;
            border: 1px solid #444;
            position: relative;
            width: fit-content;
        }

        #image-preview-container .image-preview {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 5px;
        }

        #image-preview-container img {
            max-height: 100px;
            max-width: 200px;
            border-radius: 4px;
            display: block;
            object-fit: contain;
        }

        #image-preview-container div {
            font-size: small;
            margin-top: 5px;
        }

        #image-preview-container button {
            margin-top: 5px;
            padding: 4px 8px;
            background: #a43131;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
        }

        #input-area.drag-active {
            border: 2px dashed #00ff88;
            background: #1a1a1a;
        }
    </style>
</head>

<body>
    <div id="container">
        <div id="sidebar">
            <div
                style="padding: 10px; font-weight: bold; font-size: 16px; border-bottom: 1px solid #444; color: #00ff88;">
                ‚ö° ROOT CODA
            </div>
            <small id="status-text" style="padding: 0 10px;">Initializing...</small>
            <div id="progress-bar" style="margin: 8px 10px 15px 10px;">
                <div id="progress"></div>
            </div>

            <!-- COLLAPSIBLE: Model Selection -->
            <details open>
                <summary>Model Selection</summary>
                <div class="panel-content">
                    <label for="hw-profile"
                        style="display:block; font-size: 11px; color: #ccc; margin-bottom: 4px;">Hardware
                        Profile:</label>
                    <select id="hw-profile" class="form-control"
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555; margin-bottom: 10px;">
                        <option value="lite">üîã Lite (Mobile / Snapdragon) - 2k Context</option>
                        <option value="mid" selected>üíª Mid (8GB VRAM) - 4k Context</option>
                        <option value="high">üöÄ High (16GB VRAM) - 16k Context</option>
                        <option value="ultra">‚ö° Ultra (24GB+ VRAM) - 32k Context</option>
                    </select>
                    <select id="model-select" disabled
                        style="width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555;">
                        <option value="" disabled>-- Select a Model --</option>

                        <optgroup label="‚ú® SOTA (Latest)">
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 (7B Distill) [Verified]</option>
                            <option value="mlc-ai/Qwen3-4B-q4f16_1-MLC">Qwen 3 4B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen3-8B-q4f16_1-MLC">Qwen 3 8B (Base) [Verified]</option>
                            <option value="mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC" selected>Qwen 2.5 7B (Instruct) [Verified]</option>
                            <option value="mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC">Phi 3.5 Mini (3.8B)</option>
                        </optgroup>

                        <optgroup label="üëÅÔ∏è Vision Models">
                            <option value="mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC">Phi 3.5 Vision (4.2B) [Multimodal]</option>
                        </optgroup>

                        <optgroup label="üåü 14B Models">
                            <option value="mlc-ai/Qwen2.5-14B-Instruct-q4f16_1-MLC">Qwen 2.5 14B (Instruct)</option>
                            <option value="mlc-ai/DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC">DeepSeek R1 (14B Distill)</option>
                        </optgroup>

                        <optgroup label="üöÄ High Performance (Small)">
                            <option value="mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 1.5B (Instruct) [Lite Choice]</option>
                            <option value="mlc-ai/SmolLM2-1.7B-Instruct-q4f16_1-MLC">SmolLM2 1.7B (Instruct)</option>
                            <option value="mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama 3.2 1B (Instruct)</option>
                            <option value="mlc-ai/Qwen3-0.6B-q4f16_1-MLC">Qwen 3 0.6B (Base) [Micro]</option>
                        </optgroup>

                        <optgroup label="üß™ Experimental / Other">
                            <option value="mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B</option>
                            <option value="mlc-ai/gemma-2-2b-it-q4f16_1-MLC">Gemma 2 2B</option>
                            <option value="mlc-ai/TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B</option>
                        </optgroup>

                        <option value="custom">-- Custom --</option>
                    </select>
                    <input id="custom-model-input" type="text" placeholder="Custom model ID"
                        style="width: 100%; padding: 5px; font-size: 11px;" disabled />
                    <button id="load-model-btn" disabled
                        style="width: 100%; padding: 6px; margin-top:5px; font-size: 11px;">Load Model</button>
                    <div style="font-size: 9px; color: #888; margin-top: 5px;">
                        * Models marked with * may require checking availability in MLC-AI repository
                    </div>
                </div>
            </details>

            <!-- COLLAPSIBLE: Controls -->
            <details>
                <summary>System Controls</summary>
                <div class="panel-content">
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px;">
                        <input type="checkbox" id="enable-bridge-toggle" onchange="toggleBridge(this.checked)">
                        <label for="enable-bridge-toggle" style="font-size: 11px; cursor: pointer;">Enable Wave Bridge
                            (ws:8080)</label>
                        <div id="bridge-status" style="font-size: 10px; color: #666; margin-left: 20px;">Disconnected
                        </div>
                    </div>
                    <div
                        style="background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px; display: flex; align-items: center; gap: 8px;">
                        <input type="checkbox" id="autosave-toggle" checked onchange="toggleAutoSave(this.checked)">
                        <label for="autosave-toggle" style="font-size: 11px; cursor: pointer;">üíæ Auto-Save Memories</label>
                    </div>
                    <button id="clear-cache-btn"
                        style="width: 100%; padding: 6px; background: #a43131; border:none; margin-top:5px; font-size: 11px;">‚ö†Ô∏è
                        Delete Model Cache</button>
                    <button id="debug-gpu-btn"
                        style="width: 100%; padding: 6px; background: #555; border:none; margin-top: 5px; font-size: 11px;">‚ùì
                        Debug GPU</button>
                    <button id="force-unlock-btn"
                        style="width: 100%; padding: 6px; background: #da3633; border:none; margin-top: 5px; font-size: 11px; color: white;">üîì
                        Force Unlock GPU</button>
                </div>
            </details>

            <!-- COLLAPSIBLE: Logs -->
            <details open style="flex: 1; display: flex; flex-direction: column;">
                <summary>System Logs</summary>
                <div class="panel-content" style="flex: 1; display: flex; flex-direction: column; overflow: hidden;">
                    <button id="copy-logs-btn" style="width: 100%; padding: 4px; background: #333; font-size: 10px;">üìã
                        Copy Logs</button>
                    <div id="status-log"></div>
                </div>
            </details>
        </div>

        <div id="resizer"></div>

        <div id="main">
            <!-- Split view for Chat and Context -->
            <div style="flex: 1; display: flex; height: 100%; overflow: hidden;">
                <!-- Chat Column -->
                <div style="flex: 1; display: flex; flex-direction: column; border-right: 1px solid #333;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üí¨ Chat Stream</div>
                    <div id="chat-box"></div>
                    
                    <!-- Input Area with Drag & Drop -->
                    <div id="input-container" style="border-top: 1px solid #333; padding: 10px; display: flex; flex-direction: column;">
                        <div id="image-preview-container"></div>
                        <div id="input-area" style="display: flex; gap: 10px; padding: 5px; border: 2px dashed transparent; border-radius: 6px; transition: all 0.2s;">
                            <textarea id="input" disabled placeholder="Type or Drag & Drop Image..."></textarea>
                            <button id="send-btn" disabled>Send</button>
                        </div>
                    </div>
                </div>

                <!-- Context Column -->
                <div style="width: 40%; display: flex; flex-direction: column; background: #151515;">
                    <div style="padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;">
                        üß† Root Memory</div>
                    <div id="context-box"
                        style="flex: 1; overflow-y: auto; padding: 10px; font-family: monospace; font-size: 12px; white-space: pre-wrap; color: #aaa;">
                        <div style="text-align: center; margin-top: 50px; color: #555;">(Active retrievals will appear
                            here)</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // --- IMPORTS ---
        import { SovereignLogger, createStore, getWebGPUConfig, initCozo, GPUController } from './modules/sovereign.js';

        // Load hot reload functionality in development
        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {
            import('./modules/gpu-hot-reloader.js').then(() => {
                console.log('üîÑ GPU Hot Reloader loaded for development');
            }).catch(err => {
                console.warn('‚ö†Ô∏è GPU Hot Reloader not available:', err);
            });
        }
        import { CozoDb } from './cozo_lib_wasm.js';
        import { loadAllFromIndexedDb, writeToIndexedDb } from './indexeddb.js';
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';
        import { CreateWebWorkerMLCEngine } from "https://esm.run/@mlc-ai/web-llm";
        import { marked } from "https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js";
        import { VisionController } from './modules/vision.js';

        env.allowLocalModels = false;

        // --- ROOT KERNEL SETUP ---
        const logger = new SovereignLogger('Root-Console');

        const { state, subscribe } = createStore({
            status: "Initializing...",
            progress: 0,
            activeModel: null,
            autoSave: true
        });

        // --- UTILS: The Archivist ---
        async function saveTurn(role, content) {
            if (!state.autoSave || !window.db) return;
            try {
                const ts = Date.now();
                const id = 'msg_' + ts + '_' + role;

                // Write to Memory (Embedding null for Dreamer to fix later)
                const query = ":put memory { id: $id, timestamp: $ts, role: $role, content: $content, source: 'root_console', embedding: null }";
                await window.db.run(query, JSON.stringify({ id, ts, role, content }));
                ui.log(`üíæ Memory Saved (${role})`, "debug");
            } catch (e) {
                ui.log(`Save Failed: ${e.message}`, "error");
            }
        }

        // Expose toggle handler globally
        window.toggleAutoSave = (checked) => { 
            state.autoSave = checked; 
            ui.log(`Auto-Save: ${checked ? 'ON' : 'OFF'}`, 'info');
        };

        // Bridge legacy UI logging to Sovereign Logger
        const ui = {
            log: (msg, type = 'info') => {
                // Bridge to Kernel Logger
                if (type === 'debug') type = 'info';
                if (logger[type]) logger[type](msg); else logger.info(msg);

                // Update on-screen log
                const el = document.getElementById('status-log');
                if (el) {
                    const div = document.createElement('div');
                    div.className = type;
                    div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
                    div.style.fontSize = '11px';
                    div.style.padding = '2px';
                    div.style.borderBottom = '1px solid #222';
                    el.insertBefore(div, el.firstChild);
                    if (el.children.length > 100) el.removeChild(el.lastChild);
                }
            },
            updateProgress: (pct, text) => {
                const bar = document.getElementById('progress');
                if (bar) bar.style.width = (pct * 100) + "%";
                if (text) {
                    const el = document.getElementById('status-text');
                    if (el) el.innerText = text;
                }
            },
            append: (role, text, meta = null) => {
                const box = document.getElementById('chat-box');
                const div = document.createElement('div');
                div.className = `msg ${role}`;
                div.innerHTML = text ? marked.parse(text) : "";

                if (meta && meta.trace && meta.trace.length > 0) {
                    const details = document.createElement('details');
                    const summary = document.createElement('summary');
                    summary.textContent = 'üìã Reasoning Trace';
                    const pre = document.createElement('pre');
                    pre.textContent = JSON.stringify(meta.trace, null, 2);
                    details.appendChild(summary);
                    details.appendChild(pre);
                    div.appendChild(details);
                }

                box.appendChild(div);
                box.scrollTop = box.scrollHeight;

                return {
                    div,
                    update: (newText, isMarkdown = true) => {
                        div.innerHTML = isMarkdown ? marked.parse(newText) : newText;
                        box.scrollTop = box.scrollHeight;
                    },
                    appendText: (chunk) => {
                        if (div.innerText.endsWith("...")) div.innerText = div.innerText.slice(0, -3);
                        div.innerText += chunk;
                        box.scrollTop = box.scrollHeight;
                    }
                };
            },
            appendContext: (title, details) => {
                const box = document.getElementById('context-box');
                const div = document.createElement('div');
                div.style.borderBottom = '1px solid #333';
                div.style.marginBottom = '10px';
                div.style.paddingBottom = '10px';

                const h4 = document.createElement('div');
                h4.style.fontWeight = 'bold';
                h4.style.color = '#00ff88';
                h4.style.marginBottom = '5px';
                h4.textContent = `[${new Date().toLocaleTimeString()}] ${title}`;

                const p = document.createElement('div');
                p.style.whiteSpace = 'pre-wrap';
                p.textContent = details;

                div.appendChild(h4);
                div.appendChild(p);
                box.appendChild(div);
                box.scrollTop = box.scrollHeight;
            }
        };

        // --- GLOBAL STATE ---
        let db;
        let embedder;
        let engine;
        let contextManager;
        let selectedModelId = null;

        // VisionController will handle all image-related functionality

        // --- UTILS: Response Pattern Matcher ---
        class ResponsePattern {
            static match(response) {
                let data = response;
                if (typeof response === 'string') {
                    try {
                        const clean = response.replace(/```json/g, '').replace(/```/g, '').trim();
                        if (clean.startsWith('{')) data = JSON.parse(clean);
                    } catch (e) { }
                }
                if (data?.rows && Array.isArray(data.rows)) return { type: 'DB_RESULT', rows: data.rows, count: data.rows.length };
                if (data?.error || (typeof response === 'string' && response.toLowerCase().includes('error:'))) return { type: 'ERROR', error: data?.error || response };
                if (data?.ok === false && data?.message) return { type: 'ERROR', error: data.message };
                return { type: 'RAW_TEXT', text: typeof response === 'string' ? response : JSON.stringify(response) };
            }
        }

        // --- LOGIC: Context Manager (SFS) ---
        class ContextManager {
            constructor(engine, db) {
                this.engine = engine;
                this.db = db;
                this.maxIterations = 3;
            }

            // NEW: Pure retrieval logic (returns raw data objects)
            async findRelevantMemories(userText) {
                // 1. GENERATE QUERY VECTOR (Semantic)
                let vectorResults = [];
                if (embedder) {
                    try {
                        const output = await embedder(userText, { pooling: 'mean', normalize: true });
                        const queryVec = Array.from(output.data);
                        // Query: Find top 10 nearest neighbors
                        const vecQuery = `
                            ?[id, content, dist, timestamp] := *memory{id, content, embedding, timestamp},
                            !is_null(embedding),
                            dist = vec_l2(embedding, $vec)
                            :sort dist
                            :limit 10
                        `;
                        const res = await this.db.run(vecQuery, JSON.stringify({ vec: queryVec }));
                        const parsed = ResponsePattern.match(res);
                        if (parsed.rows) vectorResults = parsed.rows.map(r => ({ id: r[0], content: r[1], dist: r[2], ts: r[3], source: 'semantic' }));
                    } catch (e) { console.warn("Vector Search failed", e); }
                }

                // 2. GENERATE KEYWORD QUERY (Lexical)
                let keywordResults = [];
                const rawWords = userText.match(/[a-zA-Z0-9_\-]+/g) || [];
                const stopWords = new Set(['the', 'and', 'is', 'in', 'at', 'of', 'on', 'for', 'to', 'it', 'this', 'that', 'what', 'who', 'how', 'why', 'when', 'where', 'tell', 'me', 'about']);
                const keywords = rawWords.map(w => w.replace(/^[-_]+|[-_]+$/g, '')).filter(w => w.length > 3 && !stopWords.has(w.toLowerCase()));
                
                if (keywords.length > 0) {
                    const conditions = keywords.map(w => `regex_matches(content, '(?i)${w}')`).join(' or ');
                    const kwQuery = `?[id, content, timestamp] := *memory{id, content, timestamp}, ${conditions} :sort -timestamp :limit 10`;
                    try {
                        const res = await this.db.run(kwQuery, "{}");
                        const parsed = ResponsePattern.match(res);
                        if (parsed.rows) keywordResults = parsed.rows.map(r => ({ id: r[0], content: r[1], dist: 0, ts: r[2], source: 'lexical' }));
                    } catch (e) { console.warn(e); }
                }

                // 3. MERGE & DEDUPLICATE
                const combined = new Map();
                // Add Semantic first
                vectorResults.forEach(item => combined.set(item.content, item));
                // Add Lexical (only if unique)
                keywordResults.forEach(item => {
                    if (!combined.has(item.content)) combined.set(item.content, item);
                });
                
                // Return top 10 unique items
                return Array.from(combined.values()).slice(0, 10);
            }

            // EXISTING: Formats the data for the R1 Reasoning Loop
            async retrieveInitialContext(userText) {
                const topItems = await this.findRelevantMemories(userText);
                if (topItems.length === 0) return "";

                const paths = [];
                const clues = topItems.map((item, index) => {
                    const id = index + 1;
                    let title = "doc_" + id;
                    // Simple title extraction
                    if (item.content.length > 0) {
                        const safeTitle = item.content.substring(0, 30).replace(/[^a-zA-Z0-9 ]/g, '').trim().replace(/\s+/g, '_').toLowerCase();
                        if (safeTitle.length > 3) title = safeTitle;
                    }
                    const path = `/knowledge/${item.source}/${title}`;
                    paths.push(path);

                    let snippet = item.content.substring(0, 300).replace(/\n/g, ' ');
                    if (item.content.length > 300) snippet += "...";

                    return `[CLUE #${id}] [Path: ${path}] [Source: ${item.source || 'unknown'}]\nSnippet: "${snippet}"`;
                });
                
                ui.log(`‚úÖ Hybrid Retrieval: ${topItems.length} active clues.`, "success");
                const treeMap = `[CURRENT REALITY MAP]\n` + paths.map(p => `- ${p}`).join('\n');
                return `${treeMap}\n\n[CONTEXT CLUES]\n${clues.join('\n\n')}`;
            }

            // ... (Keep buildVirtualPrompt and executeR1Loop exactly as they were in combined_text.txt source 1976-1996)
            buildVirtualPrompt(systemPrompt, retrievedMemories, chatHistory, userText) {
                const now = new Date().toLocaleString('en-US', { timeZoneName: 'short' });
                const physicsInstruction = `
### COGNITIVE PROTOCOL:
1. **TRUST THE CLUES:** You are operating in a "Sovereign" environment. The [CONTEXT CLUES] provided are the Ground Truth.
2. **MINIMIZE ENTROPY:** If a Clue contains the answer, OUTPUT IT DIRECTLY. Do not generate "filler".
3. **PATH OF LEAST ACTION:** The most efficient answer is the correct one.
`;
                return `### SYSTEM TIME\n${now}\n\n### BACKGROUND INFORMATION\n${retrievedMemories ? retrievedMemories : "No specific context available."} \n\n### CHAT HISTORY\n${chatHistory.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\n')}\n\n### INSTRUCTION\n${systemPrompt}\n${physicsInstruction}\nBased on the Background Information above, answer the following question.\n\n### USER QUESTION\n${userText}`;
            }

            async executeR1Loop(userText, history) {
                let iteration = 0;
                let currentContext = await this.retrieveInitialContext(userText);
                if (currentContext) ui.appendContext("Reflex Retrieval", currentContext);
                while (iteration < this.maxIterations) {
                    iteration++;
                    const fullPrompt = this.buildVirtualPrompt(
                        "You are Root Coda, a Sovereign Agent.\nPROTOCOL: Trust clues. Answer directly. No refusals.",
                        currentContext,
                        history.slice(-5),
                        userText
                    );
                    ui.log(`üß† Reasoning (Iter ${iteration})...`, "info");
                    let reply = "";
                    try {
                        const response = await this.engine.chat.completions.create({
                            messages: [{ role: "user", content: fullPrompt }],
                            temperature: 0.1,
                            max_tokens: 100
                        });
                        reply = response.choices[0].message.content.trim();
                    } catch (genErr) {
                        const errMsg = genErr?.message || String(genErr);
                        if (errMsg.includes("disposed")) return { context: currentContext, finalAnswer: "‚ö†Ô∏è System Crash: GPU Driver lost." };
                        throw genErr;
                    }

                    if (!reply) reply = ""; 

                    if (reply.includes("NEED_CONTEXT:")) {
                        const searchTerm = reply.split("NEED_CONTEXT:")[1].trim();
                        ui.log(`ü§ñ Requested search: "${searchTerm}"`, "warn");
                        const extraData = await this.retrieveInitialContext(searchTerm);
                        if (extraData) {
                            currentContext += `\n--- Additional (${searchTerm}) ---\n${extraData}`;
                            ui.appendContext(`Requested: ${searchTerm}`, extraData);
                        }
                        continue;
                    }
                    return { context: currentContext, finalAnswer: reply };
                }
                return { context: currentContext, finalAnswer: null };
            }
        }

        // --- HANDLERS ---
        async function handleSend() {
            const input = document.getElementById('input');
            const text = input.value.trim();

            // Get image from VisionController
            const imageBase64 = vision ? vision.getImage() : null;

            // Allow sending if there is text OR an image
            if ((!text && !imageBase64) || !engine) return;

            input.value = "";
            input.disabled = true;
            document.getElementById('send-btn').disabled = true;

            // Display Logic
            if (imageBase64) {
                ui.append("user", `![Uploaded Image](${imageBase64})\n\n${text}`);
            } else {
                ui.append("user", text);
            }

            if (state.autoSave) await saveTurn("user", text + (imageBase64 ? " [Image Attached]" : ""));

            try {
                // Construct Message Payload
                let messages = [];
                let context = "";

                // Wrap GPU-heavy ops in lock
                await GPUController.withLock("Root-Console-Chat", async () => {
                    // If Image: Bypass R1 Loop (Vision models typically don't do R1 reasoning yet or complex context mixing)
                    // We use a direct shot for now.
                    if (imageBase64) {
                         messages = [
                            { role: "system", content: "You are a helpful assistant. Analyze the user's image and text." },
                            {
                                role: "user",
                                content: [
                                    { type: "text", text: text || "What is in this image?" },
                                    { type: "image_url", image_url: { url: imageBase64 } }
                                ]
                            }
                        ];
                        ui.log("üëÅÔ∏è Processing Visual Data...", "warn");
                    } else {
                        // Standard Text Path (R1 Loop)
                        const r1Result = await contextManager.executeR1Loop(text, []);
                        context = r1Result.context;
                        const finalAnswer = r1Result.finalAnswer;

                        if (finalAnswer && finalAnswer.includes("System Crash")) return ui.log("üõë Execution halted (GPU Crash).", "error");

                        ui.log("üß™ Synthesizing...", "warn");
                        const sysPrompt = `You are a helpful assistant with access to retrieved context. Use it to answer the user's question.\n\nCONTEXT:\n${context || "(No relevant context)"}`;
                        messages = [{ role: "system", content: sysPrompt }, { role: "user", content: text }];
                    }

                    const msgHandle = ui.append("assistant", "");

                    const stream = await engine.chat.completions.create({
                        messages: messages,
                        max_tokens: 1024,
                        temperature: 0.7,
                        stream: true
                    });

                    let fullAnswer = "";
                    for await (const chunk of stream) {
                        const delta = chunk.choices[0]?.delta?.content || "";
                        fullAnswer += delta;
                        msgHandle.update(fullAnswer);
                    }

                    // Cleanup - Clear the image from VisionController
                    if (imageBase64 && vision) {
                        vision.clear();
                    }

                    if (state.autoSave) await saveTurn("assistant", fullAnswer);
                    ui.log("‚úÖ Response generated.", "success");
                }); // End Lock

            } catch (e) {
                const msg = e.message || String(e) || "Unknown Error";
                ui.log(`Error: ${msg}`, "error");
                ui.append("assistant", `**Error:** ${msg}`);
            } finally {
                input.disabled = false;
                document.getElementById('send-btn').disabled = false;
                input.focus();
            }
        }

        async function loadModel() {
            const select = document.getElementById('model-select');
            const customInput = document.getElementById('custom-model-input');
            const modelInput = select.value === 'custom' ? customInput.value : select.value;
            if (!modelInput) return alert("Please select a model.");

            selectedModelId = modelInput.split('/').pop();
            document.getElementById('load-model-btn').disabled = true;

            try {
                // 0. THE BLOCKER (Model Load Lock) - Serialize model loading
                ui.log("‚è≥ Queueing for Model Load (Sequential)...", "info");

                await GPUController.withModelLoadLock("Root-Console-Init", async () => {
                    ui.log(`Initializing Engine (${selectedModelId})...`, "info");

                    // --- KERNEL: Hardware Config ---
                    const hwProfile = document.getElementById('hw-profile').value;
                    const gpuConfig = await getWebGPUConfig(hwProfile);

                    if (gpuConfig.isConstrained) {
                        ui.log(`‚ö†Ô∏è Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, "warn");
                    } else {
                        ui.log(`‚úÖ GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, "success");
                    }

                    // --- Config Generation ---
                    // (Simplified Logic for cleaner file)
                    const libBase = "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/"; // Fixed URL
                    let modelLib = null;
                    const lowerId = selectedModelId.toLowerCase();
                    let qTag = "q4f16_1"; // Default

                    // Mapper
                    // SOTA / New
                    if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('deepseek-r1') && lowerId.includes('14b')) modelLib = libBase + `Qwen2.5-14B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-14b')) modelLib = libBase + `Qwen2.5-14B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;

                    // Vision Models
                    else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;

                    // Small / Efficient
                    else if (lowerId.includes('qwen3-0.6b')) modelLib = libBase + `Qwen3-0.6B-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('llama-3.2-1b')) modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;

                    // Fallbacks
                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;
                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;

                    if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B

                    let appConfig = {
                        useIndexedDBCache: true,
                        model_list: [{
                            model: "https://huggingface.co/" + modelInput + "/resolve/main/",
                            model_id: selectedModelId,
                            model_lib: modelLib,
                            vram_required_MB: 2000,
                            low_resource_required: true,
                            buffer_size_required_bytes: gpuConfig.maxBufferSize,
                        }]
                    };

                    // Create the Worker
                    const worker = new Worker('./modules/llm-worker.js', { type: 'module' });

                    // Initialize Engine via Worker
                    engine = await CreateWebWorkerMLCEngine(
                        worker,
                        selectedModelId,
                        {
                            appConfig, // Passing the VRAM/Buffer limits we calculated
                            initProgressCallback: (rep) => ui.updateProgress(rep.progress, rep.text)
                        }
                    );
                }); // Uses the default 5-minute timeout for model loading

                ui.log("üéâ Root Console Online", "success");
                contextManager = new ContextManager(engine, db);
                document.getElementById('input').disabled = false;
                document.getElementById('send-btn').disabled = false;
                document.getElementById('input').focus();

            } catch (e) {
                const errorMsg = e.message || String(e);
                ui.log(`Load Failed: ${errorMsg}`, "error");
                // Provide suggestion for common model name issues
                if (errorMsg && errorMsg.includes("Network response was not ok")) {
                    ui.log(`üí° Hint: Model may not exist or be temporarily unavailable. Try another model.`, "warn");
                }
                document.getElementById('load-model-btn').disabled = false;
            }
        }

        let vision = null; // Global VisionController instance

        async function init() {
            try {
                ui.log("üöÄ Root Kernel Starting...", "info");

                // 1. CozoDB
                await initCozo('./cozo_lib_wasm_bg.wasm');
                // Recovery Logic
                try {
                    const [keys] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });
                    if (keys.length > 0) {
                        db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        window.db = db;
                        ui.log("‚úÖ Root Graph Connected (Persistent)", "success");
                    } else {
                        db = CozoDb.new();
                        window.db = db;
                        ui.log("‚úÖ Root Graph Created (Memory)", "info");
                    }
                } catch (e) {
                    db = CozoDb.new();
                    window.db = db;
                    ui.log("‚ö†Ô∏è Fallback to Memory Graph", "warn");
                }

                // 2. Embedder (Optional)
                ui.updateProgress(0.3, "Loading Embedder...");
                const embedderPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', { device: 'wasm' });
                try {
                    embedder = await Promise.race([embedderPromise, new Promise((_, r) => setTimeout(() => r(new Error("Timeout")), 10000))]);
                    ui.log("‚úÖ Neural Embedder Ready", "success");
                } catch (e) { ui.log("‚ö†Ô∏è Embedder Skipped (Timeout)", "warn"); }

            } catch (e) {
                ui.log(`Init Error: ${e.message}`, "error");
            } finally {
                // Initialize VisionController
                vision = new VisionController();
                vision.setup('input-area', 'image-preview-container', 'input');

                // Ensure controls are unlocked even if init fails (partial functionality)
                ui.updateProgress(1.0, "Ready");

                // Fix: Copy Logs Button
                const copyBtn = document.getElementById('copy-logs-btn');
                if (copyBtn) {
                    copyBtn.onclick = () => {
                        const logs = document.getElementById('status-log').innerText;
                        navigator.clipboard.writeText(logs).then(() => ui.log("üìã Logs copied to clipboard", "success"));
                    };
                }

                document.getElementById('model-select').disabled = false;
                document.getElementById('load-model-btn').disabled = false;
                document.getElementById('load-model-btn').addEventListener('click', loadModel);

                // Input Handlers (idempotent)
                const sendBtn = document.getElementById('send-btn');
                const input = document.getElementById('input');

                if (sendBtn && input) {
                    sendBtn.onclick = handleSend; // Use property to avoid duplicates
                    input.onkeydown = (e) => {
                        if (e.key === 'Enter' && !e.shiftKey && !sendBtn.disabled) {
                            e.preventDefault();
                            handleSend();
                        }
                    };
                }
            }
        }

        // --- BRIDGE LOGIC ---
        let bridgeWs = null;
        
        // GPU Status Check and Force Unlock Handler
        async function checkGPUStatus() {
            try {
                const res = await fetch("http://localhost:8080/v1/gpu/status", {
                    method: "GET",
                    headers: { "Authorization": "Bearer sovereign-secret" }
                });

                if (res.ok) {
                    const status = await res.json();
                    ui.log(`GPU Status: ${status.locked ? `LOCKED by ${status.owner}` : 'FREE'}. Queue: ${status.queue_depth} items.`, "info");
                    if (status.queued && status.queued.length > 0) {
                        ui.log(`Queued: ${status.queued.join(', ')}`, "info");
                    }

                    // Check model loading status
                    const modelLoadStatus = GPUController.getModelLoadStatus();
                    if (modelLoadStatus.hasPendingLoad || modelLoadStatus.queueSize > 0) {
                        ui.log(`Model Load Status: Queue: ${modelLoadStatus.queueSize}, Active: ${modelLoadStatus.activeLoaders.join(', ')}`, "info");
                    }

                    return status;
                } else {
                    ui.log(`Status check failed: ${res.status}`, "warn");
                    return null;
                }
            } catch (e) {
                ui.log(`Status check error: ${e.message}`, "warn");
                return null;
            }
        }

        document.getElementById('debug-gpu-btn').addEventListener('click', async () => {
            await checkGPUStatus();
        });

        document.getElementById('force-unlock-btn').addEventListener('click', async () => {
            if (!confirm("‚ö†Ô∏è Force Unlock GPU?\nOnly do this if the system is stuck waiting for a lock.")) return;

            try {
                ui.log("Sending Force Unlock signal...", "warn");

                // First try the standard reset
                let res = await fetch("http://localhost:8080/v1/gpu/reset", {
                    method: "POST",
                    headers: { "Authorization": "Bearer sovereign-secret" }
                });

                if (res.ok) {
                    ui.log("üîì GPU Lock Force Released (Standard).", "success");
                } else {
                    ui.log(`Standard unlock failed: ${res.status}. Trying emergency release...`, "warn");

                    // If standard unlock failed, try the emergency endpoint
                    res = await fetch("http://localhost:8080/v1/gpu/force-release-all", {
                        method: "POST",
                        headers: { "Authorization": "Bearer sovereign-secret" }
                    });

                    if (res.ok) {
                        ui.log("üîì GPU Locks Force Released (Emergency).", "success");
                    } else {
                        ui.log(`Emergency unlock failed: ${res.status}`, "error");
                    }
                }
            } catch (e) {
                ui.log(`Unlock Error: ${e.message}`, "error");
            }
        });

        window.toggleBridge = function (enabled) {
            if (enabled) {
                if (!engine) { alert("Load model first!"); document.getElementById('enable-bridge-toggle').checked = false; return; }
                bridgeWs = new WebSocket("ws://localhost:8080/ws/chat");
                bridgeWs.onopen = () => { document.getElementById('bridge-status').innerText = "üü¢ Connected"; ui.log("Bridge Online", "success"); };
                // [Replace the existing bridgeWs.onmessage handler with this]
                bridgeWs.onmessage = async (e) => {
                    const msg = JSON.parse(e.data);
                    
                    // 1. STANDARD CHAT REQUEST (e.g. from VS Code)
                    if (msg.type === 'chat') {
                        ui.log(`Bridge Chat: ${msg.id}`, "info");
                        try {
                            const completion = await engine.chat.completions.create({ 
                                messages: msg.data.messages, 
                                stream: true 
                            });
                            for await (const chunk of completion) {
                                bridgeWs.send(JSON.stringify({ id: msg.id, chunk }));
                            }
                            bridgeWs.send(JSON.stringify({ id: msg.id, done: true }));
                        } catch (err) {
                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));
                        }
                    } 
                    
                    // 2. MEMORY SEARCH REQUEST (From Chrome Extension)
                    else if (msg.type === 'memory_query') {
                        const queryText = msg.data.query;
                        ui.log(`üîé Bridge Memory Search: "${queryText}"`, "info");
                        try {
                            // Use the new decoupled method
                            const results = await contextManager.findRelevantMemories(queryText);
                            
                            // Send pure JSON back to the bridge
                            bridgeWs.send(JSON.stringify({ 
                                id: msg.id, 
                                result: results 
                            }));
                            ui.log(`‚úÖ Returned ${results.length} memories`, "success");
                        } catch (err) {
                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));
                            ui.log(`‚ùå Search Failed: ${err.message}`, "error");
                        }
                    }
                };
            } else if (bridgeWs) { bridgeWs.close(); bridgeWs = null; document.getElementById('bridge-status').innerText = "Disconnected"; }
        };

        window.addEventListener('load', init);
    </script>
</body>

</html>
--- END OF FILE: tools\model-server-chat.html ---

--- START OF FILE: tools\orchestrator.py ---
import os
import requests
import json
import logging

# Config - Defaults match start-bridge.bat
BRIDGE_PORT = os.getenv("BRIDGE_PORT", "8080")
BRIDGE_HOST = os.getenv("BRIDGE_HOST", "localhost")
BRIDGE_URL = f"http://{BRIDGE_HOST}:{BRIDGE_PORT}"
MLC_INFERENCE_ENDPOINT = f"{BRIDGE_URL}/v1/chat/completions"
BRIDGE_TOKEN = os.getenv("BRIDGE_TOKEN", "sovereign-secret")

logger = logging.getLogger("Orchestrator")
logging.basicConfig(level=logging.INFO)

class MLCConnectionError(Exception):
    """Raised when connection to the MLC Bridge fails."""
    pass

class Orchestrator:
    def __init__(self, endpoint=MLC_INFERENCE_ENDPOINT, token=BRIDGE_TOKEN):
        self.endpoint = endpoint
        self.token = token
        self.active_model = None
        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json"
        }

    def load_mlc_model(self, model_name):
        """
        Configures the orchestrator for a specific model.
        Verifies connectivity to the Bridge and logs available models.
        """
        logger.info(f"Preparing Orchestrator for model: {model_name}")
        
        try:
            # Health check / Model list
            r = requests.get(f"{BRIDGE_URL}/v1/models", headers=self.headers, timeout=2.0)
            r.raise_for_status()
            models_data = r.json().get("data", [])
            available_ids = [m['id'] for m in models_data]
            logger.info(f"Bridge connected. Active Workers: {available_ids}")
            
            if not available_ids:
                logger.warning("No WebGPU workers connected to Bridge. Open model-server-chat.html")
            
        except requests.exceptions.RequestException as e:
            raise MLCConnectionError(f"Failed to connect to Wave Bridge at {BRIDGE_URL}. Ensure start-bridge.bat is running. Error: {e}")

        self.active_model = model_name
        return True

    def invoke_mlc_inference(self, prompt, system_prompt="You are a helpful AI orchestrator."):
        """
        Sends a prompt to the MLC engine via the Bridge.
        """
        if not self.active_model:
            raise ValueError("No model loaded. Call load_mlc_model() first.")

        payload = {
            "model": self.active_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }

        try:
            logger.info(f"Sending inference request to {self.endpoint}...")
            response = requests.post(
                self.endpoint, 
                json=payload, 
                headers=self.headers,
                timeout=60 # Inference can be slow
            )
            response.raise_for_status()
            
            result = self._parse_response(response.json())
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Inference request failed: {e}")
            raise MLCConnectionError(f"Inference failed: {e}")

    def _parse_response(self, mlc_response):
        """
        Extracts the content from the OpenAI-compatible JSON response.
        """
        try:
            # Check for bridge-reported errors
            if "error" in mlc_response:
                raise Exception(mlc_response["error"])

            choices = mlc_response.get("choices", [])
            if not choices:
                logger.warning("Empty choices in response")
                return ""
            
            content = choices[0].get("message", {}).get("content", "")
            return content
        except Exception as e:
            logger.error(f"Error parsing MLC response: {e}")
            return f"[Error parsing output: {e}]"

# CLI usage
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Orchestrator CLI")
    parser.add_argument("--prompt", type=str, help="Prompt to send")
    parser.add_argument("--model", type=str, default="webgpu-chat", help="Model ID")
    args = parser.parse_args()

    if args.prompt:
        orc = Orchestrator()
        try:
            orc.load_mlc_model(args.model)
            print(f"\nResponse:\n{orc.invoke_mlc_inference(args.prompt)}")
        except Exception as e:
            print(f"Error: {e}")
    else:
        print("Usage: python orchestrator.py --prompt 'Hello'")
--- END OF FILE: tools\orchestrator.py ---

--- START OF FILE: tools\prepare_cozo_import.py ---
#!/usr/bin/env python3
"""
Prepare a CozoDB import payload from an existing combined_memory.json file.
Usage:
  python tools/prepare_cozo_import.py [input_path] [output_path]
If input_path is omitted the script will search likely locations.
"""
import json
import os
import sys
from pathlib import Path

# Defaults
POSSIBLE_INPUTS = [
    Path("combined_memory.json")
]
DEFAULT_OUTPUT = Path("cozo_import_memory.json")

def find_input(path_arg=None):
    if path_arg:
        p = Path(path_arg)
        if p.exists():
            return p
        else:
            print(f"‚ùå Specified input not found: {p}")
            return None
    for p in POSSIBLE_INPUTS:
        if p.exists():
            return p
    # fallback: search workspace for first combined_memory.json
    for p in Path('.').rglob('combined_memory.json'):
        return p
    return None


def normalize_record(rec):
    # Ensure the fields Cozo expects. Return None to skip invalid records.
    uid = rec.get("id") or rec.get("uid") or rec.get("uuid") or None
    if not uid:
        # try deterministic id from source+timestamp
        src = rec.get("source") or rec.get("file") or ""
        ts = rec.get("timestamp") or rec.get("created_at") or 0
        uid = f"auto:{abs(hash(src + str(ts)))}"
    try:
        uid = str(uid)
    except Exception:
        uid = str(uid)

    try:
        ts = int(rec.get("timestamp", rec.get("created_at", 0)) or 0)
    except Exception:
        try:
            ts = int(float(rec.get("timestamp", 0)))
        except Exception:
            ts = 0

    role = str(rec.get("role", "system"))
    content = rec.get("content", "")
    if not isinstance(content, str):
        try:
            content = json.dumps(content, ensure_ascii=False)
        except Exception:
            content = str(content)
    # cap content size to be safe
    MAX_CONTENT = 200_000
    if len(content) > MAX_CONTENT:
        content = content[:MAX_CONTENT]

    source = rec.get("source", rec.get("file", "historical_import"))
    try:
        source = str(source)
    except Exception:
        source = "historical_import"

    embedding = rec.get("embedding", None)
    # Keep embedding as-is if present and looks like a list of numbers
    if isinstance(embedding, list):
        # Optionally validate length later; leave as-is
        pass
    else:
        embedding = None

    return [uid, ts, role, content, source, embedding]


def main():
    input_arg = sys.argv[1] if len(sys.argv) > 1 else None
    output_arg = sys.argv[2] if len(sys.argv) > 2 else None

    inp = find_input(input_arg)
    if not inp:
        print("‚ùå Could not find a combined_memory.json file. Provide the path as the first argument.")
        return 1

    out = Path(output_arg) if output_arg else DEFAULT_OUTPUT

    print(f"Reading {inp}...")
    try:
        raw = json.loads(inp.read_text(encoding='utf-8'))
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON decode error: {e}")
        return 1

    if not isinstance(raw, list):
        print("‚ùó Warning: input root is not a list. Attempting to find list under 'records' or 'data'.")
        if isinstance(raw, dict):
            if 'records' in raw and isinstance(raw['records'], list):
                raw = raw['records']
            elif 'data' in raw and isinstance(raw['data'], list):
                raw = raw['data']
            else:
                print("‚ùå Could not find the expected list of records in input JSON.")
                return 1

    print(f"Found {len(raw)} records. Normalizing and formatting...")

    rows = []
    for rec in raw:
        nr = normalize_record(rec)
        if nr is None:
            continue
        rows.append(nr)

    payload = {
        "relations": [
            {
                "name": "memory",
                "headers": ["id", "timestamp", "role", "content", "source", "embedding"],
                "rows": rows,
            }
        ]
    }

    print(f"Writing {len(rows)} rows to {out}...")
    out.write_text(json.dumps(payload, ensure_ascii=False, separators=(',', ':')), encoding='utf-8')
    print("‚úÖ Done. You can now drag 'cozo_import_memory.json' into the Builder or use the console import helper.")
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
--- END OF FILE: tools\prepare_cozo_import.py ---

--- START OF FILE: tools\README.md ---
# Tools Directory

Contains the browser-native HTML/JS applications (Console, Builder, Mic) and the Python WebGPU bridge for the Sovereign Context Engine.
--- END OF FILE: tools\README.md ---

--- START OF FILE: tools\root-dreamer.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Root Dreamer (Subconscious)</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåô</text></svg>">
    <style>
        :root {
            --bg-color: #0d1117;
            --text-color: #c9d1d9;
            --border-color: #30363d;
            --accent-color: #58a6ff;
            --success-color: #238636;
            --danger-color: #da3633;
            --thought-color: #d29922;
            --raw-color: #8b949e;
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 20px;
            display: flex;
            flex-direction: column;
            height: 100vh;
            box-sizing: border-box;
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        h1 { margin: 0; font-size: 1.5rem; }

        .status-panel {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 10px;
            margin-bottom: 20px;
        }

        .card {
            background: #161b22;
            border: 1px solid var(--border-color);
            border-radius: 6px;
            padding: 15px;
        }

        .stat-value {
            font-size: 2rem;
            font-weight: bold;
            color: var(--accent-color);
        }

        .stat-label {
            font-size: 0.9rem;
            color: #8b949e;
        }

        .controls {
            display: flex;
            gap: 10px;
        }

        button {
            background-color: #21262d;
            color: var(--text-color);
            border: 1px solid var(--border-color);
            padding: 8px 16px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: background-color 0.2s;
        }

        button:hover { background-color: #30363d; }
        
        button.active {
            background-color: var(--success-color);
            color: white;
            border-color: var(--success-color);
        }
        
        button.stop {
            background-color: var(--danger-color);
            border-color: var(--danger-color);
        }

        /* Tabs */
        .tabs {
            display: flex;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 0;
        }
        
        .tab {
            padding: 10px 20px;
            cursor: pointer;
            border-bottom: 2px solid transparent;
            color: #8b949e;
        }
        
        .tab:hover { color: var(--text-color); }
        
        .tab.active {
            color: var(--text-color);
            border-bottom-color: var(--accent-color);
        }

        /* Logs */
        .log-container {
            flex-grow: 1;
            background: #000;
            border: 1px solid var(--border-color);
            border-top: none;
            border-bottom-left-radius: 6px;
            border-bottom-right-radius: 6px;
            padding: 10px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.85rem;
            overflow-y: auto;
            white-space: pre-wrap;
            display: none; /* Hidden by default */
        }
        
        .log-container.active { display: block; }

        .log-entry { margin-bottom: 5px; border-bottom: 1px solid #222; padding-bottom: 2px; }
        .log-info { color: #58a6ff; }
        .log-success { color: #238636; }
        .log-warn { color: #d29922; }
        .log-error { color: #f85149; }
        
        .log-thought { color: var(--thought-color); font-style: italic; border-left: 2px solid var(--thought-color); padding-left: 10px; margin: 5px 0; }
        .log-raw { color: var(--raw-color); font-size: 0.8rem; opacity: 0.8; }
        .log-json { color: var(--success-color); font-weight: bold; }

    </style>
</head>
<body>
    <header>
        <h1>Root Dreamer (Subconscious)</h1>
        <div class="controls">
            <button id="btn-reset" class="stop" style="margin-right: 10px;">Reset Engine</button>
            <button id="btn-toggle">Wake Up</button>
        </div>
    </header>

    <div class="status-panel">
        <div class="card">
            <div class="stat-value" id="stat-processed">0</div>
            <div class="stat-label">Memories Processed</div>
        </div>
        <div class="card">
            <div class="stat-value" id="stat-synapses">0</div>
            <div class="stat-label">Synapses Formed</div>
        </div>
        <div class="card">
            <div class="stat-value" id="stat-model">Loading...</div>
            <div class="stat-label">Model Status</div>
        </div>
    </div>

    <div class="tabs">
        <div class="tab active" data-target="log-main">Stream</div>
        <div class="tab" data-target="log-thoughts">Thoughts (Internal Monologue)</div>
        <div class="tab" data-target="log-synapses">Synapses (Connections)</div>
        <div class="tab" data-target="log-raw">Raw Protocol</div>
    </div>

    <div id="log-main" class="log-container active"></div>
    <div id="log-thoughts" class="log-container"></div>
    <div id="log-synapses" class="log-container"></div>
    <div id="log-raw" class="log-container"></div>

    <script type="module">
        import { SovereignLogger, createStore, getWebGPUConfig, initCozo, GPUController } from './modules/sovereign.js';
        import { CreateWebWorkerMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        // Load hot reload functionality in development
        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {
            import('./modules/gpu-hot-reloader.js').then(() => {
                console.log('üîÑ GPU Hot Reloader loaded for development');
            }).catch(err => {
                console.warn('‚ö†Ô∏è GPU Hot Reloader not available:', err);
            });
        }

        const logger = new SovereignLogger('Dreamer');
        const MODEL_ID = "Qwen2.5-1.5B-Instruct-q4f32_1-MLC";
        
        // State
        const store = createStore({
            processedCount: 0,
            synapseCount: 0,
            isDreaming: false,
            modelReady: false
        });

        // DOM Elements
        const elProcessed = document.getElementById('stat-processed');
        const elSynapses = document.getElementById('stat-synapses');
        const elModel = document.getElementById('stat-model');
        const elToggle = document.getElementById('btn-toggle');
        const elReset = document.getElementById('btn-reset');
        
        const logs = {
            main: document.getElementById('log-main'),
            thoughts: document.getElementById('log-thoughts'),
            synapses: document.getElementById('log-synapses'),
            raw: document.getElementById('log-raw')
        };

        // Tab Switching
        document.querySelectorAll('.tab').forEach(t => {
            t.addEventListener('click', () => {
                document.querySelectorAll('.tab').forEach(x => x.classList.remove('active'));
                document.querySelectorAll('.log-container').forEach(x => x.classList.remove('active'));
                t.classList.add('active');
                document.getElementById(t.dataset.target).classList.add('active');
            });
        });

        // Logging Helper
        function appendLog(targetId, msg, type = 'info') {
            const container = logs[targetId];
            if (!container) return;
            const div = document.createElement('div');
            if (type === 'thought') div.className = 'log-thought';
            else if (type === 'json') div.className = 'log-json';
            else if (type === 'raw') div.className = 'log-raw';
            else div.className = `log-entry log-${type}`;
            const ts = `[${new Date().toLocaleTimeString()}] `;
            div.textContent = (type === 'thought' || type === 'json' || type === 'raw') ? msg : ts + msg;
            container.appendChild(div);
            container.scrollTop = container.scrollHeight;
        }

        // State Subscription
        store.subscribe((prop, val) => {
            if (prop === 'processedCount') elProcessed.textContent = val;
            if (prop === 'synapseCount') elSynapses.textContent = val;
            if (prop === 'modelReady') elModel.textContent = val ? "Ready" : "Loading...";
            if (prop === 'isDreaming') {
                elToggle.textContent = val ? "Sleep" : "Wake Up";
                elToggle.className = val ? "active" : "";
            }
        });

        let db;
        async function runQuery(query, params = "{}", immutable = false) {
            if (!db) throw new Error("Database not initialized");
            const res = await db.run(query, params, immutable);
            let parsed = res;
            if (typeof res === 'string') {
                try { parsed = JSON.parse(res); } catch (e) { logger.warn("Failed to parse Cozo response"); }
            }
            if (parsed && parsed.ok === false) throw new Error(parsed.message);
            return parsed;
        }

        async function loadModel() {
            appendLog('main', `Loading Model: ${MODEL_ID}...`);
            const worker = new Worker('./modules/llm-worker.js', { type: 'module' });

            try {
                await GPUController.withModelLoadLock("Dreamer-Init", async () => {
                    // Use a more reliable model URL for the Dreamer
                    const config = {
                        model_list: [{
                            model: "https://huggingface.co/mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC/resolve/main/",
                            model_id: "Qwen2.5-1.5B-Instruct-q4f16_1-MLC",
                            model_lib: "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                            vram_required_MB: 2000,
                            low_resource_required: true
                        }]
                    };

                    window.engine = await CreateWebWorkerMLCEngine(worker, "Qwen2.5-1.5B-Instruct-q4f16_1-MLC", {
                        appConfig: config,
                        initProgressCallback: (report) => {
                            elModel.textContent = report.text;
                            if (report.progress === 1) store.state.modelReady = true;
                        }
                    });
                });
                appendLog('main', "Model Loaded. Ready to dream.", 'success');
                store.state.modelReady = true;
            } catch (error) {
                appendLog('main', `Model Load Error: ${error.message}`, 'error');

                // Try to check GPU status for more information
                try {
                    const status = await GPUController.checkStatus();
                    if (status && status.locked) {
                        appendLog('main', `GPU currently locked by: ${status.owner || 'unknown'}`, 'warn');
                        if (status.queued && status.queued.length > 0) {
                            appendLog('main', `Queue: ${status.queued.join(', ')}`, 'warn');
                        }
                    }
                } catch (statusErr) {
                    appendLog('main', `Could not get GPU status: ${statusErr.message}`, 'warn');
                }

                // Retry after a delay
                setTimeout(() => {
                    appendLog('main', "Retrying model load in 5 seconds...", 'info');
                    loadModel();
                }, 5000);
            }
        }

        async function init() {
            try {
                appendLog('main', "Initializing CozoDB...");
                const CozoDb = await initCozo();
                try {
                    db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => {});
                    appendLog('main', "Connected to Persistent Brain.", 'success');
                } catch (e) {
                    db = CozoDb.new();
                    appendLog('main', "Connected to Transient Brain.", 'warn');
                }
                
                // Tables - Idempotent Creation
                try {
                    await runQuery(":create memory { id: String => timestamp: Int, role: String, content: String, source: String, embedding: <F32; 384>? }");
                } catch (e) {
                    if (!e.message.includes('conflicts')) throw e; // Ignore if exists
                }

                try {
                    await runQuery(":create relationships { source_id: String, target_id: String, type: String => weight: Float }");
                } catch (e) {
                    if (!e.message.includes('conflicts')) throw e;
                }
                
                await loadModel();
            } catch (e) {
                const msg = e.message || String(e);
                appendLog('main', `Initialization Error: ${msg}`, 'error');
            }
        }

        let isLoopRunning = false;
        async function dreamLoop() {
            if (isLoopRunning || !store.state.isDreaming) return;
            if (!store.state.modelReady || !window.engine) {
                await new Promise(r => setTimeout(r, 1000));
                return dreamLoop();
            }

            isLoopRunning = true;
            try {
                const query = `?[id, content] := *memory{id, content}, not *relationships{source_id: id} :limit 1`;
                const result = await runQuery(query);

                if (!result.rows || result.rows.length === 0) {
                    appendLog('main', "No orphans found. Resting...", 'info');
                    await new Promise(r => setTimeout(r, 5000));
                    isLoopRunning = false;
                    if (store.state.isDreaming) dreamLoop();
                    return;
                }

                const [id, content] = result.rows[0];
                appendLog('main', `Dreaming on memory: ${id}...`);
                const safeContent = content.length > 3000 ? content.substring(0, 3000) + "..." : content;
                
                const prompt = `Analyze this memory. Output Step 1: Thoughts (monologue) and Step 2: JSON array of concepts. Use <thought> and <json> tags. Memory: ${safeContent}`;

                await GPUController.withLock("Dreamer-Think", async () => {
                    const response = await window.engine.chat.completions.create({
                        messages: [{ role: "user", content: prompt }],
                        temperature: 0.3,
                        max_tokens: 1024
                    });
                    
                    const rawReply = response.choices[0].message.content;
                    const thoughtMatch = rawReply.match(/<thought>([\s\S]*?)<\/thought>/);
                    const jsonMatch = rawReply.match(/<json>([\s\S]*?)<\/json>/);

                    if (thoughtMatch) appendLog('thoughts', `[${id}] ${thoughtMatch[1].trim()}`, 'thought');
                    
                    let relationships = [];
                    try {
                        let jsonStr = jsonMatch ? jsonMatch[1] : rawReply;
                        jsonStr = jsonStr.replace(/```json/g, '').replace(/```/g, '').trim();
                        const start = jsonStr.indexOf('[');
                        const end = jsonStr.lastIndexOf(']');
                        if (start !== -1 && end !== -1) {
                            relationships = JSON.parse(jsonStr.substring(start, end + 1));
                            appendLog('synapses', `[${id}] Synthesized ${relationships.length} links.`, 'json');
                        }
                    } catch (pe) {
                        relationships = [{ target: "failed_parse", type: "system_flag", weight: 0.0 }];
                    }

                    const rows = relationships.map(r => [String(id), String(r.target || "unknown"), String(r.type || 'relates_to'), parseFloat(r.weight || 0.5)]);
                    if (rows.length > 0) {
                        await runQuery(`?[src, tgt, type, w] <- $rows :put relationships {source_id: src, target_id: tgt, type: type, weight: w}`, JSON.stringify({ rows }));
                        store.state.processedCount++;
                        store.state.synapseCount += rows.length;
                    }
                });
                await new Promise(r => setTimeout(r, 2000));
            } catch (err) {
                const msg = err.message || String(err);
                appendLog('main', `Dream Error: ${msg}`, 'error');
                if (msg.includes("Model not loaded") || msg.includes("Tokenizer")) {
                    store.state.modelReady = false;
                    await loadModel();
                }
                await new Promise(r => setTimeout(r, 5000));
            } finally {
                isLoopRunning = false;
            }
            if (store.state.isDreaming) dreamLoop();
        }

        elToggle.addEventListener('click', () => {
            store.state.isDreaming = !store.state.isDreaming;
            if (store.state.isDreaming) dreamLoop();
        });
        
        elReset.addEventListener('click', async () => {
            store.state.isDreaming = false;
            store.state.modelReady = false;
            appendLog('main', "üõë Resetting Engine...", 'warn');
            await loadModel();
        });

        init();
    </script>
</body>
</html>
--- END OF FILE: tools\root-dreamer.html ---

--- START OF FILE: tools\root-mic.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Root Mic üéôÔ∏è</title>
    <style>
        :root {
            --bg-color: #0f0f11;
            --surface-color: #1a1a1d;
            --primary-color: #00ff88;
            --accent-color: #00ccff;
            --text-color: #eeeeee;
            --danger-color: #ff4444;
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            overflow: hidden;
        }

        .container {
            text-align: center;
            width: 100%;
            max-width: 500px;
            padding: 20px;
        }

        h1 {
            font-weight: 300;
            letter-spacing: 2px;
            margin-bottom: 30px;
            text-transform: uppercase;
            font-size: 1.5rem;
            color: var(--accent-color);
            text-shadow: 0 0 10px rgba(0, 204, 255, 0.3);
        }

        #mic-btn {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            background: radial-gradient(circle at 30% 30%, #444, #222);
            border: 4px solid #333;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5), 0 0 0 4px var(--bg-color);
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 30px;
            position: relative;
            outline: none;
            -webkit-tap-highlight-color: transparent;
        }

        #mic-btn::after {
            content: '';
            position: absolute;
            top: -10px;
            left: -10px;
            right: -10px;
            bottom: -10px;
            border-radius: 50%;
            border: 2px solid var(--primary-color);
            opacity: 0;
            transform: scale(0.8);
            transition: all 0.3s;
        }

        #mic-btn:hover {
            transform: scale(1.05);
            background: radial-gradient(circle at 30% 30%, #555, #333);
        }

        #mic-btn.active {
            background: radial-gradient(circle at 30% 30%, #ff5555, #aa0000);
            box-shadow: 0 0 30px rgba(255, 68, 68, 0.6);
            border-color: #ff4444;
            transform: scale(0.95);
        }

        #mic-btn.active::after {
            animation: pulse 1.5s infinite;
            opacity: 1;
        }

        #mic-icon {
            font-size: 64px;
            color: #888;
            transition: color 0.3s;
        }

        #mic-btn.active #mic-icon {
            color: white;
        }

        #clarify-btn {
            background: transparent;
            color: var(--accent-color);
            border: 1px solid var(--accent-color);
            padding: 8px 16px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.9rem;
            margin-bottom: 20px;
            transition: all 0.3s;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        #clarify-btn:hover:not(:disabled) {
            background: rgba(0, 204, 255, 0.1);
            transform: translateY(-2px);
        }

        #clarify-btn:disabled {
            opacity: 0.3;
            cursor: not-allowed;
            border-color: #555;
            color: #555;
        }

        #status {
            font-size: 1.2rem;
            margin-bottom: 20px;
            height: 1.5em;
            color: #888;
        }

        .visualizer {
            width: 100%;
            height: 60px;
            background: var(--surface-color);
            border-radius: 12px;
            margin-bottom: 20px;
            position: relative;
            overflow: hidden;
            border: 1px solid #333;
        }

        .bar-container {
            display: flex;
            align-items: center;
            justify-content: center;
            height: 100%;
            gap: 2px;
        }

        .bar {
            width: 4px;
            background: var(--primary-color);
            border-radius: 2px;
            height: 4px;
            transition: height 0.1s ease;
        }

        #output {
            background: var(--surface-color);
            padding: 20px;
            border-radius: 12px;
            border: 1px solid #333;
            min-height: 100px;
            text-align: left;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            color: #ccc;
            position: relative;
            overflow-y: auto;
            max-height: 200px;
        }

        #copy-toast {
            position: absolute;
            top: 20px;
            right: 20px;
            background: var(--primary-color);
            color: #000;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            opacity: 0;
            transform: translateY(-20px);
            transition: all 0.3s;
            pointer-events: none;
        }

        #copy-toast.show {
            opacity: 1;
            transform: translateY(0);
        }

        footer {
            margin-top: 40px;
            font-size: 0.7rem;
            color: #444;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
                opacity: 1;
                border-color: var(--danger-color);
            }

            100% {
                transform: scale(1.5);
                opacity: 0;
                border-color: var(--danger-color);
            }
        }

        #loading-overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.9);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            z-index: 1000;
            transition: opacity 0.5s;
        }

        .loader {
            width: 48px;
            height: 48px;
            border: 5px solid #FFF;
            border-bottom-color: var(--primary-color);
            border-radius: 50%;
            display: inline-block;
            box-sizing: border-box;
            animation: rotation 1s linear infinite;
        }

        .progress-text {
            margin-top: 20px;
            font-family: monospace;
            color: var(--primary-color);
        }

        @keyframes rotation {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }
    </style>
</head>
<body>
    <div id="loading-overlay">
        <span class="loader"></span>
        <div id="loading-text" class="progress-text">Initializing Neural Engines...</div>
    </div>
    <div id="copy-toast">Copied to Clipboard!</div>
    <div class="container">
        <h1>Root Mic üéôÔ∏è</h1>
        <div class="visualizer"><div class="bar-container" id="bars"></div></div>
        <button id="mic-btn"><div id="mic-icon">üéôÔ∏è</div></button>
        <button id="clarify-btn" disabled>Refine Text</button>
        <div id="status">Ready</div>
        <div id="output">...</div>
        <footer>Running Locally: Whisper-Tiny (Audio) + Qwen2.5-1.5B (Text)</footer>
    </div>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";

        // THE NEW KERNEL
        import { SovereignLogger, createStore, getWebGPUConfig, GPUController } from './modules/sovereign.js';

        // Load hot reload functionality in development
        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {
            import('./modules/gpu-hot-reloader.js').then(() => {
                console.log('üîÑ GPU Hot Reloader loaded for development');
            }).catch(err => {
                console.warn('‚ö†Ô∏è GPU Hot Reloader not available:', err);
            });
        }

        const logger = new SovereignLogger('Root-Mic');
        
        // Reactive Store
        const { state, subscribe } = createStore({
            status: 'Ready',
            output: '...',
            isLoading: true,
            loadingText: 'Initializing Neural Engines...',
            isRecording: false
        });

        // UI Bindings
        subscribe((prop, val) => {
            if (prop === 'status') document.getElementById('status').innerText = val;
            if (prop === 'output') {
                document.getElementById('output').innerText = val;
                // Enable clarify button if there is text (and not just placeholder/loading)
                const btn = document.getElementById('clarify-btn');
                if (val && val !== '...' && val.length > 10) {
                    btn.disabled = false;
                } else {
                    btn.disabled = true;
                }
            }
            if (prop === 'loadingText') document.getElementById('loading-text').innerText = val;
            if (prop === 'isLoading') {
                const ol = document.getElementById('loading-overlay');
                ol.style.opacity = val ? '1' : '0';
                setTimeout(() => ol.style.display = val ? 'flex' : 'none', 500);
            }
            if (prop === 'isRecording') {
                const btn = document.getElementById('mic-btn');
                if (val) btn.classList.add('active'); else btn.classList.remove('active');
            }
        });

        let whisperWorker = null;
        let llmEngine = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let audioContext = null;
        let analyser = null;
        let dataArray = null;
        let animationId = null;
        let silenceStart = 0;

        async function init() {
            try {
                // 1. Whisper Init (Worker)
                state.loadingText = "Step 1/2: Initializing Whisper Worker...";
                whisperWorker = new Worker('./modules/whisper-worker.js', { type: 'module' });
                
                // Wait for worker init
                await new Promise((resolve, reject) => {
                    whisperWorker.onmessage = (e) => {
                        if (e.data.type === 'init_done') resolve();
                        if (e.data.type === 'error') reject(new Error(e.data.error));
                    };
                    whisperWorker.postMessage({ type: 'init' });
                });
                logger.success("Whisper Worker Ready");

                // 2. LLM Init (Using Kernel)
                state.loadingText = "Step 2/2: Config Qwen2.5 (Brain)...";
                await initLLM();

                state.isLoading = false;
                initVisualizer();
                logger.success("Root Mic Online");
            } catch (e) {
                logger.error(e.message);
                state.loadingText = `Error: ${e.message}`;
            }
        }

        // Clarify Logic
        document.getElementById('clarify-btn').addEventListener('click', async () => {
            if (!state.output || state.output === '...' || state.output.length < 5) return;
            
            const originalText = state.output;
            state.status = "Refining...";
            state.output = originalText + "\n\n[Refining...]";

            try {
                const reply = await llmEngine.chat.completions.create({
                    messages: [
                        { role: "system", content: "You are a professional text editor. Your task is to refine the provided speech-to-text output into clear, coherent, and polished text suitable for reading or pasting. Fix grammar and flow, but keep the original meaning intact. Output ONLY the refined text." },
                        { role: "user", content: `Refine this text:\n\n"${originalText}"` }
                    ],
                    temperature: 0.5,
                    max_tokens: 512,
                });

                const summary = reply.choices[0].message.content;
                state.output = summary; // Replace output with summary
                state.status = "Clarified";
                
                if (document.hasFocus()) {
                    navigator.clipboard.writeText(summary);
                    const t = document.getElementById('copy-toast');
                    t.innerText = "Refined Text Copied!";
                    t.classList.add('show');
                    setTimeout(() => { 
                        t.classList.remove('show');
                        t.innerText = "Copied to Clipboard!";
                    }, 2000);
                }

            } catch (e) {
                logger.error("Clarify failed: " + e.message);
                state.status = "Error";
                state.output = originalText; // Revert
            }
        });

        async function initLLM() {
            const modelId = "mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC";
            const snapdragonId = "snapdragon-mic-qwen";

            // 0. THE BLOCKER (Model Load Lock) - Serialize model loading
            logger.info("Requesting Model Load Lock...");

            try {
                await GPUController.withModelLoadLock("Root-Mic", async () => {
                    logger.success("Model Load Lock Acquired.");

                    // KERNEL CALL: Get safe GPU config
                    const gpuConfig = await getWebGPUConfig('lite');

                    if (gpuConfig.isConstrained) {
                        logger.warn(`Clamping Buffer to ${Math.round(gpuConfig.maxBufferSize/1024/1024)}MB for Mobile/XPS compatibility.`);
                    }

                    // Create device explicitly with limits
                    const device = await gpuConfig.adapter.requestDevice(gpuConfig.deviceConfig);

                    const appConfig = {
                        model_list: [{
                            model: "https://huggingface.co/" + modelId + "/resolve/main/",
                            model_id: snapdragonId,
                            model_lib: "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
                            vram_required_MB: 2000,
                            low_resource_required: true,
                            buffer_size_required_bytes: gpuConfig.maxBufferSize
                        }]
                    };

                    llmEngine = await webllm.CreateMLCEngine(snapdragonId, {
                        appConfig,
                        device,
                        initProgressCallback: (report) => {
                            state.loadingText = `Loading Brain: ${Math.round(report.progress * 100)}%`;
                        }
                    });
                });
            } catch (error) {
                state.loadingText = `Model Load Error: ${error.message}`;

                // Try to check GPU status for more information
                try {
                    const status = await GPUController.checkStatus();
                    if (status && status.locked) {
                        logger.warn(`GPU currently locked by: ${status.owner || 'unknown'}`);
                        if (status.queued && status.queued.length > 0) {
                            logger.warn(`Queue: ${status.queued.join(', ')}`);
                        }
                    }
                } catch (statusErr) {
                    logger.warn(`Could not get GPU status: ${statusErr.message}`);
                }

                throw error;
            }
        }

        function initVisualizer() {
            const container = document.getElementById('bars');
            for (let i = 0; i < 30; i++) {
                const bar = document.createElement('div');
                bar.className = 'bar';
                container.appendChild(bar);
            }
        }

        // Recording Logic
        document.getElementById('mic-btn').addEventListener('click', async () => {
            if (!state.isRecording) startRecording(); else stopRecording();
        });

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];
                mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
                mediaRecorder.onstop = processAudio;

                // Visualizer
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 64;
                source.connect(analyser);
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                animateVisualizer();

                mediaRecorder.start();
                state.isRecording = true;
                silenceStart = Date.now();
                state.status = "Listening...";
                state.output = "...";
            } catch (e) { alert(e.message); }
        }

        function stopRecording() {
            mediaRecorder.stop();
            state.isRecording = false;
            state.status = "Processing...";
            cancelAnimationFrame(animationId);
            if (audioContext) audioContext.close();
            document.querySelectorAll('.bar').forEach(b => b.style.height = '4px');
        }

        function animateVisualizer() {
            if (!state.isRecording) return;
            animationId = requestAnimationFrame(animateVisualizer);
            analyser.getByteFrequencyData(dataArray);
            const bars = document.querySelectorAll('.bar');
            let maxVol = 0;
            for (let i = 0; i < bars.length; i++) {
                const val = dataArray[i];
                if (val > maxVol) maxVol = val;
                bars[i].style.height = `${Math.max(4, (val / 255) * 50)}px`;
            }
            if (maxVol > 10) silenceStart = Date.now();
            else if (Date.now() - silenceStart > 3000) state.status = "‚ö†Ô∏è No Audio?";
        }

        async function processAudio() {
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); // Default browser format

            // 1. Decode to System Sample Rate (e.g., 48000Hz)
            const audioCtx = new AudioContext();
            const arrayBuffer = await audioBlob.arrayBuffer();
            const decodedBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            // 2. Resample to 16000Hz (Required by Whisper)
            const targetRate = 16000;
            const offlineCtx = new OfflineAudioContext(1, decodedBuffer.duration * targetRate, targetRate);
            const source = offlineCtx.createBufferSource();
            source.buffer = decodedBuffer;
            source.connect(offlineCtx.destination);
            source.start(0);
            
            const resampledBuffer = await offlineCtx.startRendering();
            let audioData = resampledBuffer.getChannelData(0);

            // NORMALIZE / AMPLIFY with Noise Gate
            let peak = 0;
            for (let i = 0; i < audioData.length; i++) {
                const val = Math.abs(audioData[i]);
                if (val > peak) peak = val;
            }

            // Cap amplification to avoid boosting silence/hiss into "Applause"
            // If peak is TOO low (silence), don't amplify at all.
            let ampFactor = 1.0;
            if (peak > 0.01 && peak < 0.5) {
                ampFactor = Math.min(0.5 / peak, 5.0); // Max 5x boost
                for (let i = 0; i < audioData.length; i++) {
                    audioData[i] = audioData[i] * ampFactor;
                }
            } else if (peak <= 0.01) {
                // Too quiet, likely silence. Don't send to Whisper or send silence.
                state.status = "Too quiet (Ignored)";
                return;
            }

            state.status = "Transcribing...";
            
            // 0. THE BLOCKER (GPU Lock)
            await GPUController.withLock("Root-Mic-Process", async () => {
                // Offload to Worker
                const rawText = await new Promise((resolve, reject) => {
                    const reqId = Date.now();
                    const handler = (e) => {
                        if (e.data.id === reqId) {
                            whisperWorker.removeEventListener('message', handler);
                            if (e.data.type === 'transcribe_result') resolve(e.data.text);
                            else reject(new Error(e.data.error));
                        }
                    };
                    whisperWorker.addEventListener('message', handler);
                    whisperWorker.postMessage({ type: 'transcribe', data: audioData, id: reqId });
                });

                // Hallucination Filter (Aggressive)
                let cleanedText = rawText.trim();
                const hallucinations = [
                    '[Music]', '[BLANK_AUDIO]', 'Computed', '*sigh*', '*breathing*', 
                    'Applause', 'Thank you', 'Subtitles', 'Amara.org', 'Copyright', 
                    '¬©', 'Caption', 'Sovereign' 
                ];
                
                hallucinations.forEach(h => { 
                    const regex = new RegExp(h.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'gi');
                    cleanedText = cleanedText.replace(regex, '').trim();
                });

                // Filter single punctuation or very short junk
                if (/^[.,?!;:]+$/.test(cleanedText) || cleanedText.length < 2) cleanedText = "";

                if (!cleanedText || cleanedText.length < 1) {
                    state.status = "Heard nothing";
                    return;
                }

                state.output = `Raw: "${cleanedText}"\n\nCleaning...`;
                state.status = "Refining...";

                const reply = await llmEngine.chat.completions.create({
                    messages: [
                        { role: "system", content: "You are a verbatim transcription corrector. Your ONLY task is to fix grammar, spelling, and punctuation. Do NOT answer questions. Do NOT add commentary. Output ONLY the corrected text." },
                        { role: "user", content: `Correct this text: "${cleanedText}"` }
                    ],
                    temperature: 0.3,
                    max_tokens: 512,
                });

                const finalText = reply.choices[0].message.content;
                state.output = finalText;
            }); // End Lock

            state.status = "Ready";
            
            // Auto Copy (Handle focus requirement)
            if (document.hasFocus()) {
                navigator.clipboard.writeText(cleanText).then(() => {
                    const t = document.getElementById('copy-toast');
                    t.classList.add('show');
                    setTimeout(() => t.classList.remove('show'), 2000);
                }).catch(err => {
                    console.warn("Clipboard write failed (focus lost?):", err);
                });
            } else {
                console.warn("Clipboard write skipped: Document not focused.");
                state.output += "\n(Copy skipped - Click to copy)";
            }
        }

        init();
    </script>
</body>
</html>
--- END OF FILE: tools\root-mic.html ---

--- START OF FILE: tools\sovereign-db-builder.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Root Memory Builder</title>
    <style>
        :root {
            --bg: #1e1e1e;
            --text: #e0e0e0;
            --accent: #00ff88;
            --danger: #ff4444;
            --warn: #ffc107;
            --surface: #252526;
        }

        body {
            font-family: 'Segoe UI', sans-serif;
            background: var(--bg);
            color: var(--text);
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
        }

        h1 {
            border-bottom: 1px solid #333;
            padding-bottom: 10px;
            color: var(--accent);
            font-weight: 300;
            letter-spacing: 1px;
        }

        .drop-zone {
            border: 2px dashed #444;
            padding: 40px;
            text-align: center;
            margin: 20px 0;
            border-radius: 8px;
            transition: all 0.3s;
            cursor: pointer;
            background: #111;
        }

        .drop-zone:hover {
            border-color: var(--accent);
            background: var(--surface);
        }

        .log-area {
            background: #111;
            border: 1px solid #333;
            padding: 10px;
            height: 300px;
            overflow-y: auto;
            font-family: 'Consolas', monospace;
            font-size: 0.85rem;
            color: #aaa;
            border-radius: 4px;
        }

        button {
            background: #333;
            color: white;
            border: 1px solid #555;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9rem;
            margin-right: 10px;
            transition: all 0.2s;
        }

        button:hover {
            border-color: var(--accent);
            color: var(--accent);
        }

        button:disabled {
            background: #222;
            color: #555;
            border-color: #333;
            cursor: not-allowed;
        }

        button.primary {
            background: var(--accent);
            color: #000;
            border: none;
        }

        button.primary:hover {
            background: #00cc6a;
        }

        button.danger {
            background: var(--danger);
            border-color: var(--danger);
        }

        button.danger:hover {
            background: #cc0000;
        }

        .stat-box {
            display: flex;
            gap: 20px;
            margin-bottom: 20px;
        }

        .stat {
            background: var(--surface);
            padding: 15px;
            border-radius: 6px;
            flex: 1;
            text-align: center;
            border: 1px solid #333;
        }

        .stat-val {
            font-size: 1.5rem;
            font-weight: bold;
            display: block;
            margin-bottom: 5px;
        }

        .stat-label {
            font-size: 0.8rem;
            color: #888;
            text-transform: uppercase;
        }

        textarea {
            width: 100%;
            height: 60px;
            background: #111;
            color: #e0e0e0;
            border: 1px solid #555;
            padding: 10px;
            border-radius: 4px;
            resize: vertical;
            margin-bottom: 10px;
            font-family: inherit;
        }

        textarea:focus {
            outline: none;
            border-color: var(--accent);
        }
    </style>
</head>

<body>

    <h1>üå± Root Memory Builder</h1>
    <p>Ingest session logs into your local Root Graph.</p>

    <div class="stat-box">
        <div class="stat">
            <span id="db-status" class="stat-val" style="color: #ff6b6b">Offline</span>
            <span class="stat-label">Status</span>
        </div>
        <div class="stat">
            <span id="mem-count" class="stat-val">0</span>
            <span class="stat-label">Memories</span>
        </div>
        <div class="stat">
            <span id="vec-status" class="stat-val">Loading...</span>
            <span class="stat-label">Embedder</span>
        </div>
    </div>

    <div class="drop-zone" id="drop-zone">
        Drag & Drop <code>combined_memory.json</code>, logs, or text files here<br>
        <small style="color: #666">JSON, MD, TXT, PY, JS, HTML</small>
    </div>
    <input type="file" id="file-input" multiple style="display:none">

    <!-- Quick Add Section -->
    <div style="margin: 20px 0; padding: 20px; border: 1px solid #333; background: var(--surface); border-radius: 8px;">
        <h3 style="margin-top: 0; font-weight: 300;">üìù Quick Add</h3>
        <textarea id="quick-mem-content" placeholder="Type a memory here (e.g. 'Project X password is...')"></textarea>
        <div style="text-align: right;">
            <button id="quick-add-btn" class="primary">Add to Graph</button>
        </div>
    </div>

    <div style="margin-bottom: 10px; display: flex; gap: 10px; flex-wrap: wrap;">
        <button id="auto-import-btn" disabled title="Import from ./cozo_import_memory.json">Auto-Import</button>
        <button id="export-db-btn" disabled>Export JSON</button>
        <button id="query-btn" disabled>Test Query</button>
        <div style="flex-grow: 1"></div>
        <button id="reset-btn" class="danger" disabled>Nuke Database</button>
    </div>

    <div class="log-area" id="logs"></div>

    <script type="module">
        import { SovereignLogger, initCozo, createStore } from './modules/sovereign.js';
        import { loadAllFromIndexedDb, closeDatabase, clearIndexedDbStore, flushPendingWrites } from './indexeddb.js';
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

        env.allowLocalModels = false;

        // --- 0. KERNEL SETUP ---
        const logger = new SovereignLogger('Root-Builder');

        // Wrap logger to output to DOM as well
        const originalInfo = logger.info.bind(logger);
        logger.info = (msg) => { originalInfo(msg); appendLog(msg, 'info'); };

        const originalWarn = logger.warn.bind(logger);
        logger.warn = (msg) => { originalWarn(msg); appendLog(msg, 'warn'); };

        const originalError = logger.error.bind(logger);
        logger.error = (msg) => { originalError(msg); appendLog(msg, 'error'); };

        const originalSuccess = logger.success.bind(logger);
        logger.success = (msg) => { originalSuccess(msg); appendLog(msg, 'success'); };

        function appendLog(msg, type) {
            const logs = document.getElementById('logs');
            const line = document.createElement('div');
            const time = new Date().toLocaleTimeString();
            line.textContent = `[${time}] ${msg}`;
            if (type === 'error') line.style.color = '#ff6b6b';
            if (type === 'success') line.style.color = '#00ff88';
            if (type === 'warn') line.style.color = '#ffc107';
            logs.appendChild(line);
            logs.scrollTop = logs.scrollHeight;
        }

        const { state, subscribe } = createStore({
            dbStatus: 'Offline',
            dbColor: '#ff6b6b',
            memCount: 0,
            embedderStatus: 'Loading...',
            embedderColor: '#ffc107',
            isReady: false
        });

        // UI Bindings
        subscribe((prop, val) => {
            if (prop === 'dbStatus') {
                const el = document.getElementById('db-status');
                el.innerText = val;
                el.style.color = state.dbColor;
            }
            if (prop === 'memCount') document.getElementById('mem-count').innerText = val;
            if (prop === 'embedderStatus') {
                const el = document.getElementById('vec-status');
                el.innerText = val;
                el.style.color = state.embedderColor;
            }
            if (prop === 'isReady' && val === true) {
                document.getElementById('auto-import-btn').disabled = false;
                document.getElementById('export-db-btn').disabled = false;
                document.getElementById('reset-btn').disabled = false;
                document.getElementById('query-btn').disabled = false;
            }
        });

        let db;
        let embedder;
        let CozoDbClass;

        // --- 1. INITIALIZATION ---
        async function init() {
            try {
                logger.info("Initializing Root Kernel...");

                // Load Cozo WASM via Kernel
                CozoDbClass = await initCozo('./cozo_lib_wasm_bg.wasm');

                // Probe IndexedDB (Recovery Logic)
                try {
                    const [keys, items] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });
                    logger.info(`Storage Probe: Found ${keys.length} items in persistence layer.`);
                    closeDatabase(); // Important: Release lock

                    // Try Persistent Load
                    try {
                        const dbPromise = CozoDbClass.new_from_indexed_db('coda_memory', 'cozo_store', () => { });
                        // Timeout protection
                        const timeoutPromise = new Promise((_, reject) => setTimeout(() => reject(new Error('DB Load Timeout')), 5000));
                        db = await Promise.race([dbPromise, timeoutPromise]);

                        state.dbStatus = "Active (Persistent)";
                        state.dbColor = "#00ff88";
                        logger.success("Root Graph Online (Persistent).");
                    } catch (e) {
                        logger.warn(`Persistence load failed: ${e.message}. Clearing corruption...`);
                        await clearIndexedDbStore('coda_memory', 'cozo_store');
                        // Fallback to In-Memory
                        db = CozoDbClass.new();
                        state.dbStatus = "Active (Memory-Only)";
                        state.dbColor = "#ffc107";
                        logger.warn("Fallback to In-Memory DB. Data will not be saved.");
                    }

                } catch (e) {
                    logger.warn("Storage probe failed. Creating fresh in-memory DB.");
                    db = CozoDbClass.new();
                    state.dbStatus = "Active (Memory-Only)";
                    state.dbColor = "#ffc107";
                }

                // Global Exposure for Debugging
                window.db = db;
                window.runQuery = safeRun;

                // Load Embedder
                pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2').then(pipe => {
                    embedder = pipe;
                    state.embedderStatus = "Ready";
                    state.embedderColor = "#00ff88";
                    logger.success("Neural Embedder Loaded.");
                }).catch(e => {
                    state.embedderStatus = "Error";
                    state.embedderColor = "#ff4444";
                    logger.error(`Embedder failed: ${e.message}`);
                });

                // Schema Sync
                await ensureSchema();
                await updateStats();
                state.isReady = true;

            } catch (e) {
                logger.error(`CRITICAL FAILURE: ${e.message}`);
            }
        }

        async function safeRun(query, params = "{}") {
            if (!db) throw new Error("DB offline");
            const res = await db.run(query, params);
            let parsed = res;
            if (typeof res === 'string') {
                try { parsed = JSON.parse(res); } catch (e) { }
            }
            if (parsed && parsed.ok === false) throw new Error(parsed.message);
            return parsed;
        }

        async function ensureSchema() {
            try {
                // 1. Check if table exists
                await safeRun("?[id] := *memory{id} :limit 1", "{}");

                // 2. Migration: Try to add new columns if they don't exist
                // (CozoDB ignores 'add' if column exists or throws specific error, we catch to be safe)
                try { await safeRun("::alter memory add mime_type: String?", "{}"); } catch (e) { }
                try { await safeRun("::alter memory add blob_ref: String?", "{}"); } catch (e) { }

            } catch (e) {
                logger.warn("Schema missing. Creating Root Graph schema...");
                await safeRun(`
                    :create memory {
                        id: String =>
                        timestamp: Int,
                        role: String,
                        content: String,
                        source: String,
                        embedding: <F32; 384>,
                        mime_type: String?,
                        blob_ref: String?
                    }
                `, "{}");
                logger.success("Schema Created.");
            }
        }

        async function updateStats() {
            try {
                const res = await safeRun(`?[count(id)] := *memory{id}`, "{}");
                if (res.rows && res.rows.length) state.memCount = res.rows[0][0];
            } catch (e) { }
        }

        // --- 2. INGESTION LOGIC ---
        const SOV_BATCH_SIZE = 50;

        async function handleFiles(files) {
            if (!embedder || !db) return logger.error("System not ready.");
            for (const file of files) {
                logger.info(`Reading ${file.name} (${file.type || 'unknown'})...`);
                try {
                    let records = [];

                    // Binary Handling (Images/Audio)
                    if (file.type.startsWith('image/') || file.type.startsWith('audio/')) {
                        records = [{
                            role: 'user',
                            source: file.name,
                            timestamp: file.lastModified,
                            content: `[BINARY_REF] ${file.name}`, // Placeholder for text search
                            mime_type: file.type,
                            blob_ref: file.name
                        }];
                    }
                    // Text Handling
                    else {
                        const text = await file.text();

                        if (file.name.endsWith('.json')) {
                            const json = JSON.parse(text);
                            if (Array.isArray(json)) records = json;
                            else if (json.conversations) records = json.conversations;
                            else if (json.relations) {
                                // Extract rows logic simplified
                                const mem = json.relations.find(r => r.name === 'memory') || json.relations[0];
                                if (mem) {
                                    const hdr = mem.headers || ['id', 'timestamp', 'role', 'content', 'source', 'embedding'];
                                    records = (mem.rows || []).map(r => {
                                        const obj = {};
                                        hdr.forEach((h, i) => obj[h] = r[i]);
                                        return obj;
                                    });
                                }
                            }
                        } else {
                            // Text file
                            records = [{
                                role: 'system',
                                source: file.name,
                                timestamp: file.lastModified,
                                content: text
                            }];
                        }
                    }

                    await processRecords(records, file.name);
                } catch (e) {
                    logger.error(`Failed ${file.name}: ${e.message}`);
                }
            }
            await updateStats();
        }

        async function processRecords(records, sourceName) {
            let batch = [];
            for (const rec of records) {
                let content = rec.content || rec.message || "";
                if (!content) continue;
                if (content.length > 20000) content = content.substring(0, 20000) + "...[TRUNCATED]";

                let embedding = rec.embedding || [];
                if (!embedding.length) {
                    try {
                        const out = await embedder(content, { pooling: 'mean', normalize: true });
                        embedding = Array.from(out.data);
                    } catch (e) {
                        logger.warn(`Embedding failed for item in ${sourceName}, using zero vector.`);
                        embedding = new Array(384).fill(0.0);
                    }
                }

                // Hardened Timestamp Logic
                let ts = rec.timestamp ? new Date(rec.timestamp).getTime() : Date.now();
                if (isNaN(ts)) ts = Date.now();

                const id = `${ts}-${Math.random().toString(36).substr(2, 9)}`;
                const mime = rec.mime_type || null;
                const ref = rec.blob_ref || null;

                batch.push([id, ts, rec.role || 'unknown', content, rec.source || sourceName, embedding, mime, ref]);

                if (batch.length >= SOV_BATCH_SIZE) {
                    await insertBatch(batch);
                    batch = [];
                }
            }
            if (batch.length) await insertBatch(batch);
            try { await flushPendingWrites(); } catch (e) { }
            logger.success(`Imported ${records.length} items from ${sourceName}`);
        }

        async function insertBatch(rows) {
            // Strict Schema Mapping: Variables must match Schema column names for :put
            const q = `
                ?[id, timestamp, role, content, source, embedding, mime_type, blob_ref] <- $data
                :put memory { id, timestamp, role, content, source, embedding, mime_type, blob_ref }
            `;
            
            try {
                await safeRun(q, JSON.stringify({ data: rows }));
            } catch (e) {
                logger.error(`Batch Insert Failed: ${e.message}`);
                
                // Fallback for Legacy Schema (6 columns)
                // We must bind all 8 items from the input array, but only use the first 6
                if (e.message.includes("column") || e.message.includes("arity")) {
                    logger.warn("Retrying with legacy schema (ignoring binary fields)...");
                    const legacyQ = `
                        ?[id, timestamp, role, content, source, embedding, _mime, _ref] <- $data
                        :put memory { id, timestamp, role, content, source, embedding }
                    `;
                    await safeRun(legacyQ, JSON.stringify({ data: rows }));
                } else {
                    throw e;
                }
            }
        }

        // --- 3. EVENT LISTENERS ---
        document.getElementById('drop-zone').addEventListener('click', () => document.getElementById('file-input').click());
        document.getElementById('file-input').addEventListener('change', (e) => handleFiles(e.target.files));

        document.getElementById('drop-zone').addEventListener('dragover', (e) => { e.preventDefault(); e.target.style.borderColor = '#00ff88'; });
        document.getElementById('drop-zone').addEventListener('drop', (e) => { e.preventDefault(); e.target.style.borderColor = '#444'; handleFiles(e.dataTransfer.files); });

        document.getElementById('quick-add-btn').addEventListener('click', async () => {
            const val = document.getElementById('quick-mem-content').value.trim();
            if (!val) return;
            await processRecords([{ role: 'manual', content: val, source: 'user_input' }], 'QuickAdd');
            document.getElementById('quick-mem-content').value = "";
            await updateStats();
        });

        document.getElementById('reset-btn').addEventListener('click', async () => {
            if (confirm("NUKE DATABASE? This destroys all data.")) {
                await clearIndexedDbStore('coda_memory', 'cozo_store');
                location.reload();
            }
        });

        document.getElementById('query-btn').addEventListener('click', async () => {
            try {
                const res = await safeRun("?[ts, content] := *memory{timestamp: ts, content} :sort -ts :limit 5");
                if (res.rows) logger.info("Recent:\n" + res.rows.map(r => `${new Date(r[0]).toLocaleTimeString()}: ${r[1].substring(0, 50)}...`).join('\n'));
                else logger.warn("No rows returned.");
            } catch (e) { logger.error(e.message); }
        });

        document.getElementById('auto-import-btn').addEventListener('click', async () => {
            try {
                const res = await fetch('./cozo_import_memory.json');
                const json = await res.json();
                let recs = Array.isArray(json) ? json : (json.conversations || []);
                if (recs.length) await processRecords(recs, 'auto-import');
                else logger.warn("No records found in auto-import file.");
            } catch (e) { logger.error(e.message); }
        });

        document.getElementById('export-db-btn').addEventListener('click', async () => {
            try {
                logger.info("Exporting Root Memory...");
                // Export the 'memory' relation including vectors
                const jsonStr = await db.export_relations(JSON.stringify({relations: ["memory"]}));
                
                // Create Blob and Download
                const blob = new Blob([jsonStr], {type: "application/json"});
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = `root_coda_memory_dump_${Date.now()}.json`;
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
                
                logger.success("Export Complete. You can now drag this file into another Root Coda instance.");
            } catch (e) {
                logger.error("Export Failed: " + e.message);
            }
        });

        init();
    </script>
</body>

</html>
--- END OF FILE: tools\sovereign-db-builder.html ---

--- START OF FILE: tools\start-bridge.bat ---
@echo off
echo Starting WebGPU Bridge...
echo This bridge allows external tools (like Wave Terminal) to talk to the browser.
echo.
set BRIDGE_PORT=8080
set BRIDGE_TOKEN=sovereign-secret
python webgpu_bridge.py
pause
--- END OF FILE: tools\start-bridge.bat ---

--- START OF FILE: tools\test_orchestrator.py ---
import unittest
from unittest.mock import patch, MagicMock
from orchestrator import Orchestrator, MLCConnectionError

class TestOrchestrator(unittest.TestCase):

    def setUp(self):
        self.orc = Orchestrator()

    @patch('requests.get')
    def test_load_mlc_model_success(self, mock_get):
        # Mock successful bridge connection
        mock_response = MagicMock()
        mock_response.json.return_value = {"data": [{"id": "webgpu-chat"}]}
        mock_response.status_code = 200
        mock_get.return_value = mock_response

        result = self.orc.load_mlc_model("my-model")
        self.assertTrue(result)
        self.assertEqual(self.orc.active_model, "my-model")

    @patch('requests.get')
    def test_load_mlc_model_failure(self, mock_get):
        # Mock connection error
        mock_get.side_effect = Exception("Connection refused")
        
        with self.assertRaises(MLCConnectionError):
            self.orc.load_mlc_model("my-model")

    @patch('requests.post')
    def test_invoke_mlc_inference_success(self, mock_post):
        self.orc.active_model = "test-model"
        
        # Mock successful inference
        mock_response = MagicMock()
        mock_response.json.return_value = {
            "choices": [{"message": {"content": "Hello from MLC"}}]
        }
        mock_response.status_code = 200
        mock_post.return_value = mock_response

        output = self.orc.invoke_mlc_inference("Hi")
        self.assertEqual(output, "Hello from MLC")

    def test_invoke_without_load(self):
        with self.assertRaises(ValueError):
            self.orc.invoke_mlc_inference("Hi")

if __name__ == '__main__':
    unittest.main()
--- END OF FILE: tools\test_orchestrator.py ---

--- START OF FILE: tools\webgpu-server-chat.html ---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>WebGPU Chat Server (Worker)</title>
    <style>
        body {
            background: #001;
            color: #0f0;
            font-family: monospace;
            padding: 20px;
        }

        .log {
            margin-top: 10px;
            color: #8c8;
            font-size: 0.9em;
        }

        .success {
            color: #0f0;
        }

        .error {
            color: #f00;
        }

        #status {
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 20px;
        }

        input,
        select {
            background: #222;
            color: #fff;
            border: 1px solid #444;
            padding: 5px;
        }

        button {
            background: #060;
            color: #fff;
            border: none;
            padding: 5px 15px;
            cursor: pointer;
        }
    </style>
</head>

<body>
    <div id="status">üî¥ Disconnected</div>
    <div>
        <label>Chat Model:</label>
        <input type="text" id="model-input" value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC" style="width: 400px;">
        <!-- Common options helper -->
        <select id="model-helper" onchange="document.getElementById('model-input').value = this.value">
            <option value="">-- Presets --</option>
            <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5-1.5B (Fast)</option>
            <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5-7B (Balanced)</option>
            <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek-R1-7B</option>
            <option value="DeepSeek-V2-Lite-Chat-q4f16_1-MLC">DeepSeek-V2-Lite</option>
            <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama-3.1-8B</option>
        </select>
    </div>
    <div style="margin-top: 10px;">
        <label>Bridge URL:</label>
        <input type="text" id="bridge-url" value="ws://localhost:8080/ws/chat" style="width: 300px;">
        <button id="connect-btn">Start Server</button>
    </div>
    <div id="logs" style="margin-top: 20px; border-top: 1px solid #333;"></div>

    <script type="module">
        import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        const statusDiv = document.getElementById('status');
        const logsDiv = document.getElementById('logs');
        const connectBtn = document.getElementById('connect-btn');
        const modelInput = document.getElementById('model-input');

        let engine = null;
        let ws = null;

        function log(msg, type = 'log') {
            const div = document.createElement('div');
            div.className = type;
            div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
            logsDiv.prepend(div);
        }

        connectBtn.onclick = async () => {
            connectBtn.disabled = true;
            const model = modelInput.value;

            try {
                log(`Loading WebGPU Engine (${model})...`);
                engine = await CreateMLCEngine(model, {
                    initProgressCallback: (report) => {
                        statusDiv.textContent = `üü° Loading Model: ${report.text}`;
                    }
                });
                log("Engine Loaded.", "success");

                connectWebSocket();
            } catch (e) {
                log(`Engine Load Failed: ${e.message}`, "error");
                connectBtn.disabled = false;
            }
        };

        function connectWebSocket() {
            const bridgeUrl = document.getElementById('bridge-url').value;
            log(`Connecting to Bridge (${bridgeUrl})...`);
            ws = new WebSocket(bridgeUrl);

            ws.onopen = () => {
                statusDiv.textContent = "üü¢ Bridge Connected (Ready for Requests)";
                log("Bridge Connected.", "success");
            };

            ws.onclose = () => {
                statusDiv.textContent = "üî¥ Disconnected";
                log("Bridge Disconnected. Retrying in 3s...", "error");
                setTimeout(connectWebSocket, 3000);
            };

            ws.onmessage = async (event) => {
                const msg = JSON.parse(event.data);
                if (msg.type === 'chat') {
                    handleChatRequest(msg.id, msg.data);
                }
            };
        }

        async function handleChatRequest(reqId, data) {
            log(`Chat Request ${reqId.slice(0, 8)}...`);
            try {
                // OpenAI Compatibility Mapping
                const messages = data.messages;
                const stream = data.stream !== false; // Default to true if undefined

                const completion = await engine.chat.completions.create({
                    messages,
                    stream: true,
                    temperature: data.temperature || 0.7,
                    max_tokens: data.max_tokens || 4096
                });

                for await (const chunk of completion) {
                    // Forward chunk to bridge
                    ws.send(JSON.stringify({
                        id: reqId,
                        chunk: chunk
                    }));
                }

                // Signal done
                ws.send(JSON.stringify({
                    id: reqId,
                    done: true
                }));

                log(`Request ${reqId.slice(0, 8)} Complete.`, "success");

            } catch (e) {
                log(`Request Failed: ${e.message}`, "error");
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ id: reqId, error: e.message }));
                }
            }
        }
    </script>
</body>

</html>
--- END OF FILE: tools\webgpu-server-chat.html ---

--- START OF FILE: tools\webgpu_bridge.py ---
import asyncio
import uvicorn
import json
import uuid
import os
import time
from collections import deque
from fastapi import FastAPI, WebSocket, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any

app = FastAPI(title="WebGPU Bridge")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Store active WebSocket connections
# We use a simple single-worker model for now (one browser tab per role)
workers: Dict[str, WebSocket] = {
    "chat": None,
    "embed": None
}

# Store pending response futures
# Map: request_id -> asyncio.Queue
active_requests: Dict[str, asyncio.Queue] = {}

import heapq

# --- GPU Priority Manager (The Traffic Cop) ---
class PriorityGPUManager:
    def __init__(self):
        self.current_owner: str = None
        self.locked_at: float = 0
        self.lock_token: str = None
        # Queue stores: (priority, timestamp, event, requester_id)
        # Using a list + sort is sufficient for small N (waiters < 10)
        self.queue = []
        # Track request start times to prevent starvation
        self.request_start_times = {}

    def _get_priority(self, requester_id: str) -> int:
        rid = requester_id.lower()
        if "mic" in rid: return 0        # üö® PRIORITY 1: Voice
        if "console" in rid: return 10   # üí¨ PRIORITY 2: Chat
        if "chat" in rid: return 10
        if "dream" in rid: return 20     # üí§ PRIORITY 3: Dreaming
        return 15                        # Default

    async def acquire(self, requester_id: str, timeout: float = 120.0) -> tuple[bool, str]:
        """
        Waits for the lock based on PRIORITY with improved timeout handling.
        """
        priority = self._get_priority(requester_id)

        # Track when this request started to prevent starvation
        self.request_start_times[requester_id] = time.time()

        # 1. Fast Path: If free, take it immediately
        if self.current_owner is None:
            self._take_lock(requester_id)
            if requester_id in self.request_start_times:
                del self.request_start_times[requester_id]
            return True, self.lock_token

        # 2. Slow Path: Queue up
        event = asyncio.Event()
        # Tuple: (Priority, Time, Event, ID) -> Sorts by Priority asc, then Time asc
        entry = (priority, time.time(), event, requester_id)
        self.queue.append(entry)
        self.queue.sort(key=lambda x: (x[0], x[1])) # Strict sorting

        log(f"‚è≥ {requester_id} QUEUED (Priority {priority}). Pos: {self.queue.index(entry)+1}/{len(self.queue)}")

        try:
            await asyncio.wait_for(event.wait(), timeout=timeout)
            if requester_id in self.request_start_times:
                del self.request_start_times[requester_id]
            return True, self.lock_token
        except asyncio.TimeoutError:
            if entry in self.queue:
                self.queue.remove(entry)
                if requester_id in self.request_start_times:
                    del self.request_start_times[requester_id]
            log(f"üíÄ Timeout dropping {requester_id}")
            return False, None

    def release(self, requester_id: str, force: bool = False):
        if self.current_owner != requester_id and not force:
            return False

        duration = int(time.time() - self.locked_at)
        log(f"üîì RELEASED by {requester_id} (Held {duration}s)")

        self.current_owner = None
        self.locked_at = 0
        self.lock_token = None

        # 3. Wake the Next Highest Priority
        if self.queue:
            # Pop index 0 (Lowest Priority Number = Highest Importance)
            prio, ts, event, next_id = self.queue.pop(0)
            if next_id in self.request_start_times:
                del self.request_start_times[next_id]
            self._take_lock(next_id)
            event.set() # Wake up the waiting coroutine
        else:
            # Clear any remaining request start times if queue is empty
            self.request_start_times.clear()

        return True

    def _take_lock(self, requester_id):
        self.current_owner = requester_id
        self.locked_at = time.time()
        self.lock_token = str(uuid.uuid4())
        log(f"üîí LOCKED by {requester_id}")

    def get_status(self):
        return {
            "locked": self.current_owner is not None,
            "owner": self.current_owner,
            "queue_depth": len(self.queue),
            "queued": [x[3] for x in self.queue], # List queued IDs
            "request_start_times": {req_id: start_time for req_id, start_time in self.request_start_times.items()}
        }

    def force_release_all(self):
        """Emergency method to clear all locks and queues"""
        self.current_owner = None
        self.locked_at = 0
        self.lock_token = None
        # Cancel all waiting events
        for _, _, event, req_id in self.queue:
            event.set()  # Wake up all waiting coroutines
        self.queue.clear()
        self.request_start_times.clear()
        log("‚ö†Ô∏è  ALL GPU LOCKS FORCE RELEASED")

gpu_lock = PriorityGPUManager()

@app.post("/v1/gpu/lock")
async def acquire_gpu_lock(request: Request):
    body = await request.json()
    requester = body.get("id", "unknown")
    # Default wait time: 60s
    success, token = await gpu_lock.acquire(requester, timeout=60.0)
    
    if success:
        return {"status": "acquired", "token": token}
    else:
        return JSONResponse(
            status_code=503, 
            content={"status": "timeout", "msg": "GPU Queue Timeout"}
        )

@app.post("/v1/gpu/unlock")
async def release_gpu_lock(request: Request):
    body = await request.json()
    requester = body.get("id", "unknown")
    gpu_lock.release(requester)
    return {"status": "released"}

# Auto-release watchdog (Optional, for now handled by timeouts)
@app.post("/v1/gpu/reset")
async def reset_gpu_lock():
    gpu_lock.release("admin", force=True)
    return {"status": "reset"}

@app.post("/v1/gpu/force-release-all")
async def force_release_all_gpu_locks():
    """Emergency endpoint to clear all GPU locks and queues"""
    gpu_lock.force_release_all()
    return {"status": "all_gpu_locks_force_released"}

@app.get("/v1/gpu/status")
async def gpu_status():
    return gpu_lock.get_status()

# --- Logging / observability ---
# Keep a ring buffer of recent bridge logs for the HTML log viewer.
_LOG_MAX_LINES = int(os.getenv("BRIDGE_LOG_MAX_LINES", "5000"))
_LOG_MAX_CHARS_PER_LINE = int(os.getenv("BRIDGE_LOG_MAX_CHARS_PER_LINE", "400"))
# If enabled, log streamed content deltas (can be noisy but is what you want for prompt-pipeline debugging)
_LOG_STREAM_DELTAS = os.getenv("BRIDGE_LOG_STREAM_DELTAS", "true").strip().lower() in ("1", "true", "yes", "on")

_bridge_logs: deque[tuple[int, str]] = deque(maxlen=_LOG_MAX_LINES)
_bridge_log_seq: int = 0

# Per-request telemetry (timing/throughput)
_req_meta: Dict[str, Dict[str, Any]] = {}


def _clip(s: str, max_chars: int) -> str:
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    return s[: max_chars - 1] + "‚Ä¶"

def log(msg: str):
    import datetime
    from pathlib import Path
    
    timestamp = datetime.datetime.now().isoformat()
    entry = f"{timestamp} - {msg}"
    print(entry)
    global _bridge_log_seq
    _bridge_log_seq += 1
    _bridge_logs.append((_bridge_log_seq, entry))
        
    # Write to file
    try:
        log_path = Path("../backend/logs/webgpu_bridge.log")
        log_path.parent.mkdir(parents=True, exist_ok=True)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
    except Exception as e:
        print(f"Failed to write to log file: {e}")

@app.get("/logs")
async def get_logs(limit: int = 200, since: int = 0):
    """Return recent bridge logs.

    Query params:
    - limit: max number of log lines to return
    - since: return only entries with seq > since

    Response:
    - logs: [{seq, line}, ...]
    - last_seq: latest sequence number currently available
    """
    # Snapshot to avoid holding references while iterating
    items = list(_bridge_logs)
    if since and since > 0:
        items = [it for it in items if it[0] > since]
    if limit and limit > 0:
        items = items[-limit:]
    last_seq = items[-1][0] if items else _bridge_log_seq
    return {
        "logs": [{"seq": seq, "line": line} for seq, line in items],
        "last_seq": last_seq,
    }


@app.post("/logs/clear")
async def clear_logs():
    _bridge_logs.clear()
    return {"ok": True}

@app.get("/v1/models")
async def list_models():
    """
    Return a list of available models.
    Since this is a bridge, we return the models currently loaded in the connected workers.
    """
    models = []
    
    if workers["chat"]:
        # We could query the worker for the actual model name, but for now we'll use a placeholder
        # or assume the client knows what it's doing.
        # Ideally, the worker should send its loaded model ID on connection.
        models.append({
            "id": "webgpu-chat",
            "object": "model",
            "created": 1677610602,
            "owned_by": "webgpu-bridge"
        })
        
    if workers["embed"]:
        models.append({
            "id": "webgpu-embedding",
            "object": "model",
            "created": 1677610602,
            "owned_by": "webgpu-bridge"
        })
        
    return {"object": "list", "data": models}


# --- Compatibility endpoint: audit/server-logs ---
# Some UI pages (log-viewer.html) expect '/audit/server-logs' provided by the full backend (ECE_Core).
# If that backend isn't running, provide a graceful fallback here that returns bridge logs so UI doesn't see 404s.
from pathlib import Path

@app.get('/audit/server-logs')
async def get_audit_server_logs(limit: int = 50):
    try:
        # Try known workspace locations for the backend server log (same heuristic used elsewhere)
        log_file = Path("logs/server.log")
        if not log_file.exists():
            log_file = Path("../logs/server.log")

        if log_file.exists():
            with log_file.open('r', encoding='utf-8', errors='ignore') as f:
                lines = f.read().splitlines()
            tail = lines[-int(limit):] if limit and len(lines) > 0 else lines
            return {"logs": tail, "count": len(tail)}

        # Fallback: serve bridge internal logs if backend log file not present
        items = list(_bridge_logs)[-int(limit):]
        tail = [line for (_seq, line) in items]
        return {"logs": tail, "count": len(tail)}
    except Exception as e:
        return {"logs": [f"Bridge audit logs error: {str(e)}"], "count": 0}

@app.websocket("/ws/{worker_type}")
async def websocket_endpoint(websocket: WebSocket, worker_type: str):
    if worker_type not in workers:
        await websocket.close(code=4000)
        return
    
    await websocket.accept()
    workers[worker_type] = websocket
    log(f"‚úÖ {worker_type.upper()} Worker Connected")
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # Log token usage if present
            if "usage" in message:
                log(f"üìä Token Usage: {json.dumps(message['usage'])}")
            elif "chunk" in message and "usage" in message["chunk"]:
                 # Sometimes usage is inside the chunk
                 log(f"üìä Token Usage (Chunk): {json.dumps(message['chunk']['usage'])}")

            req_id = message.get("id")

            # Capture streamed token/chunk content for debugging.
            # The worker typically sends OpenAI-compatible SSE chunks under `chunk`.
            if req_id and isinstance(message, dict):
                # Handle chunk deltas
                if _LOG_STREAM_DELTAS and "chunk" in message:
                    try:
                        ch = message.get("chunk")
                        # Try chat-completions delta
                        if isinstance(ch, dict) and isinstance(ch.get("choices"), list) and ch["choices"]:
                            choice0 = ch["choices"][0] if isinstance(ch["choices"][0], dict) else {}
                            delta = choice0.get("delta") if isinstance(choice0, dict) else None
                            if isinstance(delta, dict) and "content" in delta:
                                piece = delta.get("content") or ""
                                if piece:
                                    log(f"Œî {req_id}: {_clip(piece, _LOG_MAX_CHARS_PER_LINE)}")
                                    meta = _req_meta.get(req_id)
                                    if meta is not None:
                                        meta["chars"] = int(meta.get("chars", 0)) + len(piece)
                                        meta["chunks"] = int(meta.get("chunks", 0)) + 1
                    except Exception:
                        pass

                # Handle token_events emitted by the browser worker
                if "token_events" in message and isinstance(message.get("token_events"), list):
                    try:
                        events = message.get("token_events")
                        meta = _req_meta.get(req_id)
                        for ev in events:
                            # Normalize fields
                            tidx = ev.get("idx")
                            ttext = _clip(ev.get("text", ""), _LOG_MAX_CHARS_PER_LINE)
                            tdt = float(ev.get("dt_ms") or ev.get("dt") or 0.0)
                            tlogp = ev.get("logprob")
                            # Log a compact token-line for the viewer to parse
                            log(f"TOK {req_id} IDX={tidx} DTms={tdt:.2f} TOK=" + ttext + (" LOGP=" + str(tlogp) if tlogp is not None else ""))
                            if meta is not None:
                                meta["chars"] = int(meta.get("chars", 0)) + len(ttext)
                                meta["chunks"] = int(meta.get("chunks", 0)) + 1
                    except Exception:
                        pass
            if req_id in active_requests:
                # Put the chunk/result into the queue for the HTTP handler to consume
                await active_requests[req_id].put(message)
                
    except Exception as e:
        log(f"‚ùå {worker_type.upper()} Worker Disconnected: {e}")
    finally:
        workers[worker_type] = None

async def stream_generator(req_id: str):
    queue = active_requests[req_id]
    try:
        while True:
            message = await queue.get()
            
            if message.get("error"):
                yield f"data: {json.dumps({'error': message['error']})}\n\n"
                break
                
            if message.get("done"):
                # Emit summary telemetry if available
                meta = _req_meta.pop(req_id, None)
                if meta is not None:
                    dt = max(1e-6, time.perf_counter() - float(meta.get("t0", time.perf_counter())))
                    chars = int(meta.get("chars", 0))
                    chunks = int(meta.get("chunks", 0))
                    log(f"‚úÖ Done {req_id}: {chars} chars in {dt:.2f}s across {chunks} chunks (~{(chars / dt):.1f} chars/s)")
                yield "data: [DONE]\n\n"
                break
            
            # Forward the chunk exactly as received (OpenAI format)
            if "chunk" in message:
                yield f"data: {json.dumps(message['chunk'])}\n\n"
            
    finally:
        if req_id in active_requests:
            del active_requests[req_id]

async def collect_full_response(req_id: str):
    """
    Collects the full response for non-streaming requests.
    Aggregates chunks if the worker sends them as a stream.
    """
    queue = active_requests[req_id]
    
    # Initialize accumulator
    accumulated_content = ""
    first_chunk = None
    finish_reason = None
    
    try:
        while True:
            message = await queue.get()
            
            if message.get("error"):
                raise HTTPException(status_code=500, detail=message['error'])
                
            if message.get("done"):
                break
            
            if "chunk" in message:
                chunk = message["chunk"]
                
                # If it's a full response object (not a delta), just use it
                if "choices" in chunk and "message" in chunk["choices"][0]:
                    return chunk
                
                # Otherwise, accumulate deltas
                if not first_chunk:
                    first_chunk = chunk
                
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0].get("delta", {})
                    content = delta.get("content", "")
                    if content:
                        accumulated_content += content
                    
                    if chunk["choices"][0].get("finish_reason"):
                        finish_reason = chunk["choices"][0]["finish_reason"]
                
    finally:
        if req_id in active_requests:
            del active_requests[req_id]

        meta = _req_meta.pop(req_id, None)
        if meta is not None:
            dt = max(1e-6, time.perf_counter() - float(meta.get("t0", time.perf_counter())))
            chars = int(meta.get("chars", 0))
            chunks = int(meta.get("chunks", 0))
            log(f"‚úÖ Done {req_id}: {chars} chars in {dt:.2f}s across {chunks} chunks (~{(chars / dt):.1f} chars/s)")
            
    if not first_chunk:
        raise HTTPException(status_code=500, detail="No response received from WebGPU worker")
        
    # Construct final response object from accumulated chunks
    final_response = first_chunk.copy()
    final_response["object"] = "chat.completion"
    final_response["choices"] = [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": accumulated_content
        },
        "finish_reason": finish_reason or "stop"
    }]
        
    return final_response

@app.post("/v1/chat/completions")
@app.post("/chat/completions")
async def chat_completions(request: Request):
    if not workers["chat"]:
        raise HTTPException(status_code=503, detail="WebGPU Chat Worker not connected. Open tools/webgpu-server-chat.html")
    
    body = await request.json()
    req_id = str(uuid.uuid4())
    active_requests[req_id] = asyncio.Queue()
    
    stream = body.get("stream", False)
    log(f"Chat Request: {req_id} - Model: {body.get('model')} - Stream: {stream}")
    # Track request timing/throughput
    _req_meta[req_id] = {"t0": time.perf_counter(), "chars": 0, "chunks": 0}

    # Forward request to browser
    await workers["chat"].send_json({
        "id": req_id,
        "type": "chat",
        "data": body
    })
    
    if stream:
        # Return streaming response
        return StreamingResponse(stream_generator(req_id), media_type="text/event-stream")
    else:
        # Return standard JSON response
        response_data = await collect_full_response(req_id)
        # Log a concise summary of the chat response for UI/bridge observability
        try:
            summary = None
            if isinstance(response_data, dict):
                # Try to extract assistant content if present
                choices = response_data.get('choices')
                if choices and isinstance(choices, list) and len(choices) > 0:
                    msg = choices[0].get('message') if isinstance(choices[0], dict) else None
                    if msg and isinstance(msg, dict):
                        content = msg.get('content') or ''
                        summary = content[:200]
            if summary is None:
                summary = str(response_data)[:200]
            log(f"[WEBGPU-CHAT] RESP {req_id}: {summary}")
        except Exception:
            pass
        return JSONResponse(content=response_data)

@app.post("/v1/embeddings")
async def embeddings(request: Request):
    if not workers["embed"]:
        raise HTTPException(status_code=503, detail="WebGPU Embed Worker not connected. Open tools/webgpu-server-embed.html")
    
    body = await request.json()
    
    # Override model name to match what the worker expects/has loaded
    # The worker is strict about the model name matching its loaded model
    # body["model"] = "snowflake-arctic-embed-m-q0f32-MLC-b32" 
    
    req_id = str(uuid.uuid4())
    active_requests[req_id] = asyncio.Queue()
    
    log(f"Embed Request: {req_id} - Input length: {len(str(body.get('input')))}")

    # Forward request to browser
    await workers["embed"].send_json({
        "id": req_id,
        "type": "embedding",
        "data": body
    })
    
    # Wait for the single response (Embeddings are usually not streamed, but we use the queue for async wait)
    response_msg = await active_requests[req_id].get()
    del active_requests[req_id]
    
    if response_msg.get("error"):
        raise HTTPException(status_code=500, detail=response_msg["error"])
        
    # Log a concise summary of embedding result for visibility
    try:
        res = response_msg.get('result')
        if res:
            summary = str(res)
            log(f"[WEBGPU-EMBED] RESP {req_id}: {summary[:200]}")
    except Exception:
        pass

    return JSONResponse(content=response_msg["result"])

import secrets
import random
import mimetypes

# --- Authentication & Config ---
AUTH_TOKEN = os.getenv("BRIDGE_TOKEN")
if not AUTH_TOKEN:
    AUTH_TOKEN = secrets.token_urlsafe(16) # Generate secure token if not set

# Middleware for Auth
@app.middleware("http")
async def verify_token(request: Request, call_next):
    # Allow OPTIONS (CORS preflight) and the mobile UI page itself
    if request.method == "OPTIONS" or request.url.path in ["/mobile", "/favicon.ico", "/logs", "/health", "/audit/server-logs"]:
        return await call_next(request)
    
    # Allow local loopback without token (optional, but convenient for localhost dev)
    # client_host = request.client.host
    # if client_host == "127.0.0.1" or client_host == "localhost":
    #     return await call_next(request)

    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer ") or auth_header.split(" ")[1] != AUTH_TOKEN:
        # Return 401 but allows 403 for specific logic. 
        # Using JSON response to keep it clean.
        return JSONResponse(status_code=401, content={"error": "Unauthorized. Invalid Token."})

    return await call_next(request)

@app.get("/health")
async def health_check():
    """
    Simple health check for extensions and external tools.
    """
    return {"status": "ok", "service": "webgpu-bridge"}

@app.get("/mobile")
async def serve_mobile_app():
    """Serves the lightweight mobile chat interface."""
    file_path = "mobile-chat.html"
    # If not in current dir, check tools/
    if not os.path.exists(file_path):
        file_path = "tools/mobile-chat.html"
    
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    else:
        return HTMLResponse(content="<h1>Mobile App Not Found</h1><p>Ensure mobile-chat.html exists.</p>", status_code=404)

@app.post("/memories/search")
async def search_memories(request: Request):
    """
    Bridge endpoint for the Chrome Extension.
    Input: { "query": "User is typing about..." }
    Output: JSON array of relevant local memories.
    """
    # 1. Check if the Brain is connected
    if not workers["chat"]:
        raise HTTPException(status_code=503, detail="WebGPU Chat Worker not connected. Open tools/model-server-chat.html")
    
    try:
        body = await request.json()
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid JSON")
        
    query = body.get("query", "").strip()
    if not query:
        return JSONResponse(content=[])

    # 2. Create a Request ID to track this specific query
    req_id = str(uuid.uuid4())
    active_requests[req_id] = asyncio.Queue()
    
    # 3. Log the "Thought"
    log(f"üîé Bridge Memory Search: {req_id} - '{_clip(query, 50)}'")

    try:
        # 4. Forward the signal to the Browser (Root Console) via WebSocket
        await workers["chat"].send_json({
            "id": req_id,
            "type": "memory_query", 
            "data": {"query": query}
        })
        
        # 5. Wait for the Brain to return the Context (Timeout: 3s for speed)
        response_msg = await asyncio.wait_for(active_requests[req_id].get(), timeout=3.0)
        
        if response_msg.get("error"):
            log(f"‚ùå Search Error {req_id}: {response_msg['error']}")
            raise HTTPException(status_code=500, detail=response_msg["error"])
        
        results = response_msg.get("result", [])
        log(f"‚úÖ Served {len(results)} memories to Extension")
        return JSONResponse(content=results)

    except asyncio.TimeoutError:
        log(f"‚è∞ Search Timeout {req_id} - Browser didn't respond in 3s")
        del active_requests[req_id]
        raise HTTPException(status_code=504, detail="Query timed out")
    except Exception as e:
        if req_id in active_requests:
            del active_requests[req_id]
        raise HTTPException(status_code=500, detail=str(e))

from fastapi.responses import HTMLResponse

if __name__ == "__main__":
    host = os.getenv("BRIDGE_HOST", "0.0.0.0")
    
    # Obfuscated / Random Port Logic
    default_port = os.getenv("BRIDGE_PORT")
    if default_port:
        port = int(default_port)
    else:
        # Pick a random port in the dynamic/private range to obscure services
        port = random.randint(9000, 9999)

    print("\n" + "="*60)
    print(f"üîí SECURE BRIDGE STARTING")
    print(f"   Host: {host}")
    print(f"   Port: {port}")
    print(f"   üîë TOKEN: {AUTH_TOKEN}")
    print(f"   üì± Mobile URL: http://{host}:{port}/mobile")
    print("="*60 + "\n")
    
    uvicorn.run(app, host=host, port=port)
--- END OF FILE: tools\webgpu_bridge.py ---

--- START OF FILE: tools\modules\gpu-hot-reloader.js ---
/*
 * GPU Management Hot Reloader
 * Provides hot reload capability for GPU management in the browser
 */

class GPUHotReloader {
    constructor() {
        this.gpuController = null;
        this.checkInterval = 5000; // Check every 5 seconds
        this.lastModified = {};
        this.isReloading = false;
        this.reloaderEnabled = true;
    }

    // Enable/disable hot reload
    setEnabled(enabled) {
        this.reloaderEnabled = enabled;
        if (enabled) {
            this.startMonitoring();
        } else {
            this.stopMonitoring();
        }
    }

    // Start monitoring for changes
    startMonitoring() {
        if (this.monitorInterval) return;
        
        console.log("üîÑ GPU Hot Reloader: Starting monitoring...");
        this.monitorInterval = setInterval(() => {
            this.checkForChanges();
        }, this.checkInterval);
    }

    // Stop monitoring
    stopMonitoring() {
        if (this.monitorInterval) {
            clearInterval(this.monitorInterval);
            this.monitorInterval = null;
            console.log("üõë GPU Hot Reloader: Stopped monitoring");
        }
    }

    // Check for changes in GPU-related files
    async checkForChanges() {
        if (!this.reloaderEnabled || this.isReloading) return;

        try {
            // Check if any GPU-related files have been modified
            const gpuFiles = [
                'tools/modules/sovereign.js',
                'tools/model-server-chat.html',
                'tools/root-mic.html',
                'tools/root-dreamer.html'
            ];

            for (const file of gpuFiles) {
                const modified = await this.checkFileModified(file);
                if (modified) {
                    console.log(`üîÑ GPU Hot Reloader: Detected change in ${file}`);
                    await this.reloadGPUManagement();
                    break; // Only reload once per check cycle
                }
            }
        } catch (error) {
            console.warn('GPU Hot Reloader: Error checking for changes:', error);
        }
    }

    // Check if a file has been modified since last check
    async checkFileModified(filepath) {
        try {
            // Try to check file modification time via server endpoint
            const response = await fetch(`/file-mod-time?path=${encodeURIComponent(filepath)}`);
            if (response.ok) {
                const data = await response.json();
                const currentModTime = data.modTime;
                
                if (filepath in this.lastModified) {
                    if (this.lastModified[filepath] !== currentModTime) {
                        this.lastModified[filepath] = currentModTime;
                        return true;
                    }
                } else {
                    this.lastModified[filepath] = currentModTime;
                }
            }
        } catch (error) {
            // Fallback: try to re-fetch and compare content
            try {
                const response = await fetch(filepath + '?t=' + Date.now(), { method: 'HEAD' });
                const lastModified = response.headers.get('Last-Modified');
                
                if (filepath in this.lastModified) {
                    if (this.lastModified[filepath] !== lastModified) {
                        this.lastModified[filepath] = lastModified;
                        return true;
                    }
                } else {
                    this.lastModified[filepath] = lastModified;
                }
            } catch (e) {
                // If we can't check, assume no change to avoid constant reloads
            }
        }
        return false;
    }

    // Reload GPU management logic
    async reloadGPUManagement() {
        if (this.isReloading) return;
        
        this.isReloading = true;
        console.log("üîÑ GPU Hot Reloader: Reloading GPU management logic...");
        
        try {
            // Force release any current GPU locks to prevent stale state
            await this.forceReleaseGPU();
            
            // Clear any cached modules if possible
            this.clearModuleCache();
            
            // Reload the GPU controller with fresh logic
            await this.reloadGPUController();
            
            console.log("‚úÖ GPU Hot Reloader: GPU management reloaded successfully");
        } catch (error) {
            console.error("‚ùå GPU Hot Reloader: Error during reload:", error);
        } finally {
            this.isReloading = false;
        }
    }

    // Force release current GPU locks
    async forceReleaseGPU() {
        try {
            // Call the emergency release endpoint if available
            await fetch("http://localhost:8080/v1/gpu/force-release-all", {
                method: "POST",
                headers: { "Authorization": "Bearer sovereign-secret" }
            });
            console.log("‚úÖ GPU Hot Reloader: Force released all GPU locks");
        } catch (e) {
            console.warn("‚ö†Ô∏è GPU Hot Reloader: Could not force release GPU locks:", e);
        }
    }

    // Clear any cached modules (attempt to force reload)
    clearModuleCache() {
        // In a real implementation, this would clear module caches
        // For now, we'll just log
        console.log("üßπ GPU Hot Reloader: Clearing module cache (not implemented in browser)");
    }

    // Reload GPU controller with fresh logic
    async reloadGPUController() {
        // Since we can't truly reload modules in the browser,
        // we'll trigger a soft reload of GPU state
        if (window.GPUController) {
            // If there's a way to refresh the GPU controller state, do it here
            console.log("üîÑ GPU Hot Reloader: Refreshing GPU controller state");
        }
    }

    // Manual trigger for hot reload
    async triggerReload() {
        console.log("üîÑ GPU Hot Reloader: Manual reload triggered");
        await this.reloadGPUManagement();
    }
}

// Global instance
window.GPU_HOT_RELOADER = new GPUHotReloader();

// Auto-start if in development mode
if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    window.GPU_HOT_RELOADER.startMonitoring();
    console.log("üîÑ GPU Hot Reloader: Auto-started in development mode");
}

// Expose for manual control
window.triggerGPUHotReload = () => window.GPU_HOT_RELOADER.triggerReload();
window.setGPUHotReloadEnabled = (enabled) => window.GPU_HOT_RELOADER.setEnabled(enabled);

console.log("üîÑ GPU Hot Reloader: Initialized and ready");
--- END OF FILE: tools\modules\gpu-hot-reloader.js ---

--- START OF FILE: tools\modules\llm-worker.js ---
import { WebWorkerMLCEngineHandler, MLCEngine } from "https://esm.run/@mlc-ai/web-llm";

// The handler bridges messages between the Main Thread and the Engine
const engine = new MLCEngine();
const handler = new WebWorkerMLCEngineHandler(engine);

self.onmessage = (msg) => {
    handler.onmessage(msg);
};
--- END OF FILE: tools\modules\llm-worker.js ---

--- START OF FILE: tools\modules\sovereign.js ---
/* tools/modules/sovereign.js */

// Import CozoDB bindings from the parent directory
import initWasm, { CozoDb } from '../cozo_lib_wasm.js';

/**
 * Sovereign Coda Kernel (v2.0)
 * Standard Library for Logging, State, Hardware, and Memory.
 */

// --- 1. THE NERVOUS SYSTEM (Unified Logging) ---
export class SovereignLogger {
    constructor(sourceId) {
        this.source = sourceId;
        this.logChannel = new BroadcastChannel('sovereign-logs');
        this.codaChannel = new BroadcastChannel('coda_logs');
    }

    info(msg) { this._emit(msg, 'info'); }
    warn(msg) { this._emit(msg, 'warn'); }
    error(msg) { this._emit(msg, 'error'); }
    success(msg) { this._emit(msg, 'success'); }

    _emit(msg, type) {
        // 1. Console Fallback
        const style = type === 'error' ? 'color:red' : (type === 'success' ? 'color:green' : 'color:blue');
        console.log(`%c[${this.source}] ${msg}`, style);
        
        // 2. Broadcast to Mission Control
        const timestamp = new Date().toISOString();
        const timeShort = new Date().toLocaleTimeString();
        
        try {
            // New JSON Channel (for Mission Control)
            this.codaChannel.postMessage({
                source: this.source,
                type,
                message: msg,
                timestamp
            });
            // Legacy Channel (for Log Viewer compatibility)
            this.logChannel.postMessage({ 
                source: 'system', 
                msg: `[${this.source}] ${msg}`, 
                type, 
                time: timeShort 
            });
        } catch (e) {
            console.warn('Logger broadcast failed', e);
        }
    }
}

// --- 2. THE STATE MANAGER (Nano Store) ---
// Zero-dependency reactive state.
export function createStore(initialState) {
    const listeners = new Set();
    
    const proxy = new Proxy(initialState, {
        set(target, property, value) {
            target[property] = value;
            listeners.forEach(fn => fn(property, value));
            return true;
        }
    });

    return {
        state: proxy,
        subscribe: (fn) => listeners.add(fn),
        unsubscribe: (fn) => listeners.delete(fn)
    };
}

// --- 3. HARDWARE DETECTOR (The XPS Fix) ---
// Centralized WebGPU configuration to prevent crashes on 256MB cards.
export async function getWebGPUConfig(profile = 'mid') {
    if (!navigator.gpu) throw new Error("WebGPU not supported");
    
    // 1. Request Adapter (Prefer High Performance)
    const adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' }) 
                 || await navigator.gpu.requestAdapter();
    
    if (!adapter) throw new Error("No WebGPU Adapter found.");

    // 2. Detect Hardware Limits
    const hardwareLimit = adapter.limits.maxStorageBufferBindingSize;
    let requested = 1024 * 1024 * 1024; // Default 1GB

    // 3. Apply Profile Strategy
    if (profile === 'lite') requested = 256 * 1024 * 1024; // 256MB
    else if (profile === 'mid') requested = 1024 * 1024 * 1024; // 1GB
    else if (profile === 'high') requested = 2048 * 1024 * 1024; // 2GB

    // 4. The Safety Clamp
    const finalLimit = Math.min(requested, hardwareLimit);
    const isConstrained = finalLimit < requested;

    return {
        adapter,
        deviceConfig: {
            requiredLimits: { maxStorageBufferBindingSize: finalLimit },
            requiredFeatures: adapter.features.has("shader-f16") ? ["shader-f16"] : []
        },
        maxBufferSize: finalLimit,
        isConstrained
    };
}

// --- 4. MEMORY CORE (CozoDB Helper) ---
export async function initCozo(wasmUrl = '../cozo_lib_wasm_bg.wasm') {
    await initWasm(wasmUrl);
    return CozoDb;
}

// --- 5. THE BLOCKER (GPU Mutex) ---
class GPUController {
    static get BRIDGE_URL() { return 'http://localhost:8080'; }

    // Separate locks for different operations
    static modelLoadPromise = null;  // Promise to serialize model loading
    static activeModelLoaders = new Set();  // Track active model loaders

    // Enhanced GPU lock with retry logic and better error handling
    static async acquireLock(agentId, timeout = 120000) { // Increased default timeout to 120 seconds
        const startTime = Date.now();

        while (Date.now() - startTime < timeout) {
            try {
                // This request will HANG until the lock is available (Queue)
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), timeout);

                const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/lock`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': 'Bearer sovereign-secret'
                    },
                    body: JSON.stringify({ id: agentId }),
                    signal: controller.signal
                });

                clearTimeout(timeoutId);

                if (res.ok) {
                    const data = await res.json();
                    return { success: true, token: data.token };
                }

                // Handle specific error codes
                if (res.status === 503) {
                    const errorData = await res.json();
                    return { success: false, error: errorData.msg || `Queue Timeout (${res.status})` };
                }

                return { success: false, error: `GPU Lock Failed (${res.status})` };
            } catch (e) {
                if (e.name === 'AbortError') {
                    return { success: false, error: 'Lock acquisition timeout' };
                }

                console.warn("Bridge unreachable (No Lock)", e);

                // If bridge is down, try direct WebGPU access as fallback
                if (e.message.includes('fetch') || e.message.includes('network')) {
                    console.warn("Bridge offline, attempting direct WebGPU access...");
                    return { success: true, token: "direct-webgpu-fallback" };
                }

                return { success: false, error: e.message };
            }

            // Small delay before retry to avoid excessive polling
            await new Promise(resolve => setTimeout(resolve, 1000));
        }

        return { success: false, error: `Lock acquisition timeout after ${timeout}ms` };
    }

    static async releaseLock(agentId) {
        try {
            await fetch(`${this.BRIDGE_URL}/v1/gpu/unlock`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': 'Bearer sovereign-secret'
                },
                body: JSON.stringify({ id: agentId }),
                keepalive: true // Critical: Ensure request survives tab close
            });
        } catch (e) {
            console.warn("Failed to release lock", e);
            // Don't throw error on release failure to avoid blocking cleanup
        }
    }

    // Enhanced withLock with better error handling and retry logic
    static async withLock(agentId, taskFn, timeout = 120000) {
        const lock = await this.acquireLock(agentId, timeout);
        if (!lock.success) {
            console.error(`GPU lock acquisition failed for ${agentId}: ${lock.error}`);
            throw new Error(`Could not acquire GPU lock: ${lock.error}`);
        }

        let taskResult;
        let taskError;

        try {
            taskResult = await taskFn();
        } catch (e) {
            taskError = e;
        } finally {
            // Always try to release the lock, even if the task fails
            await this.releaseLock(agentId);
        }

        if (taskError) {
            throw taskError;
        }

        return taskResult;
    }

    // NEW: Serialize model loading to prevent multiple models loading simultaneously
    static async withModelLoadLock(agentId, taskFn, timeout = 300000) { // 5-minute timeout for model loading
        // Create a promise chain to serialize model loading at the application level
        // This ensures only one model loading operation happens at a time across all components
        const previousLoad = this.modelLoadPromise;

        // Create a new promise that waits for the previous one to complete
        this.modelLoadPromise = (async () => {
            if (previousLoad) {
                try {
                    await previousLoad;
                } catch (e) {
                    console.warn("Previous model load failed, continuing:", e);
                }
            }

            // Mark this loader as active
            this.activeModelLoaders.add(agentId);

            try {
                console.log(`[${agentId}] Starting sequential model loading (Queue: ${this.activeModelLoaders.size - 1} waiting)`);
                // The task function itself will handle GPU lock acquisition as needed
                const result = await taskFn();
                console.log(`[${agentId}] Sequential model loading completed`);
                return result;
            } finally {
                // Remove from active loaders
                this.activeModelLoaders.delete(agentId);
            }
        })();

        try {
            return await this.modelLoadPromise;
        } catch (error) {
            // Clean up on error
            this.activeModelLoaders.delete(agentId);
            throw error;
        }
    }

    // Get status of model loading queue
    static getModelLoadStatus() {
        return {
            queueSize: this.activeModelLoaders.size,
            activeLoaders: Array.from(this.activeModelLoaders),
            hasPendingLoad: this.modelLoadPromise !== null,
            modelLoadQueueInfo: `Model Load Queue: ${this.activeModelLoaders.size} active, ${this.modelLoadPromise ? 'loading' : 'idle'}`
        };
    }

    // New: Check GPU status to help with debugging
    static async checkStatus() {
        try {
            const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/status`, {
                method: 'GET',
                headers: {
                    'Authorization': 'Bearer sovereign-secret'
                }
            });

            if (res.ok) {
                return await res.json();
            }
            return { error: `Status check failed (${res.status})` };
        } catch (e) {
            return { error: e.message };
        }
    }
}

// Ensure explicit export if previous style failed in some browsers
export { GPUController };
--- END OF FILE: tools\modules\sovereign.js ---

--- START OF FILE: tools\modules\vision.js ---
/**
 * VisionController - Handles drag-and-drop, clipboard paste, and image preview logic
 */
export class VisionController {
    constructor() {
        this.imageBase64 = null;
        this.dropZone = null;
        this.previewContainer = null;
        this.inputId = null;
    }

    /**
     * Setup event listeners for the vision functionality
     * @param {string} dropZoneId - ID of the drop zone element
     * @param {string} previewContainerId - ID of the preview container element
     * @param {string} inputId - ID of the input element
     */
    setup(dropZoneId, previewContainerId, inputId) {
        this.dropZone = document.getElementById(dropZoneId);
        this.previewContainer = document.getElementById(previewContainerId);
        this.inputId = inputId;

        if (!this.dropZone || !this.previewContainer) {
            console.error('VisionController: Required elements not found');
            return;
        }

        // Prevent default drag behaviors
        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {
            this.dropZone.addEventListener(eventName, this.preventDefaults, false);
            document.body.addEventListener(eventName, this.preventDefaults, false);
        });

        // Highlight drop area when item is dragged over it
        ['dragenter', 'dragover'].forEach(eventName => {
            this.dropZone.addEventListener(eventName, this.highlight, false);
        });

        ['dragleave', 'drop'].forEach(eventName => {
            this.dropZone.addEventListener(eventName, this.unhighlight, false);
        });

        // Handle dropped files
        this.dropZone.addEventListener('drop', (e) => {
            this.handleDrop(e);
        }, false);

        // Handle clipboard paste
        document.addEventListener('paste', (e) => {
            this.handlePaste(e);
        });

        // Also allow regular file input if needed
        const fileInput = document.createElement('input');
        fileInput.type = 'file';
        fileInput.accept = 'image/*';
        fileInput.style.display = 'none';
        fileInput.id = 'vision-file-input';
        document.body.appendChild(fileInput);

        // If the drop zone is clicked, it can trigger file selection
        this.dropZone.addEventListener('click', () => {
            fileInput.click();
        });

        fileInput.addEventListener('change', (e) => {
            if (e.target.files && e.target.files[0]) {
                this.handleFile(e.target.files[0]);
            }
        });
    }

    preventDefaults(e) {
        e.preventDefault();
        e.stopPropagation();
    }

    highlight(e) {
        this.dropZone.classList.add('drag-over');
    }

    unhighlight(e) {
        this.dropZone.classList.remove('drag-over');
    }

    handleDrop(e) {
        const dt = e.dataTransfer;
        const files = dt.files;

        if (files.length > 0) {
            this.handleFile(files[0]);
        }
    }

    handlePaste(e) {
        const items = e.clipboardData.items;
        for (let i = 0; i < items.length; i++) {
            if (items[i].type.indexOf('image') !== -1) {
                const blob = items[i].getAsFile();
                if (blob) {
                    this.handleFile(blob);
                    break;
                }
            }
        }
    }

    /**
     * Handle a file by reading it as DataURL (Base64)
     * @param {File} file - The file to process
     */
    handleFile(file) {
        if (!file.type.match('image.*')) {
            console.error('VisionController: Only image files are supported');
            return;
        }

        const reader = new FileReader();
        
        reader.onload = (e) => {
            this.imageBase64 = e.target.result;
            this.showPreview(this.imageBase64, file.name);
        };
        
        reader.onerror = (e) => {
            console.error('VisionController: Error reading file', e);
        };
        
        reader.readAsDataURL(file);
    }

    /**
     * Show image preview in the container
     * @param {string} base64Url - The Base64 encoded image URL
     * @param {string} fileName - The original file name
     */
    showPreview(base64Url, fileName) {
        if (!this.previewContainer) return;
        
        // Clear previous preview
        this.previewContainer.innerHTML = '';
        
        // Create preview elements
        const previewDiv = document.createElement('div');
        previewDiv.className = 'image-preview';
        
        const img = document.createElement('img');
        img.src = base64Url;
        img.alt = `Preview: ${fileName}`;
        img.style.maxWidth = '200px';
        img.style.maxHeight = '150px';
        img.style.objectFit = 'contain';
        
        const caption = document.createElement('div');
        caption.textContent = `Image: ${fileName}`;
        caption.style.fontSize = 'small';
        caption.style.marginTop = '5px';
        
        const removeBtn = document.createElement('button');
        removeBtn.textContent = 'Remove Image';
        removeBtn.type = 'button';
        removeBtn.style.marginTop = '5px';
        removeBtn.onclick = () => {
            this.clear();
        };
        
        previewDiv.appendChild(img);
        previewDiv.appendChild(caption);
        previewDiv.appendChild(removeBtn);
        this.previewContainer.appendChild(previewDiv);
    }

    /**
     * Clear the current image and preview
     */
    clear() {
        this.imageBase64 = null;
        if (this.previewContainer) {
            this.previewContainer.innerHTML = '';
        }
        // If there's a file input, reset it too
        const fileInput = document.getElementById('vision-file-input');
        if (fileInput) {
            fileInput.value = '';
        }
    }

    /**
     * Get the current Base64 image string
     * @returns {string|null} The Base64 image string or null if no image is loaded
     */
    getImage() {
        return this.imageBase64;
    }
}
--- END OF FILE: tools\modules\vision.js ---

--- START OF FILE: tools\modules\whisper-worker.js ---
import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';

// Configure for WASM
env.allowLocalModels = false;
env.useBrowserCache = true;

class WhisperWorker {
    constructor() {
        this.pipe = null;
    }

    async init() {
        if (!this.pipe) {
            console.log("[WhisperWorker] Loading model...");
            this.pipe = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', { 
                device: 'wasm'
            });
            console.log("[WhisperWorker] Model loaded.");
        }
    }

    async transcribe(audioData) {
        if (!this.pipe) await this.init();
        
        console.log("[WhisperWorker] Processing audio...");
        const result = await this.pipe(audioData, { 
            language: 'english',
            chunk_length_s: 30,
            stride_length_s: 5
        });
        
        return result.text.trim();
    }
}

const worker = new WhisperWorker();

self.onmessage = async (e) => {
    const { type, data, id } = e.data;

    try {
        if (type === 'init') {
            await worker.init();
            self.postMessage({ type: 'init_done', id });
        } 
        else if (type === 'transcribe') {
            const text = await worker.transcribe(data);
            self.postMessage({ type: 'transcribe_result', text, id });
        }
    } catch (err) {
        self.postMessage({ type: 'error', error: err.message, id });
    }
};
--- END OF FILE: tools\modules\whisper-worker.js ---

