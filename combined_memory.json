[
  {
    "id": "CHANGELOG.md",
    "timestamp": 1767241434,
    "role": "file",
    "content": "# Context-Engine Changelog\n\n## [2.2.10] - 2025-12-31 \"Log File System Implementation\"\n\n### Added\n- **Logs Directory**: Created `logs/` directory to store individual component logs\n- **File-based Logging**: Each system component now writes to its own log file (e.g., `chat-api.log`, `memory-api.log`, `websocket-bridge.log`)\n- **Log Truncation**: Implemented automatic log truncation to keep only last 1000 lines per file\n- **Individual Log Files**: Separate log files for each component for easier debugging\n\n### Changed\n- **Log Storage**: Moved from in-memory only to file-based persistent logging\n- **Log Management**: Added automatic log rotation and truncation to prevent disk space issues\n- **API Endpoints**: All API endpoints now write to both central buffer and individual log files\n\n## [2.2.9] - 2025-12-31 \"Complete Process Log Capture\"\n\n### Added\n- **Process Logging**: Added logging for all major system processes (chat, memory search, WebSocket connections)\n- **Error Tracking**: Enhanced error logging with detailed context and request IDs\n- **Status Monitoring**: Added detailed status messages for connection states and process flow\n\n### Changed\n- **Log Collection**: Enhanced centralized log collection with comprehensive process monitoring\n- **API Endpoints**: All API endpoints now log detailed request/response information\n- **WebSocket Handler**: Improved WebSocket connection logging with detailed status updates\n- **Error Handling**: Enhanced error messages with better context and correlation IDs\n\n## [2.2.8] - 2025-12-31 \"Universal Log Collection System\"\n\n### Added\n- **Log Collection**: Added centralized log collection system in `webgpu_bridge.py` with global log buffer\n- **API Endpoints**: Added `/logs/recent` and `/logs/collect` endpoints for log aggregation\n- **Standard 013**: Created universal log collection standard for all system components\n- **Cross-Platform Logging**: Implemented logging from all system components (Python, JavaScript, WebSocket)\n\n### Changed\n- **Log Viewer**: Updated `log-viewer.html` to consume logs from the new centralized endpoint\n- **WebSocket Logging**: Enhanced WebSocket connection to send detailed status messages to log viewer\n- **System Integration**: All components now route logs through the central collection system\n\n## [2.2.7] - 2025-12-31 \"Ghost Engine Startup Improvements\"\n\n### Fixed\n- **Connection Issues**: Fixed Ghost Engine startup to ensure proper WebSocket connection establishment\n- **Process Launch**: Improved startup scripts to properly launch Ghost Engine with correct parameters\n- **CPU-Only Mode**: Enhanced CPU-only mode startup with appropriate browser flags\n- **Low-Resource Mode**: Fixed low-resource mode startup with conservative GPU settings\n\n### Changed\n- **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` with better Ghost Engine launch parameters\n- **Connection Timing**: Improved timing between server and Ghost Engine startup\n- **User Feedback**: Added clearer status messages during startup process\n\n## [2.2.6] - 2025-12-31 \"WebGPU Adapter Error Handling\"\n\n### Fixed\n- **WebGPU Errors**: Added specific handling for \"No WebGPU Adapter found\" errors on Snapdragon/limited GPU devices\n- **Error Messages**: Improved error messages to guide users when WebGPU is unavailable\n- **Graceful Degradation**: System now provides helpful guidance instead of failing silently\n\n### Changed\n- **Chat Endpoint**: Enhanced `/v1/chat/completions` to handle WebGPU adapter errors gracefully\n- **Error Response**: More informative error messages for GPU-related issues\n- **User Guidance**: Clear instructions for users with unsupported GPU configurations\n\n## [2.2.5] - 2025-12-31 \"Log Viewer Consolidation\"\n\n### Added\n- **Single Panel**: Consolidated all logs into one unified panel for easier monitoring\n- **Stream Collection**: All app processes now stream to single consolidated view\n- **Efficiency**: Removed unused chat and context panels that were empty\n\n### Changed\n- **Log Viewer**: `tools/log-viewer.html` now shows all logs in single panel\n- **UI Simplification**: Streamlined interface for better usability\n- **Copy Functionality**: Simplified copy to clipboard for all logs at once\n\n## [2.2.4] - 2025-12-31 \"Chat Client & Bridge Reorientation\"\n\n### Added\n- **Chat Client**: Converted `tools/anchor.py` from Shell Executor to Chat Client interface\n- **Stream Accumulation**: Enhanced bridge to properly accumulate chat stream responses\n- **Terminal Chat**: Added conversation history and context management to CLI\n\n### Fixed\n- **Chat Response**: Fixed issue where chat responses were cut off due to stream handling\n- **Bridge Protocol**: Improved WebSocket message handling for complete response delivery\n- **Conversation Flow**: Added proper conversation history management in CLI\n\n### Changed\n- **Architecture**: Shifted from command execution to chat interface in terminal client\n- **API Handling**: Bridge now accumulates streaming responses for non-streaming API compatibility\n- **User Experience**: Terminal client now provides full chat experience with context\n\n## [2.2.3] - 2025-12-31 \"Ghost Engine Startup Fix\"\n\n### Fixed\n- **JavaScript Disabled**: Removed `--disable-javascript` flag that was preventing Ghost Engine from starting\n- **WASM Engine**: Fixed issue where WebAssembly AI engine couldn't load with JavaScript disabled\n- **WebSocket Connection**: Resolved \"Failed to fetch\" errors by enabling JavaScript in headless browser\n- **Memory Search**: Fixed context search functionality by ensuring Ghost Engine starts properly\n\n### Changed\n- **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` to enable JavaScript\n- **Ghost Engine**: Headless browser now properly loads WASM AI engine and connects to bridge\n\n## [2.2.2] - 2025-12-31 \"Context Search Fix\"\n\n### Fixed\n- **Memory Search**: Fixed 503 errors when Ghost Engine is disconnected by providing helpful error messages\n- **WebSocket Handling**: Improved handling of search result responses from Ghost Engine\n- **Timeout Management**: Added proper timeout handling for search requests\n- **Error Messages**: Enhanced error messages to guide users when Ghost Engine is not connected\n\n### Changed\n- **Search Endpoint**: Improved `/v1/memory/search` to handle disconnected Ghost Engine gracefully\n- **Chat Endpoint**: Enhanced error handling for chat completions when Ghost Engine is unavailable\n\n## [2.2.1] - 2025-12-31 \"UI Consolidation\"\n\n### Removed\n- **Sidecar Interface**: Removed duplicate sidecar.html interface to consolidate to single Context UI\n- **Redundant Endpoints**: Streamlined UI access to focus on single interface\n\n### Added\n- **Context UI**: Single, focused interface for retrieval and search functionality\n- **Endpoint Consolidation**: Both `/sidecar` and `/context` now serve the same Context UI\n\n### Changed\n- **UI Strategy**: Shifted from multiple similar interfaces to single, focused Context UI\n- **User Experience**: Simplified navigation with single interface for context retrieval\n\n## [2.2.0] - 2025-12-31 \"Text-Only Architecture Pivot\"\n\n### Removed\n- **Vision Engine**: Removed Python-based vision_engine.py and Ollama dependency\n- **Image Processing**: Removed all /v1/vision/* endpoints and image-related functionality\n- **External Dependencies**: Eliminated heavy Python/Ollama dependencies for lightweight operation\n\n### Added\n- **Text-Only Focus**: Streamlined architecture focusing purely on text context and memory\n- **Simplified Bridge**: Cleaned webgpu_bridge.py with only essential context relay functionality\n- **Memory Builder**: Reinforced tools/memory-builder.html as the primary background processor\n- **Browser-Native Processing**: Leverage Ghost Engine (WebGPU) for all processing needs\n\n### Changed\n- **Architecture**: Shifted from multi-component system to lightweight, browser-native approach\n- **Processing Model**: Memory processing now handled by Qwen 1.5B in WebGPU (memory-builder.html)\n- **Dependency Management**: Eliminated external inference servers (Ollama) in favor of browser-native models\n- **Sidecar Interface**: Simplified to focus solely on retrieval and search functionality\n\n## [2.1.0] - 2025-12-31 \"Daemon Eyes & Passive Observation\"\n\n### Added\n- **Daemon Eyes**: Implemented \"Digital Proprioception\". System now observes user screen activity via `sidecar.html` toggle.\n- **Vision Pipeline**: Integrated `vision_engine.py` to convert images/screenshots into semantic text memories.\n- **Live Context Loop**: Added `POST /v1/vision/screenshot` for non-blocking background context ingestion.\n- **Unified Sidecar**: Merged Retrieval and Vision tools into `tools/sidecar.html`.\n- **Context UI**: Added `tools/context.html` for simplified read-only context retrieval with scrollable display and one-click copy\n- **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:\n    - `GET /sidecar` - Serves the unified control center\n    - `GET /context` - Serves the read-only context retrieval UI\n    - `POST /v1/vision/ingest` - Handles image upload and VLM processing\n    - `POST /v1/vision/screenshot` - Handles background screenshot processing\n    - `POST /v1/memory/search` - Implements memory graph search functionality\n\n### Changed\n- **Context Strategy**: Shifted from \"Manual Copy-Paste\" to \"Passive Accumulation + Manual Retrieval\".\n- **Bridge Architecture**: `webgpu_bridge.py` now manages background tasks (FastAPI `BackgroundTasks`) for image processing to prevent UI freezing.\n- **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs\n\n## [2.0.3] - 2025-12-31 \"Browser-Based Control Center & VLM Integration\"\n\n### Added\n- **Vision Engine**: Created `tools/vision_engine.py` for Python-powered image analysis using Ollama backend\n- **Browser Control Center**: Implemented `tools/sidecar.html` with dual tabs for context retrieval and vision ingestion\n- **Context UI**: Added `tools/context.html` for manual context retrieval with scrollable display and one-click copy\n- **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:\n    - `GET /sidecar` - Serves the sidecar dashboard\n    - `GET /context` - Serves the context retrieval UI\n    - `POST /v1/vision/ingest` - Handles image upload and VLM processing\n    - `POST /v1/memory/search` - Implements memory graph search functionality\n- **VLM Integration**: Full integration pipeline from image upload â†’ Python VLM â†’ memory graph ingestion\n\n### Changed\n- **Bridge Enhancement**: Extended `webgpu_bridge.py` to serve UI files and orchestrate vision processing\n- **Memory Search**: Implemented placeholder search functionality with realistic response structure\n- **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs\n\n## [2.0.2] - 2025-12-31 \"Test Suite Organization & Pipeline Verification\"\n\n### Added\n- **Test Directory Structure**: Created dedicated `tests/` directory in project root for all test files\n- **Test File Migration**: Moved all test files from `tools/` and `scripts/` to new `tests/` directory:\n    - `test_model_loading.py` â†’ `tests/test_model_loading.py`\n    - `test_model_availability.py` â†’ `tests/test_model_availability.py`\n    - `test_orchestrator.py` â†’ `tests/test_orchestrator.py`\n    - `model_test.html` â†’ `tests/model_test.html`\n    - `test_gpu_fixes.py` â†’ `tests/test_gpu_fixes.py`\n- **Test Configuration Updates**: Updated test files to use correct port (8000 instead of 8080) for current architecture\n- **Comprehensive Test Suite**: Enhanced test coverage for model loading, endpoint accessibility, and data pipeline verification\n\n### Changed\n- **Project Organization**: Consolidated all test assets into dedicated directory for better maintainability\n- **Test Architecture**: Updated test configurations to match current Anchor Core unified architecture (port 8000)\n\n## [2.0.1] - 2025-12-30 \"Server Stability & Endpoint Fixes\"\n\n### Fixed\n- **Server Startup Issues**: Resolved server hanging issues caused by problematic path parameter syntax (`:path`) in route definitions that prevented proper server startup\n- **Missing Endpoints**: Added critical missing endpoints (`/v1/models/pull`, `/v1/models/pull/status`, `/v1/gpu/lock`, `/v1/gpu/status`, etc.) that were documented but missing from implementation\n- **Endpoint Accessibility**: Verified all documented endpoints are now accessible and responding properly\n- **Model Availability**: Improved model availability testing showing that `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` and `Qwen2.5-7B-Instruct-q4f16_1-MLC` have most files available (missing only `params.json`)\n\n### Architecture Shift\n- **Unified Anchor Core**: Consolidated Bridge, File Server, and UI into a single process (`webgpu_bridge.py`) running on **Port 8000**.\n- **Single Origin**: Eliminated CORS issues and port confusion. UI, API, and Models are served from the same origin.\n- **Protocol**:\n    - Brain: `http://localhost:8000/chat.html`\n    - Terminal: `http://localhost:8000/terminal.html`\n    - API: `http://localhost:8000/v1/...`\n\n### Removed / Archived\n- **CLI Bloat**: Deleted `anchor.py` and `sov.py` in favor of web-based `terminal.html` or direct API calls.\n- **Legacy Scripts**: Archived `start-bridge.bat`, `start-ghost-shell.bat`, `launch-ghost.ps1`, `hot_reload_gpu.py`, and others to `archive/v2_ghost_shell/`.\n\n### Added\n- **start-anchor.bat**: Single-click launcher that starts the Core and the Ghost Engine (Minimized Browser).\n\n## [1.2.4] - 2025-12-29 \"Ghost & Shell Architecture\"\n\n### Added\n- **Ghost & Shell Architecture**: Implemented headless Ghost engine with native Anchor shell for OS integration\n- **Auto-Ignition Protocol**: Added auto-start sequence for headless browser with `?headless=true` parameter\n- **Anchor Terminal**: Created `tools/anchor.py` for native PowerShell interface with natural language processing\n- **Spawn Endpoint**: Added `/v1/system/spawn_shell` to launch native terminals from dashboard\n- **Neural Shell Protocol**: Enhanced `/v1/shell/exec` to process natural language to PowerShell commands\n- **UTF-8 Encoding Fix**: Added Windows encoding enforcement to prevent Unicode crashes in bridge\n- **Minimized Window Approach**: Updated `scripts/launch-ghost.ps1` to use `--start-minimized` for proper GPU access\n- **Unified Startup**: Consolidated to single `start-ghost-shell.bat` script launching complete architecture\n\n### Changed\n- **Renamed Kernel**: Migrated from `sovereign.js` to `anchor.js` with updated imports across all components\n- **Simplified Bridge**: Streamlined `webgpu_bridge.py` with essential functionality only\n- **Updated Neural Terminal**: Modified `tools/neural-terminal.html` to use new shell protocol\n- **Dashboard Integration**: Added Anchor Shell button to `tools/index.html`\n- **Startup Scripts**: Consolidated multiple startup scripts to single unified approach\n\n### Fixed\n- **Windows Encoding**: Resolved Unicode encoding crashes with UTF-8 enforcement\n- **Bridge Authorization**: Fixed authentication token consistency across components\n- **Headless GPU Access**: Resolved WebGPU initialization issues with minimized window approach\n- **Model Loading**: Fixed auto-load sequence for Ghost engine with proper model selection\n- **Cache Bypass Protocol**: Implemented \"Stealth Mode\" for browser AI engine by overriding Cache API and modifying static file headers to force browser to treat models as \"data in RAM\" rather than \"persistent storage\", bypassing strict security policies.\n- **NoCacheStaticFiles**: Custom StaticFiles class with `Cache-Control: no-store` headers to prevent browser cache API usage when serving models through the bridge.\n- **Neural Shell Protocol**: Activated \"The Hands\" (Layer 3) in `webgpu_bridge.py` (`/v1/shell/exec`), allowing the browser to execute system commands on the host.\n- **Neural Terminal**: `tools/neural-terminal.html` provides a matrix-style command interface for direct shell access from the browser.\n- **Hot Reload Improvement**: Fixed `start-sovereign-console-hotreload.bat` port conflicts and added `/file-mod-time` endpoint to `smart_gpu_bridge.py` for correct frontend reloading.\n- **Root Mic (Audio Input)**: Renamed `sovereign-mic.html` to `root-mic.html` and added \"Summarize & Clarify\" feature using the local Qwen2.5 model.\n- **Long-Form Transcription**: Fixed Whisper pipeline to support recordings >30s using chunking and striding.\n- **CozoDB Corruption Recovery**: Enhanced error handling for IndexedDB corruption with automatic fallback to in-memory database, manual recovery button, and timeout protection against hanging WASM calls.\n- **Bulk CozoDB Import Tool**: Added `tools/prepare_cozo_import.py` to transform `combined_memory.json` into the canonical `relations` payload (`cozo_import_memory.json`) for atomic bulk imports into CozoDB.\n- **Import Safety & Verification**: Added recommended import procedure and a post-import verification + backup step to avoid Schema Detachment.\n- **WebGPU Bridge**: `webgpu_bridge.py` for proxying OpenAI API requests to browser workers.\n- **Chat Worker**: `webgpu-server-chat.html` for running LLMs in the browser.\n- **Embed Worker**: `webgpu-server-embed.html` for running embedding models in the browser.\n- **Mobile Chat**: `mobile-chat.html` for a lightweight, mobile-friendly UI.\n- **Log Viewer**: `log-viewer.html` for real-time server log monitoring.\n\n### Changed\n- **Model Loading**: Updated `model-server-chat.html` to use bridge-based model URLs (`http://localhost:8080/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.\n- **Ingestion Defaults**: Recommended batch size increased to 100 to prevent long-running slow writes that can desync CozoDB's in-memory metadata.\n- **Git Configuration**: Added `models/` directory to `.gitignore` to prevent committing large binary model files.\n\n---\n\n## [1.2.3] - 2025-12-19 \"Snapdragon Optimization\"\n\n### Added\n- **Qwen3 Support**: Added `Qwen3-4B-Instruct` to the verified model list.\n- **Llama 3.2 Support**: Added `Llama-3.2-1B-Instruct` as the recommended lightweight model.\n- **Buffer Override**: Implemented `appConfig` overrides to force high-end performance on 256MB GPUs (fixing Adreno throttling).\n\n### Changed\n- **Portable Launchers**: All scripts now use `--user-data-dir=\"%~dp0browser_data\"` for fully portable, clean-running instances.\n- **Model Config**: Refactored `CreateMLCEngine` initialization to handle both URL-based and ID-based model definitions reliably.\n\n---\n\n## [1.2.2] - 2025-12-18 \"Hermes & CozoDB Fixes\"\n\n### Fixed\n- **Hermes Model Support**: Fixed 404 errors for OpenHermes and NeuralHermes by mapping them to the verified `Mistral-v0.3` WASM library.\n- **CozoDB Date Formatting**: Removed `strftime` dependency from WASM queries (causing `no_implementation` errors) and moved date formatting to client-side JavaScript.\n- **Drag-and-Drop Import**: Fixed handling of CozoDB `relations` export format in drag-and-drop ingestion.\n- **Documentation**: Established `specs/mlc-urls.md` as a registry for verified WASM binaries.\n\n---\n\n## [1.2.1] - 2025-12-15 \"DeepSeek & CozoDB Stabilization\"\n\n### Fixed\n- **CozoDB Initialization**: Resolved `CozoDb.new_from_path is not a function` error by switching to `CozoDb.new_from_indexed_db` for persistent browser storage (IndexedDB backend).\n- **WASM Memory Access**: Fixed \"memory access out of bounds\" error in `sovereign-db-builder.html` and `unified-coda.html` by correctly stringifying JSON parameters passed to `db.run()`.\n- **DeepSeek Configuration**: Fixed \"Cannot find model record\" error in `unified-coda.html` by decoupling the internal model ID from the HuggingFace URL.\n\n### Added\n- **Sovereign Hub**: Created `tools/index.html` as a central dashboard for the Console, Builder, and Log Viewer.\n- **Log Viewer Upgrade**: Refactored `tools/log-viewer.html` to use `BroadcastChannel` for real-time, polling-free log updates from the console.\n- **Expanded File Support**: Updated `sovereign-db-builder.html` to support ingestion of a wider range of code and config files (ts, rs, go, sql, ini, xml, etc.).\n\n## [1.2.0] - 2025-12-15 \"Sovereign Architecture\"\n\n### Added\n- **Sovereign Console**: Created `tools/unified-coda.html`, a standalone WASM-based chat console with local CozoDB (OPFS) and Transformers.js.\n- **Sovereign DB Builder**: Created `tools/sovereign-db-builder.html` for ingesting JSON logs into the browser-based database.\n- **Model Support**: Expanded `unified-coda.html` to support the full range of MLC-compatible models (Llama 3.2, Qwen 2.5, Gemma 2, etc.).\n\n### Changed\n- **Log Management**: Updated backend logging to truncate files at 500KB to prevent disk bloat.\n\n## [1.1.0] - 2025-12-14 \"Browser Stability & Bridge Fixes\"\n\n### Fixed\n- **WebGPU Bridge**: Patched `tools/webgpu_bridge.py` to accept any model name, resolving 503 errors during embedding requests.\n- **LLM Client**: Updated `backend/src/llm.py` to correctly identify and use the configured embedding model (`nomic-embed-text-v1.5`).\n- **Coda Chat**: Modified `backend/src/recipes/coda_chat.py` to sanitize and truncate `retrieve_memory` outputs. Large JSON payloads were causing `Maximum call stack size exceeded` errors in the browser-based LLM worker.\n\n## [1.0.0] - 2025-12-08 \"Infinite Context Pipeline\"\n\n### Added\n- **Phase 1: Hardware Foundation**: All LLM servers now boot with 65,536 context window and Flash Attention enabled\n- **Phase 2: Context Rotation Protocol**: ContextManager automatically rotates context when exceeding 55k tokens\n- **Phase 3: Graph-R1 Enhancement**: GraphReasoner now retrieves ContextGist memories for historical continuity\n- **ContextGist Nodes**: Neo4j storage for compressed historical context summaries with chronological links\n- **Context Shifting Logic**: Intelligent distillation of old content using Distiller agent with gist creation\n- **Documentation Structure**: Organized specs/ directories at root, backend, and anchor levels with spec.md, plan.md, tasks.md\n- **Infinite Context Pipeline**: Complete end-to-end implementation enabling unlimited context window management\n\n### Changed\n- **Upgraded Context Windows**: All start scripts now default to 64k context for infinite work capability\n- **Enhanced Memory Architecture**: Neo4j now stores both active memories and ContextGist historical summaries\n- **Improved ContextManager**: Added check_and_rotate_context() logic with automatic gist creation and storage\n- **Extended GraphReasoner**: Updated retrieval queries to include ContextGist nodes alongside regular memories\n- **Optimized Distiller Integration**: Enhanced _chunk_and_distill functionality for context rotation use cases\n- **Refined Archivist Agent**: Now coordinates context rotation and gist management operations\n\n### Fixed\n- **Context Limit Elimination**: Fixed issue where systems would crash when reaching context limits\n- **Memory Continuity**: Resolved problems with historical context access across conversation boundaries\n- **Performance Optimization**: Fixed inefficiencies in large context handling with 64k window support\n- **Rotation Logic**: Fixed issues with context preservation during rotation cycles\n\n---\n\n## [0.9.0] - 2025-12-07 \"Reka & Local Proxy\"\n\n### Added\n- **Reka Configuration**: Full support for Reka-Flash-3-21B (Q4_K_S) with 16k context, stop tokens, and optimized LLaMa server flags.\n- **Local API Proxy**: Added `scripts/local_api_proxy.py` to enforce static API keys for local LLaMa instances (fixes Cline extension \"OpenAI API Key\" requirement).\n- **VS Code Integration**: Added `.vscode/settings.json` template and `VSCODE_CLINE_SETUP.md` for seamless local development.\n- **MCP Health**: Added `/health` endpoint to Unified Launcher for better compatibility.\n\n### Fixed\n- **MCP Routing**: Resolved duplicate `/mcp` prefix in Unified Launcher routes (`/mcp/tools` is now accessible).\n- **LLM Client**: Added `stop` token support to API payloads and local GGUF generation.\n\n## [0.8.0] - 2025-12-06 \"Archivist Protocol\"\n\n### Added\n- **Archivist Ingestion**: Implemented `POST /archivist/ingest` endpoint to accept live data from the browser.\n- **Memory Schema**: Enforced **Directive INJ-A1** (`PlaintextMemory`) for immutable \"Page-Store\" records.\n- **Modular DOM Adapters**:\n    - `GeminiAdapter`: Clean extraction for Google Gemini.\n    - `ChatGPTAdapter`: Clean extraction for ChatGPT.\n    - `ClaudeAdapter`: Clean extraction for Claude.ai.\n    - `GenericAdapter`: Universal fallback for any webpage.\n- **Extension UI**: Added **[Save to Memory]** button to the Side Panel for manual ingestion.\n\n### Fixed\n- **Encoding Crash**: Resolved Windows `charmap` error by enforcing `PYTHONIOENCODING='utf-8'`.\n- **Server Stability**: Fixed startup crashes caused by `MemoryWeaver` resource contention.\n\n## [0.7.0] - 2025-12-06 \"Operation Concrete\"\n\n### Added\n- **Browser Bridge**: A Chrome Extension (MV3) capable of:\n    - **Voice**: Streaming chat interface via Side Panel.\n    - **Sight**: Context injection (reading active tab).\n    - **Hands**: JavaScript execution on active pages (User-ratified).\n- **Backend Architecture**: Migrated from monolithic scripts to **Modular Recipes** (MAX Agentic Cookbook standard).\n    - `CodaChatRecipe`: Handles orchestration, context, and tool execution.\n- **Persistence**: Side panel now saves chat history to local storage.\n- **Markdown Support**: Chat interface renders code blocks and syntax highlighting.\n\n### Changed\n- **Identity**: System formally renamed from \"Sybil\" to **\"Coda\"**.\n- **Documentation**: Adopted `specs/` based documentation policy.\n\n### Fixed\n- **Audit Logger**: Patched critical `NameError` in streaming endpoints.\n- **Security**: Hardened extension execution via `world: \"MAIN\"` to bypass strict CSP on some sites.\n\n---\n\n## [0.6.0] - 2025-11-30 \"Operation MCP Integrated\"\n\n### Added\n- **MCP Integration**: Complete integration of MCP server into main ECE Core server\n- **Unified Endpoint**: All MCP functionality now available at `/mcp` on main server (port 8000)\n- **Memory Tools**: Enhanced MCP tools for memory operations:\n    - `add_memory` - Add to Neo4j memory graph\n    - `search_memories` - Search memory graph with relationships\n    - `get_summaries` - Get session summaries\n- **Configuration**: New `mcp_enabled` setting in config.yaml to toggle integration\n- **Authentication**: MCP endpoints now inherit main server authentication settings\n\n### Changed\n- **Architecture**: MCP server no longer runs as separate process, now integrated into main ECE server\n- **Endpoints**: MCP tools now accessed via `/mcp/tools` and `/mcp/call` instead of separate server\n- **Deployment**: Simplified deployment - no need to start separate MCP service\n- **Resources**: Reduced memory footprint by eliminating duplicate server processes\n\n### Fixed\n- **Connection Issues**: Resolved intermittent connection failures between ECE and external MCP server\n- **Latency**: Reduced tool call latency by eliminating inter-service communication overhead\n- **Synchronization**: Fixed race conditions in concurrent tool executions\n\n---\n\n## [0.5.1] - 2025-11-29 \"Memory Weaver Security Audit\"\n\n### Added\n- **Security Hardening**: Added input validation for all GraphReasoner queries\n- **Audit Trail**: Enhanced logging for all automated relationship repairs\n- **Circuit Breakers**: Added fail safes for Weaver operations\n\n### Changed\n- **Weaver Engine**: Refactored to use parameterized queries, preventing Cypher injection\n- **Permission Model**: Strengthened access controls for relationship modification operations\n\n### Fixed\n- **Cypher Injection**: Patched vulnerability in Neo4j relationship queries\n- **Race Conditions**: Fixed concurrency issues in automated repair operations\n- **Resource Exhaustion**: Added limits to prevent DoS via excessive repair requests\n\n---\n\n## [0.5.0] - 2025-11-28 \"Memory Weaver (Automated Repair)\"\n\n### Added\n- **Memory Weaver Engine**: Automated system for detecting and repairing broken relationships in Neo4j\n- **Similarity Detection**: Embedding-based relationship discovery for linking related memories\n- **Audit System**: Complete traceability for all automated repairs with `auto_commit_run_id`\n- **Rollback Capability**: Deterministic reversal of automated changes via `rollback_commits_by_run.py`\n- **Scheduler**: Background maintenance tasks for continuous graph integrity\n\n### Changed\n- **Graph Maintenance**: Automated relationship repair now runs as background process\n- **Quality Assurance**: Enhanced relationship validation with similarity scoring\n- **Traceability**: All automated changes now logged with unique run identifiers\n\n### Fixed\n- **Orphaned Nodes**: Automatically discovers and connects isolated memories\n- **Broken Links**: Repairs missing relationships between related concepts\n- **Data Drift**: Corrects inconsistent metadata across related nodes\n\n---\n\n## [0.4.0] - 2025-11-25 \"Graph-R1 Implementation\"\n\n### Added\n- **Graph Reasoner**: Iterative \"Think â†’ Query â†’ Retrieve â†’ Rethink\" reasoning engine\n- **Q-Learning Retrieval**: Reinforcement learning for optimized memory access patterns\n- **Markovian Reasoning**: Chunked thinking with state preservation across context shifts\n- **Multi-Hop Queries**: Complex graph traversal for answering compound questions\n- **Cognitive Agents**: Plugin architecture for specialized reasoning tasks\n\n### Changed\n- **Retrieval Method**: Replaced simple vector search with Graph-R1 retrieval\n- **Memory Access**: Graph-based traversal now primary method for context assembly\n- **Agent Architecture**: Modular cognitive agents for specialized tasks\n- **Context Building**: Enhanced context with relationship-aware retrieval\n\n### Fixed\n- **Context Relevance**: Improved precision of memory retrieval\n- **Chain of Thought**: Better preservation of reasoning pathways\n- **Memory Decay**: Reduced loss of historical context in long conversations\n\n---\n\n## [0.3.1] - 2025-11-20 \"Security Hardening\"\n\n### Added\n- **API Authentication**: Token-based authentication for all endpoints\n- **Rate Limiting**: Request throttling to prevent abuse\n- **Input Sanitization**: Enhanced validation for all user inputs\n- **Audit Logging**: Comprehensive logging of all sensitive operations\n- **Secure Defaults**: Safe configuration presets for common deployment scenarios\n\n### Changed\n- **Security Model**: Implemented zero-trust architecture\n- **Credential Handling**: Secure storage and transmission of API keys\n- **Access Controls**: Granular permissions for different API endpoints\n\n### Fixed\n- **Authentication Bypass**: Patched critical vulnerability in API access\n- **Data Exposure**: Resolved information disclosure in error messages\n- **Injection Attacks**: Fixed potential SQL injection in Neo4j queries\n\n---\n\n## [0.3.0] - 2025-11-15 \"Neo4j Migration Complete\"\n\n### Added\n- **Neo4j Integration**: Complete migration from SQLite to Neo4j graph database\n- **Redis Cache**: Hot cache layer for active session management\n- **Graph Schema**: Formal schema definition for memory relationships\n- **Migration Tools**: Scripts to migrate existing SQLite data to Neo4j\n- **Backup System**: Automated graph backup and restoration procedures\n\n### Changed\n- **Storage Architecture**: Tiered storage (Redis hot cache + Neo4j persistent)\n- **Query Language**: Cypher queries for graph operations\n- **Relationship Modeling**: Graph-based connections between memories\n- **Indexing Strategy**: Graph-based indices for faster retrieval\n\n### Fixed\n- **Performance**: Significantly improved query performance for complex relationships\n- **Scalability**: Better handling of large-scale memory graphs\n- **Consistency**: Stronger data integrity with ACID-compliant transactions\n\n---\n\n## [0.2.0] - 2025-10-30 \"Cognitive Agents\"\n\n### Added\n- **Verifier Agent**: Fact-checking via empirical distrust protocol\n- **Archivist Agent**: Memory maintenance and staleness detection\n- **Distiller Agent**: Content summarization and extraction\n- **Agent Framework**: Plugin system for extensible cognitive capabilities\n- **Truth Scoring**: Provenance-aware fact-checking with primary source priority\n\n### Changed\n- **Memory Hygiene**: Automated maintenance of memory quality\n- **Verification Process**: Evidence-based fact-checking system\n- **Quality Assurance**: Continuous assessment of memory reliability\n- **Maintenance Schedule**: Regular memory grooming operations\n\n### Fixed\n- **Hallucinations**: Reduced false information in responses\n- **Stale Information**: Automatic detection and updating of outdated memories\n- **Data Quality**: Improved content validation and cleaning procedures\n\n---\n\n## [0.1.0] - 2025-09-15 \"Initial Architecture\"\n\n### Added\n- **Core Backend**: Initial ECE_Core with SQLite memory system\n- **Anchor Interface**: Terminal interface for user interaction\n- **Basic Memory**: Text-based memory storage and retrieval\n- **LLM Integration**: Support for various local LLM servers\n- **Plugin System**: Extensible tool architecture (UTCP)\n\n### Changed\n- **Foundation**: Established core architecture patterns\n- **API Design**: Defined RESTful API structure for components\n\n### Fixed\n- **Basic Functionality**: Initial implementation of core features",
    "source": "CHANGELOG.md"
  },
  {
    "id": "kill-edge.bat",
    "timestamp": 1766310900,
    "role": "file",
    "content": "@echo off\r\necho ðŸ”ª Killing all Microsoft Edge processes...\r\ntaskkill /F /IM msedge.exe /T\r\necho.\r\necho âœ… Edge terminated. You can now run 'launch-edge-unsafe.bat' cleanly.\r\npause\r\n",
    "source": "kill-edge.bat"
  },
  {
    "id": "launch-chromium-d3d12.bat",
    "timestamp": 1767207068,
    "role": "file",
    "content": "@echo off\nsetlocal EnableDelayedExpansion\n\n:: Define the User Data Directory (Project Relative)\nset \"USER_DATA=%~dp0browser_data\"\nif not exist \"%USER_DATA%\" mkdir \"%USER_DATA%\"\n\n:: Define Flags for D3D12 (Default for Windows)\nset \"FLAGS=--user-data-dir=\"%USER_DATA%\" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --disable-gpu-watchdog --disable-web-security --disable-site-isolation-trials --disable-features=IsolateOrigins,site-per-process\"\nset \"URL=http://localhost:8000/chat.html\"\n\necho ---------------------------------------------------\necho ðŸ” Detecting Browsers...\necho ---------------------------------------------------\n\nset \"count=0\"\n\n:: 1. Check Microsoft Edge\nif exist \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Microsoft Edge\"\n    set \"path[!count!]=C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n)\n\n:: 2. Check Google Chrome\nif exist \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome\"\n    set \"path[!count!]=C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n) else if exist \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome (x86)\"\n    set \"path[!count!]=C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\n) else if exist \"%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome (User)\"\n    set \"path[!count!]=%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\"\n)\n\n:: 3. Check Brave\nif exist \"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Brave Browser\"\n    set \"path[!count!]=C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\n)\n\n:: Check if any found\nif %count%==0 (\n    echo âŒ No compatible Chromium browser found.\n    pause\n    exit /b\n)\n\n:: Display Menu\necho Select a browser to launch:\nfor /L %%i in (1,1,%count%) do (\n    echo [%%i] !name[%%i]!\n)\necho.\n\n:prompt\nset /p \"choice=Enter number (1-%count%): \"\n\n:: Validate Input\nif \"%choice%\"==\"\" goto prompt\nif %choice% LSS 1 goto prompt\nif %choice% GTR %count% goto prompt\n\nset \"BROWSER=!path[%choice%]!\"\nset \"BROWSER_NAME=!name[%choice%]!\"\n\necho.\necho ðŸš€ Launching %BROWSER_NAME% with D3D12 (Default) backend...\necho Path: \"%BROWSER%\"\necho Data: \"%USER_DATA%\"\necho URL: \"%URL%\"\necho.\necho Executing: \"%BROWSER%\" %FLAGS% %URL%\necho.\n\n\"%BROWSER%\" %FLAGS% %URL%\npause\n",
    "source": "launch-chromium-d3d12.bat"
  },
  {
    "id": "launch-chromium-vulkan.bat",
    "timestamp": 1767207115,
    "role": "file",
    "content": "@echo off\nsetlocal EnableDelayedExpansion\n\n:: Define the User Data Directory (Project Relative)\nset \"USER_DATA=%~dp0browser_data\"\nif not exist \"%USER_DATA%\" mkdir \"%USER_DATA%\"\n\n:: Define Flags (Critical for Snapdragon)\nset \"FLAGS=--user-data-dir=\"%USER_DATA%\" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --enable-features=Vulkan --use-angle=vulkan --disable-gpu-watchdog\"\nset \"URL=http://localhost:8000/chat.html\"\n\necho ---------------------------------------------------\necho ðŸ” Detecting Browsers (Vulkan Mode)...\necho ---------------------------------------------------\n\nset \"count=0\"\n\n:: 1. Check Microsoft Edge\nif exist \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Microsoft Edge\"\n    set \"path[!count!]=C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n)\n\n:: 2. Check Google Chrome\nif exist \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome\"\n    set \"path[!count!]=C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n) else if exist \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome (x86)\"\n    set \"path[!count!]=C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\n) else if exist \"%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Google Chrome (User)\"\n    set \"path[!count!]=%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\"\n)\n\n:: 3. Check Brave\nif exist \"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\" (\n    set /a count+=1\n    set \"name[!count!]=Brave Browser\"\n    set \"path[!count!]=C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\n)\n\n:: Check if any found\nif %count%==0 (\n    echo âŒ No compatible Chromium browser found.\n    pause\n    exit /b\n)\n\n:: Display Menu\necho Select a browser to launch:\nfor /L %%i in (1,1,%count%) do (\n    echo [%%i] !name[%%i]!\n)\necho.\n\n:prompt\nset /p \"choice=Enter number (1-%count%): \"\n\n:: Validate Input\nif \"%choice%\"==\"\" goto prompt\nif %choice% LSS 1 goto prompt\nif %choice% GTR %count% goto prompt\n\nset \"BROWSER=!path[%choice%]!\"\nset \"BROWSER_NAME=!name[%choice%]!\"\n\necho.\necho ðŸš€ Launching %BROWSER_NAME% with VULKAN backend...\necho Path: \"%BROWSER%\"\necho Data: \"%USER_DATA%\"\necho.\n\n\"%BROWSER%\" %FLAGS% %URL%\npause\n",
    "source": "launch-chromium-vulkan.bat"
  },
  {
    "id": "README.md",
    "timestamp": 1767241567,
    "role": "file",
    "content": "# Context Engine (Sovereign Edition)\n\n> **Philosophy:** Your mind, augmented. Your data, sovereign. Your tools, open.\n\nA **Browser-Native** cognitive extraction system. No servers. No cloud. No installation.\nJust you, your browser, and your infinite context.\n\n---\n\n## âš¡ Quick Start\n\n1.  **Download** this repository.\n2.  **Open** `tools/index.html` in Chrome or Edge.\n3.  **Click** \"Double Click to Launch\" on the Console.\n\n*That's it. You are running a local LLM with persistent Graph Memory.*\n\n---\n\n## ðŸ—ï¸ Architecture\n\nThe system runs entirely in `tools/` using WebAssembly (WASM).\n\n### 1. The Sovereign Loop\n```mermaid\ngraph TD\n    User -->|Input| HTML[model-server-chat.html]\n\n    subgraph Browser_Memory [\"Two Birds, One Stone\"]\n        HTML -->|Store/Retrieve| Cozo[\"CozoDB WASM\"]\n        Cozo -->|Persist| IDB[\"IndexedDB/OPFS\"]\n    end\n\n    subgraph Cognitive_Engine\n        HTML -->|Context + Prompt| WebLLM[\"DeepSeek-R1 (WASM)\"]\n        WebLLM -->|Reasoning Trace| HTML\n    end\n```\n\n### 2. Core Components\n*   **Brain**: `chat.html` - Runs the Graph-R1 Reasoning Loop. Now uses **Hybrid Search** (Vector + BM25 FTS) and supports SOTA models (Qwen 3, Gemma 3).\n*   **Memory**: `CozoDB (WASM)` - Stores relations (`*memory`) and vectors. Persists to browser IndexedDB.\n*   **Stomach**: `db_builder.html` - Ingests files into the graph. Now \"Multisensory-Ready\" (Phase A): accepts images/audio as references.\n\n---\n\n## ðŸ”¥ Hot Reload System\n\nThe system includes a comprehensive hot reload mechanism for GPU management and development:\n\n*   **Automatic Reload**: Changes to GPU-related files trigger automatic reloads\n*   **Browser Integration**: Hot reload functionality built into all components\n*   **No Service Restart**: Updates occur without restarting services\n*   **Stale Lock Prevention**: Automatic cleanup during reloads\n*   **Development Mode**: Automatically activates when running on localhost\n*   **File Monitoring**: Monitors GPU-related files every 2 seconds for changes\n*   **Enhanced Monitoring**: Includes GPU manager with status checking capabilities\n\n### Getting Started with Hot Reload\n1. Use the unified startup script: `start-ghost-shell.bat`\n2. Monitor changes in real-time with the GPU manager: `python scripts/gpu_manager.py`\n3. Manual reload triggers available in browser console: `window.triggerGPUHotReload()`\n4. Enable/disable hot reload in browser: `window.setGPUHotReloadEnabled(true/false)`\n5. Manual trigger for hot reload: `python scripts/gpu_manager.py --hot-reload`\n\n### Files Monitored\n- `tools/webgpu_bridge.py` - Backend bridge logic\n- `tools/modules/anchor.js` - Frontend GPU controller\n- `tools/model-server-chat.html` - Main console interface\n- `tools/anchor-mic.html` - Voice input interface\n- `tools/memory-builder.html` - Background processing\n\n### Benefits\n- **Faster Development**: Changes take effect immediately\n- **No Service Interruption**: Updates occur without restarting services\n- **Stale Lock Prevention**: Automatic cleanup during reloads\n- **Development Convenience**: Built-in triggers for manual reloads\n\n## ðŸ§  Text-Only Architecture\n\nThe system now focuses purely on text context and memory processing using browser-native WebGPU.\n\n* **Memory Builder (`memory-builder.html`)**: Background processor using Qwen 1.5B in WebGPU\n* **CozoDB (WASM)**: Local storage in browser IndexedDB\n* **Ghost Engine**: Headless browser handles all processing via WebGPU\n\n## ðŸŽ›ï¸ Control Center\n\n* **Context UI (`/context` or `/sidecar`)**: Single, focused interface for retrieval and search in local memory graph. Both endpoints serve the same interface.\n\n## ðŸ“Š Logging & Monitoring\n\nThe system provides comprehensive logging for debugging and monitoring:\n\n*   **Central Log Viewer**: Access all logs at `http://localhost:8000/log-viewer.html`\n*   **File-based Logs**: Individual component logs in the `logs/` directory\n*   **Log Truncation**: Automatic truncation to last 1000 lines per file\n*   **Real-time Streaming**: Live log updates from all system components\n\n### Log Access\n1. **Web Interface**: Visit `http://localhost:8000/log-viewer.html` for real-time logs\n2. **File System**: Check individual log files in the `logs/` directory\n3. **API Endpoint**: Access recent logs via `/logs/recent` endpoint\n\n## ðŸ“± Low-Resource & Mobile Optimization\n\nThe system now supports low-resource devices (phones, small laptops) and CPU-only operation:\n\n*   **Low-Resource Mode**: Set `LOW_RESOURCE_MODE=true` for conservative settings (64MB GPU buffer, single-threaded)\n*   **CPU-Only Mode**: Set `CPU_ONLY_MODE=true` to force CPU processing when GPU is unavailable\n*   **Mobile Ready**: Optimized for phones and tablets with reduced memory usage\n*   **Small Models**: Default to smallest available models (Phi-3.5-mini) for constrained hardware\n\n### Getting Started with Low-Resource Mode\n1. Set environment variable: `set LOW_RESOURCE_MODE=true`\n2. Start the system: `start-anchor.bat`\n3. For CPU-only: `set CPU_ONLY_MODE=true` before starting\n\n## ðŸ›ï¸ Ghost & Shell Architecture\n\nThe system now features the Ghost & Shell architecture for native OS integration:\n\n*   **Ghost Engine**: Headless browser running inference in background (`launch-ghost.ps1`)\n*   **Shell Interface**: Native terminal with natural language processing (`anchor.py`, `neural-terminal.html`)\n*   **Bridge Protocol**: Secure communication via WebGPU bridge on port 8080\n*   **Spawn Endpoint**: Launch native terminals from dashboard (`/v1/system/spawn_shell`)\n\n### Getting Started with Ghost & Shell\n1. Start the complete system: `start-ghost-shell.bat`\n2. Access the dashboard: `http://localhost:8000`\n3. Click \"Anchor Shell\" to spawn native terminal, or use `python tools/anchor.py`\n4. Use natural language commands (prefix with `?` in neural terminal)\n\n### Known Issues\n* **Headless GPU Access**: WebGPU may not initialize in some headless environments. Ensure GPU drivers are up to date and hardware acceleration is enabled in browser settings.\n* **Model Loading**: Large models may require significant VRAM and take time to load initially.\n\n## ðŸ”„ Model Loading Serialization\n\nThe system now includes model loading serialization to prevent GPU overload:\n\n*   **Sequential Loading**: Models load one at a time to prevent GPU resource contention\n*   **Queue Management**: Proper queuing of model loading requests\n*   **Resource Protection**: Prevents multiple models from loading simultaneously\n*   **Improved Stability**: Reduces GPU memory allocation conflicts during startup\n*   **Model URL Fixes**: Corrected model URLs to use reliable endpoints\n\n---\n\n## ðŸ“š Documentation\n\n*   **Architecture**: [specs/spec.md](specs/spec.md)\n*   **Roadmap**: [specs/plan.md](specs/plan.md)\n*   **WASM Layer**: [specs/architecture/sovereign-wasm.spec.md](specs/architecture/sovereign-wasm.spec.md)\n*   **Hot Reload System**: Integrated into [WASM Layer Spec](specs/architecture/sovereign-wasm.spec.md)\n\n---\n\n## ðŸ§¹ Legacy Support\nThe old Python/Neo4j backend has been **archived**.\n*   Legacy README: [archive/v1_python_backend/README_LEGACY.md](archive/v1_python_backend/README_LEGACY.md)\n*   Legacy Code: `archive/v1_python_backend/`",
    "source": "README.md"
  },
  {
    "id": "read_all.py",
    "timestamp": 1766451022,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nRoot Reader: Aggregates content from relevant project files for orchestration.\r\n\r\nThis script scans the project directory and combines the content of source code,\r\nconfiguration, and documentation files into a single text file (combined_text.txt).\r\n\r\nIt respects the current project structure:\r\n- Root level scripts and docs\r\n- tools/: Sovereign Core (JS/HTML/CSS)\r\n- specs/: System Specifications\r\n- scripts/: CI/Utility scripts\r\n\"\"\"\r\nimport argparse\r\nimport json\r\nimport os\r\nfrom typing import List, Tuple, Any\r\n\r\ndef find_project_root(start_path: str | None = None) -> str:\r\n    \"\"\"\r\n    Locate project root by looking for indicators like .git, package.json, README.md\r\n    \"\"\"\r\n    if start_path is None:\r\n        start_path = os.path.abspath(__file__)\r\n\r\n    path = os.path.abspath(start_path)\r\n    if os.path.isfile(path):\r\n        path = os.path.dirname(path)\r\n\r\n    root_indicators = (\".git\", \"package.json\", \"README.md\")\r\n    while True:\r\n        if any(os.path.exists(os.path.join(path, ind)) for ind in root_indicators):\r\n            return path\r\n        parent = os.path.dirname(path)\r\n        if parent == path:\r\n            return os.getcwd()\r\n        path = parent\r\n\r\ndef get_allowed_files(project_root: str) -> List[Tuple[str, str]]:\r\n    \"\"\"\r\n    Returns list of (file_path, section_name) for all relevant project files.\r\n    \"\"\"\r\n    allowed_files = []\r\n    \r\n    # Extensions we care about\r\n    code_exts = {'.py', '.js', '.ts', '.html', '.css', '.json', '.md', '.bat', '.ps1', '.sh', '.yaml', '.yml'}\r\n    \r\n    # Directories to completely ignore\r\n    ignored_dirs = {'.git', '.venv', 'browser_data', 'archive', '__pycache__', 'node_modules', '.github'}\r\n    \r\n    # Files to ignore\r\n    ignored_files = {\r\n        'package-lock.json', \r\n        'combined_text.txt', \r\n        'cozo_lib_wasm_bg.wasm',\r\n        'combined_memory.json',\r\n        'cozo_import_memory.json'\r\n    }\r\n\r\n    for root, dirs, files in os.walk(project_root):\r\n        # Filter directories in-place to avoid walking into ignored ones\r\n        dirs[:] = [d for d in dirs if d not in ignored_dirs and not d.startswith('.')]\r\n        \r\n        rel_root = os.path.relpath(root, project_root)\r\n        section = \"ROOT\" if rel_root == \".\" else rel_root.replace(os.sep, \"_\").upper()\r\n\r\n        for f in files:\r\n            if f in ignored_files:\r\n                continue\r\n            \r\n            ext = os.path.splitext(f)[1].lower()\r\n            if ext in code_exts:\r\n                full_path = os.path.join(root, f)\r\n                allowed_files.append((full_path, section))\r\n                \r\n    return allowed_files\r\n\r\ndef to_yaml_style(obj: Any, indent: int = 0) -> str:\r\n    \"\"\"\r\n    Recursively converts a JSON-compatible object to a YAML-like string.\r\n    \"\"\"\r\n    lines = []\r\n    prefix = \"  \" * indent\r\n    \r\n    if isinstance(obj, dict):\r\n        for k, v in obj.items():\r\n            if isinstance(v, (dict, list)):\r\n                lines.append(f\"{prefix}{k}:\")\r\n                lines.append(to_yaml_style(v, indent + 1))\r\n            else:\r\n                # Handle multiline strings safely\r\n                v_str = str(v)\r\n                if '\\n' in v_str:\r\n                     lines.append(f\"{prefix}{k}: |\")\r\n                     for line in v_str.split('\\n'):\r\n                         lines.append(f\"{prefix}  {line}\")\r\n                else:\r\n                    lines.append(f\"{prefix}{k}: {v}\")\r\n    elif isinstance(obj, list):\r\n        for item in obj:\r\n            if isinstance(item, (dict, list)):\r\n                lines.append(f\"{prefix}-\")\r\n                # For list items that are objects, we want the properties to align slightly differently\r\n                # But for simplicity in this custom dumper:\r\n                sub = to_yaml_style(item, indent + 1)\r\n                lines.append(sub)\r\n            else:\r\n                lines.append(f\"{prefix}- {item}\")\r\n    else:\r\n        return f\"{prefix}{obj}\"\r\n\r\n    return \"\\n\".join(lines)\r\n\r\ndef create_project_corpus(\r\n    output_file: str | None = None,\r\n    dry_run: bool = False,\r\n):\r\n    \"\"\"\r\n    Aggregates content from project files into a single corpus.\r\n    \"\"\"\r\n    project_root = find_project_root()\r\n    output_file = output_file or os.path.join(project_root, \"combined_text.txt\")\r\n\r\n    print(f\"Project Root Detected: {project_root}\")\r\n    allowed_files = get_allowed_files(project_root)\r\n\r\n    if not allowed_files:\r\n        print(f\"No relevant files found in '{project_root}'.\")\r\n        return\r\n\r\n    print(f\"Found {len(allowed_files)} files to process.\")\r\n\r\n    if dry_run:\r\n        print(f\"Dry run enabled â€” would process {len(allowed_files)} files:\")\r\n        for file_path, section in allowed_files:\r\n            print(f\"  - {os.path.relpath(file_path, project_root)} ({section})\")\r\n        return\r\n\r\n    memory_records = []\r\n\r\n    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\r\n        # Add a file map at the very top for the orchestrator\r\n        outfile.write(\"=== PROJECT FILE MAP ===\\n\")\r\n        for file_path, section in allowed_files:\r\n            rel_path = os.path.relpath(file_path, project_root)\r\n            outfile.write(f\"- {rel_path} ({section})\\n\")\r\n        outfile.write(\"========================\\n\\n\")\r\n\r\n        for file_path, section in allowed_files:\r\n            rel_path = os.path.relpath(file_path, project_root)\r\n            print(f\"Processing '{rel_path}'...\")\r\n            try:\r\n                with open(file_path, \"rb\") as raw_file:\r\n                    raw_data = raw_file.read()\r\n                if not raw_data:\r\n                    continue\r\n                \r\n                try:\r\n                    decoded_content = raw_data.decode(\"utf-8\")\r\n                except UnicodeDecodeError:\r\n                    decoded_content = raw_data.decode(\"utf-8\", errors=\"replace\")\r\n\r\n                ext = os.path.splitext(file_path)[1].lower()\r\n                final_content = decoded_content\r\n                \r\n                # Upgrade: Convert JSON to YAML-like text\r\n                if ext == '.json':\r\n                    try:\r\n                        json_obj = json.loads(decoded_content)\r\n                        # Use pretty print json as a reliable fallback or strict yaml style\r\n                        # The user asked for \"YAML-like string (key: value) or pretty-printed JSON (indent=2)\"\r\n                        # Let's try our YAML converter first, it's cleaner for reading.\r\n                        final_content = to_yaml_style(json_obj)\r\n                    except Exception:\r\n                        # Fallback to original content if parsing fails\r\n                        pass\r\n\r\n                outfile.write(f\"--- START OF FILE: {rel_path} ---\\n\")\r\n                outfile.write(final_content)\r\n                if not final_content.endswith('\\n'):\r\n                    outfile.write('\\n')\r\n                outfile.write(f\"--- END OF FILE: {rel_path} ---\\n\\n\")\r\n\r\n                # Store for JSON memory export (Node structure)\r\n                memory_records.append({\r\n                    \"id\": rel_path,\r\n                    \"timestamp\": int(os.path.getmtime(file_path)),\r\n                    \"role\": \"file\",\r\n                    \"content\": final_content,\r\n                    \"source\": rel_path\r\n                })\r\n\r\n            except Exception as e:\r\n                print(f\"Error processing '{rel_path}': {e}\")\r\n\r\n    # Save the combined memory records for Builder ingestion\r\n    memory_file = os.path.join(project_root, \"combined_memory.json\")\r\n    with open(memory_file, \"w\", encoding=\"utf-8\") as f:\r\n        json.dump(memory_records, f, indent=2, ensure_ascii=False)\r\n    print(f\"Memory records saved to '{memory_file}'.\")\r\n\r\n    print(f\"\\nAggregation complete. Corpus saved to '{output_file}'.\")\r\n\r\ndef _parse_cli() -> argparse.Namespace:\r\n    p = argparse.ArgumentParser(description=\"Aggregate project code and docs for orchestration.\")\r\n    p.add_argument(\r\n        \"--out\",\r\n        \"-o\",\r\n        default=None,\r\n        help=\"Output file path (defaults to combined_text.txt in project root)\",\r\n    )\r\n    p.add_argument(\r\n        \"--dry-run\",\r\n        action=\"store_true\",\r\n        help=\"Show what would be processed without writing the combined file\",\r\n    )\r\n    return p.parse_args()\r\n\r\nif __name__ == \"__main__\":\r\n    args = _parse_cli()\r\n    create_project_corpus(\r\n        output_file=args.out,\r\n        dry_run=args.dry_run,\r\n    )",
    "source": "read_all.py"
  },
  {
    "id": "start-anchor.bat",
    "timestamp": 1767237731,
    "role": "file",
    "content": "@echo off\necho âš“ STARTING ANCHOR SYSTEM IN BACKGROUND...\n\nREM Check for low-resource mode\nif \"%LOW_RESOURCE_MODE%\"==\"true\" (\n    echo ðŸ“± Low-Resource Mode Enabled\n) else (\n    if \"%CPU_ONLY_MODE%\"==\"true\" (\n        echo ðŸ’» CPU-Only Mode Enabled\n    )\n)\n\nREM 1. Start the Unified Server (Truly Background - no window)\nif \"%LOW_RESOURCE_MODE%\"==\"true\" (\n    start \"Anchor Core\" /min cmd /c \"cd tools && set LOW_RESOURCE_MODE=true && python webgpu_bridge.py\"\n) else (\n    if \"%CPU_ONLY_MODE%\"==\"true\" (\n        start \"Anchor Core\" /min cmd /c \"cd tools && set CPU_ONLY_MODE=true && python webgpu_bridge.py\"\n    ) else (\n        start \"Anchor Core\" /min cmd /c \"cd tools && python webgpu_bridge.py\"\n    )\n)\n\nREM 2. Wait for Server to initialize\necho Waiting for server to initialize...\ntimeout /t 5 /nobreak >nul\n\nREM 3. Launch the Ghost Engine (Truly Background - no window)\nREM Points to the new chat.html on the single port in headless mode (FIXED: JavaScript Enabled)\necho ðŸ‘» Launching Ghost Engine...\nif \"%LOW_RESOURCE_MODE%\"==\"true\" (\n    start \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost --max-active-webgl-contexts=1 --max-webgl-contexts-per-group=1 --disable-gpu-memory-buffer-compositor-resources --force-gpu-mem-available-mb=64 --force-low-power-gpu\"\n) else (\n    if \"%CPU_ONLY_MODE%\"==\"true\" (\n        start \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost --force-low-power-gpu --disable-gpu-sandbox --disable-features=VizDisplayCompositor\"\n    ) else (\n        start \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost\"\n    )\n)\n\necho.\necho âœ… Anchor System Started in Background\necho    Open http://localhost:8000 in your browser when ready\necho    Services will start automatically when you access the UI\necho.\necho ðŸ’¡ For low-resource devices: set LOW_RESOURCE_MODE=true before running\necho ðŸ’¡ For CPU-only: set CPU_ONLY_MODE=true before running\necho.\necho ðŸ”„ Ghost Engine should connect automatically to enable chat and memory search",
    "source": "start-anchor.bat"
  },
  {
    "id": "start-low-resource.bat",
    "timestamp": 1767237775,
    "role": "file",
    "content": "@echo off\necho ðŸ“± STARTING ANCHOR SYSTEM IN LOW-RESOURCE MODE...\n\necho ðŸ“‹ Configuration:\necho   - GPU Buffer: 64MB (conservative)\necho   - Single-threaded operations\necho   - Small model defaults (Phi-3.5-mini)\necho   - Reduced cache sizes\necho   - Longer timeouts for stability\n\nREM Set environment variables for low-resource mode\nset LOW_RESOURCE_MODE=true\n\nREM 1. Start the Unified Server with low-resource settings\necho Starting Anchor Core...\nstart \"Anchor Core\" /min cmd /c \"cd tools && set LOW_RESOURCE_MODE=true && python webgpu_bridge.py\"\n\nREM 2. Wait for Server to initialize\necho Waiting for server to initialize...\ntimeout /t 5 /nobreak >nul\n\nREM 3. Launch the Ghost Engine with conservative GPU settings (FIXED: JavaScript Enabled)\necho ðŸ‘» Launching Ghost Engine...\nstart \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost --max-active-webgl-contexts=1 --max-webgl-contexts-per-group=1 --disable-gpu-memory-buffer-compositor-resources --force-gpu-mem-available-mb=64 --force-low-power-gpu --disable-gpu-driver-workarounds --disable-gpu-sandbox --disable-features=VizDisplayCompositor --disable-gpu-memory-buffer-video-frames --disable-gpu-memory-buffer-compositor-resources\"\n\necho.\necho âœ… Anchor System Started in Low-Resource Mode\necho    Open http://localhost:8000 in your browser when ready\necho    Services will start automatically when you access the UI\necho.\necho ðŸ’¡ Tips for low-resource devices:\necho   - Use Phi-3.5-mini or smaller models\necho   - Expect slower response times\necho   - Close other GPU-intensive applications\necho   - Consider using CPU-only mode if GPU crashes persist",
    "source": "start-low-resource.bat"
  },
  {
    "id": "backend\\README.md",
    "timestamp": 1766451022,
    "role": "file",
    "content": "# Backend Directory\r\n\r\nContains server-side logs and legacy Python backend configurations (minimized in favor of the browser-native tools).\r\n",
    "source": "backend\\README.md"
  },
  {
    "id": "docs\\low_resource_mode.md",
    "timestamp": 1767229834,
    "role": "file",
    "content": "# Low-Resource Mode for Anchor Core\n\nThis guide explains how to optimize the Anchor Core for phones, small laptops, and other low-resource devices.\n\n## Environment Variables\n\nThe system supports two environment variables for optimization:\n\n### `LOW_RESOURCE_MODE`\n- Set to `true` to enable conservative settings for low-resource devices\n- Reduces GPU buffer size to 64MB\n- Limits concurrent operations to 1\n- Uses smaller models and reduced context windows\n\n### `CPU_ONLY_MODE`\n- Set to `true` to force CPU-only processing (no GPU)\n- Useful when GPU is unavailable or causing crashes\n- Slower but more stable on constrained hardware\n\n## Setting Environment Variables\n\n### Windows Command Prompt:\n```cmd\nset LOW_RESOURCE_MODE=true\nstart-anchor.bat\n```\n\n### Windows PowerShell:\n```powershell\n$env:LOW_RESOURCE_MODE=\"true\"\n.\\start-anchor.bat\n```\n\n### Linux/Mac:\n```bash\nexport LOW_RESOURCE_MODE=true\n./start-anchor.sh\n```\n\n## Conservative Settings Applied\n\nWhen `LOW_RESOURCE_MODE` is enabled, the system applies:\n\n- **GPU Buffer**: 64MB (vs 256MB default)\n- **Model**: Phi-3.5-mini (smallest recommended)\n- **Context Window**: 2048 tokens (vs 4096+ default)\n- **Batch Size**: 1 (vs 4+ default)\n- **WebGL Contexts**: 1 max (vs 16+ default)\n- **Cache Size**: 128MB (vs 1GB+ default)\n- **Timeouts**: 120 seconds (vs 30s default)\n\n## For Phones and Tablets\n\nFor mobile devices, use both settings:\n```cmd\nset LOW_RESOURCE_MODE=true\nset CPU_ONLY_MODE=true\nstart-anchor.bat\n```\n\n## Model Recommendations for Low-Resource Devices\n\n- `Phi-3.5-mini-instruct-q4f16_1-MLC` - Smallest recommended model\n- `Qwen2-0.5B-Instruct-q4f16_1-MLC` - If available, even smaller\n- Avoid models > 1.5B parameters on devices with < 1GB VRAM\n\n## Troubleshooting\n\n### GPU Crashes\n- Enable `LOW_RESOURCE_MODE=true`\n- Consider `CPU_ONLY_MODE=true` for stability\n\n### Slow Performance\n- Use the smallest available models\n- Reduce context window size\n- Close other GPU-intensive applications\n\n### Memory Issues\n- Enable conservative memory settings\n- Clear browser cache regularly\n- Use single-threaded mode",
    "source": "docs\\low_resource_mode.md"
  },
  {
    "id": "docs\\sidecar_vision_guide.md",
    "timestamp": 1767225159,
    "role": "file",
    "content": "# Anchor Core: Browser-Based Control Center & Vision Integration\n\n## Overview\n\nThe Anchor Core now includes a browser-based control center that provides unified access to system functionality through two main interfaces:\n\n1. **Sidecar Dashboard** (`http://localhost:8000/sidecar`) - Dual-tab interface for context retrieval and vision processing\n2. **Context UI** (`http://localhost:8000/context`) - Manual context retrieval with scrollable display and copy functionality\n\n## Components\n\n### Vision Engine (`tools/vision_engine.py`)\n- Python-powered Vision Language Model (VLM) integration\n- Currently configured for Ollama backend with LLaVA model\n- Handles image analysis and converts to text descriptions for memory storage\n\n### Sidecar Dashboard (`tools/sidecar.html`)\n- **Retrieve Tab**: Query the memory graph and retrieve context\n- **Vision Tab**: Drag-and-drop image processing with VLM analysis\n- Real-time processing logs\n\n### Context UI (`tools/context.html`)\n- Manual context retrieval interface\n- Scrollable text display for reviewing context\n- One-click copy functionality for pasting into other tools\n\n## Setup\n\n### Prerequisites\n1. Ensure Ollama is installed and running:\n   ```bash\n   ollama serve\n   ```\n\n2. Pull a vision model (e.g., LLaVA):\n   ```bash\n   ollama pull llava\n   ```\n\n### Launch\n1. Start the Anchor Core:\n   ```bash\n   start-anchor.bat\n   ```\n\n2. Access the interfaces:\n   - Sidecar: `http://localhost:8000/sidecar`\n   - Context UI: `http://localhost:8000/context`\n\n## Usage\n\n### Context Retrieval\n1. Open `http://localhost:8000/context`\n2. Enter a query in the search field (e.g., \"Project Specs\")\n3. Click \"Fetch Context\" to retrieve relevant information\n4. Review the context in the scrollable text area\n5. Click \"ðŸ“‹ Copy to Clipboard\" to copy the context for use elsewhere\n\n### Vision Processing\n1. Open `http://localhost:8000/sidecar`\n2. Go to the \"Vision\" tab\n3. Drag and drop an image or click to upload\n4. The image will be processed by the VLM\n5. Results will be stored in the memory graph automatically\n\n### Memory Search\n1. Use the \"Retrieve\" tab in the sidecar\n2. Enter your query and click \"Fetch Context\"\n3. Copy the results to use in other applications\n\n## Endpoints\n\n- `GET /sidecar` - Serve the sidecar dashboard\n- `GET /context` - Serve the context UI\n- `POST /v1/vision/ingest` - Process uploaded images with VLM\n- `POST /v1/memory/search` - Search the memory graph\n\n## Architecture\n\nThe system follows a unified architecture where:\n- The WebGPU Bridge (`webgpu_bridge.py`) serves UI files and orchestrates components\n- Vision processing happens in Python via the Vision Engine\n- Memory storage uses the graph database\n- Communication between components happens via WebSockets",
    "source": "docs\\sidecar_vision_guide.md"
  },
  {
    "id": "docs\\webgpu_snapdragon_issues.md",
    "timestamp": 1767236881,
    "role": "file",
    "content": "# WebGPU Issues on Snapdragon/XPS13 Devices\n\nThis document explains how to handle WebGPU adapter issues on Snapdragon-based devices like the XPS13.\n\n## The Issue\n\nOn Snapdragon/XPS13 devices, you may encounter this error:\n```\nLoad Failed: No WebGPU Adapter found. This often happens after a GPU crash. Please RESTART YOUR BROWSER or check for driver updates.\n```\n\nThis occurs because:\n- The Adreno GPU in Snapdragon processors has limited WebGPU support\n- The WebGPU implementation may not be available in all browser contexts\n- Power management settings may disable GPU acceleration\n\n## Solutions\n\n### 1. Use CPU-Only Mode\nSet the environment variable before starting:\n```cmd\nset CPU_ONLY_MODE=true\nstart-anchor.bat\n```\n\n### 2. Browser Launch Parameters\nThe system now includes conservative GPU parameters for Snapdragon devices:\n- `--force-low-power-gpu` - Forces low-power GPU mode\n- `--disable-gpu-driver-workarounds` - Bypasses problematic driver workarounds\n- `--max-active-webgl-contexts=1` - Limits GPU context usage\n\n### 3. Alternative Models\nConsider using models that are more CPU-friendly:\n- Use smaller models (0.5B or 1.5B parameters)\n- Use quantized models (q4f16_1 format)\n\n### 4. Manual Browser Settings\nIf launching manually:\n1. Open Edge/Chrome with `--enable-unsafe-webgpu --force-low-power-gpu`\n2. Navigate to `edge://flags` or `chrome://flags`\n3. Enable \"Unsafe WebGPU\" experiment\n4. Restart browser\n\n## Error Handling\n\nThe system now handles WebGPU errors gracefully by:\n- Providing informative error messages\n- Suggesting alternative solutions\n- Continuing to operate where possible\n\n## Future Improvements\n\nWe're working on:\n- CPU-based fallback models for unsupported GPUs\n- Better hardware detection and automatic configuration\n- Optimized parameters for ARM-based processors",
    "source": "docs\\webgpu_snapdragon_issues.md"
  },
  {
    "id": "extension\\background.js",
    "timestamp": 1766619659,
    "role": "file",
    "content": "chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n  if (request.action === 'queryMemories') {\n    queryMemoriesFromCozoDB(request.query)\n      .then(memories => ({\n        success: true,\n        memories: memories,\n        summary: generateSummary(memories)\n      }))\n      .then(result => sendResponse(result))\n      .catch(err => {\n        console.error('[Sovereign] Error querying memories:', err);\n        sendResponse({\n          success: false,\n          error: err.message,\n          summary: null\n        });\n      });\n    return true; // Keep channel open for async response\n  }\n});\n\nasync function queryMemoriesFromCozoDB(userInput) {\n  try {\n    // Attempt to hit the Local Bridge (webgpu_bridge.py)\n    // The bridge runs on port 8080 if using start-bridge.bat, or random port 9000-9999\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), 10000); // 10 second timeout\n\n    // Try ports for the bridge (8080 is the default when using start-bridge.bat, others are random)\n    const commonPorts = [8080, 9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015];\n    let response = null;\n    let lastError = null;\n\n    for (const port of commonPorts) {\n      try {\n        // First, try to ping the health endpoint to see if there's a bridge running\n        const healthUrl = `http://localhost:${port}/health`;\n        let healthResponse;\n\n        try {\n          healthResponse = await fetch(healthUrl, {\n            method: 'GET',\n            signal: controller.signal\n          });\n        } catch (healthError) {\n          console.log(`[Sovereign] Health check failed for port ${port}:`, healthError.message);\n          continue; // Try next port\n        }\n\n        if (!healthResponse.ok) {\n          console.log(`[Sovereign] Health check failed for port ${port}, status: ${healthResponse.status}`);\n          continue; // Try next port\n        }\n\n        console.log(`[Sovereign] Bridge detected on port ${port}, testing memory search...`);\n\n        // Now try the memory search endpoint\n        const searchUrl = `http://localhost:${port}/memories/search`;\n\n        // Try with the default token first\n        response = await fetch(searchUrl, {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n            'Authorization': 'Bearer sovereign-secret'  // Default token from start-bridge.bat\n          },\n          body: JSON.stringify({ query: userInput }),\n          signal: controller.signal\n        });\n\n        if (response.ok) {\n          console.log(`[Sovereign] Successfully connected to bridge at port ${port}`);\n          clearTimeout(timeoutId);\n          return response.json();\n        } else if (response.status === 401) {\n          // If unauthorized, try without token (some configurations might not require it)\n          console.log(`[Sovereign] Trying without auth token for port ${port}`);\n          response = await fetch(searchUrl, {\n            method: 'POST',\n            headers: {\n              'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({ query: userInput }),\n            signal: controller.signal\n          });\n\n          if (response.ok) {\n            console.log(`[Sovereign] Successfully connected to bridge at port ${port} (no auth)`);\n            clearTimeout(timeoutId);\n            return response.json();\n          }\n        }\n      } catch (e) {\n        lastError = e;\n        console.log(`[Sovereign] Trying bridge port ${port} failed:`, e.message);\n        continue;\n      }\n    }\n\n    // If all ports failed, use the last error\n    clearTimeout(timeoutId);\n    throw lastError || new Error(`All bridge ports failed. No connection to WebGPU bridge available. Is the bridge running?`);\n  } catch (e) {\n    console.warn('[Sovereign] Backend unavailable, falling back to simulated response...', e.message);\n    // In the future, this can connect directly to IndexedDB via shared worker\n    // For now, return simulated data\n    return [\n      {\n        content: \"This is a simulated memory based on your input: \" + userInput.substring(0, 100) + \"...\",\n        timestamp: new Date().toISOString(),\n        relevance: 0.8\n      }\n    ];\n  }\n}\n\nfunction generateSummary(memories) {\n  if (!memories || memories.length === 0) return null;\n\n  const maxMemories = 3;\n  const relevant = memories.slice(0, maxMemories);\n\n  return relevant\n    .map((m, idx) => `[Memory ${idx + 1}] ${m.content.substring(0, 150)}...`)\n    .join('\\n');\n}",
    "source": "extension\\background.js"
  },
  {
    "id": "extension\\content.js",
    "timestamp": 1766916224,
    "role": "file",
    "content": "// Platform-specific DOM selectors\r\nconst SELECTORS = {\r\n    'gemini.google.com': 'div[contenteditable=\"true\"], textarea',\r\n    'chatgpt.openai.com': 'textarea, div[contenteditable=\"true\"]'\r\n};\r\n\r\nlet textArea = null;\r\nlet inputTimeout = null;\r\nconst PAUSE_THRESHOLD = 3000; // 3 seconds\r\n\r\n// 1. Detect the active text input\r\nfunction detectTextArea() {\r\n    const domain = window.location.hostname;\r\n    const selector = SELECTORS[domain];\r\n    if (!selector) return null;\r\n    return document.querySelector(selector);\r\n}\r\n\r\n// 2. Extract text from the input\r\nfunction getVisibleText() {\r\n    if (!textArea) return \"\";\r\n    return textArea.value || textArea.textContent || \"\";\r\n}\r\n\r\n// 3. Monitor for user pauses\r\nfunction setupPauseDetector() {\r\n    if (!textArea) return;\r\n\r\n    textArea.addEventListener('input', () => {\r\n        clearTimeout(inputTimeout);\r\n        inputTimeout = setTimeout(() => {\r\n            const text = getVisibleText();\r\n            if (text.length > 10) { // Only query if meaningful text exists\r\n                console.log('[Sovereign] 3-second pause detected, querying memories...');\r\n                chrome.runtime.sendMessage(\r\n                    { action: 'queryMemories', query: text },\r\n                    (response) => {\r\n                        if (response && response.success) injectContext(response);\r\n                    }\r\n                );\r\n            }\r\n        }, PAUSE_THRESHOLD);\r\n    });\r\n}\r\n\r\n// 4. Inject the retrieved context\r\nfunction injectContext(contextData) {\r\n    if (!contextData.summary) return;\r\n\r\n    const timestamp = new Date().toLocaleTimeString();\r\n    const summary = `\\n\\n[Sovereign Context Injection at ${timestamp}]\\n${contextData.summary}\\n---\\n`;\r\n\r\n    // For contenteditable (Gemini/modern apps)\r\n    if (textArea.isContentEditable || textArea.getAttribute('contenteditable') === 'true') {\r\n        // Simple append - in production this might need Range/Selection manipulation for cursors\r\n        textArea.textContent = textArea.textContent + summary;\r\n    }\r\n    // For standard textarea (ChatGPT legacy)\r\n    else {\r\n        textArea.value += summary;\r\n    }\r\n\r\n    // Notify user\r\n    displayIndicator('\\u2713 Context injected', 'success'); // Using Unicode checkmark\r\n}\r\n\r\n// 5. UI Feedback\r\nfunction displayIndicator(message, type) {\r\n    let indicator = document.getElementById('sovereign-indicator');\r\n    if (!indicator) {\r\n        indicator = document.createElement('div');\r\n        indicator.id = 'sovereign-indicator';\r\n        indicator.style.position = 'fixed';\r\n        indicator.style.bottom = '20px';\r\n        indicator.style.right = '20px';\r\n        indicator.style.padding = '10px 15px';\r\n        indicator.style.borderRadius = '5px';\r\n        indicator.style.zIndex = '9999';\r\n        indicator.style.fontFamily = 'monospace';\r\n        document.body.appendChild(indicator);\r\n    }\r\n\r\n    indicator.textContent = message;\r\n    indicator.style.background = type === 'success' ? '#238636' : '#da3633';\r\n    indicator.style.color = '#ffffff';\r\n\r\n    setTimeout(() => indicator.remove(), 5000);\r\n}\r\n\r\n// Handle messages from popup\r\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\r\n    if (request.action === 'testInjection') {\r\n        // For testing purposes, inject a sample context\r\n        const testData = {\r\n            summary: \"This is a test injection from Sovereign Context Bridge.\\n\\n[Sample Memory] Example context for testing purposes...\"\r\n        };\r\n        injectContext(testData);\r\n        sendResponse({ success: true });\r\n        return true; // Keep channel open for async response\r\n    }\r\n});\r\n\r\n// --- 6. Robust Initialization ---\r\nfunction startSovereignObserver() {\r\n    // Safety Check: If body isn't ready, wait for next frame\r\n    if (!document.body) {\r\n        console.warn(\"[Sovereign] document.body not ready, retrying...\");\r\n        requestAnimationFrame(startSovereignObserver);\r\n        return;\r\n    }\r\n\r\n    console.log(\"[Sovereign] Body detected. Eyes opening...\");\r\n\r\n    // Main Logic\r\n    if (!textArea) {\r\n        textArea = detectTextArea();\r\n        if (textArea) {\r\n            console.log(\"[Sovereign] Input Found on Init.\");\r\n            setupPauseDetector();\r\n        }\r\n    }\r\n\r\n    // Watch for dynamic changes\r\n    const observer = new MutationObserver(() => {\r\n        if (!textArea) {\r\n            textArea = detectTextArea();\r\n            if (textArea) console.log(\"[Sovereign] Input Found via Mutation.\");\r\n        }\r\n    });\r\n\r\n    observer.observe(document.body, { childList: true, subtree: true });\r\n}\r\n\r\n// Start only when DOM is ready\r\nif (document.readyState === 'loading') {\r\n    document.addEventListener('DOMContentLoaded', startSovereignObserver);\r\n} else {\r\n    startSovereignObserver();\r\n}\r\n\r\n// --- DEBUG: CLICK-TO-LOG REFLEX ---\r\n// This allows us to manually verify what the extension sees when we touch the UI.\r\ndocument.addEventListener('click', (event) => {\r\n    const target = event.target;\r\n    const detected = detectTextArea();\r\n\r\n    console.group(\"ðŸ‘ï¸ [Sovereign Debug] Retina Scan\");\r\n    console.log(\"ðŸ–±ï¸ Clicked Element:\", target);\r\n    console.log(\"ðŸ·ï¸ Clicked Class:\", target.className);\r\n\r\n    if (detected) {\r\n        console.log(\"%câœ… Active Input Detected:\", \"color:green;font-weight:bold\", detected);\r\n        console.log(\"ðŸ“ Current Value:\", detected.value || detected.innerText || detected.textContent);\r\n    } else {\r\n        console.log(\"%câŒ No Input Detected via Selector\", \"color:red;font-weight:bold\");\r\n        console.log(\"ðŸ” Current Selector for Domain:\", SELECTORS[window.location.hostname] || \"NONE\");\r\n    }\r\n    console.groupEnd();\r\n});",
    "source": "extension\\content.js"
  },
  {
    "id": "extension\\manifest.json",
    "timestamp": 1766916224,
    "role": "file",
    "content": "manifest_version: 3\nname: Sovereign Context Bridge\nversion: 1.0.1\ndescription: Silent context injection for LLM conversations\npermissions:\n  - activeTab\n  - scripting\n  - storage\n  - webRequest\nhost_permissions:\n  - *://gemini.google.com/*\n  - *://chatgpt.com/*\n  - http://localhost/*\n  - http://127.0.0.1/*\ncontent_scripts:\n  -\n    matches:\n      - *://gemini.google.com/*\n      - *://chatgpt.com/*\n    js:\n      - content.js\n    run_at: document_idle\nbackground:\n  service_worker: background.js\naction:\n  default_popup: popup.html\n  default_title: Sovereign Context Bridge",
    "source": "extension\\manifest.json"
  },
  {
    "id": "extension\\popup.html",
    "timestamp": 1766458261,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"UTF-8\">\n  <style>\n    body { background: #0f1115; color: #e2e8f0; font-family: sans-serif; width: 250px; padding: 15px; }\n    h3 { margin-top: 0; color: #00ff88; font-weight: 300; border-bottom: 1px solid #333; padding-bottom: 10px; }\n    .stat-row { display: flex; justify-content: space-between; margin-bottom: 8px; font-size: 0.9rem; }\n    .val { font-weight: bold; color: #58a6ff; }\n    button { width: 100%; padding: 8px; margin-top: 10px; background: #2d2d2d; border: 1px solid #444; color: #fff; cursor: pointer; border-radius: 4px; }\n    button:hover { background: #333; border-color: #00ff88; }\n    .status-ok { color: #00ff88; }\n    .status-err { color: #ff4444; }\n  </style>\n</head>\n<body>\n  <h3>&#129300 Sovereign Bridge</h3>\n\n  <div class=\"stat-row\">\n    <span>Status:</span>\n    <span id=\"status-badge\" class=\"status-err\">&#9679; Offline</span>\n  </div>\n\n  <div class=\"stat-row\">\n    <span>Memories:</span>\n    <span id=\"mem-count\" class=\"val\">0</span>\n  </div>\n\n  <div class=\"stat-row\">\n    <span>Last Inject:</span>\n    <span id=\"last-inject\" class=\"val\">-</span>\n  </div>\n\n  <button id=\"settings-btn\">&#9881;&#65039 Settings</button>\n  <button id=\"test-inject-btn\">&#129512 Test Injection</button>\n\n  <script src=\"popup.js\"></script>\n</body>\n</html>",
    "source": "extension\\popup.html"
  },
  {
    "id": "extension\\popup.js",
    "timestamp": 1766526948,
    "role": "file",
    "content": "document.addEventListener('DOMContentLoaded', async () => {\n    const statusBadge = document.getElementById('status-badge');\n\n    // Check connection to Local Bridge\n    try {\n        // Using a more robust approach to handle CORS issues\n        const controller = new AbortController();\n        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout\n\n        const res = await fetch('http://localhost:8080/health', {\n            signal: controller.signal,\n            mode: 'cors', // Explicitly set CORS mode\n            credentials: 'omit' // Don't send credentials\n        });\n\n        clearTimeout(timeoutId);\n\n        if (res.ok) {\n            statusBadge.textContent = \"â— Online\";\n            statusBadge.className = \"status-ok\";\n        } else {\n            statusBadge.textContent = \"â— Offline\";\n            statusBadge.className = \"status-err\";\n        }\n    } catch (e) {\n        // Handle network errors, CORS errors, and timeouts\n        console.warn('[Sovereign] Backend connection failed:', e.message);\n        statusBadge.textContent = \"â— Offline\";\n        statusBadge.className = \"status-err\";\n    }\n\n    document.getElementById('test-inject-btn').addEventListener('click', () => {\n        // Trigger manual test injection\n        chrome.tabs.query({active: true, currentWindow: true}, (tabs) => {\n            chrome.tabs.sendMessage(tabs[0].id, { action: 'testInjection' }, (response) => {\n                if (chrome.runtime.lastError) {\n                    console.log('[Sovereign] Test injection not available on this page');\n                } else {\n                    console.log('[Sovereign] Test injection triggered');\n                }\n            });\n        });\n    });\n\n    // Add settings button functionality\n    document.getElementById('settings-btn').addEventListener('click', () => {\n        // For now, just show a message - in the future this could open options page\n        alert('Sovereign Context Bridge Settings\\n\\nConfigure extension preferences here.');\n    });\n});",
    "source": "extension\\popup.js"
  },
  {
    "id": "extension\\images\\README.md",
    "timestamp": 1766458004,
    "role": "file",
    "content": "# Extension Icons\n\nThis directory contains the following icon files:\n\n- `icon-16.png` - 16x16 pixel icon (minimal valid PNG)\n- `icon-32.png` - 32x32 pixel icon (minimal valid PNG)\n- `icon-128.png` - 128x128 pixel icon (minimal valid PNG)\n\nThese icons represent the Sovereign Context Bridge extension.\n\nNote: The current files are minimal valid PNGs for extension loading. Replace with actual designed icons for production use.",
    "source": "extension\\images\\README.md"
  },
  {
    "id": "logs\\README.md",
    "timestamp": 1767241530,
    "role": "file",
    "content": "# Logs Directory\n\nThis directory contains individual log files for each system component to facilitate debugging and monitoring.\n\n## Log File Naming Convention\n\nEach component writes to its own log file named after the source:\n\n- `system.log` - System startup and general operations\n- `chat_api.log` - Chat API requests and responses\n- `memory_api.log` - Memory search API operations\n- `websocket_bridge.log` - WebSocket connection events\n- `python_stdout.log` - Python standard output\n- `python_stderr.log` - Python standard error\n\n## Log Rotation\n\nEach log file is automatically truncated to keep only the last 1000 lines to prevent excessive disk usage.\n\n## Log Format\n\nEach log entry follows this format:\n```\n[YYYY-MM-DD HH:MM:SS] [LEVEL] Message content\n```\n\nWhere LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG\n\n## Accessing Logs\n\n- **Real-time viewing**: Use the log viewer at `http://localhost:8000/log-viewer.html`\n- **File access**: Individual log files are available in this directory\n- **API access**: Recent logs available via `/logs/recent` endpoint",
    "source": "logs\\README.md"
  },
  {
    "id": "scripts\\CHANGELOG.md",
    "timestamp": 1766916224,
    "role": "file",
    "content": "# Changelog\r\n\r\nAll notable changes to the `scripts/` module will be documented in this file.\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **Smart GPU Bridge**: Added `StaticFiles` mount at `/models` to serve local model artifacts.\r\n- **On-Demand Downloads**: Added `POST /v1/models/pull` and `GET /v1/models/pull/status` to handle server-side model downloading from Hugging Face.\r\n- **Shared Module**: Integrated `scripts.download_models` for reusable download logic.\r\n\r\n### Fixed\r\n- **CORS/Auth Collision**: Reordered `CORSMiddleware` to wrap the entire application (including Auth middleware) to ensure CORS headers are sent even on 401 Unauthorized responses.\r\n- **Authentication**: Exempted `/models` path from Token Verification to allow browser-side fetching of artifacts without credentials.\r\n- **Import Error**: Fixed `ModuleNotFoundError` by changing import to `from download_models import ...` for direct script execution.\r\n",
    "source": "scripts\\CHANGELOG.md"
  },
  {
    "id": "scripts\\download_models.py",
    "timestamp": 1767199928,
    "role": "file",
    "content": "\nimport os\nimport json\nimport requests\nimport sys\nfrom pathlib import Path\n\n# Configuration\nMODELS_DIR = Path(\"models\").resolve()\nHF_ENDPOINT = \"https://huggingface.co\"\n\ndef download_file(url, dest_path, progress_callback=None):\n    \"\"\"Download a file with progress indication\"\"\"\n    if dest_path.exists():\n        if progress_callback: progress_callback(f\"Skipping {dest_path.name} (exists)\", 1.0)\n        return\n\n    if progress_callback: progress_callback(f\"Downloading {dest_path.name}...\", 0.0)\n    \n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        total_size = int(response.headers.get('content-length', 0))\n        block_size = 8192\n        wrote = 0\n        \n        with open(dest_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=block_size):\n                f.write(chunk)\n                wrote += len(chunk)\n                # Optional: detailed progress\n        \n        if progress_callback: progress_callback(f\"Saved {dest_path.name}\", 1.0)\n        \n    except Exception as e:\n        if progress_callback: progress_callback(f\"Error {dest_path.name}: {e}\", 0.0)\n        raise e\n\ndef download_model(model_id, repo_url=None, base_dir=None, progress_callback=None):\n    \"\"\"\n    Downloads an MLC model from Hugging Face.\n    \n    Args:\n        model_id (str): The ID of the model (e.g. \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\")\n        repo_url (str, optional): Full HF URL. Defaults to constructing from model_id.\n        base_dir (Path, optional): Directory to store models. Defaults to ./models\n        progress_callback (func, optional): Function(msg, progress_float)\n    \"\"\"\n    if base_dir is None:\n        base_dir = MODELS_DIR\n    \n    base_dir.mkdir(exist_ok=True)\n    \n    # Handle model_id / repo_url\n    if not repo_url:\n        repo_url = f\"{HF_ENDPOINT}/mlc-ai/{model_id}\"\n    \n    # Strip prefix from model_id for directory name\n    dir_name = model_id.split(\"/\")[-1]\n    model_dir = base_dir / dir_name\n    model_dir.mkdir(exist_ok=True)\n    \n    if progress_callback: progress_callback(f\"Starting download for {dir_name}\", 0.0)\n\n    # 1. Download ndarray-cache.json\n    cache_url = f\"{repo_url}/resolve/main/ndarray-cache.json\"\n    cache_path = model_dir / \"ndarray-cache.json\"\n    \n    try:\n        download_file(cache_url, cache_path, progress_callback)\n    except Exception as e:\n        print(f\"âŒ Failed to fetch ndarray-cache.json: {e}\")\n        raise e\n\n    # 2. Parse cache\n    with open(cache_path, 'r') as f:\n        cache_data = json.load(f)\n        \n    records = cache_data.get(\"records\", [])\n    total_files = len(records) + 5\n    completed = 1\n\n    # 3. Download Shards\n    for record in records:\n        # Check both keys for safety (older MLC mappings used 'name')\n        file_name = record.get(\"dataPath\", record.get(\"name\"))\n        if not file_name:\n            continue\n            \n        url = f\"{repo_url}/resolve/main/{file_name}\"\n        dest = model_dir / file_name\n        \n        download_file(url, dest)\n        \n        completed += 1\n        if progress_callback: \n            progress_callback(f\"Downloading {file_name}\", completed/total_files)\n\n    # 4. Download Configs\n    config_files = [\"mlc-chat-config.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\", \"merges.txt\"]\n    for fname in config_files:\n        url = f\"{repo_url}/resolve/main/{fname}\"\n        dest = model_dir / fname\n        try:\n            download_file(url, dest)\n        except:\n            pass # Optional\n        \n        completed += 1\n        if progress_callback: \n            progress_callback(f\"Checked {fname}\", completed/total_files)\n\n    if progress_callback: progress_callback(\"Download Complete\", 1.0)\n    print(f\"Serve at: http://localhost:8080/models/{dir_name}\")\n\ndef main():\n    # Default behavior: Download Qwen2.5-Coder-1.5B\n    default_model = \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\"\n    \n    def print_progress(msg, p):\n        print(f\"[{int(p*100)}%] {msg}\")\n\n    download_model(default_model, progress_callback=print_progress)\n\nif __name__ == \"__main__\":\n    main()\n",
    "source": "scripts\\download_models.py"
  },
  {
    "id": "scripts\\gpu_manager.py",
    "timestamp": 1766555341,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nGPU Resource Manager for ECE_Core\nProvides utilities to monitor and manage GPU locks in the WebGPU bridge\n\"\"\"\n\nimport requests\nimport json\nimport time\nimport argparse\nfrom typing import Dict, Any\n\nclass GPUResourceManager:\n    def __init__(self, bridge_url: str = \"http://localhost:8080\"):\n        self.bridge_url = bridge_url\n        self.headers = {\"Authorization\": \"Bearer sovereign-secret\"}\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get current GPU status\"\"\"\n        try:\n            response = requests.get(f\"{self.bridge_url}/v1/gpu/status\", headers=self.headers)\n            if response.status_code == 200:\n                return response.json()\n            else:\n                print(f\"Error getting status: {response.status_code} - {response.text}\")\n                return {}\n        except Exception as e:\n            print(f\"Error connecting to bridge: {e}\")\n            return {}\n    \n    def reset_lock(self) -> bool:\n        \"\"\"Reset the current GPU lock\"\"\"\n        try:\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/reset\", headers=self.headers)\n            if response.status_code == 200:\n                print(\"âœ… GPU lock reset successfully\")\n                return True\n            else:\n                print(f\"âŒ Failed to reset GPU lock: {response.status_code} - {response.text}\")\n                return False\n        except Exception as e:\n            print(f\"âŒ Error resetting GPU lock: {e}\")\n            return False\n    \n    def force_release_all(self) -> bool:\n        \"\"\"Force release all GPU locks (emergency)\"\"\"\n        try:\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/force-release-all\", headers=self.headers)\n            if response.status_code == 200:\n                print(\"âœ… All GPU locks force released successfully\")\n                return True\n            else:\n                print(f\"âŒ Failed to force release GPU locks: {response.status_code} - {response.text}\")\n                return False\n        except Exception as e:\n            print(f\"âŒ Error force releasing GPU locks: {e}\")\n            return False\n    \n    def monitor(self, interval: int = 5):\n        \"\"\"Monitor GPU status continuously\"\"\"\n        print(f\"ðŸ“Š Monitoring GPU status every {interval}s (Ctrl+C to stop)\")\n        try:\n            while True:\n                status = self.get_status()\n                if status:\n                    locked = status.get('locked', False)\n                    owner = status.get('owner', 'None')\n                    queue_depth = status.get('queue_depth', 0)\n                    queued = status.get('queued', [])\n                    \n                    status_str = f\"GPU: {'LOCKED' if locked else 'FREE'}\"\n                    if locked:\n                        status_str += f\" by {owner}\"\n                    if queue_depth > 0:\n                        status_str += f\" | Queue: {queue_depth} | Queued: {', '.join(queued) if queued else 'None'}\"\n                    \n                    print(f\"[{time.strftime('%H:%M:%S')}] {status_str}\")\n                else:\n                    print(f\"[{time.strftime('%H:%M:%S')}] âŒ Unable to get GPU status\")\n                \n                time.sleep(interval)\n        except KeyboardInterrupt:\n            print(\"\\nâ¹ï¸  Monitoring stopped\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"GPU Resource Manager for ECE_Core\")\n    parser.add_argument(\"--bridge-url\", default=\"http://localhost:8080\", \n                       help=\"WebGPU bridge URL (default: http://localhost:8080)\")\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Get current GPU status\")\n    parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset GPU lock\")\n    parser.add_argument(\"--force-release\", action=\"store_true\", help=\"Force release all GPU locks\")\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Monitor GPU status continuously\")\n    parser.add_argument(\"--interval\", type=int, default=5, help=\"Monitor interval in seconds (default: 5)\")\n    \n    args = parser.parse_args()\n    \n    manager = GPUResourceManager(args.bridge_url)\n    \n    if args.status:\n        status = manager.get_status()\n        if status:\n            print(json.dumps(status, indent=2))\n        else:\n            print(\"âŒ Failed to get status\")\n    \n    elif args.reset:\n        manager.reset_lock()\n    \n    elif args.force_release:\n        manager.force_release_all()\n    \n    elif args.monitor:\n        manager.monitor(args.interval)\n    \n    else:\n        # Default: show status\n        status = manager.get_status()\n        if status:\n            locked = status.get('locked', False)\n            owner = status.get('owner', 'None')\n            queue_depth = status.get('queue_depth', 0)\n            queued = status.get('queued', [])\n            \n            print(f\"GPU Status: {'LOCKED' if locked else 'FREE'}\", end=\"\")\n            if locked:\n                print(f\" by {owner}\", end=\"\")\n            print(f\" | Queue: {queue_depth} items\")\n            \n            if queued:\n                print(f\"Queued: {', '.join(queued)}\")\n        else:\n            print(\"âŒ Failed to get status\")\n\nif __name__ == \"__main__\":\n    main()",
    "source": "scripts\\gpu_manager.py"
  },
  {
    "id": "scripts\\README.md",
    "timestamp": 1766451022,
    "role": "file",
    "content": "# Scripts Directory\r\n\r\nContains utility scripts for continuous integration and local environment setup.\r\n",
    "source": "scripts\\README.md"
  },
  {
    "id": "scripts\\ci\\check_docs.py",
    "timestamp": 1766310900,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"CI doc check script (Sovereign Era).\r\n\r\nVerifies the presence of critical spec files defined in specs/doc_policy.md.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n\r\nREPO_ROOT = Path(__file__).resolve().parents[2]\r\n\r\n\r\ndef main() -> int:\r\n    # 1. Check Core Specs (per specs/doc_policy.md Rule 3)\r\n    expected = [\r\n        REPO_ROOT / \"specs\" / \"spec.md\",\r\n        REPO_ROOT / \"specs\" / \"plan.md\",\r\n        REPO_ROOT / \"specs\" / \"tasks.md\",\r\n        REPO_ROOT / \"specs\" / \"doc_policy.md\",\r\n    ]\r\n    \r\n    missing = [str(p) for p in expected if not p.exists()]\r\n    if missing:\r\n        print(\"[FAIL] Missing core specification files:\")\r\n        for m in missing:\r\n            print(f\"  - {m}\")\r\n        return 2\r\n\r\n    # 2. Check README\r\n    readme = REPO_ROOT / \"README.md\"\r\n    if not readme.exists():\r\n        print(\"[FAIL] README.md not found\")\r\n        return 2\r\n\r\n    text = readme.read_text(encoding=\"utf-8\")\r\n    lower = text.lower()\r\n    \r\n    # 3. Simple Content Check (Sovereign Context Engine)\r\n    # We relax the strict \"UTCP\" check as architecture evolves.\r\n    checks = [\r\n        (\"context engine\", \"Project name 'Context Engine' not found in README\"),\r\n    ]\r\n    \r\n    failed = []\r\n    for token, msg in checks:\r\n        if token not in lower:\r\n            failed.append(msg)\r\n            \r\n    if failed:\r\n        print(\"[FAIL] README checks failed:\")\r\n        for f in failed:\r\n            print(f\"  - {f}\")\r\n        return 2\r\n\r\n    print(\"[OK] Sovereign Doc Checks Passed\")\r\n    return 0\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n",
    "source": "scripts\\ci\\check_docs.py"
  },
  {
    "id": "scripts\\models\\Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\\ndarray-cache.json",
    "timestamp": 1767199941,
    "role": "file",
    "content": "metadata:\n  ParamSize: 311\n  ParamBytes: 868547584.0\n  BitsPerParam: 4.501079412165634\nrecords:\n  -\n    dataPath: params_shard_0.bin\n    format: raw-shard\n    nbytes: 116686848\n    records:\n      -\n        name: model.embed_tokens.q_weight\n        shape:\n          - 151936\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 116686848\n        byteOffset: 0\n    md5sum: f4ca09395d1c686c5fd4b218af1af093\n  -\n    dataPath: params_shard_1.bin\n    format: raw-shard\n    nbytes: 22330368\n    records:\n      -\n        name: model.embed_tokens.q_scale\n        shape:\n          - 151936\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 14585856\n        byteOffset: 0\n      -\n        name: model.layers.0.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 14585856\n      -\n        name: model.layers.0.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 14588928\n      -\n        name: model.layers.0.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 21470208\n    md5sum: 613a8925a1901da401f93575a239f02a\n  -\n    dataPath: params_shard_2.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.0.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.0.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.0.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.0.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.0.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.0.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.0.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.0.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.1.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.1.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.1.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: e92c0dc827dee36d699145a413a9064a\n  -\n    dataPath: params_shard_3.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.1.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.1.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.1.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.1.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.1.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.1.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.1.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.1.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.10.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.10.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.10.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 0e78a810f25f40d7cab6b1b455e99c3d\n  -\n    dataPath: params_shard_4.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.10.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.10.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.10.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.10.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.10.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.10.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.10.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.10.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.11.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.11.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.11.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 6d6593ebd4a628cb2e44bc26cd4f1e62\n  -\n    dataPath: params_shard_5.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.11.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.11.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.11.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.11.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.11.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.11.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.11.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.11.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.12.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.12.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.12.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: b2dab743b9f219aa1b7ac6395704438f\n  -\n    dataPath: params_shard_6.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.12.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.12.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.12.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.12.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.12.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.12.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.12.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.12.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.13.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.13.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.13.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: aea741363013010464655fe00fc7befb\n  -\n    dataPath: params_shard_7.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.13.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.13.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.13.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.13.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.13.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.13.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.13.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.13.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.14.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.14.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.14.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 988f7cf5454c9d4cbbc29838a4da60fe\n  -\n    dataPath: params_shard_8.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.14.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.14.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.14.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.14.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.14.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.14.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.14.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.14.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.15.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.15.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.15.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 1b589178f29c61c1c5d19caae2088e11\n  -\n    dataPath: params_shard_9.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.15.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.15.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.15.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.15.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.15.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.15.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.15.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.15.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.16.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.16.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.16.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 43a36182d328d5d02f819ab2449e1848\n  -\n    dataPath: params_shard_10.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.16.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.16.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.16.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.16.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.16.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.16.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.16.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.16.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.17.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.17.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.17.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: e8261af07fa7a6e6a1cff5b1cde708b1\n  -\n    dataPath: params_shard_11.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.17.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.17.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.17.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.17.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.17.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.17.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.17.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.17.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.18.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.18.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.18.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: c65a79d136f6d903bdfc22e8712e1bd1\n  -\n    dataPath: params_shard_12.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.18.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.18.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.18.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.18.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.18.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.18.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.18.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.18.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.19.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.19.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.19.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 8bf25fbd0f684d60a251d41c50513d45\n  -\n    dataPath: params_shard_13.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.19.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.19.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.19.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.19.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.19.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.19.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.19.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.19.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.2.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.2.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.2.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 067d3dff4fa4f560c081426e8cbbc88c\n  -\n    dataPath: params_shard_14.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.2.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.2.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.2.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.2.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.2.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.2.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.2.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.2.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.20.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.20.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.20.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: a131e415f972b1c6c0dbe882b35ef41c\n  -\n    dataPath: params_shard_15.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.20.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.20.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.20.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.20.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.20.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.20.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.20.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.20.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.21.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.21.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.21.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: f5d4a75a6a2135a939af5713ac5f500e\n  -\n    dataPath: params_shard_16.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.21.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.21.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.21.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.21.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.21.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.21.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.21.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.21.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.22.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.22.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.22.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 05bfa52e634e98ca5093f65646a50e74\n  -\n    dataPath: params_shard_17.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.22.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.22.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.22.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.22.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.22.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.22.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.22.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.22.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.23.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.23.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.23.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: dd10bea779342657ebbf62764f430f8b\n  -\n    dataPath: params_shard_18.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.23.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.23.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.23.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.23.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.23.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.23.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.23.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.23.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.24.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.24.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.24.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 6f70a236930fd706a7918f82d8ae0534\n  -\n    dataPath: params_shard_19.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.24.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.24.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.24.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.24.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.24.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.24.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.24.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.24.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.25.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.25.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.25.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: f6271193876887244227c2067de5a961\n  -\n    dataPath: params_shard_20.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.25.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.25.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.25.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.25.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.25.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.25.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.25.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.25.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.26.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.26.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.26.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 83a0dd18752358f98513fdc6d9dbcb8a\n  -\n    dataPath: params_shard_21.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.26.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.26.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.26.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.26.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.26.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.26.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.26.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.26.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.27.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.27.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.27.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: cfdce01cfadf5f34dc6682cdec8d41a5\n  -\n    dataPath: params_shard_22.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.27.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.27.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.27.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.27.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.27.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.27.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.27.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.27.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.3.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.3.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.3.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 597a27c278a6c7e2d9d9d94f1e72c40f\n  -\n    dataPath: params_shard_23.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.3.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.3.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.3.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.3.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.3.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.3.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.3.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.3.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.4.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.4.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.4.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 4ec89f383c158a5f58fb0b6536bef6aa\n  -\n    dataPath: params_shard_24.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.4.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.4.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.4.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.4.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.4.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.4.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.4.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.4.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.5.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.5.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.5.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 56f51061c54df41f2fdca2a31c9c051b\n  -\n    dataPath: params_shard_25.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.5.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.5.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.5.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.5.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.5.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.5.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.5.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.5.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.6.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.6.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.6.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: a614a06096888e420d39c3ec6d104c3e\n  -\n    dataPath: params_shard_26.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.6.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.6.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.6.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.6.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.6.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.6.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.6.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.6.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.7.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.7.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.7.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 4d8348a350ff35052fb2763a764b777e\n  -\n    dataPath: params_shard_27.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.7.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.7.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.7.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.7.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.7.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.7.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.7.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.7.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.8.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.8.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.8.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: 565b8ef6d7e1fbe05ecddd36e5dd80cc\n  -\n    dataPath: params_shard_28.bin\n    format: raw-shard\n    nbytes: 26331136\n    records:\n      -\n        name: model.layers.8.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.8.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.8.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.8.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.8.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.8.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.8.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.8.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.layers.9.input_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n      -\n        name: model.layers.9.mlp.down_proj.q_weight\n        shape:\n          - 1536\n          - 1120\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 6881280\n        byteOffset: 18589696\n      -\n        name: model.layers.9.mlp.down_proj.q_scale\n        shape:\n          - 1536\n          - 280\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 860160\n        byteOffset: 25470976\n    md5sum: eb4e6bce77c7ada018c69550a401fe96\n  -\n    dataPath: params_shard_29.bin\n    format: raw-shard\n    nbytes: 18589696\n    records:\n      -\n        name: model.layers.9.mlp.gate_up_proj.q_weight\n        shape:\n          - 17920\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 13762560\n        byteOffset: 0\n      -\n        name: model.layers.9.mlp.gate_up_proj.q_scale\n        shape:\n          - 17920\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 1720320\n        byteOffset: 13762560\n      -\n        name: model.layers.9.post_attention_layernorm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 15482880\n      -\n        name: model.layers.9.self_attn.c_attn.bias\n        shape:\n          - 2048\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 4096\n        byteOffset: 15485952\n      -\n        name: model.layers.9.self_attn.c_attn.q_weight\n        shape:\n          - 2048\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1572864\n        byteOffset: 15490048\n      -\n        name: model.layers.9.self_attn.c_attn.q_scale\n        shape:\n          - 2048\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 196608\n        byteOffset: 17062912\n      -\n        name: model.layers.9.self_attn.o_proj.q_weight\n        shape:\n          - 1536\n          - 192\n        dtype: uint32\n        format: f32-to-bf16\n        nbytes: 1179648\n        byteOffset: 17259520\n      -\n        name: model.layers.9.self_attn.o_proj.q_scale\n        shape:\n          - 1536\n          - 48\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 147456\n        byteOffset: 18439168\n      -\n        name: model.norm.weight\n        shape:\n          - 1536\n        dtype: float16\n        format: f32-to-bf16\n        nbytes: 3072\n        byteOffset: 18586624\n    md5sum: be0c75aaa4d3430fbb48b14586e191b2",
    "source": "scripts\\models\\Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\\ndarray-cache.json"
  },
  {
    "id": "specs\\architecture-v2.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Sovereign Architecture V2: The Ghost & The Shell\r\n\r\n## 1. Overview\r\nArchitecture V2 decouples the **Interface** from the **Inference Engine**. \r\nInstead of a monolithic \"Chat UI\" inside a browser, the system splits into a background service (Ghost) and a lightweight client (Shell).\r\n\r\n## 2. Component A: The Ghost (Headless Engine)\r\nThe Ghost is a background process responsible solely for loading the LLM into VRAM and exposing an API.\r\n\r\n* **Current Implementation:** Headless Chromium (`launch-ghost.ps1`).\r\n* **Future Implementation:** C++ Native Binary (`neural-ghost.exe`) using Dawn/WebGPU.\r\n* **Responsibility:**\r\n    * Manage WebGPU Context.\r\n    * Load Weights (MLC-LLM).\r\n    * Serve `localhost:8080/v1/chat/completions`.\r\n    * **Stealth Mode:** Uses `NoCacheStaticFiles` to treat models as RAM-only data, bypassing browser storage quotas.\r\n    * **Search Engine:** Provides hybrid search (Vector + BM25 FTS) via CozoDB WASM.\r\n\r\n## 3. Component B: The Shell (Native Client)\r\nThe Shell is the user interface, residing in the user's native terminal environment (PowerShell, Bash, etc.).\r\n\r\n* **Implementation:** Python Client (`tools/sov.py`).\r\n* **Responsibility:**\r\n    * Capture user input (`stdin`).\r\n    * Send JSON payload to The Ghost.\r\n    * Render streamed response to `stdout`.\r\n    * Execute system commands (Agency).\r\n\r\n## 4. The Data Flow\r\n1.  **User:** Types `sov \"List large files\"` in PowerShell.\r\n2.  **Shell:** Sends POST request to `localhost:8080`.\r\n3.  **Bridge:** Forwards request to Headless Browser (Ghost) via Websocket/Fetch.\r\n4.  **Ghost:** Runs inference on RTX 4090 via WebGPU.\r\n5.  **Ghost:** Returns tokens -> Bridge -> Shell.\r\n6.  **Shell:** Displays output or executes `Get-ChildItem` command.\r\n\r\n## 5. Roadmap\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 4:** Neural Shell Protocol (`/v1/shell/exec` endpoint).\r\n- [x] **Phase 4.5:** Ghost Auto-Ignition (Auto-start with ?headless=true flag).\r\n- [x] **Phase 5:** Native Shell Implementation (Anchor terminal with spawn endpoint).\r\n- [ ] **Phase 6:** Migration to C++ Native Runtime (Removing Chrome entirely).",
    "source": "specs\\architecture-v2.md"
  },
  {
    "id": "specs\\doc_policy.md",
    "timestamp": 1767238219,
    "role": "file",
    "content": "# Documentation Policy (Root Coda)\n\n**Status:** Active | **Authority:** Human-Locked\n\n## Core Philosophy\n1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.\n2. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.\n3. **Brevity:** Text sections must be <500 characters.\n4. **Pain into Patterns:** Every major bug must become a Standard.\n\n## Structure\n\n### 1. The Blueprint (`specs/spec.md`)\n*   **Role:** The single architectural source of truth.\n*   **Format:** \"Visual Monolith\".\n*   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.\n\n### 2. The Tracker (`specs/tasks.md`)\n*   **Role:** Current work queue.\n*   **Format:** Checklist.\n*   **Maintenance:** Updated by Agents after every major task.\n\n### 3. The Roadmap (`specs/plan.md`)\n*   **Role:** Strategic vision.\n*   **Format:** Phased goals.\n\n### 4. Standards (`specs/standards/*.md`)\n*   **Role:** Institutional Memory (The \"Laws\" of the codebase).\n*   **Trigger:** Created after any bug that took >1 hour to fix OR any systemic improvement that affects multiple components.\n*   **Format:** \"The Triangle of Pain\"\n    1.  **What Happened:** The specific failure mode (e.g., \"Bridge crashed on start\").\n    2.  **The Cost:** The impact (e.g., \"3 hours debugging Unicode errors\").\n    3.  **The Rule:** The permanent constraint (e.g., \"Force UTF-8 encoding on Windows stdout\").\n\n### 5. Local Context (`*/README.md`)\n*   **Role:** Directory-specific context.\n*   **Limit:** 1 sentence explaining the folder's purpose.\n\n### 6. System-Wide Standards\n*   **Universal Logging:** All system components must route logs to the central log collection system (Standard 013)\n*   **Single Source of Truth:** The log viewer at `/log-viewer.html` is the single point for all system diagnostics\n\n## LLM Protocol\n1. **Read-First:** Always read `specs/spec.md` AND `specs/standards/` before coding.\n2. **Drafting:** When asked to document, produce **Mermaid diagrams** and short summaries.\n3. **Editing:** Do not modify `specs/doc_policy.md` or `specs/spec.md` structure unless explicitly instructed.\n4. **Archival:** Move stale docs to `archive/` immediately.\n5. **Enforcement:** If a solution violates a Standard, reject it immediately.\n\n---\n*Verified by Architecture Council. Edited by Humans Only.*\n",
    "source": "specs\\doc_policy.md"
  },
  {
    "id": "specs\\models.md",
    "timestamp": 1766955235,
    "role": "file",
    "content": "# Verified MLC-LLM Model URLs\n\n**Status:** Registry of verified WebLLM-compatible model URLs.\n**Last Updated:** Dec 22, 2025\n**Source:** `mlc-ai/web-llm` config and verified HTTP checks.\n\n## Technology: WASM + WebGPU Inference\n\nThis project uses **WebLLM** (by MLC-AI) to run Large Language Models directly in the browser.\n\n1.  **Compilation:** Models (Llama 3, Qwen 2.5, etc.) are compiled into **WebAssembly (WASM)** modules (`.wasm`). These modules contain the model's architecture and logic, optimized for execution in a web environment.\n2.  **Acceleration:** The WASM module uses the **WebGPU API** to access the user's local GPU. This allows for massive parallelism, enabling 7B+ parameter models to run at interactive speeds (20-100+ tokens/sec) on consumer hardware.\n3.  **Zero-Server:** No data leaves the browser. The \"Backend\" is the user's own GPU.\n\n## Official Model Registry\n\n**CRITICAL REFERENCE:** The definitive list of supported models and their WASM binaries can be found at:\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293\n- This contains the official `prebuiltAppConfig` with verified model configurations\n\n**ADDITIONAL VERIFIED LINKS:**\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293 - Contains verified MLC model configurations and working WASM URLs\n\n---\n\n## 1. Verified Models (Ready for Production)\n\nThese models have been verified to exist in the `v0_2_80` library and are compatible with the current `web-llm` version.\n\n### ðŸŒŸ 7B - 8B Class (Recommended)\nBalanced performance for reasoning and chat. Requires 6GB+ VRAM.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Qwen2.5-7B-Instruct-q4f16_1-MLC` | **Best All-Rounder.** Fast, smart, 32k context. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3.1-8B-Instruct-q4f32_1-MLC` | **Meta's Latest.** Strong reasoning. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3-8B-Instruct-q4f32_1-MLC` | Llama 3 Base. Reliable. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC` | **Reasoning Specialist.** | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` (Uses Qwen2 base) |\n\n### ðŸš€ High Performance (Small)\nFastest start times. Works on most laptops/integrated graphics.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Phi-3.5-mini-instruct-q4f16_1-MLC` | **Microsoft Phi.** 3.8B params. Very smart for size. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` | **Ultra-Lite.** 1.5B params. Blazing fast. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `SmolLM2-1.7B-Instruct-q4f16_1-MLC` | Efficient 1.7B model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n\n### ðŸ§  Code Specialist Models\nModels optimized for code generation and command translation.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` | **Qwen Coder 1.5B.** Specialized for code and command generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` | **Qwen Coder 7B.** Advanced code reasoning and command translation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n\n### ðŸŒŸ Gemma Models (Google)\nLightweight and efficient models from Google.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `gemma-2-9b-it-q4f16_1-MLC` | **Gemma 2 9B.** High performance text generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `gemma-2-9b-it-q4f32_1-MLC` | **Gemma 2 9B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `gemma-2-2b-it-q4f16_1-MLC` | **Gemma 2 2B.** Lightweight option. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `gemma-2-2b-it-q4f32_1-MLC` | **Gemma 2 2B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `gemma-3-1b-it-q4f16_1-MLC` | **Gemma 3 1B.** Latest generation small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-3-1b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n\n### ðŸ‘ï¸ Vision Models (Multimodal)\nModels that can process both text and images.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Phi-3.5-vision-instruct-q4f16_1-MLC` | **Microsoft Phi Vision.** 4.2B params, multimodal. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm` |\n| `Phi-3.5-vision-instruct-q4f32_1-MLC` | **Microsoft Phi Vision.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f32_1-ctx4k_cs2k-webgpu.wasm` |\n\n### ðŸš€ Larger Models (For High VRAM Systems)\nModels for systems with 12GB+ VRAM like RTX 4090.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Llama-2-13b-chat-hf-q4f16_1-MLC` | **Llama 2 13B.** For high-end systems. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-13b-chat-hf-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-8B-q4f16_1-MLC` | **Qwen 3 8B.** Latest generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-8B-q4f32_1-MLC` | **Qwen 3 8B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-4B-q4f16_1-MLC` | **Qwen 3 4B.** Balanced performance. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-4B-q4f32_1-MLC` | **Qwen 3 4B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n\n### ðŸš€ Smaller Models (For Low VRAM Systems)\nModels optimized for systems with limited VRAM like the XPS 13.\n\n| Model ID | Details | WASM URL |\n| :--- | :--- | :--- |\n| `Qwen2-0.5B-Instruct-q4f16_1-MLC` | **Ultra-Lightweight.** 0.5B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen2-0.5B-Instruct-q4f32_1-MLC` | **Ultra-Lightweight.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-0.6B-q4f16_1-MLC` | **Qwen 3 Tiny.** 0.6B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-0.6B-q4f32_1-MLC` | **Qwen 3 Tiny.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-1.7B-q4f16_1-MLC` | **Qwen 3 Small.** 1.7B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Qwen3-1.7B-q4f32_1-MLC` | **Qwen 3 Small.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3.2-1B-Instruct-q4f16_1-MLC` | **Llama 3.2 1B.** Efficient small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3.2-1B-Instruct-q4f32_1-MLC` | **Llama 3.2 1B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3.2-3B-Instruct-q4f16_1-MLC` | **Llama 3.2 3B.** Balanced small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\n| `Llama-3.2-3B-Instruct-q4f32_1-MLC` | **Llama 3.2 3B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\n\n---\n\n## 2. Known Issues / Missing Binaries\n\nThese models are listed in the official config but their WASM binaries are not currently hosted in the `v0_2_80` folder. **Avoid using these until binaries are available.**\n\n*   `Qwen2.5-14B-Instruct-q4f16_1-MLC` (404 Not Found) - **Known Issue**\n*   `DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC` (404 Not Found) - **Known Issue**\n*   `Qwen2-VL-7B-Instruct-q4f16_1-MLC` (404 Not Found)\n*   `gemma-3-2b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 2B available, only 1B exists**\n*   `gemma-3-12b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 12B available in current library**\n\n## 3. Verified Working Models (Recommended)\n\nBased on the official config at the GitHub link above, these models are confirmed to have working WASM binaries:\n\n### High Performance (7B-8B Range)\n*   `Qwen2.5-7B-Instruct-q4f16_1-MLC` - Verified working\n*   `Llama-3.1-8B-Instruct-q4f32_1-MLC` - Verified working\n*   `Phi-3.5-mini-instruct-q4f16_1-MLC` - Verified working\n*   `gemma-2-9b-it-q4f16_1-MLC` - Verified working\n*   `Qwen3-8B-q4f16_1-MLC` - Verified working\n\n### Lightweight Options (1.5B and below)\n*   `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` - Verified working\n*   `Qwen2-0.5B-Instruct-q4f16_1-MLC` - Verified working\n*   `Llama-3.2-1B-Instruct-q4f16_1-MLC` - Verified working\n*   `SmolLM2-1.7B-Instruct-q4f16_1-MLC` - Verified working\n\n### Vision Models\n*   `Phi-3.5-vision-instruct-q4f16_1-MLC` - Verified working\n\n### ðŸ’» Code Specialists\n*   `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` - **Best for Neural Terminal.** Fast & Smart.\n*   `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` - Verified working\n\n---\n\n## 3. URL Construction Logic\n\nIf you need to construct a URL manually:\n\n```javascript\nconst libBase = \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/\";\nconst version = \"v0_2_80\"; // Check src/config.ts for 'modelVersion'\nconst modelSpecificName = \"Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\";\n\nconst fullUrl = `${libBase}${version}/${modelSpecificName}`;\n```\n\n**Note:** The `modelSpecificName` often differs slightly from the Hugging Face repo name (e.g., `Llama-3_1` vs `Llama-3.1` or `Qwen2` base for `DeepSeek`). Always check `mlc_config.ts` mapping.",
    "source": "specs\\models.md"
  },
  {
    "id": "specs\\plan.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Anchor Core Roadmap (V2.2)\r\n\r\n**Status:** Unified Architecture Deployed\r\n**Focus:** Model Loading Reliability & Endpoint Completeness.\r\n\r\n## Phase 1: Foundation (Completed)\r\n- [x] Pivot to WebLLM/WebGPU stack.\r\n- [x] Implement CozoDB (WASM) for memory.\r\n- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).\r\n\r\n## Phase 2: Stabilization (Completed)\r\n- [x] Fix Model Loading (Quota/VRAM config).\r\n- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).\r\n- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).\r\n\r\n## Phase 2.5: Root Refactor (Completed)\r\n- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).\r\n- [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.\r\n- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.\r\n- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).\r\n\r\n## Phase 3: Expansion & Hardening (Completed)\r\n- [x] **Resource Hardening**: Implemented \"Consciousness Semaphore\" in `sovereign.js`.\r\n- [x] **Documentation Refactor**: Executed \"Visual Monolith\" strategy.\r\n- [x] **Memory Hygiene**: Implement \"Forgetting Curve\" in `root-dreamer.html`.\r\n- [x] **Active Memory Persistence**: Enable chat to write back to the Graph.\r\n- [x] **Temporal Awareness**: Ground the model in real-time.\r\n- [x] **Mobile Optimization**: Polish mobile UX for `model-server-chat.html`.\r\n\r\n## Phase 4: Ghost & Shell Architecture (Completed)\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 4:** Neural Shell Protocol (`/v1/shell/exec` endpoint).\r\n- [x] **Phase 4.5:** Ghost Auto-Ignition (Auto-start with ?headless=true flag).\r\n- [x] **Phase 5:** Native Shell Implementation (Anchor terminal with spawn endpoint).\r\n- [x] **Phase 6:** Unified Anchor Core (Single-process architecture on port 8000).\r\n- [ ] **Phase 7:** Migration to C++ Native Runtime (Removing Chrome entirely).\r\n\r\n## Phase 5: Model Loading Reliability (Completed)\r\n- [x] **URL Construction Fix**: Implemented `/models/{model}/resolve/main/{file}` redirect for MLC-LLM compatibility.\r\n- [x] **File Renaming**: Standardized component names (`anchor-mic.html`, `memory-builder.html`, `db_builder.html`).\r\n- [x] **Server Stability**: Fixed hanging issues with problematic path parameter syntax.\r\n- [x] **Endpoint Completeness**: Verified all documented endpoints are accessible.\r\n\r\n## Phase 5.5: Search Enhancement (Completed)\r\n- [x] **BM25 Implementation**: Replaced regex-based search with CozoDB FTS using BM25 algorithm.\r\n- [x] **Hybrid Search**: Combined vector search (semantic) with BM25 (lexical) for better results.\r\n- [x] **Index Creation**: Added FTS index creation in memory initialization routines.\r\n- [x] **Stemming Support**: Enabled English stemming for improved word variation matching.\r\n\r\n## Phase 6: Federation\r\n- [ ] **Device Sync**: Sync IndexedDB across devices (Peer-to-Peer).\r\n- [ ] **Local-First Cloud**: Optional encrypted backup.\r\n",
    "source": "specs\\plan.md"
  },
  {
    "id": "specs\\spec.md",
    "timestamp": 1767231133,
    "role": "file",
    "content": "# Anchor Core: The Visual Monolith (v3.0)\n\n**Status:** Text-Only Architecture | **Philosophy:** Browser-Native, Lightweight.\n\n## 1. The Anchor Architecture\nThe **Anchor Core** (`webgpu_bridge.py`) is the unified server. The **Ghost Engine** is a headless browser window acting as the GPU Worker.\n\n```mermaid\ngraph TD\n    subgraph Anchor_Core [Localhost:8000]\n        Bridge[WebGPU Bridge (Python)]\n\n        subgraph Assets\n            UI[chat.html]\n            Context[context.html]\n            Dreamer[memory-builder.html]\n        end\n\n        subgraph API_Endpoints\n            ChatAPI[\"/v1/chat/completions\"]\n            SearchAPI[\"/v1/memory/search\"]\n        end\n    end\n\n    subgraph Ghost_Engine [Headless Browser]\n        Worker[WebLLM (WASM)]\n        Memory[CozoDB (WASM)]\n        Search[Hybrid Search]\n    end\n\n    User -->|HTTP| Context\n\n    Context -->|Search| SearchAPI\n    SearchAPI -->|Query| Ghost_Engine\n    Ghost_Engine -->|Ground Truth| Context\n\n    Bridge -->|API| Ghost_Engine\n    Ghost_Engine -->|GPU| Worker\n    ChatAPI -->|MLC-LLM| Worker\n    SearchAPI -->|Memory Query| Memory\n    Dreamer -->|Background Processing| Memory\n    Resolver -->|File Redirect| Models[Local Model Files]\n    UI -->|Context Retrieval| Search_Engine\n    Search_Engine -->|Hybrid Results| UI\n```\n\n## 2. Port Map\n\n* **8000**: **The One Port.** Serves UI, API, Models, and WebSocket connections.\n\n## 3. Search Architecture\n\n* **Hybrid Retrieval**: Combines Vector search (semantic) with BM25 (lexical) for optimal results.\n* **BM25 FTS**: CozoDB Full Text Search with stemming and relevance scoring.\n* **Context Manager**: Intelligent retrieval system in `ContextManager` class.\n",
    "source": "specs\\spec.md"
  },
  {
    "id": "specs\\tasks.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Context-Engine Implementation Tasks\r\n\r\n## Current Work Queue (Unified Anchor Architecture)\r\n\r\n### Phase 5: Unified Anchor (Completed)\r\n- [x] **Consolidation**: Merge File Server and Bridge into `webgpu_bridge.py` (Port 8000).\r\n- [x] **Renaming**: `model-server-chat` -> `chat.html`, `neural-terminal` -> `terminal.html`.\r\n- [x] **Cleanup**: Archive legacy startup scripts.\r\n- [x] **Launcher**: Create `start-anchor.bat`.\r\n- [x] **Native Shell Spawning**: Implement `/v1/system/spawn_shell` endpoint.\r\n- [x] **Dashboard Integration**: Add Anchor Shell spawn button to `index.html`.\r\n- [x] **Native Client**: Create `anchor.py` for PowerShell terminal spawning.\r\n- [x] **Architecture Documentation**: Create Anchor Core specification.\r\n- [x] **Testing Suite**: Create `test_model_loading.py` and `model_test.html` for endpoint verification.\r\n- [x] **Troubleshooting Documentation**: Add model loading troubleshooting standard.\r\n\r\n### Phase 5.1: Model Loading Fixes (Completed)\r\n- [x] **URL Construction Fix**: Implement `/models/{model}/resolve/main/{file}` redirect endpoint for MLC-LLM compatibility.\r\n- [x] **File Renaming**: Rename `root-mic.html` -> `anchor-mic.html`, `root-dreamer.html` -> `memory-builder.html`, `sovereign-db-builder.html` -> `db_builder.html`.\r\n- [x] **UI Layout Fix**: Add proper margins to prevent elements from being cut off at top of browser window.\r\n- [x] **Server Stability**: Fix server hanging issues caused by problematic path parameter syntax.\r\n- [x] **Endpoint Verification**: Ensure all documented endpoints are accessible and responding properly.\r\n\r\n### Phase 5.2: Search Enhancement (Completed)\r\n- [x] **BM25 Implementation**: Replace regex-based search with CozoDB FTS using BM25 algorithm in `tools/chat.html`.\r\n- [x] **Index Creation**: Add FTS index creation in `memory-builder.html`, `db_builder.html`, and `chat.html` initialization.\r\n- [x] **Hybrid Search**: Maintain vector search alongside BM25 for semantic + lexical retrieval.\r\n- [x] **Fallback Mechanism**: Implement regex fallback if FTS index is unavailable.\r\n- [x] **Stemming Support**: Enable English stemming for better word variation matching.\r\n\r\n### Completed - Root Refactor âœ…\r\n- [x] **Kernel**: Implement `tools/modules/sovereign.js`.\r\n- [x] **Mic**: Refactor `root-mic.html` to use Kernel.\r\n- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.\r\n- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).\r\n- [x] **Docs**: Update all specs to reflect Root Architecture.\r\n\r\n### Completed - Hardware Optimization ðŸ‰\r\n- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.\r\n- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.\r\n- [x] **Crash Prevention**: Context clamping for constrained drivers.\r\n- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.\r\n- [x] **Consciousness Semaphore**: Implemented resource arbitration in `sovereign.js`.\r\n\r\n### Completed - The Subconscious âœ…\r\n- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.\r\n- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.\r\n- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).\r\n- [x] **Memory Hygiene**: Implemented \"Forgetting Curve\" in `root-dreamer.html`.\r\n\r\n### Completed - Active Cognition âœ…\r\n- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.\r\n- [x] **User Control**: Add \"Auto-Save\" toggle to System Controls.\r\n- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.\r\n- [x] **Multimodal**: Add Drag-and-Drop Image support to Console.\r\n\r\n### Phase 4.1: The Neural Shell (Completed) ðŸš§\r\n**Objective:** Decouple Intelligence (Chat) from Agency (Terminal).\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 3.5:** Ghost Auto-Ignition (Headless auto-start with ?headless=true flag).\r\n- [x] **Phase 4:** Migration to C++ Native Runtime (Removing Chrome entirely).\r\n- [x] **Bridge Repair**: Debug and stabilize `extension-bridge` connectivity.\r\n- [x] **Neural Shell Protocol**: Implement `/v1/shell/exec` in `webgpu_bridge.py`.\r\n- [x] **The \"Coder\" Model**: Add `Qwen2.5-Coder-1.5B` to Model Registry.\r\n- [x] **Terminal UI**: Create `tools/neural-terminal.html` for natural language command execution.\r\n\r\n### Phase 4.2: Agentic Expansion (Deferred)\r\n- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.\r\n- [ ] **Voice Output**: Add TTS to Console.\r\n\r\n## Phase 5: The Specialist Array\r\n- [ ] **Dataset Generation**: Samsung TRM / Distillation.\r\n- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.\r\n- [ ] **Model Merging**: FrankenMoE construction.\r\n\r\n## Backlog\r\n- [ ] **Federation Protocol**: P2P sync.\r\n- [ ] **Android App**: Wrapper for Root Coda.",
    "source": "specs\\tasks.md"
  },
  {
    "id": "specs\\test-suite-model-verification.md",
    "timestamp": 1767205246,
    "role": "file",
    "content": "# Model Loading Test Suite Documentation\n\n## Overview\nThis document describes how the updated test suite has helped debug model loading issues in the Anchor Core system, particularly the dual loading system problems that caused inconsistent behavior across different UI components.\n\n## The Dual Loading System Problem\n\n### Background\nThe Anchor Core system had two different model loading pathways:\n1. **Bridge-based loading**: Uses local file resolution via `/models/{model}/resolve/main/{file}` endpoint\n2. **Direct online loading**: Uses direct HuggingFace URLs in the browser\n\n### Issues Identified\n- Models worked in some components (like `anchor-mic.html`) but not others (like `chat.html`)\n- Confusion about which approach to use\n- Inconsistent model availability across UI components\n- Debugging time wasted trying to fix local file resolution when online loading worked\n\n## Test Suite Updates and Their Debugging Impact\n\n### 1. `verify_hf_models.py` - Hugging Face Verification\n**Purpose**: Verify models are available on Hugging Face before local testing\n\n**Debugging Impact**:\n- Identified which models actually exist on Hugging Face\n- Prevented wasted time on models that don't exist online\n- Clarified the source of truth for model availability\n- Revealed that some models listed in documentation don't have available binaries\n\n### 2. `verify_local_models.py` - Local File Verification  \n**Purpose**: Check if required model files exist locally in the models directory\n\n**Debugging Impact**:\n- Identified which models are properly downloaded and available locally\n- Revealed that the models directory was empty in many cases\n- Showed the difference between online availability and local presence\n- Helped understand why local file resolution was failing\n\n### 3. `verify_model_complete.py` - Complete Pipeline Verification\n**Purpose**: End-to-end verification including Hugging Face â†’ Local â†’ Bridge availability\n\n**Debugging Impact**:\n- Revealed the complete pathway for model loading\n- Identified exactly where in the chain models were failing\n- Showed that some models were available via bridge redirects but not local files\n- Provided clear categorization of model status (locally available, bridge available, needs download, unavailable)\n\n## How the Test Suite Resolved the Issues\n\n### 1. Separation of Concerns\n**Before**: Single test tried to check both online and local availability, causing confusion\n**After**: Separate tests for each verification type, allowing clear diagnosis\n\n### 2. Online-First Verification with Redirect Handling\n**Before**: Tests assumed local files existed without verifying online availability first\n**After**: Tests first verify models exist on Hugging Face, then check local availability\n**Key Discovery**: 307/302 redirect status codes indicate files exist on Hugging Face (not missing)\n\n### 3. Clear Status Categorization\n**Before**: Models were just \"available\" or \"not available\" with unclear reasons\n**After**: Models categorized as:\n- Available locally (ready to use immediately)\n- Available via bridge (redirects to online sources)\n- Need download (via `/v1/models/pull` endpoint)\n- Completely unavailable (not on Hugging Face)\n\n### 4. Bridge Redirect Validation\n**Before**: No verification that the bridge redirect endpoint was working properly\n**After**: Explicit testing of bridge redirect functionality to ensure it properly serves files\n\n### 5. Key Finding - Models Are Available Online\n**Critical Discovery**: All tested models are available on Hugging Face with 307 redirects, indicating they exist and can be downloaded. The issue was not with online availability but with local presence and download status.\n\n## Resolution of Dual Loading System Issues\n\n### Problem: Inconsistent Behavior\n- `anchor-mic.html` worked with online loading\n- `chat.html` failed with local file resolution\n- Confusion about which approach to use\n\n### Solution: Test Suite Insights\nThe test suite revealed:\n1. Online loading (like in `anchor-mic.html`) works when models are available on Hugging Face\n2. Local file resolution (like in `chat.html`) fails when files aren't properly downloaded\n3. Bridge redirects can serve as a fallback when local files don't exist\n4. The system needs to handle both pathways gracefully\n\n### Recommended Approach\nBased on test results and Standard 008 (Online-Only Approach):\n1. First attempt to load via bridge redirect (which can serve local or redirect to online)\n2. Fallback to direct online loading if bridge fails\n3. Use consistent configuration patterns across all UI components\n4. Test both pathways during development to ensure compatibility\n\n### Actual Solution: Download Required Models (Enhanced Bridge Redirect)\nBased on the verification results, all models are available on Hugging Face but need to be downloaded to the local models directory:\n\n```bash\n# Example: Download the Qwen2.5-Coder-1.5B model using the API\ncurl -X POST http://localhost:8000/v1/models/pull \\\n  -H \"Authorization: Bearer sovereign-secret\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_id\": \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n    \"url\": \"https://huggingface.co/mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\"\n  }'\n```\n\n**Enhanced Bridge Redirect Implementation**: The system now implements Standard 009's bridge redirect logic:\n- **Check Local First**: When requesting `/models/{file_path}`, the bridge first checks for the file locally\n- **Serve Local**: If found locally, serves with proper no-cache headers\n- **Redirect Online**: If missing locally, redirects to HuggingFace with HTTP 302\n- **Fallback Resilience**: This ensures models work even if not downloaded locally\n\nAfter downloading models, they will be available for both local loading and bridge redirects. However, the enhanced bridge now provides automatic fallback to online sources when local files are missing.\n\n## Verification Workflow\n\n### For New Models:\n1. Run `verify_hf_models.py` to ensure model exists on Hugging Face\n2. Download model if needed using `/v1/models/pull` endpoint\n3. Run `verify_local_models.py` to confirm local availability\n4. Run `verify_model_complete.py` for end-to-end verification\n5. Test in browser components\n\n### For Troubleshooting:\n1. If model fails to load in UI, run complete verification to identify where it fails\n2. Check Hugging Face availability first\n3. Verify local file presence\n4. Test bridge redirect functionality\n5. Apply appropriate fix based on verification results\n\n## Conclusion\n\nThe updated test suite has successfully resolved the dual loading system confusion by:\n- Separating online and local verification concerns\n- Providing clear status categorization\n- Enabling systematic debugging of model loading issues\n- Supporting both loading pathways with clear fallback strategies\n- Following the \"online-first\" approach documented in standards\n\nThis systematic approach prevents the \"groundhog day\" effect where the same model loading issues are debugged repeatedly without understanding the root cause.",
    "source": "specs\\test-suite-model-verification.md"
  },
  {
    "id": "specs\\architecture\\anchor-core.spec.md",
    "timestamp": 1767231057,
    "role": "file",
    "content": "# Anchor Core Architecture Specification\n\n**Status:** Active | **Component:** `tools/webgpu_bridge.py`\n\n## Overview\nThe Anchor Core unifies the Bridge, UI, and API into a single process running on port 8000, eliminating CORS issues and port conflicts.\n\n## Architecture Diagram\n\n```mermaid\ngraph TD\n    User[\"ðŸ‘¤ User\"] --> Dashboard[\"ðŸ  Dashboard (index.html)\"]\n    \n    subgraph \"ANCHOR CORE (Single Process)\"\n        Bridge[\"ðŸ”— WebGPU Bridge (8000)\"]\n        API[\"âš¡ API Endpoints\"]\n        UI[\"ðŸŒ UI Server\"]\n        WS[\"ðŸ“¡ WebSocket Layer\"]\n    end\n    \n    Dashboard --> Bridge\n    Bridge --> API\n    Bridge --> UI\n    Bridge --> WS\n    \n    subgraph \"SHELL PROTOCOL\"\n        API --> ShellExec[\"ì‰˜ /v1/shell/exec\"]\n        API --> Spawn[\"ðŸš€ /v1/system/spawn_shell\"]\n    end\n    \n    ShellExec --> Host[\"ðŸ–¥ï¸ Host System\"]\n    Spawn --> PowerShell[\"ðŸªŸ PowerShell Window\"]\n```\n\n## Components\n\n### 1. The Unified Core (`webgpu_bridge.py`)\n- **Role:** Single server for API, UI, and WebSockets\n- **Port:** 8000 (The One Port)\n- **Function:** Bridges browser WebGPU to system commands\n\n### 2. Shell Protocol (The Hands)\n- **`/v1/shell/exec`**: Execute system commands via bridge\n\n### 3. Context System\n- **Context UI**: Read-only interface for quick context retrieval and copy-paste.\n- **Memory Search**: Query the Ghost Engine's Graph (Vector + BM25) for relevant context.\n- **Memory Builder**: Background processor using Qwen 1.5B in WebGPU for memory processing.\n\n## Endpoints\n\n### `GET /sidecar`\n- **Function:** Serves the Context UI (consolidated interface).\n\n### `GET /context`\n- **Function:** Serves the Context UI (consolidated interface).\n\n### `POST /v1/memory/search`\n- **Function:** Queries the Ghost Engine's Graph (Vector + BM25).\n- **Input:** `{ \"query\": \"string\" }`\n- **Output:** `{ \"context\": \"Formatted Ground Truth...\" }`\n\n### `POST /v1/chat/completions`\n- **Function:** Proxy to browser engine\n- **Auth:** Bearer token\n\n### `POST /v1/shell/exec`\n- **Function:** Execute system commands\n- **Format:** `{ \"cmd\": \"command\" }`\n\n### `POST /v1/system/spawn_shell`\n- **Function:** Launch native PowerShell client\n- **Result:** New `anchor.py` terminal window\n\n## Search Architecture\n\n### Hybrid Retrieval System\n* **Vector Search**: Semantic search using cosine similarity (`vec_l2`)\n* **BM25 FTS**: Lexical search using CozoDB Full Text Search with stemming\n* **Context Manager**: Intelligent retrieval in `ContextManager.findRelevantMemories()`\n* **Fallback Mechanism**: Regex-based search when FTS index unavailable\n\n## Security\n- **Token Auth:** `Authorization: Bearer sovereign-secret`\n- **CORS Policy:** Open for internal use only\n- **System Access:** Restricted to authorized commands",
    "source": "specs\\architecture\\anchor-core.spec.md"
  },
  {
    "id": "specs\\architecture\\api.spec.md",
    "timestamp": 1766916224,
    "role": "file",
    "content": "# API Specification (WebGPU Bridge)\r\n\r\n**Status:** Production\r\n**Component:** `tools/webgpu_bridge.py`\r\n\r\n## Overview\r\nThe \"Bridge\" acts as a reverse-proxy, exposing the browser's WebLLM engine as an OpenAI-compatible API.\r\n\r\n## Endpoints\r\n\r\n### `POST /v1/chat/completions`\r\n- **Format:** OpenAI Standard.\r\n- **Flow:** \r\n  1. Client sends JSON to Python Bridge.\r\n  2. Bridge forwards via WebSocket to `model-server-chat.html`.\r\n  3. Browser computes response (WebGPU).\r\n  4. Result streamed back to Bridge -> Client.\r\n\r\n### `POST /v1/embeddings`\r\n- **Format:** OpenAI Standard.\r\n- **Flow:** Forwards to `webgpu-server-embed.html`.\r\n\r\n### `POST /v1/shell/exec`\r\n- **Purpose:** Neural Shell Protocol (The Hands). Executes arbitrary shell commands on host.\r\n- **Format:** JSON `{ \"cmd\": \"string\" }`.\r\n- **Response:** JSON `{ \"stdout\": \"...\", \"stderr\": \"...\", \"code\": 0 }`.\r\n- **Security:** Strict Token Auth required. 30s timeout.\r\n\r\n## WebSockets\r\n- `/ws/chat`: Connection for Chat Worker.\r\n- `/ws/embed`: Connection for Embedding Worker.\r\n\r\n## Security\r\n- **Token Auth:** `Authorization: Bearer <BRIDGE_TOKEN>` required.\r\n- **Network:** Binds to random port (obfuscation) or `8000` (default).\r\n",
    "source": "specs\\architecture\\api.spec.md"
  },
  {
    "id": "specs\\architecture\\sovereign-wasm.spec.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Sovereign WASM Specification (Root Kernel)\r\n\r\n## Architecture Overview\r\nThe **Root Coda** system runs entirely in the browser using a unified Kernel (`sovereign.js`) that manages Compute (WebLLM) and Memory (CozoDB).\r\n\r\n## 1. The Kernel (`tools/modules/sovereign.js`)\r\nThe Kernel is the standard library for all Root Tools. It enforces consistency and safety.\r\n\r\n### 1.1 Hardware Abstraction (\"Snapdragon Fix\")\r\n**Problem**: Adreno GPUs (Snapdragon X Elite) and some mobile chips crash if a WebGPU buffer >256MB is requested, or if context exceeds 4k tokens without specific driver flags.\r\n**Solution**: `getWebGPUConfig(profile)`\r\n- **Lite**: Clamps buffer to 256MB, Context to 2048.\r\n- **Mid**: Clamps buffer to 1GB, Context to 4096.\r\n- **High/Ultra**: Unlocked.\r\n\r\n### 1.2 Unified Logging\r\n**Problem**: `console.log` is invisible on mobile or when running as a PWA.\r\n**Solution**: `SovereignLogger`\r\n- Broadcasts all logs to `BroadcastChannel('sovereign-logs')`.\r\n- Consumed by `log-viewer.html` for real-time remote debugging.\r\n\r\n### 1.3 Reactive State\r\n**Problem**: Spaghetti code updating DOM elements manually.\r\n**Solution**: `createStore(initialState)`\r\n- Lightweight `Proxy`-based store.\r\n- Components subscribe to changes: `subscribe((key, val) => updateUI(key, val))`.\r\n\r\n## 2. Memory Layer (CozoDB WASM)\r\nThe Kernel provides a standardized loader: `initCozo(wasmPath)`.\r\n\r\n### Data Portability\r\n- **Lossless Export**: The Root Builder features a \"Lossless Export\" button.\r\n- **Mechanism**: Dumps full Cozo relations (including vectors) to a JSON file.\r\n- **Use Case**: Transfer full \"Brain\" state between devices or backup.\r\n\r\n### Search Enhancement (BM25 FTS)\r\n- **Hybrid Retrieval**: Combines vector search (semantic) with BM25 FTS (lexical) for optimal results.\r\n- **Index Creation**: FTS index created during initialization: `::fts create memory:content_fts`\r\n- **Stemming Support**: Uses English stemming for better word variation matching.\r\n- **Fallback Mechanism**: Maintains regex-based search when FTS index unavailable.\r\n\r\n### Schema\r\n```datalog\r\n:create memory {\r\n    id: String\r\n    =>\r\n    timestamp: Int,\r\n    role: String,\r\n    content: String,\r\n    source: String,\r\n    embedding: <F32; 384>\r\n}\r\n```\r\n\r\n## 3. Tool Bridge (Legacy Support)\r\nThe `webgpu_bridge.py` acts as a secure relay (websocket <-> http) for external tools (like VS Code extensions) to access the Browser's LLM.\r\n- **Input**: HTTP/REST (`/v1/chat/completions`)\r\n- **Output**: WebSocket (`ws://localhost:8080/ws/chat`)\r\n\r\n### 3.1 Local Model Serving (Storage Quota Bypass)\r\n**Problem**: Browsers (especially in Incognito/Guest modes) strictly limit persistent storage (e.g., <300MB), preventing the caching of large LLM weights (~2GB+).\r\n**Solution**: The Bridge acts as a local HTTP File Server.\r\n- **Endpoint**: `http://localhost:8080/models/{model_id}/...`\r\n- **Mechanism**: \r\n    1. Frontend requests model from localhost.\r\n    2. If 404, Frontend triggers `POST /v1/models/pull`.\r\n    3. Bridge downloads artifacts from Hugging Face to `./models`.\r\n    4. Frontend polls status and loads the model into RAM (bypassing IndexedDB quota).\r\n\r\n## 4. Audio Input (Root Mic)\r\n**Goal**: Pure client-side speech-to-text without sending audio to a cloud.\r\n\r\n### 4.1 Pipeline\r\n1. **Capture**: `MediaRecorder` (WebM) -> 48kHz decoding.\r\n2. **Preprocessing**:\r\n   - Downsampling to 16kHz (Whisper Native).\r\n   - **Noise Gate**: Discards audio if peak amplitude < 0.01 (Prevents transcribing silence).\r\n   - **Amplification**: Smart gain (max 5x) for quiet voices, but capped to avoid boosting noise floor.\r\n3. **Inference (WASM)**: \r\n   - Model: `Xenova/whisper-tiny.en` (Quantized).\r\n   - **Long-form Strategy**: Uses `chunk_length_s: 30` and `stride_length_s: 5` to process audio exceeding the model's native 30s window.\r\n4. **Post-Processing (Refinement)**:\r\n   - **Hallucination Filter**: Regex removal of common Whisper artifacts (e.g., \"[Music]\", \"Applause\", \"Amara.org\").\r\n   - **LLM Cleanup**: The raw transcript is passed to the local Qwen2.5 instance with a system prompt to fix grammar/punctuation without altering meaning.\r\n\r\n### 4.2 Summarization Loop\r\n- **Trigger**: User clicks \"Summarize & Clarify\" after a successful transcription.\r\n- **Process**: The cleaned transcript is sent back to the Local Kernel (Qwen2.5) with a prompt to \"summarize and clarify core meaning.\"\r\n- **Output**: The transcript is replaced by the summary, which is automatically copied to the clipboard.\r\n\r\n## 5. Parallel Compute (The Worker)\r\nTo prevent UI freezing during heavy inference, the LLM runs in a dedicated Web Worker.\r\n\r\n### 5.1 `tools/modules/llm-worker.js`\r\n- **Role**: Hosts the `MLCEngine` instance.\r\n- **Communication**: Uses `WebWorkerMLCEngineHandler` to bridge messages between the main thread and the worker.\r\n- **Benefit**: Ensures the UI remains responsive (scrolling, typing) even while the GPU is crunching tokens.\r\n\r\n## 6. Resource Management (Orchestrator)\r\n**Problem**: Multiple browser tabs (Mic, Console, Dreamer) competing for the single GPU resource led to deadlocks, timeouts, and \"Device Lost\" errors.\r\n**Solution**: A Priority-Queue based Locking System with enhanced timeout handling and emergency procedures.\r\n\r\n### 6.1 GPU Controller (`tools/modules/sovereign.js`)\r\n- **Serialized Loading**: `withModelLoadLock()` ensures only one tab loads a model at a time, preventing GPU overload during initial loading.\r\n- **Access Priority**:\r\n  - **Priority 0 (High)**: Root Mic (Voice Input - cannot wait)\r\n  - **Priority 10 (Med)**: Root Console (Chat - user waiting)\r\n  - **Priority 15 (Med)**: Default priority\r\n  - **Priority 20 (Low)**: Root Dreamer (Background tasks)\r\n- **Timeouts**: Increased broken-lock timeout from 60s to **120s** (2 minutes) to accommodate large model loading.\r\n- **Retry Logic**: Added retry mechanism with proper error handling.\r\n- **Fallback Mechanism**: Direct WebGPU access when bridge unavailable.\r\n- **Status Checking**: Added GPU status check functionality.\r\n- **Emergency Release**: If a lock is held >120s, it is forcibly broken to prevent system deadlock.\r\n\r\n### 6.2 Bridge Orchestration (`smart_gpu_bridge.py`)\r\n- **Queue Tracking**: Tracks request start times to prevent starvation.\r\n- **Enhanced Timeouts**: Increased timeout from 60s to 120s for lock acquisition.\r\n- **Request Tracking**: Added request_start_times to prevent queue starvation.\r\n- **Enhanced Status**: Detailed queue information in status endpoint.\r\n- **Endpoints**:\r\n  - `GET /v1/gpu/status`: Monitor active locks and queue depth.\r\n  - `POST /v1/gpu/lock`: Acquire lock (blocking).\r\n  - `POST /v1/gpu/unlock`: Release GPU lock.\r\n  - `POST /v1/gpu/reset`: Standard reset.\r\n  - `POST /v1/gpu/force-release-all`: Nuclear option for stuck states.\r\n  - `POST /v1/gpu/force-release`: Emergency release endpoint.\r\n  - `POST /v1/hot-reload`: Hot reload endpoint for development.\r\n\r\n### 6.3 GPU Management Utilities\r\n- **GPU Manager Script** (`scripts/gpu_manager.py`): Command-line tool to monitor and manage GPU resources.\r\n- **Test Script** (`scripts/test_gpu_fixes.py`): Comprehensive testing of GPU resource management.\r\n- **Monitoring Commands**:\r\n  - `python scripts/gpu_manager.py --status`: Check GPU status\r\n  - `python scripts/gpu_manager.py --monitor --interval 10`: Monitor continuously\r\n  - `python scripts/gpu_manager.py --force-release`: Force release GPU locks\r\n  - `python scripts/gpu_manager.py --reset`: Standard reset\r\n  - `python scripts/gpu_manager.py --hot-reload`: Trigger hot reload\r\n\r\n## 7. Development Infrastructure\r\n**Problem**: Restarting the Python bridge and refreshing 3 browser tabs for every small code change is slow.\r\n\r\n### 7.1 Hot Reload System\r\n- **Backend**: `smart_gpu_bridge.py` monitors its own source code (and `download_models.py`) for changes. It automatically reloads the Python process while preserving active WebSocket connections if possible.\r\n- **Frontend**: `gpu-hot-reloader.js` connects to the bridge via WebSocket. When the bridge signals a reload (or detects an HTML update), the browser auto-refreshes.\r\n- **Safety**: Automatically releases all GPU locks during a reload event to prevent \"Ghost Locks\".\r\n",
    "source": "specs\\architecture\\sovereign-wasm.spec.md"
  },
  {
    "id": "specs\\standards\\001-windows-console-encoding.md",
    "timestamp": 1767059367,
    "role": "file",
    "content": "# Standard 001: Windows Console Encoding\n\n## What Happened?\nThe Python Bridge (`webgpu_bridge.py`) crashed immediately upon launch on Windows 11. The error was `UnicodeEncodeError: 'charmap' codec can't encode character...`.\n\n## The Cost\n- 3 failed integration attempts.\n- \"Integration Hell\" state requiring full manual intervention.\n- Bridge stability compromised during demos.\n\n## The Rule\n1. **Explicit Encoding:** All Python scripts outputting to stdout must explicitly handle encoding.\n2. **The Fix:** Include this snippet at the top of all entry points:\n   ```python\n   import sys\n   if sys.platform == \"win32\":\n       sys.stdout.reconfigure(encoding='utf-8')\n   ```\n\n3. **Validation:** CI/CD or startup scripts must verify the bridge launches without encoding errors.",
    "source": "specs\\standards\\001-windows-console-encoding.md"
  },
  {
    "id": "specs\\standards\\002-cache-api-security-policy.md",
    "timestamp": 1767059453,
    "role": "file",
    "content": "# Standard 002: Cache API Security Policy\n\n## What Happened?\nBrowser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as \"data in RAM\" rather than \"persistent storage\", causing initialization failures.\n\n## The Cost\n- Multiple failed model loading attempts\n- Browser security errors preventing WebGPU initialization\n- \"Stealth Mode\" required to bypass strict policies\n- Significant debugging time to understand browser security model\n\n## The Rule\n1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:\n   ```python\n   class NoCacheStaticFiles(StaticFiles):\n       async def __call__(self, scope, receive, send):\n           async def send_wrapper(message):\n               if message['type'] == 'http.response.start':\n                   headers = message.get('headers', [])\n                   headers.extend([\n                       (b\"Cache-Control\", b\"no-store, no-cache, must-revalidate, proxy-revalidate\"),\n                       (b\"Pragma\", b\"no-cache\"),\n                       (b\"Expires\", b\"0\"),\n                   ])\n                   message['headers'] = headers\n               await send(message)\n           await super().__call__(scope, receive, send_wrapper)\n   ```\n\n2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.\n\n3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components.",
    "source": "specs\\standards\\002-cache-api-security-policy.md"
  },
  {
    "id": "specs\\standards\\003-webgpu-initialization-stability.md",
    "timestamp": 1767059513,
    "role": "file",
    "content": "# Standard 003: WebGPU Initialization Stability\n\n## What Happened?\nWebGPU failed to initialize properly in headless browsers, causing GPU access failures and preventing AI model execution. This occurred because browsers require visible windows for GPU access in some configurations.\n\n## The Cost\n- Failed AI model execution in headless environments\n- Hours of debugging GPU initialization issues\n- Unreliable AI processing in automated systems\n- Need for complex workarounds to achieve stable GPU access\n\n## The Rule\n1. **Minimized Window Approach:** Always use `--start-minimized` flag when launching headless browsers that require GPU access:\n   ```bash\n   start \"Ghost Engine\" /min msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222\n   ```\n\n2. **GPU Buffer Configuration:** Implement 256MB override for Adreno GPUs and other constrained hardware:\n   ```javascript\n   // In WebGPU configuration\n   const adapter = await navigator.gpu.requestAdapter({\n       powerPreference: 'high-performance',\n       forceFallbackAdapter: false\n   });\n   ```\n\n3. **Hardware Abstraction Layer:** Use clamp buffer techniques for Snapdragon/Mobile stability to prevent VRAM crashes.\n\n4. **Consciousness Semaphore:** Ensure resource arbitration between different components to prevent GPU memory conflicts.",
    "source": "specs\\standards\\003-webgpu-initialization-stability.md"
  },
  {
    "id": "specs\\standards\\004-wasm-memory-management.md",
    "timestamp": 1767059547,
    "role": "file",
    "content": "# Standard 004: WASM Memory Management\n\n## What Happened?\nWASM applications experienced \"memory access out of bounds\" errors and crashes when handling large JSON payloads or complex database operations. This was particularly problematic in `sovereign-db-builder.html` and `unified-coda.html` where JSON parameters were passed to `db.run()`.\n\n## The Cost\n- Crashes during database operations in browser-based CozoDB\n- \"Maximum call stack size exceeded\" errors with large JSON payloads\n- Unreliable memory operations in browser-based systems\n- Hours of debugging memory access violations in WASM\n\n## The Rule\n1. **JSON Stringification:** Always properly stringify JSON parameters before passing to WASM functions:\n   ```javascript\n   // Before calling db.run() or similar WASM functions\n   const jsonString = JSON.stringify(data);\n   db.run(query, jsonString);\n   ```\n\n2. **Payload Size Limits:** Implement size checks before processing large JSON payloads in browser workers:\n   ```javascript\n   if (JSON.stringify(payload).length > MAX_SAFE_SIZE) {\n       // Handle large payloads differently or chunk them\n   }\n   ```\n\n3. **Error Handling:** Add timeout protection and fallback mechanisms for hanging WASM calls:\n   ```javascript\n   try {\n       const result = await Promise.race([\n           db.run(query),\n           new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))\n       ]);\n   } catch (error) {\n       // Handle timeout or memory errors gracefully\n   }\n   ```\n\n4. **IndexedDB Fallback:** Use `CozoDb.new_from_indexed_db` instead of `new_from_path` for persistent browser storage to avoid filesystem access issues.",
    "source": "specs\\standards\\004-wasm-memory-management.md"
  },
  {
    "id": "specs\\standards\\005-model-loading-configuration.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard 005: Model Loading Configuration & Endpoint Verification\r\n\r\n## What Happened?\r\nModel loading failed due to various configuration issues including \"Cannot find model record\" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.\r\n\r\n## The Cost\r\n- Failed model initialization preventing AI functionality\r\n- Multiple 404 errors for specific model types\r\n- 503 and 405 errors during embedding and model download requests\r\n- Hours spent debugging model configuration issues\r\n- Unreliable model loading across different model types\r\n- Significant time wasted discovering that documented endpoints didn't exist in the backend\r\n- Frontend-backend integration failures due to missing API endpoints\r\n\r\n## The Rule\r\n1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:\r\n   ```python\r\n   # Example mapping for problematic models\r\n   MODEL_MAPPINGS = {\r\n       'OpenHermes': 'Mistral-v0.3',\r\n       'NeuralHermes': 'Mistral-v0.3',\r\n       # Add other mappings as needed\r\n   }\r\n   ```\r\n\r\n2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:\r\n   ```python\r\n   # In webgpu_bridge.py - ensure flexible model name handling\r\n   # Don't validate model names strictly on the bridge side\r\n   ```\r\n\r\n3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:\r\n   ```javascript\r\n   // In frontend code\r\n   const internalModelId = getModelInternalId(userModelName);\r\n   const modelUrl = getModelUrl(internalModelId);\r\n   ```\r\n\r\n4. **Verification Registry:** Maintain `specs/mlc-urls.md` as a registry for verified WASM binaries to ensure compatibility.\r\n\r\n5. **Bridge-Based URLs:** Use bridge-based model URLs (`http://localhost:8000/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.\r\n\r\n6. **Endpoint Verification Protocol:** Always verify that documented endpoints exist in the backend implementation before deploying frontend code that depends on them:\r\n   ```python\r\n   # Example: Required endpoints for model management\r\n   REQUIRED_ENDPOINTS = [\r\n       \"/v1/models/pull\",\r\n       \"/v1/models/pull/status\",\r\n       \"/v1/gpu/lock\",\r\n       \"/v1/gpu/unlock\",\r\n       \"/v1/gpu/status\",\r\n       \"/v1/gpu/reset\",\r\n       \"/v1/gpu/force-release-all\",\r\n       \"/v1/system/spawn_shell\",\r\n       \"/v1/shell/exec\"\r\n   ]\r\n   ```\r\n\r\n7. **Documentation-Implementation Synchronization:** When documenting an endpoint, immediately implement it in the backend to prevent documentation-code drift.\r\n\r\n8. **Server Startup Verification:** After adding new endpoints, always verify that the server starts properly and doesn't hang due to problematic async operations or path parameter syntax:\r\n   - Test import functionality: `python -c \"import webgpu_bridge; print('Import successful')\"`\r\n   - Verify server startup and response to requests\r\n   - Avoid problematic syntax like `:path` in route definitions that can cause server hangs\r\n   - Use simple synchronous operations when possible to avoid blocking the event loop",
    "source": "specs\\standards\\005-model-loading-configuration.md"
  },
  {
    "id": "specs\\standards\\006-model-url-construction-fix.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard 006: Model URL Construction for MLC-LLM Compatibility\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) failed to load models with the error \"TypeError: Failed to construct 'URL': Invalid URL\", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.\r\n\r\n## The Cost\r\n- 4+ hours debugging model loading failures\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Multiple failed attempts with different URL construction approaches\r\n\r\n## The Rule\r\n1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path}` endpoint to redirect MLC-LLM requests to local model files:\r\n   ```python\r\n   @app.get(\"/models/{model_name}/resolve/main/{file_path:path}\")\r\n   async def model_resolve_redirect(model_name: str, file_path: str):\r\n       import os\r\n       from fastapi.responses import FileResponse, JSONResponse\r\n\r\n       models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\r\n       actual_path = os.path.join(models_base, model_name, file_path)\r\n\r\n       if os.path.exists(actual_path) and os.path.isfile(actual_path):\r\n           return FileResponse(actual_path)\r\n       else:\r\n           return JSONResponse(status_code=404, content={\r\n               \"error\": f\"File {file_path} not found for model {model_name}\"\r\n           })\r\n   ```\r\n\r\n2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.\r\n\r\n3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.\r\n\r\n4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.\r\n\r\n5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly.",
    "source": "specs\\standards\\006-model-url-construction-fix.md"
  },
  {
    "id": "specs\\standards\\007-model-loading-transition-standard.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard 007: Model Loading Transition - Online-Only Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.\r\n\r\nThe old implementation in `chat.html` was trying to:\r\n1. Check for local model files using the `/models/{model}/resolve/main/` pattern\r\n2. Download models through the bridge if not found locally\r\n3. Use a complex configuration with multiple model entries and local file resolution\r\n\r\nThis approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.\r\n\r\n## The Cost\r\n- Hours spent debugging model loading failures in `chat.html`\r\n- Confusion between working and failing components (anchor-mic.html vs chat.html)\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:\r\n   ```javascript\r\n   // Archive the old function with a descriptive name\r\n   async function loadModel_archived() {\r\n       // Original complex implementation\r\n   }\r\n   \r\n   // Implement the working online-only approach\r\n   async function loadModel() {\r\n       // Simplified online-only implementation\r\n   }\r\n   ```\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Progressive Enhancement**: Start with a working online-only solution, then add local model loading capabilities in a separate iteration after the basic functionality is stable.\r\n\r\n6. **Model Loading Verification**: Always test model loading with the same models across different UI components to ensure consistency.\r\n\r\n## Implementation Pattern\r\n\r\n### Working Online-Only Format (Recommended):\r\n```javascript\r\n// Based on the working anchor-mic.html implementation\r\nasync function loadModel() {\r\n    // ... setup code ...\r\n    \r\n    const appConfig = {\r\n        model_list: [{\r\n            model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n            model_id: selectedModelId,\r\n            model_lib: modelLib,  // WASM library URL from mapper\r\n            vram_required_MB: 2000,\r\n            low_resource_required: true,\r\n            buffer_size_required_bytes: gpuConfig.maxBufferSize,\r\n            overrides: {\r\n                context_window_size: gpuConfig.isConstrained ? 2048 : 4096\r\n            }\r\n        }],\r\n        useIndexedDBCache: false, // Disable caching to prevent issues\r\n    };\r\n\r\n    engine = await CreateWebWorkerMLCEngine(\r\n        new Worker('./modules/llm-worker.js', { type: 'module' }),\r\n        selectedModelId,\r\n        {\r\n            initProgressCallback: (report) => {\r\n                // Progress reporting\r\n            },\r\n            appConfig: appConfig,\r\n            logLevel: \"INFO\",\r\n            useIndexedDBCache: false, // Force disable cache\r\n        }\r\n    );\r\n}\r\n```\r\n\r\n### Complex Local Resolution (Problematic - Avoid):\r\n```javascript\r\n// DO NOT USE - This causes hangs after GPU configuration\r\n// Complex local file checking and bridge download logic\r\nconst localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;\r\nconst check = await fetch(localModelUrl, { method: 'HEAD' });\r\n// ... complex download and resolution logic that causes hangs\r\n```\r\n\r\n## Transition Protocol\r\n\r\nWhen transitioning model loading implementations:\r\n\r\n1. **Identify Working Component**: Find a UI component that successfully loads models (e.g., `anchor-mic.html`)\r\n2. **Analyze Working Pattern**: Study the model loading approach in the working component\r\n3. **Archive Complex Logic**: Preserve the old implementation for future reference\r\n4. **Implement Simple Approach**: Adopt the working pattern from the successful component\r\n5. **Test Thoroughly**: Verify the new implementation works with multiple models\r\n6. **Document Changes**: Record the transition in standards documentation\r\n\r\n## Future Considerations\r\n\r\nThe archived local model loading approach should be revisited after further prototyping and debugging. The online-only approach provides immediate functionality while the more complex local approach can be refined separately without blocking development progress.",
    "source": "specs\\standards\\007-model-loading-transition-standard.md"
  },
  {
    "id": "specs\\standards\\008-model-loading-online-only-approach.md",
    "timestamp": 1767197594,
    "role": "file",
    "content": "# Standard 008: Model Loading - Online-Only Approach for Browser Implementation\n\n## What Happened?\nThe Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.\n\nThe issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.\n\n## The Cost\n- All models showing as unavailable in API tests\n- Confusion between working and failing components\n- Inconsistent model loading across different UI components\n- User frustration with limited model availability\n- Time wasted on attempting to fix complex local model resolution logic\n- Delayed development due to complex debugging of the local file + bridge download approach\n\n## The Rule\n1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:\n   ```javascript\n   // Use direct HuggingFace URLs like anchor-mic.html\n   const appConfig = {\n       model_list: [{\n           model: window.location.origin + \"/models/\" + selectedModelId, // This will redirect to online source\n           model_id: selectedModelId,\n           model_lib: modelLib,  // WASM library URL\n           // ... other config\n       }],\n       useIndexedDBCache: false, // Disable caching to prevent issues\n   };\n   ```\n\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\n\n3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.\n\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\n\n5. **Bridge Redirect Endpoint**: Ensure the `/models/{model}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist.",
    "source": "specs\\standards\\008-model-loading-online-only-approach.md"
  },
  {
    "id": "specs\\standards\\009-model-loading-configuration-bridge-vs-direct.md",
    "timestamp": 1767197626,
    "role": "file",
    "content": "# Standard 009: Model Loading Configuration - Bridge vs Direct Online\n\n## What Happened?\nThe Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:\n\n1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources\n2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser\n\nThe inconsistency occurred because:\n- Some components (like `anchor-mic.html`) work with direct online URLs\n- Other components (like `chat.html`) were configured for local file resolution\n- The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist\n\n## The Cost\n- Confusion about which model loading approach to use\n- Inconsistent behavior across different UI components\n- Models working in some components but not others\n- Debugging time spent on understanding different loading mechanisms\n- Users experiencing different model availability depending on which UI they use\n\n## The Rule\n1. **Consistent Model Configuration**: All UI components should use the same model loading approach:\n   ```javascript\n   // Recommended configuration pattern\n   const modelConfig = {\n       model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect\n       model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID\n       model_lib: modelLib,                                  // WASM library URL\n   };\n   ```\n\n2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:\n   - First check for local files in the models directory\n   - If local file doesn't exist, redirect to the corresponding HuggingFace URL:\n     `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`\n\n3. **Fallback Handling**: Implement proper fallback when local files are not available:\n   ```javascript\n   // In the UI, handle both local and online availability\n   async function loadModel(modelId) {\n       try {\n           // Try bridge-based loading first\n           await initializeEngine(bridgeConfig(modelId));\n       } catch (error) {\n           // Fallback to direct online loading if bridge fails\n           await initializeEngine(onlineConfig(modelId));\n       }\n   }\n   ```\n\n4. **Testing Protocol**: Test model loading through both pathways:\n   - Verify local file resolution works when files exist\n   - Verify online fallback works when local files don't exist\n   - Test both the redirect endpoint and direct access patterns\n\n5. **Documentation Consistency**: All UI components should follow the same documented approach to avoid confusion.",
    "source": "specs\\standards\\009-model-loading-configuration-bridge-vs-direct.md"
  },
  {
    "id": "specs\\standards\\010-bridge-redirect-implementation.md",
    "timestamp": 1767205268,
    "role": "file",
    "content": "# Standard 010: Bridge Redirect Implementation - Smart Model Loading\n\n## What Happened?\nThe Anchor Core system had inconsistent model loading behavior where some models worked and others didn't. The issue was that the bridge was only serving local files and returning 404 errors when files were missing, instead of providing fallback to online sources.\n\nThe browser components (like chat.html) were requesting model files from the local bridge (localhost:8000), but if the model hadn't been downloaded locally, they would fail with 404 errors instead of falling back to online loading.\n\n## The Cost\n- Models failing to load when not downloaded locally\n- Inconsistent behavior across different model requests\n- User frustration when models appear unavailable\n- Complex debugging to understand the local vs online loading pathway\n\n## The Rule\n1. **Smart Redirect Pattern**: Implement the following pattern for model file requests:\n   - **Check Local First**: When receiving a request for `/models/{file_path}`, first check if the file exists in the local models directory\n   - **Serve Local**: If found locally, serve the file with proper no-cache headers to prevent browser caching issues\n   - **Redirect Online**: If not found locally, redirect to the corresponding HuggingFace URL with HTTP 302 status\n\n2. **NoCache Headers**: When serving local files, ensure proper cache-control headers are applied:\n   ```python\n   # Headers to apply to local file responses\n   Cache-Control: no-store, no-cache, must-revalidate\n   Pragma: no-cache\n   Expires: 0\n   ```\n\n3. **Request Method Handling**: Handle both GET and HEAD requests appropriately:\n   - **GET**: Return the file content or redirect\n   - **HEAD**: Return headers with file size if local file exists, or redirect if missing\n\n4. **Logging**: Log when files are not found locally and redirected to HuggingFace for debugging purposes\n\n5. **Resilience**: The system must never fail to provide model files when they exist online, regardless of local download status\n\n## Implementation Example\n```python\n@app.get(\"/models/{file_path:path}\")\nasync def models_redirect(file_path: str):\n    \"\"\"Smart redirect: Check for local file first, redirect to HuggingFace if missing\"\"\"\n    import os\n    from fastapi.responses import FileResponse, RedirectResponse\n    \n    # Construct path to local model file\n    models_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\n    local_path = os.path.join(models_dir, file_path)\n    \n    # Check if the file exists locally\n    if os.path.exists(local_path) and os.path.isfile(local_path):\n        # Serve the local file with no-cache headers\n        return NoCacheFileResponse(local_path)\n    else:\n        # File doesn't exist locally, redirect to HuggingFace\n        print(f\"âš ï¸ File not found locally, redirecting to HuggingFace: {file_path}\")\n        hf_url = f\"https://huggingface.co/{file_path}\"\n        return RedirectResponse(url=hf_url, status_code=302)\n```\n\nThis ensures the system provides maximum resilience by falling back to online sources when local files are missing.",
    "source": "specs\\standards\\010-bridge-redirect-implementation.md"
  },
  {
    "id": "specs\\standards\\011-comprehensive-testing-verification.md",
    "timestamp": 1767209996,
    "role": "file",
    "content": "# Standard 011: Comprehensive Testing and Verification Protocol\n\n## What Happened?\nThe Anchor Core system required a comprehensive testing approach to prevent issues like missing endpoints, function syntax errors, model loading failures, and data pipeline problems. Previously, these issues were discovered reactively during development or deployment, causing delays and debugging overhead.\n\n## The Cost\n- Hours spent debugging missing endpoints after deployment\n- Time wasted on syntax errors in critical files\n- Model loading failures discovered during user testing\n- Data pipeline issues found late in the development cycle\n- Lack of systematic verification leading to inconsistent quality\n\n## The Rule\n1. **Dedicated Test Directory**: All test files must be organized in a dedicated `tests/` directory in the project root\n    ```bash\n    tests/\n    â”œâ”€â”€ comprehensive_test_suite.py\n    â”œâ”€â”€ endpoint_syntax_verification.py\n    â”œâ”€â”€ test_model_loading.py\n    â”œâ”€â”€ test_model_availability.py\n    â”œâ”€â”€ test_gpu_fixes.py\n    â”œâ”€â”€ test_orchestrator.py\n    â”œâ”€â”€ model_test.html\n    â””â”€â”€ README.md\n    ```\n\n2. **Comprehensive Test Coverage**: Tests must cover:\n   - Model loading functionality\n   - Data pipeline verification\n   - Endpoint accessibility\n   - Missing endpoint detection\n   - Function syntax error detection\n   - System health verification\n\n3. **Endpoint Verification Protocol**: All critical endpoints must be tested for accessibility:\n   ```python\n   # Example endpoint test pattern\n   critical_endpoints = [\n       (\"/health\", \"GET\", 200),\n       (\"/v1/chat/completions\", \"POST\", 400),  # Expected 400 due to missing body\n       (\"/v1/gpu/status\", \"GET\", 200),\n       # ... add all critical endpoints\n   ]\n   ```\n\n4. **Syntax Verification**: Critical Python files must be checked for syntax errors:\n   ```python\n   # Use AST parsing to verify syntax\n   import ast\n   with open(file_path, 'r') as f:\n       source_code = f.read()\n   ast.parse(source_code)  # Will raise SyntaxError if invalid\n   ```\n\n5. **Test Documentation**: All test files must be documented in `tests/README.md` with:\n   - Purpose of each test file\n   - How to run the tests\n   - Test coverage details\n   - Expected outputs\n\n6. **Pre-Deployment Verification**: Before any deployment, run the comprehensive test suite:\n   ```bash\n   python tests/comprehensive_test_suite.py\n   ```\n\n7. **Continuous Verification**: Implement automated testing in CI/CD pipelines to catch issues early\n\n## Implementation Example\n\n### Running the Comprehensive Test Suite:\n```bash\n# Basic test run\npython tests/comprehensive_test_suite.py\n\n# With custom parameters\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output report.json\n\n# Endpoint and syntax verification only\npython tests/endpoint_syntax_verification.py\n```\n\n### Expected Test Coverage:\n- Model loading: 100% coverage of model files and configurations\n- API endpoints: 100% verification of all documented endpoints\n- Syntax: 100% verification of critical Python files\n- Data pipeline: End-to-end verification of data flow\n- System health: Verification of all core services\n\n## Verification Checklist\n- [ ] All test files organized in `tests/` directory\n- [ ] Comprehensive test suite covers all major components\n- [ ] Endpoint verification tests all critical endpoints\n- [ ] Syntax verification tests all critical Python files\n- [ ] Tests are documented in `tests/README.md`\n- [ ] Test suite runs without errors\n- [ ] Test reports are generated and reviewed",
    "source": "specs\\standards\\011-comprehensive-testing-verification.md"
  },
  {
    "id": "specs\\standards\\012-context-utility-manifest.md",
    "timestamp": 1767226368,
    "role": "file",
    "content": "# Standard 012: Context Utility Manifest\n\n**Authority:** Active | **Philosophy:** Invisible Infrastructure\n\n## The Principle\nThe Anchor Core is not a \"Chat App\". It is a **Context Utility** (like electricity or WiFi).\n1.  **Headless First**: The system must provide value without a visible UI window.\n2.  **Passive Observation**: Data ingestion should happen automatically (Daemon Eyes) rather than requiring manual user input.\n3.  **Universal Availability**: Context must be accessible via standard HTTP endpoints (`/v1/memory/search`) to any client (Terminal, VS Code, Browser).\n\n## The Rules\n1.  **No UI Blocking**: Long-running tasks (like VLM analysis) MUST run in background threads/processes.\n2.  **Zero-Touch Ingestion**: Screen/Audio capture must require zero clicks after initial activation.\n3.  **Ground Truth**: All ingested context is immutable \"Ground Truth\" until proven otherwise.",
    "source": "specs\\standards\\012-context-utility-manifest.md"
  },
  {
    "id": "specs\\standards\\013-universal-log-collection.md",
    "timestamp": 1767241502,
    "role": "file",
    "content": "# Standard 013: Universal Log Collection System\n\n## What Happened?\nThe system had fragmented logging across multiple sources (browser console, Python stdout, WebSocket events) making debugging difficult. Users had to check multiple places to understand system behavior.\n\n## The Cost\n- 4+ hours spent debugging connection issues by checking browser console, Python terminal, and WebSocket messages separately\n- Inefficient troubleshooting workflow requiring multiple monitoring tools\n- Missed error correlations between different system components\n- Poor visibility into system-wide operation\n\n## The Rule\n1. **Universal Collection**: All system logs (Python, JavaScript, WebSocket, browser, model loading, GPU status) must be aggregated in a single location: `tools/log-viewer.html`\n2. **Broadcast Channel Protocol**: All components must use the `sovereign-logs` or `coda_logs` BroadcastChannel to send messages to the log viewer:\n   ```javascript\n   // From browser components\n   const logChannel = new BroadcastChannel('sovereign-logs');\n   logChannel.postMessage({\n       source: 'component-name',\n       type: 'info|success|error|warning|debug',\n       time: new Date().toISOString(),\n       msg: 'message content'\n   });\n   ```\n\n3. **Python Integration**: Python scripts must send log data via API endpoints that feed into the log viewer\n4. **Centralized Access**: The single point of truth for all system diagnostics is `http://localhost:8000/log-viewer.html`\n5. **File-based Logging**: Each component must also write to its own log file in the `logs/` directory for persistent storage\n6. **Log Truncation**: Individual log files must be truncated to last 1000 lines to prevent disk space issues\n7. **Source Tagging**: All log entries must be clearly tagged with their source for easy identification",
    "source": "specs\\standards\\013-universal-log-collection.md"
  },
  {
    "id": "specs\\standards\\anchor-spawn-protocol.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard: Anchor Shell Spawn Protocol\r\n\r\n**Authority:** Active Standard | **Trigger:** Native Shell Integration\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nThe system needed a way to spawn native PowerShell terminals from the web dashboard that connect to the bridge API. Initial attempts failed due to missing endpoints and incorrect process spawning. Additionally, the Ghost engine (headless browser) had connection issues with the unified Anchor Core architecture due to hardcoded port references and incorrect model URL configurations.\r\n\r\n### 2. The Cost\r\n- 2 hours debugging process spawning on Windows\r\n- 1 hour fixing endpoint registration issues\r\n- 30 minutes updating UI to match new architecture\r\n- 2 hours debugging WebSocket connection failures (hardcoded port 8080 vs 8000)\r\n- 1.5 hours fixing model loading issues (multiple iterations with relative vs absolute paths)\r\n- 1 hour understanding MLC engine model configuration requirements\r\n\r\n### 3. The Rule\r\n- **Endpoint Path:** Always use `/v1/system/spawn_shell` for native shell spawning\r\n- **Authentication:** Must use `Bearer sovereign-secret` token\r\n- **Process Spawning:** Use `subprocess.Popen()` with `shell=True` for Windows compatibility\r\n- **UI Integration:** Dashboard must provide clear feedback on spawn success/failure\r\n- **Port Consistency:** Use `http://localhost:8000` for unified architecture\r\n- **WebSocket Connections:** Use `window.location.host` to dynamically connect to current port\r\n- **API Calls:** Use relative paths (e.g., `/v1/gpu/status`) for same-server requests\r\n- **Model Configuration:** Use local paths (e.g., `/models/model-name`) in model field, Hugging Face IDs in model_id field\r\n- **Model Loading:** Local model checks should use relative paths like `/models/{id}/ndarray-cache.json`",
    "source": "specs\\standards\\anchor-spawn-protocol.md"
  },
  {
    "id": "specs\\standards\\model-loading-troubleshooting.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard: Model Loading & Bridge Connectivity Testing Protocol\r\n\r\n**Authority:** Active Standard | **Trigger:** Recurring Bridge/Model Loading Issues\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nThe Ghost engine (headless browser) repeatedly had connection and model loading failures with the unified Anchor Core architecture. Issues included:\r\n- WebSocket connection failures (hardcoded port 8080 vs 8000)\r\n- Model loading failures due to incorrect URL construction\r\n- API endpoint accessibility problems\r\n- Multiple iterations required to fix configuration\r\n\r\n### 2. The Cost\r\n- 6+ hours across multiple debugging sessions\r\n- Multiple failed attempts with different URL configuration approaches\r\n- Confusion between Hugging Face URLs and local model paths\r\n- Time lost to iterative fixing process\r\n\r\n### 3. The Rule\r\n- **Always test first:** Before debugging, run `test_model_loading.py` to identify which components fail\r\n- **Use relative paths:** For same-server API calls (e.g., `/v1/gpu/status`)\r\n- **Use dynamic host:** For WebSocket connections (`window.location.host`)\r\n- **Model configuration:** Use local paths in `model` field, Hugging Face IDs in `model_id` field\r\n- **Verify endpoints:** Use `model_test.html` to test endpoint accessibility interactively\r\n- **Document changes:** Update `specs/standards/` when fixing similar issues\r\n\r\n## Testing Protocol\r\n\r\n### Pre-Deployment Checklist\r\n1. Run `python tools/test_model_loading.py`\r\n2. Verify WebSocket connection with `model_test.html`\r\n3. Test model accessibility for all configured models\r\n4. Confirm API endpoints respond correctly\r\n\r\n### Troubleshooting Flow\r\n```mermaid\r\nflowchart TD\r\n    A[Connection Issue] --> B{Test Suite Result?}\r\n    B -->|Pass| C[System Configuration Issue]\r\n    B -->|Fail| D{Which Tests Fail?}\r\n    D -->|API Endpoints| E[Check Port Configuration]\r\n    D -->|Model Paths| F[Check Model URL Format]\r\n    D -->|WebSocket| G[Check Dynamic Host Usage]\r\n    E --> H[Fix: Use Relative Paths]\r\n    F --> I[Fix: Local Paths in Model Config]\r\n    G --> J[Fix: Dynamic Host for WebSocket]\r\n    H --> K[Re-run Tests]\r\n    I --> K\r\n    J --> K\r\n    K --> L{All Pass?}\r\n    L -->|Yes| M[Issue Resolved]\r\n    L -->|No| A\r\n```\r\n\r\n### Test Files\r\n- `tools/test_model_loading.py` - Command-line model loading test suite\r\n- `tools/model_test.html` - Interactive browser-based testing interface\r\n- Both verify endpoint accessibility and model path configuration",
    "source": "specs\\standards\\model-loading-troubleshooting.md"
  },
  {
    "id": "specs\\standards\\model-url-construction-fix.md",
    "timestamp": 1767195827,
    "role": "file",
    "content": "# Standard: Model URL Construction and Availability Testing for MLC-LLM Integration\r\n\r\n**Authority:** Active Standard | **Trigger:** Model Loading URL Construction Issues\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nThe Anchor Console (`chat.html`) failed to load models with the error \"TypeError: Failed to construct 'URL': Invalid URL\", while the Root Mic (`anchor-mic.html`) loaded models successfully. The issue was in the dynamic model configuration where the system was constructing HuggingFace URLs instead of using the local path format expected by the MLC-LLM library. Additionally, model availability testing was needed to verify which models can be loaded before attempting to initialize the engine.\r\n\r\n### 2. The Cost\r\n- 3+ hours debugging model loading failures\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Time wasted on attempting to load models that don't exist locally\r\n\r\n### 3. The Rule\r\n- **Full URL Format**: When configuring models for MLC-LLM, use full URL format with `window.location.origin`: `${window.location.origin}/models/{model-name}` instead of relative paths\r\n- **Model ID Extraction**: Extract just the model name part from the full model ID using `selectedModelId.split('/').pop()`\r\n- **Path Sanitization**: Sanitize model paths to remove special characters that could cause URL parsing errors: `path.replace(/[^a-zA-Z0-9._-]/g, '_')`\r\n- **Consistent Format**: Ensure all dynamically added models follow the same URL pattern as pre-configured models in appConfig\r\n- **Model Availability Testing**: Always verify model files exist locally before attempting engine initialization\r\n- **URL Validation**: Always validate constructed URLs before passing to MLC-LLM engine\r\n\r\n## Implementation Pattern\r\n\r\n### Correct Format:\r\n```javascript\r\n// Predefined models in appConfig\r\nconst appConfig = {\r\n    model_list: [\r\n        {\r\n            model: window.location.origin + \"/models/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",  // Full URL format\r\n            model_id: \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n            // ... other config\r\n        }\r\n    ]\r\n};\r\n\r\n// Dynamic model addition\r\nconst strippedModelId = selectedModelId.split('/').pop();\r\nconst safeModelPath = strippedModelId.replace(/[^a-zA-Z0-9._-]/g, '_'); // Sanitize path\r\nappConfig.model_list.push({\r\n    model: window.location.origin + `/models/${safeModelPath}`,  // Full URL format\r\n    model_id: selectedModelId,                                   // Full HuggingFace ID\r\n    model_lib: modelLib,                                        // WASM library URL\r\n    // ... other config\r\n});\r\n```\r\n\r\n### Model Availability Testing:\r\n```javascript\r\n// Test if model files exist before engine initialization\r\nasync function testModelAvailability(modelName) {\r\n    const configFiles = [\r\n        `/models/${modelName}/ndarray-cache.json`,\r\n        `/models/${modelName}/tokenizer.json`,\r\n        `/models/${modelName}/mlc-chat-config.json`\r\n    ];\r\n\r\n    for (const configFile of configFiles) {\r\n        try {\r\n            const response = await fetch(configFile, { method: 'HEAD' });\r\n            if (response.status !== 200) {\r\n                return { available: false, missingFile: configFile };\r\n            }\r\n        } catch (error) {\r\n            return { available: false, error: error.message };\r\n        }\r\n    }\r\n    return { available: true };\r\n}\r\n```\r\n\r\n### Incorrect Format:\r\n```javascript\r\n// DO NOT USE - This causes \"TypeError: Failed to construct 'URL'\"\r\nappConfig.model_list.push({\r\n    model: `/models/${modelName}`,  // Relative path only\r\n    // ...\r\n});\r\n\r\n// DO NOT USE - Without availability check\r\nawait CreateWebWorkerMLCEngine(worker, selectedModelId, { appConfig });\r\n```\r\n\r\n## File Renaming Standards\r\n\r\nAs part of this fix, the following files were renamed for consistency:\r\n- `root-mic.html` â†’ `anchor-mic.html` (Audio input interface)\r\n- `root-dreamer.html` â†’ `memory-builder.html` (Background processing)\r\n- `sovereign-db-builder.html` â†’ `db_builder.html` (Database management)\r\n\r\nUpdate all references in HTML, documentation, and configuration files when renaming components.\r\n\r\n## Model Availability Testing Protocol\r\n\r\nUse the provided testing tools to verify model availability before attempting to load:\r\n\r\n1. **Browser-based testing**: Use `tools/model_test.html` to test model accessibility via web interface\r\n2. **Command-line testing**: Use `tools/test_model_loading.py` to test model availability programmatically\r\n3. **Pre-loading verification**: Always check model availability before engine initialization\r\n4. **Model download**: If model is not available locally, trigger download via `/v1/models/pull` endpoint\r\n\r\nThe testing tools verify:\r\n- API endpoint accessibility\r\n- Model path existence\r\n- Required configuration files (ndarray-cache.json, tokenizer.json, mlc-chat-config.json)\r\n- Overall system health",
    "source": "specs\\standards\\model-url-construction-fix.md"
  },
  {
    "id": "templates\\waveai_ece.json",
    "timestamp": 1766310900,
    "role": "file",
    "content": "ece-local:\n  display:name: Sovereign Console (ECE)\n  display:order: 1\n  display:icon: microchip\n  display:description: Local WebGPU Model via ECE Bridge\n  ai:provider: custom\n  ai:apitype: openai-chat\n  ai:model: webgpu-chat\n  ai:thinkinglevel: medium\n  ai:endpoint: http://127.0.0.1:8080/v1/chat/completions\n  ai:apitoken: not-needed\n  ai:capabilities:\n    - tools",
    "source": "templates\\waveai_ece.json"
  },
  {
    "id": "tests\\comprehensive_test_suite.py",
    "timestamp": 1767209786,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive Test Suite for Anchor Core\n\nThis script provides a complete test suite covering:\n1. Model loading functionality\n2. Data pipeline verification\n3. Endpoint accessibility\n4. Missing endpoint detection\n5. Function syntax error detection\n\"\"\"\n\nimport requests\nimport sys\nimport time\nimport json\nfrom urllib.parse import urljoin\nfrom pathlib import Path\nimport subprocess\nimport importlib.util\nfrom typing import Dict, List, Tuple, Any\n\n\nclass ComprehensiveTestSuite:\n    def __init__(self, base_url: str = \"http://localhost:8000\", token: str = \"sovereign-secret\"):\n        self.base_url = base_url\n        self.token = token\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n        self.results = {\n            \"model_loading\": {},\n            \"data_pipeline\": {},\n            \"endpoint_verification\": {},\n            \"syntax_check\": {},\n            \"overall\": {\"passed\": 0, \"failed\": 0, \"total\": 0}\n        }\n\n    def run_all_tests(self) -> bool:\n        \"\"\"Run all test categories\"\"\"\n        print(\"ðŸš€ Running Comprehensive Test Suite for Anchor Core...\")\n        print(f\"Testing against: {self.base_url}\")\n        print(\"=\" * 80)\n\n        # Run all test categories\n        model_loading_ok = self.test_model_loading()\n        data_pipeline_ok = self.test_data_pipeline()\n        endpoint_ok = self.test_endpoint_verification()\n        syntax_ok = self.test_syntax_verification()\n\n        # Summary\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ðŸ“Š COMPREHENSIVE TEST SUITE SUMMARY\")\n        print(\"=\" * 80)\n        \n        print(f\"Model Loading Tests: {'âœ… PASS' if model_loading_ok else 'âŒ FAIL'}\")\n        print(f\"Data Pipeline Tests: {'âœ… PASS' if data_pipeline_ok else 'âŒ FAIL'}\")\n        print(f\"Endpoint Verification: {'âœ… PASS' if endpoint_ok else 'âŒ FAIL'}\")\n        print(f\"Syntax Verification: {'âœ… PASS' if syntax_ok else 'âŒ FAIL'}\")\n\n        overall_success = all([model_loading_ok, data_pipeline_ok, endpoint_ok, syntax_ok])\n        print(f\"\\nðŸŽ¯ Overall Result: {'âœ… ALL TESTS PASSED' if overall_success else 'âŒ SOME TESTS FAILED'}\")\n        \n        print(f\"\\nðŸ“ˆ Test Statistics:\")\n        print(f\"  Total Tests: {self.results['overall']['total']}\")\n        print(f\"  Passed: {self.results['overall']['passed']}\")\n        print(f\"  Failed: {self.results['overall']['failed']}\")\n\n        return overall_success\n\n    def test_model_loading(self) -> bool:\n        \"\"\"Test model loading functionality\"\"\"\n        print(\"\\nðŸ” Testing Model Loading...\")\n        print(\"-\" * 40)\n\n        # Test model availability\n        models_to_test = [\n            \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n            \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\n            \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n        ]\n\n        all_models_ok = True\n        for model_name in models_to_test:\n            print(f\"  Testing model: {model_name}\")\n            \n            # Test model config files accessibility\n            config_files = [\n                f\"models/{model_name}/resolve/main/ndarray-cache.json\",\n                f\"models/{model_name}/resolve/main/mlc-chat-config.json\",\n                f\"models/{model_name}/resolve/main/tokenizer.json\",\n                f\"models/{model_name}/resolve/main/tokenizer_config.json\"\n            ]\n\n            model_ok = True\n            for config_file in config_files:\n                try:\n                    url = urljoin(self.base_url, config_file)\n                    response = requests.head(url, timeout=10)\n                    status_ok = response.status_code in [200, 404]  # 404 is expected if file doesn't exist locally\n                    print(f\"    {'âœ…' if status_ok else 'âŒ'} {config_file} -> {response.status_code}\")\n                    \n                    test_key = f\"model_{model_name}_{config_file.replace('/', '_')}\"\n                    self.results[\"model_loading\"][test_key] = {\n                        \"url\": url,\n                        \"status\": response.status_code,\n                        \"success\": status_ok\n                    }\n                    \n                    if not status_ok:\n                        model_ok = False\n                        all_models_ok = False\n                        \n                except Exception as e:\n                    print(f\"    âŒ {config_file} -> ERROR: {e}\")\n                    test_key = f\"model_{model_name}_{config_file.replace('/', '_')}\"\n                    self.results[\"model_loading\"][test_key] = {\n                        \"url\": urljoin(self.base_url, config_file),\n                        \"status\": \"ERROR\",\n                        \"success\": False,\n                        \"error\": str(e)\n                    }\n                    model_ok = False\n                    all_models_ok = False\n\n            if model_ok:\n                print(f\"  âœ… Model {model_name} is accessible\")\n            else:\n                print(f\"  âŒ Model {model_name} has issues\")\n\n        # Update overall counters\n        total_model_tests = len(self.results[\"model_loading\"])\n        passed_model_tests = sum(1 for r in self.results[\"model_loading\"].values() if r[\"success\"])\n        self.results[\"overall\"][\"total\"] += total_model_tests\n        self.results[\"overall\"][\"passed\"] += passed_model_tests\n        self.results[\"overall\"][\"failed\"] += total_model_tests - passed_model_tests\n\n        return all_models_ok\n\n    def test_data_pipeline(self) -> bool:\n        \"\"\"Test data pipeline functionality\"\"\"\n        print(\"\\n PIPELINE Testing Data Pipeline...\")\n        print(\"-\" * 40)\n\n        pipeline_tests = [\n            (\"/health\", \"GET\", 200, \"Health Check\"),\n            (\"/v1/gpu/status\", \"GET\", 200, \"GPU Status\"),\n            (\"/v1/system/spawn_shell\", \"POST\", 400, \"Spawn Shell (expects 400 due to missing body)\"),\n        ]\n\n        all_pipeline_ok = True\n        for endpoint, method, expected_status, description in pipeline_tests:\n            print(f\"  Testing {description}: {method} {endpoint}\")\n            try:\n                url = urljoin(self.base_url, endpoint)\n                if method == \"GET\":\n                    response = requests.get(url, headers=self.headers, timeout=10)\n                elif method == \"POST\":\n                    response = requests.post(url, headers=self.headers, timeout=10)\n                \n                status_ok = response.status_code == expected_status\n                print(f\"    {'âœ…' if status_ok else 'âŒ'} Status: {response.status_code} (expected {expected_status})\")\n                \n                test_key = f\"pipeline_{endpoint.replace('/', '_')}\"\n                self.results[\"data_pipeline\"][test_key] = {\n                    \"url\": url,\n                    \"method\": method,\n                    \"status\": response.status_code,\n                    \"expected\": expected_status,\n                    \"success\": status_ok\n                }\n                \n                if not status_ok:\n                    all_pipeline_ok = False\n                    \n            except Exception as e:\n                print(f\"    âŒ ERROR: {e}\")\n                test_key = f\"pipeline_{endpoint.replace('/', '_')}\"\n                self.results[\"data_pipeline\"][test_key] = {\n                    \"url\": urljoin(self.base_url, endpoint),\n                    \"method\": method,\n                    \"status\": \"ERROR\",\n                    \"expected\": expected_status,\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                all_pipeline_ok = False\n\n        # Update overall counters\n        total_pipeline_tests = len(self.results[\"data_pipeline\"])\n        passed_pipeline_tests = sum(1 for r in self.results[\"data_pipeline\"].values() if r[\"success\"])\n        self.results[\"overall\"][\"total\"] += total_pipeline_tests\n        self.results[\"overall\"][\"passed\"] += passed_pipeline_tests\n        self.results[\"overall\"][\"failed\"] += total_pipeline_tests - passed_pipeline_tests\n\n        return all_pipeline_ok\n\n    def test_endpoint_verification(self) -> bool:\n        \"\"\"Test endpoint accessibility and detect missing endpoints\"\"\"\n        print(\"\\nðŸ” Testing Endpoint Verification...\")\n        print(\"-\" * 40)\n\n        # Define critical endpoints that should exist\n        critical_endpoints = [\n            (\"/health\", \"GET\", 200),\n            (\"/v1/chat/completions\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/embeddings\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/shell/exec\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/gpu/lock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/gpu/unlock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/gpu/status\", \"GET\", 200),\n            (\"/v1/gpu/reset\", \"POST\", 200),\n            (\"/v1/gpu/force-release-all\", \"POST\", 200),\n            (\"/v1/system/spawn_shell\", \"POST\", 200),\n            (\"/v1/models/pull\", \"POST\", 400),  # Expected to fail with 400 due to missing body\n            (\"/v1/models/pull/status\", \"GET\", 400),  # Expected to fail with 400 due to missing id param\n        ]\n\n        all_endpoints_ok = True\n        missing_endpoints = []\n\n        for endpoint, method, expected_status in critical_endpoints:\n            print(f\"  Testing endpoint: {method} {endpoint}\")\n            try:\n                url = urljoin(self.base_url, endpoint)\n                if method == \"GET\":\n                    response = requests.get(url, headers=self.headers, timeout=10)\n                elif method == \"POST\":\n                    response = requests.post(url, headers=self.headers, timeout=10)\n                \n                # For endpoints that are expected to fail due to missing body/params, \n                # we consider them accessible if they return 400/404/422 rather than 404/405\n                accessible = response.status_code != 404 and response.status_code != 405\n                success = accessible  # We consider it a success if the endpoint exists\n                \n                status_msg = f\"Status: {response.status_code}\"\n                if not accessible:\n                    status_msg += \" (MISSING/INACCESSIBLE)\"\n                    missing_endpoints.append(f\"{method} {endpoint}\")\n                    all_endpoints_ok = False\n                \n                print(f\"    {'âœ…' if accessible else 'âŒ'} {status_msg}\")\n                \n                test_key = f\"endpoint_{endpoint.replace('/', '_')}\"\n                self.results[\"endpoint_verification\"][test_key] = {\n                    \"url\": url,\n                    \"method\": method,\n                    \"status\": response.status_code,\n                    \"expected\": expected_status,\n                    \"accessible\": accessible,\n                    \"success\": success\n                }\n                \n            except Exception as e:\n                print(f\"    âŒ ERROR: {e}\")\n                missing_endpoints.append(f\"{method} {endpoint}\")\n                test_key = f\"endpoint_{endpoint.replace('/', '_')}\"\n                self.results[\"endpoint_verification\"][test_key] = {\n                    \"url\": urljoin(self.base_url, endpoint),\n                    \"method\": method,\n                    \"status\": \"ERROR\",\n                    \"expected\": expected_status,\n                    \"accessible\": False,\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                all_endpoints_ok = False\n\n        if missing_endpoints:\n            print(f\"\\n  âš ï¸  Missing/Inaccessible Endpoints:\")\n            for ep in missing_endpoints:\n                print(f\"    - {ep}\")\n        else:\n            print(f\"\\n  âœ… All critical endpoints are accessible!\")\n\n        # Update overall counters\n        total_endpoint_tests = len(self.results[\"endpoint_verification\"])\n        passed_endpoint_tests = sum(1 for r in self.results[\"endpoint_verification\"].values() if r[\"success\"])\n        self.results[\"overall\"][\"total\"] += total_endpoint_tests\n        self.results[\"overall\"][\"passed\"] += passed_endpoint_tests\n        self.results[\"overall\"][\"failed\"] += total_endpoint_tests - passed_endpoint_tests\n\n        return all_endpoints_ok\n\n    def test_syntax_verification(self) -> bool:\n        \"\"\"Test for function syntax errors in critical files\"\"\"\n        print(\"\\nðŸ” Testing Syntax Verification...\")\n        print(\"-\" * 40)\n\n        # Define critical Python files to check for syntax errors\n        critical_files = [\n            \"tools/webgpu_bridge.py\",\n            \"tools/anchor.py\",\n            \"tools/orchestrator.py\",\n            \"tests/test_model_loading.py\",\n            \"tests/test_model_availability.py\",\n            \"tests/test_gpu_fixes.py\",\n            \"tests/test_orchestrator.py\",\n        ]\n\n        all_syntax_ok = True\n        syntax_errors = []\n\n        for file_path in critical_files:\n            print(f\"  Checking syntax: {file_path}\")\n            try:\n                file_abs_path = Path(file_path)\n                if file_abs_path.exists():\n                    # Use Python's built-in compile to check syntax\n                    with open(file_abs_path, 'r', encoding='utf-8') as f:\n                        source_code = f.read()\n                    \n                    compile(source_code, str(file_abs_path), 'exec')\n                    print(f\"    âœ… Syntax OK\")\n                    \n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                    self.results[\"syntax_check\"][test_key] = {\n                        \"file\": str(file_abs_path),\n                        \"success\": True,\n                        \"error\": None\n                    }\n                else:\n                    print(f\"    âš ï¸  File not found\")\n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                    self.results[\"syntax_check\"][test_key] = {\n                        \"file\": str(file_abs_path),\n                        \"success\": False,\n                        \"error\": \"File not found\"\n                    }\n                    syntax_errors.append(f\"{file_path}: File not found\")\n                    all_syntax_ok = False\n                    \n            except SyntaxError as e:\n                print(f\"    âŒ Syntax Error: {e}\")\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                self.results[\"syntax_check\"][test_key] = {\n                    \"file\": str(file_abs_path),\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                all_syntax_ok = False\n            except Exception as e:\n                print(f\"    âŒ Error checking file: {e}\")\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                self.results[\"syntax_check\"][test_key] = {\n                    \"file\": str(file_abs_path),\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                all_syntax_ok = False\n\n        if syntax_errors:\n            print(f\"\\n  âŒ Syntax errors found:\")\n            for error in syntax_errors:\n                print(f\"    - {error}\")\n        else:\n            print(f\"\\n  âœ… All critical files have valid syntax!\")\n\n        # Update overall counters\n        total_syntax_tests = len(self.results[\"syntax_check\"])\n        passed_syntax_tests = sum(1 for r in self.results[\"syntax_check\"].values() if r[\"success\"])\n        self.results[\"overall\"][\"total\"] += total_syntax_tests\n        self.results[\"overall\"][\"passed\"] += passed_syntax_tests\n        self.results[\"overall\"][\"failed\"] += total_syntax_tests - passed_syntax_tests\n\n        return all_syntax_ok\n\n    def generate_detailed_report(self) -> Dict[str, Any]:\n        \"\"\"Generate a detailed test report\"\"\"\n        report = {\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_url\": self.base_url,\n            \"results\": self.results,\n            \"summary\": {\n                \"total_tests\": self.results[\"overall\"][\"total\"],\n                \"passed_tests\": self.results[\"overall\"][\"passed\"],\n                \"failed_tests\": self.results[\"overall\"][\"failed\"],\n                \"success_rate\": self.results[\"overall\"][\"passed\"] / max(self.results[\"overall\"][\"total\"], 1) * 100\n            }\n        }\n        return report\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Run Comprehensive Test Suite for Anchor Core\")\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for detailed test report (JSON format)\")\n\n    args = parser.parse_args()\n\n    # Run the comprehensive test suite\n    test_suite = ComprehensiveTestSuite(base_url=args.url, token=args.token)\n    success = test_suite.run_all_tests()\n\n    # Generate and save detailed report if requested\n    if args.output:\n        report = test_suite.generate_detailed_report()\n        with open(args.output, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n        print(f\"\\nðŸ“„ Detailed test report saved to: {args.output}\")\n\n    # Exit with appropriate code\n    sys.exit(0 if success else 1)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tests\\comprehensive_test_suite.py"
  },
  {
    "id": "tests\\endpoint_syntax_verification.py",
    "timestamp": 1767209923,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nEndpoint and Syntax Verification Tests for Anchor Core\n\nThis script specifically tests for:\n1. Missing endpoints\n2. Function syntax errors\n3. API endpoint accessibility\n4. System health verification\n\"\"\"\n\nimport requests\nimport sys\nimport json\nimport subprocess\nimport ast\nfrom urllib.parse import urljoin\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\n\nclass EndpointAndSyntaxTester:\n    def __init__(self, base_url: str = \"http://localhost:8000\", token: str = \"sovereign-secret\"):\n        self.base_url = base_url\n        self.token = token\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n        self.results = {\n            \"endpoints\": {},\n            \"syntax\": {},\n            \"system_health\": {}\n        }\n\n    def test_all_endpoints(self) -> Dict[str, bool]:\n        \"\"\"Test all defined endpoints for accessibility\"\"\"\n        print(\"ðŸ” Testing All Endpoints for Accessibility...\")\n        print(\"-\" * 50)\n\n        # Define all expected endpoints with their methods and expected status codes\n        endpoints = [\n            # Core endpoints\n            (\"/\", \"GET\", [200, 404]),  # Root may serve UI or return 404\n            (\"/health\", \"GET\", [200]),\n            \n            # API endpoints\n            (\"/v1/chat/completions\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            (\"/v1/embeddings\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            (\"/v1/shell/exec\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            \n            # GPU management endpoints\n            (\"/v1/gpu/lock\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            (\"/v1/gpu/unlock\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            (\"/v1/gpu/status\", \"GET\", [200]),\n            (\"/v1/gpu/reset\", \"POST\", [200, 401]),\n            (\"/v1/gpu/force-release-all\", \"POST\", [200, 401]),\n            \n            # System endpoints\n            (\"/v1/system/spawn_shell\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            \n            # Model endpoints\n            (\"/v1/models/pull\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\n            (\"/v1/models/pull/status\", \"GET\", [400, 401, 404]),  # Expected to fail with 400 due to missing id param\n            \n            # Model file endpoints (these should return 404 if files don't exist locally, but endpoint should be accessible)\n            (\"/models/test-model/resolve/main/ndarray-cache.json\", \"GET\", [200, 404]),\n            (\"/models/test-model/resolve/main/mlc-chat-config.json\", \"GET\", [200, 404]),\n            (\"/models/test-model/resolve/main/tokenizer.json\", \"GET\", [200, 404]),\n        ]\n\n        missing_endpoints = []\n        accessible_endpoints = []\n\n        for endpoint, method, expected_statuses in endpoints:\n            print(f\"  Testing: {method} {endpoint}\")\n            \n            try:\n                url = urljoin(self.base_url, endpoint)\n                \n                if method == \"GET\":\n                    response = requests.get(url, headers=self.headers, timeout=10)\n                elif method == \"POST\":\n                    # Send minimal payload to avoid 400 errors due to missing body\n                    if endpoint in [\"/v1/chat/completions\", \"/v1/embeddings\", \"/v1/shell/exec\"]:\n                        response = requests.post(url, headers=self.headers, timeout=10, json={})\n                    else:\n                        response = requests.post(url, headers=self.headers, timeout=10)\n                else:\n                    response = requests.request(method, url, headers=self.headers, timeout=10)\n                \n                status_ok = response.status_code in expected_statuses\n                accessible = response.status_code != 404 and response.status_code != 405  # 404 = not found, 405 = method not allowed\n                \n                status_icon = \"âœ…\" if status_ok else \"âŒ\"\n                print(f\"    {status_icon} Status: {response.status_code} (expected: {expected_statuses})\")\n                \n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\n                self.results[\"endpoints\"][test_key] = {\n                    \"url\": url,\n                    \"method\": method,\n                    \"status\": response.status_code,\n                    \"expected\": expected_statuses,\n                    \"accessible\": accessible,\n                    \"status_ok\": status_ok,\n                    \"success\": accessible  # Endpoint exists if not 404/405\n                }\n                \n                if accessible:\n                    accessible_endpoints.append(f\"{method} {endpoint}\")\n                else:\n                    missing_endpoints.append(f\"{method} {endpoint}\")\n                    \n            except requests.exceptions.ConnectionError:\n                print(f\"    âŒ Connection Error - Server may not be running\")\n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\n                self.results[\"endpoints\"][test_key] = {\n                    \"url\": url,\n                    \"method\": method,\n                    \"status\": \"CONNECTION_ERROR\",\n                    \"expected\": expected_statuses,\n                    \"accessible\": False,\n                    \"status_ok\": False,\n                    \"success\": False\n                }\n                missing_endpoints.append(f\"{method} {endpoint}\")\n            except Exception as e:\n                print(f\"    âŒ Error: {e}\")\n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\n                self.results[\"endpoints\"][test_key] = {\n                    \"url\": urljoin(self.base_url, endpoint),\n                    \"method\": method,\n                    \"status\": \"ERROR\",\n                    \"expected\": expected_statuses,\n                    \"accessible\": False,\n                    \"status_ok\": False,\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                missing_endpoints.append(f\"{method} {endpoint}\")\n\n        print(f\"\\n  Summary:\")\n        print(f\"    âœ… Accessible Endpoints: {len(accessible_endpoints)}\")\n        print(f\"    âŒ Missing/Inaccessible: {len(missing_endpoints)}\")\n        \n        if missing_endpoints:\n            print(f\"\\n  Missing Endpoints:\")\n            for ep in missing_endpoints:\n                print(f\"    - {ep}\")\n        \n        endpoint_success = len(missing_endpoints) == 0\n        return {\"success\": endpoint_success, \"missing\": missing_endpoints, \"accessible\": accessible_endpoints}\n\n    def test_syntax_in_files(self) -> Dict[str, bool]:\n        \"\"\"Test syntax in critical Python files\"\"\"\n        print(\"\\nðŸ” Testing Syntax in Critical Files...\")\n        print(\"-\" * 50)\n\n        # Define critical files to check for syntax errors\n        critical_files = [\n            \"tools/webgpu_bridge.py\",\n            \"tools/anchor.py\", \n            \"tools/orchestrator.py\",\n            \"tests/comprehensive_test_suite.py\",\n            \"tests/test_model_loading.py\",\n            \"tests/test_model_availability.py\",\n            \"tests/test_gpu_fixes.py\",\n            \"tests/test_orchestrator.py\",\n        ]\n\n        syntax_errors = []\n        valid_files = []\n\n        for file_path in critical_files:\n            print(f\"  Checking: {file_path}\")\n            \n            try:\n                path_obj = Path(file_path)\n                if not path_obj.exists():\n                    print(f\"    âš ï¸  File not found\")\n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                    self.results[\"syntax\"][test_key] = {\n                        \"file\": str(path_obj),\n                        \"exists\": False,\n                        \"valid_syntax\": False,\n                        \"success\": False,\n                        \"error\": \"File not found\"\n                    }\n                    syntax_errors.append(f\"{file_path}: File not found\")\n                    continue\n\n                # Read and parse the file to check for syntax errors\n                with open(path_obj, 'r', encoding='utf-8') as f:\n                    source_code = f.read()\n                \n                # Parse the AST to check for syntax errors\n                ast.parse(source_code)\n                \n                print(f\"    âœ… Valid syntax\")\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                self.results[\"syntax\"][test_key] = {\n                    \"file\": str(path_obj),\n                    \"exists\": True,\n                    \"valid_syntax\": True,\n                    \"success\": True,\n                    \"error\": None\n                }\n                valid_files.append(file_path)\n                \n            except SyntaxError as e:\n                print(f\"    âŒ Syntax Error: {e}\")\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                self.results[\"syntax\"][test_key] = {\n                    \"file\": str(path_obj),\n                    \"exists\": True,\n                    \"valid_syntax\": False,\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\n            except Exception as e:\n                print(f\"    âŒ Error: {e}\")\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\n                self.results[\"syntax\"][test_key] = {\n                    \"file\": str(path_obj),\n                    \"exists\": True,\n                    \"valid_syntax\": False,\n                    \"success\": False,\n                    \"error\": str(e)\n                }\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\n\n        print(f\"\\n  Summary:\")\n        print(f\"    âœ… Valid Syntax: {len(valid_files)}\")\n        print(f\"    âŒ Syntax Errors: {len(syntax_errors)}\")\n        \n        if syntax_errors:\n            print(f\"\\n  Files with Syntax Errors:\")\n            for error in syntax_errors:\n                print(f\"    - {error}\")\n        \n        syntax_success = len(syntax_errors) == 0\n        return {\"success\": syntax_success, \"errors\": syntax_errors, \"valid\": valid_files}\n\n    def test_system_health(self) -> Dict[str, bool]:\n        \"\"\"Test overall system health\"\"\"\n        print(\"\\nðŸ” Testing System Health...\")\n        print(\"-\" * 50)\n\n        health_checks = []\n\n        # Test health endpoint\n        try:\n            response = requests.get(urljoin(self.base_url, \"/health\"), timeout=10)\n            health_ok = response.status_code == 200\n            health_checks.append((\"Health Endpoint\", health_ok, response.status_code))\n            print(f\"  Health Endpoint: {'âœ…' if health_ok else 'âŒ'} Status {response.status_code}\")\n        except Exception as e:\n            health_checks.append((\"Health Endpoint\", False, str(e)))\n            print(f\"  Health Endpoint: âŒ Error {e}\")\n\n        # Test if server is responding\n        try:\n            response = requests.get(self.base_url, timeout=10)\n            server_ok = response.status_code in [200, 404]  # 200 = UI served, 404 = no root handler\n            health_checks.append((\"Server Response\", server_ok, response.status_code))\n            print(f\"  Server Response: {'âœ…' if server_ok else 'âŒ'} Status {response.status_code}\")\n        except Exception as e:\n            health_checks.append((\"Server Response\", False, str(e)))\n            print(f\"  Server Response: âŒ Error {e}\")\n\n        # Test authentication\n        try:\n            response = requests.get(urljoin(self.base_url, \"/v1/gpu/status\"), timeout=10)\n            auth_ok = response.status_code in [200, 401, 403]  # 401/403 = auth required, 200 = success\n            auth_msg = \"Auth OK\" if auth_ok else \"Unexpected status\"\n            health_checks.append((\"Authentication\", auth_ok, auth_msg))\n            print(f\"  Authentication: {'âœ…' if auth_ok else 'âŒ'} {auth_msg}\")\n        except Exception as e:\n            health_checks.append((\"Authentication\", False, str(e)))\n            print(f\"  Authentication: âŒ Error {e}\")\n\n        all_healthy = all(check[1] for check in health_checks)\n        return {\"success\": all_healthy, \"checks\": health_checks}\n\n    def run_all_tests(self) -> bool:\n        \"\"\"Run all verification tests\"\"\"\n        print(\"ðŸš€ Running Endpoint and Syntax Verification Tests...\")\n        print(f\"Testing against: {self.base_url}\")\n        print(\"=\" * 80)\n\n        # Run all test categories\n        endpoint_results = self.test_all_endpoints()\n        syntax_results = self.test_syntax_in_files()\n        health_results = self.test_system_health()\n\n        # Summary\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ðŸ“Š VERIFICATION TEST RESULTS\")\n        print(\"=\" * 80)\n        \n        print(f\"Endpoint Accessibility: {'âœ… PASS' if endpoint_results['success'] else 'âŒ FAIL'}\")\n        print(f\"Syntax Verification: {'âœ… PASS' if syntax_results['success'] else 'âŒ FAIL'}\")\n        print(f\"System Health: {'âœ… PASS' if health_results['success'] else 'âŒ FAIL'}\")\n\n        overall_success = all([\n            endpoint_results['success'],\n            syntax_results['success'],\n            health_results['success']\n        ])\n\n        print(f\"\\nðŸŽ¯ Overall Result: {'âœ… ALL VERIFICATIONS PASSED' if overall_success else 'âŒ SOME VERIFICATIONS FAILED'}\")\n\n        if not endpoint_results['success']:\n            print(f\"\\n  Missing Endpoints: {len(endpoint_results['missing'])}\")\n        \n        if not syntax_results['success']:\n            print(f\"  Syntax Errors: {len(syntax_results['errors'])}\")\n\n        return overall_success\n\n    def generate_report(self) -> Dict:\n        \"\"\"Generate a detailed verification report\"\"\"\n        report = {\n            \"timestamp\": __import__('datetime').datetime.now().isoformat(),\n            \"base_url\": self.base_url,\n            \"results\": self.results,\n            \"summary\": {\n                \"endpoints\": {\n                    \"total\": len(self.results[\"endpoints\"]),\n                    \"accessible\": len([r for r in self.results[\"endpoints\"].values() if r[\"success\"]]),\n                    \"missing\": len([r for r in self.results[\"endpoints\"].values() if not r[\"success\"]])\n                },\n                \"syntax\": {\n                    \"total\": len(self.results[\"syntax\"]),\n                    \"valid\": len([r for r in self.results[\"syntax\"].values() if r[\"success\"]]),\n                    \"errors\": len([r for r in self.results[\"syntax\"].values() if not r[\"success\"]])\n                }\n            }\n        }\n        return report\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Verify Endpoints and Syntax for Anchor Core\")\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for verification report (JSON format)\")\n\n    args = parser.parse_args()\n\n    # Run the verification tests\n    tester = EndpointAndSyntaxTester(base_url=args.url, token=args.token)\n    success = tester.run_all_tests()\n\n    # Generate and save report if requested\n    if args.output:\n        report = tester.generate_report()\n        with open(args.output, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n        print(f\"\\nðŸ“„ Verification report saved to: {args.output}\")\n\n    sys.exit(0 if success else 1)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tests\\endpoint_syntax_verification.py"
  },
  {
    "id": "tests\\model_test.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Anchor Core Model Test Suite</title>\r\n    <style>\r\n        body {\r\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\r\n            max-width: 1200px;\r\n            margin: 0 auto;\r\n            padding: 20px;\r\n            background-color: #f5f5f5;\r\n        }\r\n        .container {\r\n            background: white;\r\n            border-radius: 8px;\r\n            padding: 20px;\r\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\r\n        }\r\n        h1 {\r\n            color: #2c3e50;\r\n            border-bottom: 2px solid #3498db;\r\n            padding-bottom: 10px;\r\n        }\r\n        .test-section {\r\n            margin: 20px 0;\r\n            padding: 15px;\r\n            border: 1px solid #ddd;\r\n            border-radius: 5px;\r\n            background-color: #fafafa;\r\n        }\r\n        .test-result {\r\n            padding: 5px 10px;\r\n            margin: 5px 0;\r\n            border-radius: 3px;\r\n            font-family: monospace;\r\n        }\r\n        .success {\r\n            background-color: #d4edda;\r\n            color: #155724;\r\n            border: 1px solid #c3e6cb;\r\n        }\r\n        .error {\r\n            background-color: #f8d7da;\r\n            color: #721c24;\r\n            border: 1px solid #f5c6cb;\r\n        }\r\n        .info {\r\n            background-color: #d1ecf1;\r\n            color: #0c5460;\r\n            border: 1px solid #bee5eb;\r\n        }\r\n        button {\r\n            background-color: #3498db;\r\n            color: white;\r\n            border: none;\r\n            padding: 10px 15px;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            margin: 5px;\r\n        }\r\n        button:hover {\r\n            background-color: #2980b9;\r\n        }\r\n        button:disabled {\r\n            background-color: #bdc3c7;\r\n            cursor: not-allowed;\r\n        }\r\n        .model-list {\r\n            display: flex;\r\n            flex-wrap: wrap;\r\n            gap: 10px;\r\n            margin: 10px 0;\r\n        }\r\n        .model-item {\r\n            background: #e3f2fd;\r\n            padding: 8px 12px;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            border: 1px solid #bbdefb;\r\n        }\r\n        .model-item:hover {\r\n            background: #bbdefb;\r\n        }\r\n    </style>\r\n</head>\r\n<body>\r\n    <div class=\"container\">\r\n        <h1>ðŸ§ª Anchor Core Model Test Suite</h1>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>ðŸŒ System Information</h2>\r\n            <div id=\"system-info\" class=\"test-result info\">Loading system information...</div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>ðŸ” API Endpoint Tests</h2>\r\n            <button id=\"test-health\">Test Health Endpoint</button>\r\n            <button id=\"test-gpu-status\">Test GPU Status</button>\r\n            <button id=\"test-spawn\">Test Spawn Endpoint</button>\r\n            <div id=\"api-results\"></div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>ðŸ“¦ Model Path Tests</h2>\r\n            <p>Click on a model to test its accessibility:</p>\r\n            <div class=\"model-list\">\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\">Qwen2.5-Coder-1.5B</div>\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\">Qwen2.5-Coder-7B</div>\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-7B-Instruct-q4f16_1-MLC\">Qwen2.5-7B</div>\r\n                <div class=\"model-item\" data-model=\"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\">DeepSeek-R1-Qwen-7B</div>\r\n                <div class=\"model-item\" data-model=\"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\">DeepSeek-R1-Llama-8B</div>\r\n            </div>\r\n            <div id=\"model-results\"></div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>ðŸ“ˆ Test Summary</h2>\r\n            <div id=\"summary\">Run tests to see summary...</div>\r\n        </div>\r\n    </div>\r\n\r\n    <script>\r\n        // Update system information\r\n        document.getElementById('system-info').textContent = \r\n            `Current Origin: ${window.location.origin} | Protocol: ${window.location.protocol} | Host: ${window.location.host}`;\r\n\r\n        // API Test Functions\r\n        async function testHealth() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing health endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/health');\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `âœ… Health: ${JSON.stringify(data)}` \r\n                    : `âŒ Health: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">âŒ Health: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        async function testGpuStatus() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing GPU status endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/v1/gpu/status', {\r\n                    headers: { 'Authorization': 'Bearer sovereign-secret' }\r\n                });\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `âœ… GPU Status: ${JSON.stringify(data)}` \r\n                    : `âŒ GPU Status: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">âŒ GPU Status: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        async function testSpawn() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing spawn endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/v1/system/spawn_shell', {\r\n                    method: 'POST',\r\n                    headers: { \r\n                        'Content-Type': 'application/json',\r\n                        'Authorization': 'Bearer sovereign-secret'\r\n                    },\r\n                    body: JSON.stringify({})\r\n                });\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `âœ… Spawn: ${JSON.stringify(data)}` \r\n                    : `âŒ Spawn: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">âŒ Spawn: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        // Model Test Functions\r\n        async function testModel(modelName) {\r\n            const resultDiv = document.getElementById('model-results');\r\n            resultDiv.innerHTML = `<div class=\"test-result info\">Testing model: ${modelName}...</div>`;\r\n\r\n            const modelPath = `/models/${modelName}`;\r\n            const configFiles = [\r\n                `${modelPath}/ndarray-cache.json`,\r\n                `${modelPath}/tokenizer.json`,\r\n                `${modelPath}/mlc-chat-config.json`,\r\n                `${modelPath}/params.json`,\r\n                `${modelPath}/tokenizer_config.json`\r\n            ];\r\n\r\n            let results = [];\r\n            let allAvailable = true;\r\n\r\n            for (const configFile of configFiles) {\r\n                try {\r\n                    const response = await fetch(configFile, { method: 'HEAD' });\r\n                    const status = response.status;\r\n                    const statusText = response.statusText;\r\n\r\n                    const fileResult = {\r\n                        file: configFile,\r\n                        status: status,\r\n                        accessible: status === 200,\r\n                        statusText: statusText\r\n                    };\r\n\r\n                    if (status !== 200) {\r\n                        allAvailable = false;\r\n                    }\r\n\r\n                    results.push(fileResult);\r\n                } catch (error) {\r\n                    results.push({\r\n                        file: configFile,\r\n                        status: 'ERROR',\r\n                        accessible: false,\r\n                        statusText: error.message\r\n                    });\r\n                    allAvailable = false;\r\n                }\r\n            }\r\n\r\n            // Display results\r\n            let html = `<h4>Results for ${modelName}:</h4>`;\r\n            for (const result of results) {\r\n                const statusClass = result.status === 200 ? 'success' :\r\n                                  result.status === 'ERROR' ? 'error' : 'info';\r\n\r\n                const statusIcon = result.status === 200 ? 'âœ…' :\r\n                                 result.status === 'ERROR' ? 'âŒ' : 'â„¹ï¸';\r\n\r\n                html += `<div class=\"test-result ${statusClass}\">${statusIcon} ${result.file} - ${result.status} ${result.statusText}</div>`;\r\n            }\r\n\r\n            // Add availability summary\r\n            const availabilityClass = allAvailable ? 'success' : 'error';\r\n            const availabilityIcon = allAvailable ? 'ðŸŽ‰' : 'âš ï¸';\r\n            const availabilityText = allAvailable ?\r\n                'Model is fully available for loading!' :\r\n                'Model is NOT available - download required before loading';\r\n\r\n            html += `<div class=\"test-result ${availabilityClass}\"><strong>${availabilityIcon} AVAILABILITY: ${availabilityText}</strong></div>`;\r\n\r\n            // Add download suggestion if not available\r\n            if (!allAvailable) {\r\n                html += `<div class=\"test-result info\">ðŸ’¡ To download: POST to /v1/models/pull with model_id: \"mlc-ai/${modelName}\"</div>`;\r\n            }\r\n\r\n            resultDiv.innerHTML = html;\r\n        }\r\n\r\n        // Event Listeners\r\n        document.getElementById('test-health').addEventListener('click', testHealth);\r\n        document.getElementById('test-gpu-status').addEventListener('click', testGpuStatus);\r\n        document.getElementById('test-spawn').addEventListener('click', testSpawn);\r\n        \r\n        // Add event listeners to model items\r\n        document.querySelectorAll('.model-item').forEach(item => {\r\n            item.addEventListener('click', () => {\r\n                const modelName = item.getAttribute('data-model');\r\n                testModel(modelName);\r\n            });\r\n        });\r\n\r\n        // Initial test of health endpoint\r\n        window.addEventListener('load', () => {\r\n            testHealth();\r\n        });\r\n    </script>\r\n</body>\r\n</html>",
    "source": "tests\\model_test.html"
  },
  {
    "id": "tests\\README.md",
    "timestamp": 1767225144,
    "role": "file",
    "content": "# Test Suite for Anchor Core\n\nThis directory contains all test files for the Anchor Core system, organized to verify functionality across different components.\n\n## Test Files\n\n### Python Tests\n- `comprehensive_test_suite.py` - Main test suite covering model loading, data pipeline, endpoint verification, and syntax checking\n- `endpoint_syntax_verification.py` - Specific tests for endpoint accessibility and syntax verification\n- `test_new_endpoints.py` - Tests for new sidecar, context, and vision endpoints\n- `test_model_loading.py` - Tests for model loading functionality and endpoint accessibility\n- `test_model_availability.py` - Tests for model availability and download capability\n- `test_gpu_fixes.py` - Tests for GPU resource management and lock functionality\n- `test_orchestrator.py` - Unit tests for the orchestrator component\n\n### HTML Tests\n- `model_test.html` - Interactive web-based test suite for model and endpoint verification\n\n## Running Tests\n\n### Comprehensive Test Suite\n```bash\npython tests/comprehensive_test_suite.py\n```\n\nWith custom parameters:\n```bash\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output test_report.json\n```\n\n### Individual Test Files\n```bash\npython tests/test_model_loading.py\npython tests/test_model_availability.py\npython tests/test_gpu_fixes.py\n```\n\n### Interactive Web Tests\nStart the Anchor Core server and navigate to:\n```\nhttp://localhost:8000/tests/model_test.html\n```\n\n## Test Coverage\n\nThe test suite covers:\n\n1. **Model Loading**: Verifies model availability and accessibility\n2. **Data Pipeline**: Tests API endpoints and data flow\n3. **Endpoint Verification**: Checks for missing or inaccessible endpoints\n4. **Syntax Verification**: Validates Python syntax in critical files\n5. **GPU Management**: Tests GPU lock, unlock, and resource management\n6. **System Integration**: Verifies end-to-end functionality\n\n## Test Categories\n\n### Model Tests\n- Model file accessibility\n- Configuration file verification\n- Download capability testing\n\n### API Tests\n- Health endpoint\n- GPU management endpoints\n- Shell execution endpoints\n- Model pull endpoints\n\n### System Tests\n- Bridge functionality\n- WebSocket connections\n- Authentication",
    "source": "tests\\README.md"
  },
  {
    "id": "tests\\test_gpu_fixes.py",
    "timestamp": 1767209613,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify GPU resource management fixes including model loading serialization\n\"\"\"\n\nimport time\nimport requests\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport json\n\ndef test_gpu_status():\n    \"\"\"Test GPU status endpoint\"\"\"\n    try:\n        response = requests.get(\"http://localhost:8000/v1/gpu/status\",\n                              headers={\"Authorization\": \"Bearer sovereign-secret\"})\n        if response.status_code == 200:\n            status = response.json()\n            print(f\"âœ… GPU Status: {status}\")\n            return True\n        else:\n            print(f\"âŒ GPU Status request failed: {response.status_code}\")\n            return False\n    except Exception as e:\n        print(f\"âŒ Error getting GPU status: {e}\")\n        return False\n\ndef test_lock_acquisition(agent_id: str, timeout: int = 120):  # Increased to 120s\n    \"\"\"Test GPU lock acquisition\"\"\"\n    try:\n        print(f\"â³ Agent {agent_id} requesting GPU lock...\")\n        start_time = time.time()\n\n        response = requests.post(\"http://localhost:8000/v1/gpu/lock\",\n                                headers={\"Authorization\": \"Bearer sovereign-secret\"},\n                                json={\"id\": agent_id},\n                                timeout=timeout)\n        \n        elapsed = time.time() - start_time\n        \n        if response.status_code == 200:\n            result = response.json()\n            print(f\"âœ… Agent {agent_id} acquired lock in {elapsed:.2f}s: {result.get('token', 'no-token')}\")\n            \n            # Release the lock\n            release_response = requests.post(\"http://localhost:8000/v1/gpu/unlock\",\n                                          headers={\"Authorization\": \"Bearer sovereign-secret\"},\n                                          json={\"id\": agent_id})\n            if release_response.status_code == 200:\n                print(f\"âœ… Agent {agent_id} released lock\")\n            else:\n                print(f\"âš ï¸  Agent {agent_id} failed to release lock: {release_response.status_code}\")\n            \n            return True\n        else:\n            print(f\"âŒ Agent {agent_id} failed to acquire lock: {response.status_code} - {response.text}\")\n            return False\n    except Exception as e:\n        print(f\"âŒ Agent {agent_id} error: {e}\")\n        return False\n\ndef test_concurrent_access():\n    \"\"\"Test concurrent GPU access with different priority agents\"\"\"\n    print(\"\\nðŸ§ª Testing concurrent GPU access...\")\n    \n    agents = [\n        (\"Root-Mic\", 5),  # High priority\n        (\"Root-Console-Init\", 10),  # Medium priority\n        (\"Dreamer-Init\", 15),  # Lower priority\n        (\"Test-Agent-4\", 20),  # Even lower priority\n    ]\n    \n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n        for agent_id, delay in agents:\n            # Add small delay to ensure proper ordering\n            future = executor.submit(test_lock_acquisition, agent_id)\n            futures.append(future)\n            time.sleep(0.5)  # Stagger the requests\n        \n        # Wait for all to complete\n        for future in as_completed(futures):\n            future.result()\n\ndef test_force_release():\n    \"\"\"Test force release functionality\"\"\"\n    print(\"\\nðŸ”§ Testing force release functionality...\")\n    \n    # First, acquire a lock manually\n    response = requests.post(\"http://localhost:8000/v1/gpu/lock\",\n                            headers={\"Authorization\": \"Bearer sovereign-secret\"},\n                            json={\"id\": \"test-force-release\"})\n    \n    if response.status_code == 200:\n        print(\"âœ… Acquired test lock\")\n        \n        # Now force release all locks\n        force_response = requests.post(\"http://localhost:8000/v1/gpu/force-release-all\",\n                                     headers={\"Authorization\": \"Bearer sovereign-secret\"})\n        \n        if force_response.status_code == 200:\n            print(\"âœ… Force release executed successfully\")\n        else:\n            print(f\"âŒ Force release failed: {force_response.status_code}\")\n    else:\n        print(f\"âŒ Failed to acquire test lock: {response.status_code}\")\n\ndef run_comprehensive_test():\n    \"\"\"Run comprehensive tests\"\"\"\n    print(\"ðŸš€ Running comprehensive GPU resource management tests...\\n\")\n    \n    # Test 1: Basic status check\n    print(\"1ï¸âƒ£ Testing GPU status endpoint...\")\n    status_ok = test_gpu_status()\n    \n    # Test 2: Force release\n    print(\"\\n2ï¸âƒ£ Testing force release functionality...\")\n    test_force_release()\n    \n    # Test 3: Concurrent access\n    print(\"\\n3ï¸âƒ£ Testing concurrent access patterns...\")\n    test_concurrent_access()\n    \n    # Test 4: Status after tests\n    print(\"\\n4ï¸âƒ£ Checking final GPU status...\")\n    final_status_ok = test_gpu_status()\n    \n    print(\"\\nâœ… Comprehensive testing completed!\")\n    return status_ok and final_status_ok\n\nif __name__ == \"__main__\":\n    run_comprehensive_test()",
    "source": "tests\\test_gpu_fixes.py"
  },
  {
    "id": "tests\\test_model_availability.py",
    "timestamp": 1767197389,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nModel Availability Test Suite for Anchor Core\n\nThis script tests model availability by checking if required model files exist\nlocally before attempting to load them into the MLC-LLM engine.\n\"\"\"\n\nimport requests\nimport sys\nimport time\nfrom urllib.parse import urljoin\nimport json\nfrom pathlib import Path\n\n\nclass ModelAvailabilityTester:\n    def __init__(self, base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\n        self.base_url = base_url\n        self.token = token\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n        self.results = {}\n\n    def test_model_availability(self, model_name):\n        \"\"\"Test if a model is available for loading by checking required files\"\"\"\n        print(f\"Testing model availability: {model_name}\")\n\n        # Define the required model files for MLC-LLM using the resolve/main pattern\n        # This matches how the MLC-LLM library actually accesses files\n        required_files = [\n            f\"models/{model_name}/resolve/main/ndarray-cache.json\",\n            f\"models/{model_name}/resolve/main/tokenizer.json\",\n            f\"models/{model_name}/resolve/main/mlc-chat-config.json\",\n            f\"models/{model_name}/resolve/main/tokenizer_config.json\"\n        ]\n\n        # Note: MLC-LLM models use sharded parameter files (params_shard_*.bin) instead of params.json\n        # So we don't check for params.json which doesn't exist for these models\n\n        model_result = {\n            \"model_name\": model_name,\n            \"available\": True,\n            \"files\": {},\n            \"download_required\": False\n        }\n\n        missing_files = []\n\n        for file_path in required_files:\n            try:\n                url = urljoin(self.base_url, file_path)\n                response = requests.head(url, timeout=10)  # Short timeout for availability check\n\n                file_status = {\n                    \"url\": url,\n                    \"status_code\": response.status_code,\n                    \"exists\": response.status_code == 200,\n                    \"checked_at\": time.time()\n                }\n\n                model_result[\"files\"][file_path] = file_status\n\n                if response.status_code == 200:\n                    print(f\"  OK {file_path}\")\n                elif response.status_code == 404:\n                    print(f\"  MISSING {file_path} - NOT FOUND\")\n                    missing_files.append(file_path)\n                    model_result[\"available\"] = False\n                    model_result[\"download_required\"] = True\n                else:\n                    print(f\"  WARNING {file_path} - Status {response.status_code}\")\n                    model_result[\"available\"] = False\n\n            except requests.exceptions.RequestException as e:\n                file_status = {\n                    \"url\": urljoin(self.base_url, file_path),\n                    \"status_code\": \"ERROR\",\n                    \"exists\": False,\n                    \"error\": str(e),\n                    \"checked_at\": time.time()\n                }\n                model_result[\"files\"][file_path] = file_status\n                print(f\"  âŒ {file_path} - ERROR: {e}\")\n                model_result[\"available\"] = False\n\n        if missing_files:\n            print(f\"  INFO Missing files: {len(missing_files)} required files not found\")\n        else:\n            print(f\"  SUCCESS Model {model_name} is fully available for loading!\")\n\n        self.results[model_name] = model_result\n        return model_result[\"available\"]\n\n    def test_model_download_capability(self, model_id):\n        \"\"\"Test if the model download endpoint works for a given model\"\"\"\n        print(f\"\\nTesting download capability for: {model_id}\")\n        \n        try:\n            url = urljoin(self.base_url, \"/v1/models/pull\")\n            payload = {\n                \"model_id\": model_id,\n                \"url\": f\"https://huggingface.co/{model_id}\"\n            }\n            \n            # Make a quick test request without waiting for full download\n            response = requests.post(url, json=payload, headers=self.headers, timeout=5)\n            \n            if response.status_code in [200, 409, 400]:  # 409=already exists, 400=bad request (but endpoint works)\n                print(f\"  OK Download endpoint accessible for {model_id}\")\n                return True\n            else:\n                print(f\"  FAILED Download endpoint failed for {model_id}: {response.status_code}\")\n                return False\n\n        except requests.exceptions.RequestException as e:\n            print(f\"  FAILED Download endpoint error for {model_id}: {e}\")\n            return False\n\n    def run_model_availability_tests(self, model_list):\n        \"\"\"Run availability tests for a list of models\"\"\"\n        print(f\"Running Model Availability Tests against: {self.base_url}\")\n        print(\"-\" * 70)\n        \n        available_models = []\n        unavailable_models = []\n        \n        for model_name in model_list:\n            print(f\"\\nINFO Testing: {model_name}\")\n            is_available = self.test_model_availability(model_name)\n\n            if is_available:\n                available_models.append(model_name)\n                print(f\"  RESULT: AVAILABLE for loading\")\n            else:\n                unavailable_models.append(model_name)\n                print(f\"  RESULT: NOT AVAILABLE (download required)\")\n                \n                # Test if download is possible\n                huggingface_id = f\"mlc-ai/{model_name}\"\n                self.test_model_download_capability(huggingface_id)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"TEST SUMMARY:\")\n        print(f\"  Available Models: {len(available_models)}\")\n        for model in available_models:\n            print(f\"    OK {model}\")\n\n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\n        for model in unavailable_models:\n            print(f\"    MISSING {model}\")\n\n        print(\"\\nRECOMMENDATION:\")\n        if unavailable_models:\n            print(\"  - Download required models using the /v1/models/pull endpoint\")\n            print(\"  - Or ensure models are pre-loaded in the models/ directory\")\n        else:\n            print(\"  - All models are ready for immediate loading!\")\n            \n        return available_models, unavailable_models\n\n    def generate_report(self):\n        \"\"\"Generate a detailed test report\"\"\"\n        report = {\n            \"base_url\": self.base_url,\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"results\": self.results,\n            \"summary\": {\n                \"total_models\": len(self.results),\n                \"available\": len([r for r in self.results.values() if r[\"available\"]]),\n                \"unavailable\": len([r for r in self.results.values() if not r[\"available\"]])\n            }\n        }\n        return report\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Test Model Availability for Anchor Core\")\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\", \n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\", \n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--models\", nargs=\"+\", \n                       help=\"Specific models to test (default: predefined list)\")\n    parser.add_argument(\"--output\", \n                       help=\"Output file for test report (JSON format)\")\n\n    args = parser.parse_args()\n\n    tester = ModelAvailabilityTester(base_url=args.url, token=args.token)\n    \n    # Default model list if none provided\n    default_models = [\n        \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n        \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\", \n        \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n        \"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\n        \"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\n        \"Phi-3.5-mini-instruct-q4f16_1-MLC\",\n        \"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\"\n    ]\n    \n    models_to_test = args.models if args.models else default_models\n    \n    available, unavailable = tester.run_model_availability_tests(models_to_test)\n    \n    if args.output:\n        report = tester.generate_report()\n        with open(args.output, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"\\nTest report saved to: {args.output}\")\n\n    # Exit with error code if no models are available\n    if not available:\n        print(\"\\nERROR: No models are currently available for loading!\")\n        sys.exit(1)\n    else:\n        print(f\"\\nSUCCESS: {len(available)} model(s) are ready for loading!\")\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tests\\test_model_availability.py"
  },
  {
    "id": "tests\\test_model_loading.py",
    "timestamp": 1767195827,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nModel Loading Test Suite for Anchor Core\r\n\r\nThis script tests the model loading functionality and verifies that all endpoints\r\nare accessible and working correctly with the unified Anchor Core architecture.\r\n\"\"\"\r\n\r\nimport requests\r\nimport sys\r\nimport os\r\nfrom urllib.parse import urljoin\r\nimport json\r\nfrom pathlib import Path\r\n\r\n\r\nclass ModelLoadingTester:\r\n    def __init__(self, base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\r\n        self.base_url = base_url\r\n        self.token = token\r\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n        self.results = {}\r\n        \r\n    def test_endpoint_accessibility(self, endpoint, method=\"GET\", expected_status=200):\r\n        \"\"\"Test if an endpoint is accessible\"\"\"\r\n        try:\r\n            url = urljoin(self.base_url, endpoint)\r\n            response = requests.request(method, url, headers=self.headers)\r\n            success = response.status_code == expected_status\r\n            self.results[f\"endpoint_{endpoint.replace('/', '_')}\"] = {\r\n                \"url\": url,\r\n                \"method\": method,\r\n                \"status\": response.status_code,\r\n                \"expected\": expected_status,\r\n                \"success\": success,\r\n                \"message\": f\"Status {response.status_code}\" if success else f\"Expected {expected_status}, got {response.status_code}\"\r\n            }\r\n            return success\r\n        except Exception as e:\r\n            self.results[f\"endpoint_{endpoint.replace('/', '_')}\"] = {\r\n                \"url\": urljoin(self.base_url, endpoint),\r\n                \"method\": method,\r\n                \"status\": \"ERROR\",\r\n                \"expected\": expected_status,\r\n                \"success\": False,\r\n                \"message\": str(e)\r\n            }\r\n            return False\r\n    \r\n    def test_model_path_accessibility(self, model_path):\r\n        \"\"\"Test if a model path is accessible\"\"\"\r\n        try:\r\n            url = urljoin(self.base_url, model_path)\r\n            response = requests.head(url, headers=self.headers)\r\n            success = response.status_code in [200, 404]  # 404 means path exists but file doesn't (which is expected for model directories)\r\n            self.results[f\"model_{model_path.replace('/', '_')}\"] = {\r\n                \"url\": url,\r\n                \"status\": response.status_code,\r\n                \"success\": success,\r\n                \"message\": f\"Model path accessible\" if success else f\"Model path not accessible: {response.status_code}\"\r\n            }\r\n            return success\r\n        except Exception as e:\r\n            self.results[f\"model_{model_path.replace('/', '_')}\"] = {\r\n                \"url\": urljoin(self.base_url, model_path),\r\n                \"status\": \"ERROR\",\r\n                \"success\": False,\r\n                \"message\": str(e)\r\n            }\r\n            return False\r\n    \r\n    def test_model_config_accessibility(self, model_name):\r\n        \"\"\"Test if model config files are accessible\"\"\"\r\n        config_files = [\r\n            f\"models/{model_name}/ndarray-cache.json\",\r\n            f\"models/{model_name}/tokenizer.json\",\r\n            f\"models/{model_name}/mlc-chat-config.json\",\r\n            f\"models/{model_name}/params.json\"\r\n        ]\r\n        \r\n        results = []\r\n        for config_file in config_files:\r\n            try:\r\n                url = urljoin(self.base_url, config_file)\r\n                response = requests.head(url)\r\n                success = response.status_code in [200, 404]  # Allow 404 as files may not exist yet\r\n                results.append({\r\n                    \"file\": config_file,\r\n                    \"url\": url,\r\n                    \"status\": response.status_code,\r\n                    \"success\": success\r\n                })\r\n            except Exception as e:\r\n                results.append({\r\n                    \"file\": config_file,\r\n                    \"url\": urljoin(self.base_url, config_file),\r\n                    \"status\": \"ERROR\",\r\n                    \"success\": False,\r\n                    \"message\": str(e)\r\n                })\r\n        \r\n        self.results[f\"model_configs_{model_name}\"] = results\r\n        return results\r\n    \r\n    def run_comprehensive_test(self):\r\n        \"\"\"Run comprehensive tests for all endpoints and model paths\"\"\"\r\n        print(\"Running Model Loading Test Suite...\")\r\n        print(f\"Testing against: {self.base_url}\")\r\n        print(\"-\" * 60)\r\n        \r\n        # Test core API endpoints\r\n        api_endpoints = [\r\n            (\"/health\", \"GET\", 200),\r\n            (\"/v1/gpu/status\", \"GET\", 200),\r\n            (\"/v1/gpu/lock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/unlock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/shell/exec\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/system/spawn_shell\", \"POST\", 200),\r\n        ]\r\n        \r\n        print(\"Testing API Endpoints:\")\r\n        for endpoint, method, expected in api_endpoints:\r\n            success = self.test_endpoint_accessibility(endpoint, method, expected)\r\n            status_icon = \"[PASS]\" if success else \"[FAIL]\"\r\n            key = f\"endpoint_{endpoint.replace('/', '_')}\"\r\n            print(f\"  {status_icon} {method} {endpoint} -> {self.results[key]['message']}\")\r\n        \r\n        print()\r\n        \r\n        # Test model paths\r\n        model_names = [\r\n            \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\r\n            \"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\r\n            \"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\"\r\n        ]\r\n        \r\n        print(\"Testing Model Paths:\")\r\n        for model_name in model_names:\r\n            success = self.test_model_path_accessibility(f\"/models/{model_name}\")\r\n            status_icon = \"[PASS]\" if success else \"[FAIL]\"\r\n            key = f\"model_/models/{model_name}\".replace(\"/\", \"_\")\r\n            if key in self.results:\r\n                print(f\"  {status_icon} /models/{model_name} -> {self.results[key]['message']}\")\r\n            else:\r\n                print(f\"  [FAIL] /models/{model_name} -> Key not found in results\")\r\n        \r\n        print()\r\n        \r\n        # Test model config files\r\n        print(\"Testing Model Config Files:\")\r\n        for model_name in model_names[:2]:  # Test just the first 2 to avoid too much output\r\n            configs = self.test_model_config_accessibility(model_name)\r\n            print(f\"  Model: {model_name}\")\r\n            for config in configs:\r\n                status_icon = \"[PASS]\" if config['success'] else \"[FAIL]\"\r\n                status_symbol = \"OK\" if config['success'] else \"X\"\r\n                print(f\"    [{status_symbol}] {config['file']} -> Status: {config['status']}\")\r\n        \r\n        print()\r\n        \r\n        # Summary\r\n        total_tests = len([r for r in self.results.values() if isinstance(r, dict) and 'success' in r]) + \\\r\n                     sum(len(r) for r in self.results.values() if isinstance(r, list))\r\n        passed_tests = 0\r\n        \r\n        for key, value in self.results.items():\r\n            if isinstance(value, dict) and 'success' in value:\r\n                if value['success']:\r\n                    passed_tests += 1\r\n            elif isinstance(value, list):\r\n                for item in value:\r\n                    if item.get('success'):\r\n                        passed_tests += 1\r\n        \r\n        print(\"-\" * 60)\r\n        print(f\"Test Summary: {passed_tests}/{total_tests} tests passed\")\r\n\r\n        if passed_tests == total_tests:\r\n            print(\"All tests passed! The Anchor Core is properly configured.\")\r\n            return True\r\n        else:\r\n            print(\"Some tests failed. Check the output above for details.\")\r\n            return False\r\n\r\n    def generate_test_report(self):\r\n        \"\"\"Generate a detailed test report\"\"\"\r\n        report = {\r\n            \"base_url\": self.base_url,\r\n            \"timestamp\": str(__import__('datetime').datetime.now()),\r\n            \"results\": self.results,\r\n            \"summary\": {\r\n                \"total_tests\": 0,\r\n                \"passed_tests\": 0,\r\n                \"failed_tests\": 0\r\n            }\r\n        }\r\n        \r\n        # Count tests\r\n        for key, value in self.results.items():\r\n            if isinstance(value, dict) and 'success' in value:\r\n                report[\"summary\"][\"total_tests\"] += 1\r\n                if value['success']:\r\n                    report[\"summary\"][\"passed_tests\"] += 1\r\n                else:\r\n                    report[\"summary\"][\"failed_tests\"] += 1\r\n            elif isinstance(value, list):\r\n                report[\"summary\"][\"total_tests\"] += len(value)\r\n                for item in value:\r\n                    if item.get('success'):\r\n                        report[\"summary\"][\"passed_tests\"] += 1\r\n                    else:\r\n                        report[\"summary\"][\"failed_tests\"] += 1\r\n        \r\n        return report\r\n\r\n\r\ndef main():\r\n    import argparse\r\n    \r\n    parser = argparse.ArgumentParser(description=\"Test Anchor Core Model Loading Functionality\")\r\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\", help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\r\n    parser.add_argument(\"--token\", default=\"sovereign-secret\", help=\"Authentication token (default: sovereign-secret)\")\r\n    parser.add_argument(\"--output\", help=\"Output file for test report (JSON format)\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    tester = ModelLoadingTester(base_url=args.url, token=args.token)\r\n    success = tester.run_comprehensive_test()\r\n    \r\n    if args.output:\r\n        report = tester.generate_test_report()\r\n        with open(args.output, 'w') as f:\r\n            json.dump(report, f, indent=2)\r\n        print(f\"ðŸ“„ Test report saved to: {args.output}\")\r\n    \r\n    sys.exit(0 if success else 1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "source": "tests\\test_model_loading.py"
  },
  {
    "id": "tests\\test_new_endpoints.py",
    "timestamp": 1767225135,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify the new sidecar and vision endpoints are working\n\"\"\"\n\nimport requests\nimport time\n\ndef test_endpoints():\n    base_url = \"http://localhost:8000\"\n    \n    print(\"Testing new Anchor Core endpoints...\")\n    print(f\"Base URL: {base_url}\")\n    print(\"-\" * 50)\n    \n    # Test sidecar endpoint\n    try:\n        response = requests.get(f\"{base_url}/sidecar\", timeout=10)\n        print(f\"GET /sidecar -> Status: {response.status_code} {'âœ…' if response.status_code == 200 else 'âŒ'}\")\n    except Exception as e:\n        print(f\"GET /sidecar -> Error: {e} âŒ\")\n    \n    # Test context endpoint\n    try:\n        response = requests.get(f\"{base_url}/context\", timeout=10)\n        print(f\"GET /context -> Status: {response.status_code} {'âœ…' if response.status_code == 200 else 'âŒ'}\")\n    except Exception as e:\n        print(f\"GET /context -> Error: {e} âŒ\")\n    \n    # Test memory search endpoint (with a sample query)\n    try:\n        response = requests.post(\n            f\"{base_url}/v1/memory/search\",\n            json={\"query\": \"test\"},\n            timeout=10\n        )\n        print(f\"POST /v1/memory/search -> Status: {response.status_code} {'âœ…' if response.status_code == 200 else 'âŒ'}\")\n        if response.status_code == 200:\n            data = response.json()\n            print(f\"  Response keys: {list(data.keys())}\")\n    except Exception as e:\n        print(f\"POST /v1/memory/search -> Error: {e} âŒ\")\n    \n    # Test vision ingest endpoint (this will fail without a proper image, but should return 400 not 404)\n    try:\n        response = requests.post(f\"{base_url}/v1/vision/ingest\", timeout=10)\n        # Should return 400 (bad request) not 404 (not found)\n        is_ok = response.status_code in [400, 405]  # 400 = bad request (no image), 405 = method not allowed\n        print(f\"POST /v1/vision/ingest -> Status: {response.status_code} {'âœ…' if is_ok else 'âŒ'} (should be 400 or 405 for valid endpoint)\")\n    except Exception as e:\n        print(f\"POST /v1/vision/ingest -> Error: {e} âŒ\")\n    \n    print(\"-\" * 50)\n    print(\"Endpoint testing complete!\")\n\nif __name__ == \"__main__\":\n    test_endpoints()",
    "source": "tests\\test_new_endpoints.py"
  },
  {
    "id": "tests\\test_orchestrator.py",
    "timestamp": 1766526074,
    "role": "file",
    "content": "import unittest\nfrom unittest.mock import patch, MagicMock\nfrom orchestrator import Orchestrator, MLCConnectionError\n\nclass TestOrchestrator(unittest.TestCase):\n\n    def setUp(self):\n        self.orc = Orchestrator()\n\n    @patch('requests.get')\n    def test_load_mlc_model_success(self, mock_get):\n        # Mock successful bridge connection\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": [{\"id\": \"webgpu-chat\"}]}\n        mock_response.status_code = 200\n        mock_get.return_value = mock_response\n\n        result = self.orc.load_mlc_model(\"my-model\")\n        self.assertTrue(result)\n        self.assertEqual(self.orc.active_model, \"my-model\")\n\n    @patch('requests.get')\n    def test_load_mlc_model_failure(self, mock_get):\n        # Mock connection error\n        mock_get.side_effect = Exception(\"Connection refused\")\n        \n        with self.assertRaises(MLCConnectionError):\n            self.orc.load_mlc_model(\"my-model\")\n\n    @patch('requests.post')\n    def test_invoke_mlc_inference_success(self, mock_post):\n        self.orc.active_model = \"test-model\"\n        \n        # Mock successful inference\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\n            \"choices\": [{\"message\": {\"content\": \"Hello from MLC\"}}]\n        }\n        mock_response.status_code = 200\n        mock_post.return_value = mock_response\n\n        output = self.orc.invoke_mlc_inference(\"Hi\")\n        self.assertEqual(output, \"Hello from MLC\")\n\n    def test_invoke_without_load(self):\n        with self.assertRaises(ValueError):\n            self.orc.invoke_mlc_inference(\"Hi\")\n\nif __name__ == '__main__':\n    unittest.main()\n",
    "source": "tests\\test_orchestrator.py"
  },
  {
    "id": "tools\\anchor-mic.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Root Mic ðŸŽ™ï¸</title>\r\n    <style>\r\n        :root {\r\n            --bg-color: #0f0f11;\r\n            --surface-color: #1a1a1d;\r\n            --primary-color: #00ff88;\r\n            --accent-color: #00ccff;\r\n            --text-color: #eeeeee;\r\n            --danger-color: #ff4444;\r\n        }\r\n\r\n        body {\r\n            background-color: var(--bg-color);\r\n            color: var(--text-color);\r\n            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\r\n            margin: 0;\r\n            display: flex;\r\n            flex-direction: column;\r\n            align-items: center;\r\n            justify-content: center;\r\n            min-height: 100vh;\r\n            padding: 8px; /* Add padding to prevent cutoff */\r\n            overflow: auto; /* Allow scrolling if needed */\r\n        }\r\n\r\n        .container {\r\n            text-align: center;\r\n            width: 100%;\r\n            max-width: 500px;\r\n            padding: 20px;\r\n        }\r\n\r\n        h1 {\r\n            font-weight: 300;\r\n            letter-spacing: 2px;\r\n            margin-bottom: 30px;\r\n            text-transform: uppercase;\r\n            font-size: 1.5rem;\r\n            color: var(--accent-color);\r\n            text-shadow: 0 0 10px rgba(0, 204, 255, 0.3);\r\n        }\r\n\r\n        #mic-btn {\r\n            width: 150px;\r\n            height: 150px;\r\n            border-radius: 50%;\r\n            background: radial-gradient(circle at 30% 30%, #444, #222);\r\n            border: 4px solid #333;\r\n            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5), 0 0 0 4px var(--bg-color);\r\n            cursor: pointer;\r\n            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);\r\n            display: flex;\r\n            align-items: center;\r\n            justify-content: center;\r\n            margin: 0 auto 30px;\r\n            position: relative;\r\n            outline: none;\r\n            -webkit-tap-highlight-color: transparent;\r\n        }\r\n\r\n        #mic-btn::after {\r\n            content: '';\r\n            position: absolute;\r\n            top: -10px;\r\n            left: -10px;\r\n            right: -10px;\r\n            bottom: -10px;\r\n            border-radius: 50%;\r\n            border: 2px solid var(--primary-color);\r\n            opacity: 0;\r\n            transform: scale(0.8);\r\n            transition: all 0.3s;\r\n        }\r\n\r\n        #mic-btn:hover {\r\n            transform: scale(1.05);\r\n            background: radial-gradient(circle at 30% 30%, #555, #333);\r\n        }\r\n\r\n        #mic-btn.active {\r\n            background: radial-gradient(circle at 30% 30%, #ff5555, #aa0000);\r\n            box-shadow: 0 0 30px rgba(255, 68, 68, 0.6);\r\n            border-color: #ff4444;\r\n            transform: scale(0.95);\r\n        }\r\n\r\n        #mic-btn.active::after {\r\n            animation: pulse 1.5s infinite;\r\n            opacity: 1;\r\n        }\r\n\r\n        #mic-icon {\r\n            font-size: 64px;\r\n            color: #888;\r\n            transition: color 0.3s;\r\n        }\r\n\r\n        #mic-btn.active #mic-icon {\r\n            color: white;\r\n        }\r\n\r\n        #clarify-btn {\r\n            background: transparent;\r\n            color: var(--accent-color);\r\n            border: 1px solid var(--accent-color);\r\n            padding: 8px 16px;\r\n            border-radius: 20px;\r\n            cursor: pointer;\r\n            font-size: 0.9rem;\r\n            margin-bottom: 20px;\r\n            transition: all 0.3s;\r\n            text-transform: uppercase;\r\n            letter-spacing: 1px;\r\n        }\r\n\r\n        #clarify-btn:hover:not(:disabled) {\r\n            background: rgba(0, 204, 255, 0.1);\r\n            transform: translateY(-2px);\r\n        }\r\n\r\n        #clarify-btn:disabled {\r\n            opacity: 0.3;\r\n            cursor: not-allowed;\r\n            border-color: #555;\r\n            color: #555;\r\n        }\r\n\r\n        #status {\r\n            font-size: 1.2rem;\r\n            margin-bottom: 20px;\r\n            height: 1.5em;\r\n            color: #888;\r\n        }\r\n\r\n        .visualizer {\r\n            width: 100%;\r\n            height: 60px;\r\n            background: var(--surface-color);\r\n            border-radius: 12px;\r\n            margin-bottom: 20px;\r\n            position: relative;\r\n            overflow: hidden;\r\n            border: 1px solid #333;\r\n        }\r\n\r\n        .bar-container {\r\n            display: flex;\r\n            align-items: center;\r\n            justify-content: center;\r\n            height: 100%;\r\n            gap: 2px;\r\n        }\r\n\r\n        .bar {\r\n            width: 4px;\r\n            background: var(--primary-color);\r\n            border-radius: 2px;\r\n            height: 4px;\r\n            transition: height 0.1s ease;\r\n        }\r\n\r\n        #output {\r\n            background: var(--surface-color);\r\n            padding: 20px;\r\n            border-radius: 12px;\r\n            border: 1px solid #333;\r\n            min-height: 100px;\r\n            text-align: left;\r\n            font-family: 'Courier New', monospace;\r\n            font-size: 0.9rem;\r\n            color: #ccc;\r\n            position: relative;\r\n            overflow-y: auto;\r\n            max-height: 200px;\r\n        }\r\n\r\n        #copy-toast {\r\n            position: absolute;\r\n            top: 20px;\r\n            right: 20px;\r\n            background: var(--primary-color);\r\n            color: #000;\r\n            padding: 8px 16px;\r\n            border-radius: 20px;\r\n            font-weight: bold;\r\n            opacity: 0;\r\n            transform: translateY(-20px);\r\n            transition: all 0.3s;\r\n            pointer-events: none;\r\n        }\r\n\r\n        #copy-toast.show {\r\n            opacity: 1;\r\n            transform: translateY(0);\r\n        }\r\n\r\n        footer {\r\n            margin-top: 40px;\r\n            font-size: 0.7rem;\r\n            color: #444;\r\n        }\r\n\r\n        @keyframes pulse {\r\n            0% {\r\n                transform: scale(1);\r\n                opacity: 1;\r\n                border-color: var(--danger-color);\r\n            }\r\n\r\n            100% {\r\n                transform: scale(1.5);\r\n                opacity: 0;\r\n                border-color: var(--danger-color);\r\n            }\r\n        }\r\n\r\n        #loading-overlay {\r\n            position: fixed;\r\n            top: 0;\r\n            left: 0;\r\n            width: 100%;\r\n            height: 100%;\r\n            background: rgba(0, 0, 0, 0.9);\r\n            display: flex;\r\n            flex-direction: column;\r\n            align-items: center;\r\n            justify-content: center;\r\n            z-index: 1000;\r\n            transition: opacity 0.5s;\r\n        }\r\n\r\n        .loader {\r\n            width: 48px;\r\n            height: 48px;\r\n            border: 5px solid #FFF;\r\n            border-bottom-color: var(--primary-color);\r\n            border-radius: 50%;\r\n            display: inline-block;\r\n            box-sizing: border-box;\r\n            animation: rotation 1s linear infinite;\r\n        }\r\n\r\n        .progress-text {\r\n            margin-top: 20px;\r\n            font-family: monospace;\r\n            color: var(--primary-color);\r\n        }\r\n\r\n        @keyframes rotation {\r\n            0% {\r\n                transform: rotate(0deg);\r\n            }\r\n\r\n            100% {\r\n                transform: rotate(360deg);\r\n            }\r\n        }\r\n    </style>\r\n</head>\r\n<body>\r\n    <div id=\"loading-overlay\">\r\n        <span class=\"loader\"></span>\r\n        <div id=\"loading-text\" class=\"progress-text\">Initializing Neural Engines...</div>\r\n    </div>\r\n    <div id=\"copy-toast\">Copied to Clipboard!</div>\r\n    <div class=\"container\">\r\n        <h1>Root Mic ðŸŽ™ï¸</h1>\r\n        <div class=\"visualizer\"><div class=\"bar-container\" id=\"bars\"></div></div>\r\n        <button id=\"mic-btn\"><div id=\"mic-icon\">ðŸŽ™ï¸</div></button>\r\n        <button id=\"clarify-btn\" disabled>Refine Text</button>\r\n        <div id=\"status\">Ready</div>\r\n        <div id=\"output\">...</div>\r\n        <footer>Running Locally: Whisper-Tiny (Audio) + Qwen2.5-1.5B (Text)</footer>\r\n    </div>\r\n\r\n    <script type=\"module\">\r\n        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\r\n        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n        // THE NEW KERNEL\r\n        import { AnchorLogger, createStore, getWebGPUConfig, GPUController } from './modules/anchor.js';\r\n\r\n        // Load hot reload functionality in development\r\n        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {\r\n            import('./modules/gpu-hot-reloader.js').then(() => {\r\n                console.log('ðŸ”„ GPU Hot Reloader loaded for development');\r\n            }).catch(err => {\r\n                console.warn('âš ï¸ GPU Hot Reloader not available:', err);\r\n            });\r\n        }\r\n\r\n        const logger = new AnchorLogger('Root-Mic');\r\n        \r\n        // Reactive Store\r\n        const { state, subscribe } = createStore({\r\n            status: 'Ready',\r\n            output: '...',\r\n            isLoading: true,\r\n            loadingText: 'Initializing Neural Engines...',\r\n            isRecording: false\r\n        });\r\n\r\n        // UI Bindings\r\n        subscribe((prop, val) => {\r\n            if (prop === 'status') document.getElementById('status').innerText = val;\r\n            if (prop === 'output') {\r\n                document.getElementById('output').innerText = val;\r\n                // Enable clarify button if there is text (and not just placeholder/loading)\r\n                const btn = document.getElementById('clarify-btn');\r\n                if (val && val !== '...' && val.length > 10) {\r\n                    btn.disabled = false;\r\n                } else {\r\n                    btn.disabled = true;\r\n                }\r\n            }\r\n            if (prop === 'loadingText') document.getElementById('loading-text').innerText = val;\r\n            if (prop === 'isLoading') {\r\n                const ol = document.getElementById('loading-overlay');\r\n                ol.style.opacity = val ? '1' : '0';\r\n                setTimeout(() => ol.style.display = val ? 'flex' : 'none', 500);\r\n            }\r\n            if (prop === 'isRecording') {\r\n                const btn = document.getElementById('mic-btn');\r\n                if (val) btn.classList.add('active'); else btn.classList.remove('active');\r\n            }\r\n        });\r\n\r\n        let whisperWorker = null;\r\n        let llmEngine = null;\r\n        let mediaRecorder = null;\r\n        let audioChunks = [];\r\n        let audioContext = null;\r\n        let analyser = null;\r\n        let dataArray = null;\r\n        let animationId = null;\r\n        let silenceStart = 0;\r\n\r\n        async function init() {\r\n            try {\r\n                // 1. Whisper Init (Worker)\r\n                state.loadingText = \"Step 1/2: Initializing Whisper Worker...\";\r\n                whisperWorker = new Worker('./modules/whisper-worker.js', { type: 'module' });\r\n                \r\n                // Wait for worker init\r\n                await new Promise((resolve, reject) => {\r\n                    whisperWorker.onmessage = (e) => {\r\n                        if (e.data.type === 'init_done') resolve();\r\n                        if (e.data.type === 'error') reject(new Error(e.data.error));\r\n                    };\r\n                    whisperWorker.postMessage({ type: 'init' });\r\n                });\r\n                logger.success(\"Whisper Worker Ready\");\r\n\r\n                // 2. LLM Init (Using Kernel)\r\n                state.loadingText = \"Step 2/2: Config Qwen2.5 (Brain)...\";\r\n                await initLLM();\r\n\r\n                state.isLoading = false;\r\n                initVisualizer();\r\n                logger.success(\"Root Mic Online\");\r\n            } catch (e) {\r\n                logger.error(e.message);\r\n                state.loadingText = `Error: ${e.message}`;\r\n            }\r\n        }\r\n\r\n        // Clarify Logic\r\n        document.getElementById('clarify-btn').addEventListener('click', async () => {\r\n            if (!state.output || state.output === '...' || state.output.length < 5) return;\r\n            \r\n            const originalText = state.output;\r\n            state.status = \"Refining...\";\r\n            state.output = originalText + \"\\n\\n[Refining...]\";\r\n\r\n            try {\r\n                const reply = await llmEngine.chat.completions.create({\r\n                    messages: [\r\n                        { role: \"system\", content: \"You are a professional text editor. Your task is to refine the provided speech-to-text output into clear, coherent, and polished text suitable for reading or pasting. Fix grammar and flow, but keep the original meaning intact. Output ONLY the refined text.\" },\r\n                        { role: \"user\", content: `Refine this text:\\n\\n\"${originalText}\"` }\r\n                    ],\r\n                    temperature: 0.5,\r\n                    max_tokens: 512,\r\n                });\r\n\r\n                const summary = reply.choices[0].message.content;\r\n                state.output = summary; // Replace output with summary\r\n                state.status = \"Clarified\";\r\n                \r\n                if (document.hasFocus()) {\r\n                    navigator.clipboard.writeText(summary);\r\n                    const t = document.getElementById('copy-toast');\r\n                    t.innerText = \"Refined Text Copied!\";\r\n                    t.classList.add('show');\r\n                    setTimeout(() => { \r\n                        t.classList.remove('show');\r\n                        t.innerText = \"Copied to Clipboard!\";\r\n                    }, 2000);\r\n                }\r\n\r\n            } catch (e) {\r\n                logger.error(\"Clarify failed: \" + e.message);\r\n                state.status = \"Error\";\r\n                state.output = originalText; // Revert\r\n            }\r\n        });\r\n\r\n        async function initLLM() {\r\n            const modelId = \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\";\r\n            const snapdragonId = \"snapdragon-mic-qwen\";\r\n\r\n            // 0. THE BLOCKER (Model Load Lock) - Serialize model loading\r\n            logger.info(\"Requesting Model Load Lock...\");\r\n\r\n            try {\r\n                await GPUController.withModelLoadLock(\"Root-Mic\", async () => {\r\n                    logger.success(\"Model Load Lock Acquired.\");\r\n\r\n                    // KERNEL CALL: Get safe GPU config\r\n                    const gpuConfig = await getWebGPUConfig('lite');\r\n\r\n                    if (gpuConfig.isConstrained) {\r\n                        logger.warn(`Clamping Buffer to ${Math.round(gpuConfig.maxBufferSize/1024/1024)}MB for Mobile/XPS compatibility.`);\r\n                    }\r\n\r\n                    // Create device explicitly with limits\r\n                    const device = await gpuConfig.adapter.requestDevice(gpuConfig.deviceConfig);\r\n\r\n                    const appConfig = {\r\n                        model_list: [{\r\n                            model: \"https://huggingface.co/\" + modelId + \"/resolve/main/\",\r\n                            model_id: snapdragonId,\r\n                            model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                            vram_required_MB: 2000,\r\n                            low_resource_required: true,\r\n                            buffer_size_required_bytes: gpuConfig.maxBufferSize\r\n                        }]\r\n                    };\r\n\r\n                    llmEngine = await webllm.CreateMLCEngine(snapdragonId, {\r\n                        appConfig,\r\n                        device,\r\n                        initProgressCallback: (report) => {\r\n                            state.loadingText = `Loading Brain: ${Math.round(report.progress * 100)}%`;\r\n                        }\r\n                    });\r\n                });\r\n            } catch (error) {\r\n                state.loadingText = `Model Load Error: ${error.message}`;\r\n\r\n                // Try to check GPU status for more information\r\n                try {\r\n                    const status = await GPUController.checkStatus();\r\n                    if (status && status.locked) {\r\n                        logger.warn(`GPU currently locked by: ${status.owner || 'unknown'}`);\r\n                        if (status.queued && status.queued.length > 0) {\r\n                            logger.warn(`Queue: ${status.queued.join(', ')}`);\r\n                        }\r\n                    }\r\n                } catch (statusErr) {\r\n                    logger.warn(`Could not get GPU status: ${statusErr.message}`);\r\n                }\r\n\r\n                throw error;\r\n            }\r\n        }\r\n\r\n        function initVisualizer() {\r\n            const container = document.getElementById('bars');\r\n            for (let i = 0; i < 30; i++) {\r\n                const bar = document.createElement('div');\r\n                bar.className = 'bar';\r\n                container.appendChild(bar);\r\n            }\r\n        }\r\n\r\n        // Recording Logic\r\n        document.getElementById('mic-btn').addEventListener('click', async () => {\r\n            if (!state.isRecording) startRecording(); else stopRecording();\r\n        });\r\n\r\n        async function startRecording() {\r\n            try {\r\n                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\r\n                mediaRecorder = new MediaRecorder(stream);\r\n                audioChunks = [];\r\n                mediaRecorder.ondataavailable = e => audioChunks.push(e.data);\r\n                mediaRecorder.onstop = processAudio;\r\n\r\n                // Visualizer\r\n                audioContext = new (window.AudioContext || window.webkitAudioContext)();\r\n                const source = audioContext.createMediaStreamSource(stream);\r\n                analyser = audioContext.createAnalyser();\r\n                analyser.fftSize = 64;\r\n                source.connect(analyser);\r\n                dataArray = new Uint8Array(analyser.frequencyBinCount);\r\n                animateVisualizer();\r\n\r\n                mediaRecorder.start();\r\n                state.isRecording = true;\r\n                silenceStart = Date.now();\r\n                state.status = \"Listening...\";\r\n                state.output = \"...\";\r\n            } catch (e) { alert(e.message); }\r\n        }\r\n\r\n        function stopRecording() {\r\n            mediaRecorder.stop();\r\n            state.isRecording = false;\r\n            state.status = \"Processing...\";\r\n            cancelAnimationFrame(animationId);\r\n            if (audioContext) audioContext.close();\r\n            document.querySelectorAll('.bar').forEach(b => b.style.height = '4px');\r\n        }\r\n\r\n        function animateVisualizer() {\r\n            if (!state.isRecording) return;\r\n            animationId = requestAnimationFrame(animateVisualizer);\r\n            analyser.getByteFrequencyData(dataArray);\r\n            const bars = document.querySelectorAll('.bar');\r\n            let maxVol = 0;\r\n            for (let i = 0; i < bars.length; i++) {\r\n                const val = dataArray[i];\r\n                if (val > maxVol) maxVol = val;\r\n                bars[i].style.height = `${Math.max(4, (val / 255) * 50)}px`;\r\n            }\r\n            if (maxVol > 10) silenceStart = Date.now();\r\n            else if (Date.now() - silenceStart > 3000) state.status = \"âš ï¸ No Audio?\";\r\n        }\r\n\r\n        async function processAudio() {\r\n            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); // Default browser format\r\n\r\n            // 1. Decode to System Sample Rate (e.g., 48000Hz)\r\n            const audioCtx = new AudioContext();\r\n            const arrayBuffer = await audioBlob.arrayBuffer();\r\n            const decodedBuffer = await audioCtx.decodeAudioData(arrayBuffer);\r\n\r\n            // 2. Resample to 16000Hz (Required by Whisper)\r\n            const targetRate = 16000;\r\n            const offlineCtx = new OfflineAudioContext(1, decodedBuffer.duration * targetRate, targetRate);\r\n            const source = offlineCtx.createBufferSource();\r\n            source.buffer = decodedBuffer;\r\n            source.connect(offlineCtx.destination);\r\n            source.start(0);\r\n            \r\n            const resampledBuffer = await offlineCtx.startRendering();\r\n            let audioData = resampledBuffer.getChannelData(0);\r\n\r\n            // NORMALIZE / AMPLIFY with Noise Gate\r\n            let peak = 0;\r\n            for (let i = 0; i < audioData.length; i++) {\r\n                const val = Math.abs(audioData[i]);\r\n                if (val > peak) peak = val;\r\n            }\r\n\r\n            // Cap amplification to avoid boosting silence/hiss into \"Applause\"\r\n            // If peak is TOO low (silence), don't amplify at all.\r\n            let ampFactor = 1.0;\r\n            if (peak > 0.01 && peak < 0.5) {\r\n                ampFactor = Math.min(0.5 / peak, 5.0); // Max 5x boost\r\n                for (let i = 0; i < audioData.length; i++) {\r\n                    audioData[i] = audioData[i] * ampFactor;\r\n                }\r\n            } else if (peak <= 0.01) {\r\n                // Too quiet, likely silence. Don't send to Whisper or send silence.\r\n                state.status = \"Too quiet (Ignored)\";\r\n                return;\r\n            }\r\n\r\n            state.status = \"Transcribing...\";\r\n            \r\n            // 0. THE BLOCKER (GPU Lock)\r\n            await GPUController.withLock(\"Root-Mic-Process\", async () => {\r\n                // Offload to Worker\r\n                const rawText = await new Promise((resolve, reject) => {\r\n                    const reqId = Date.now();\r\n                    const handler = (e) => {\r\n                        if (e.data.id === reqId) {\r\n                            whisperWorker.removeEventListener('message', handler);\r\n                            if (e.data.type === 'transcribe_result') resolve(e.data.text);\r\n                            else reject(new Error(e.data.error));\r\n                        }\r\n                    };\r\n                    whisperWorker.addEventListener('message', handler);\r\n                    whisperWorker.postMessage({ type: 'transcribe', data: audioData, id: reqId });\r\n                });\r\n\r\n                // Hallucination Filter (Aggressive)\r\n                let cleanedText = rawText.trim();\r\n                const hallucinations = [\r\n                    '[Music]', '[BLANK_AUDIO]', 'Computed', '*sigh*', '*breathing*', \r\n                    'Applause', 'Thank you', 'Subtitles', 'Amara.org', 'Copyright', \r\n                    'Â©', 'Caption', 'Sovereign' \r\n                ];\r\n                \r\n                hallucinations.forEach(h => { \r\n                    const regex = new RegExp(h.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'), 'gi');\r\n                    cleanedText = cleanedText.replace(regex, '').trim();\r\n                });\r\n\r\n                // Filter single punctuation or very short junk\r\n                if (/^[.,?!;:]+$/.test(cleanedText) || cleanedText.length < 2) cleanedText = \"\";\r\n\r\n                if (!cleanedText || cleanedText.length < 1) {\r\n                    state.status = \"Heard nothing\";\r\n                    return;\r\n                }\r\n\r\n                state.output = `Raw: \"${cleanedText}\"\\n\\nCleaning...`;\r\n                state.status = \"Refining...\";\r\n\r\n                const reply = await llmEngine.chat.completions.create({\r\n                    messages: [\r\n                        { role: \"system\", content: \"You are a verbatim transcription corrector. Your ONLY task is to fix grammar, spelling, and punctuation. Do NOT answer questions. Do NOT add commentary. Output ONLY the corrected text.\" },\r\n                        { role: \"user\", content: `Correct this text: \"${cleanedText}\"` }\r\n                    ],\r\n                    temperature: 0.3,\r\n                    max_tokens: 512,\r\n                });\r\n\r\n                const finalText = reply.choices[0].message.content;\r\n                state.output = finalText;\r\n            }); // End Lock\r\n\r\n            state.status = \"Ready\";\r\n            \r\n            // Auto Copy (Handle focus requirement)\r\n            if (document.hasFocus()) {\r\n                navigator.clipboard.writeText(cleanText).then(() => {\r\n                    const t = document.getElementById('copy-toast');\r\n                    t.classList.add('show');\r\n                    setTimeout(() => t.classList.remove('show'), 2000);\r\n                }).catch(err => {\r\n                    console.warn(\"Clipboard write failed (focus lost?):\", err);\r\n                });\r\n            } else {\r\n                console.warn(\"Clipboard write skipped: Document not focused.\");\r\n                state.output += \"\\n(Copy skipped - Click to copy)\";\r\n            }\r\n        }\r\n\r\n        init();\r\n    </script>\r\n</body>\r\n</html>\r\n",
    "source": "tools\\anchor-mic.html"
  },
  {
    "id": "tools\\anchor.py",
    "timestamp": 1767234450,
    "role": "file",
    "content": "import requests\nimport sys\nimport json\nimport argparse\n\n# Configuration\nBRIDGE_URL = \"http://localhost:8000\"\n\ndef check_connection():\n    try:\n        requests.get(f\"{BRIDGE_URL}/health\", timeout=1)\n        return True\n    except:\n        return False\n\ndef chat_loop():\n    print(\"\\nâš“ Anchor Terminal (Chat Mode)\")\n    print(\"--------------------------------\")\n    print(\"Connecting to Ghost Engine...\")\n    \n    if not check_connection():\n        print(f\"âŒ Could not connect to {BRIDGE_URL}\")\n        print(\"   -> Run 'start-anchor.bat' first.\")\n        return\n\n    # Conversation History\n    history = [\n        {\"role\": \"system\", \"content\": \"You are Anchor, a helpful AI assistant running locally.\"}\n    ]\n    \n    print(\"âœ… Connected. Type 'exit' to quit, 'clear' to reset.\\n\")\n\n    while True:\n        try:\n            user_input = input(\"You: \").strip()\n            if not user_input: continue\n            \n            if user_input.lower() in ['exit', 'quit']:\n                print(\"ðŸ‘‹ Disconnecting.\")\n                break\n                \n            if user_input.lower() == 'clear':\n                history = [history[0]] # Keep system prompt\n                print(\"--- Context Cleared ---\")\n                continue\n\n            # Add User Message\n            history.append({\"role\": \"user\", \"content\": user_input})\n            print(\"Anchor: \", end=\"\", flush=True)\n\n            # Send to Bridge\n            try:\n                response = requests.post(\n                    f\"{BRIDGE_URL}/v1/chat/completions\",\n                    json={\n                        \"messages\": history,\n                        \"stream\": False # Bridge accumulates stream for us\n                    },\n                    timeout=120\n                )\n                \n                if response.status_code == 200:\n                    data = response.json()\n                    ai_text = data['choices'][0]['message']['content']\n                    print(ai_text + \"\\n\")\n                    history.append({\"role\": \"assistant\", \"content\": ai_text})\n                else:\n                    print(f\"âŒ Error {response.status_code}: {response.text}\")\n                    \n            except Exception as e:\n                print(f\"âŒ Request Failed: {e}\")\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted.\")\n            break\n\nif __name__ == \"__main__\":\n    chat_loop()",
    "source": "tools\\anchor.py"
  },
  {
    "id": "tools\\CHANGELOG.md",
    "timestamp": 1766916224,
    "role": "file",
    "content": "# Tools Changelog\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **On-Demand Model Serving**: `model-server-chat.html` now checks `http://localhost:8080/models/{id}` before loading. If missing, it triggers a download via the Bridge.\r\n- **Quota Bypass**: Using `useIndexedDBCache: false` for large models to bypass browser storage limits, relying on the Bridge's local file server instead.\r\n\r\n### Fixed\r\n- **Model ID Mismatch**: Fixed logic in `loadModel` where the `mlc-ai/` prefix was being aggressively stripped, causing config lookups to fail.\r\n- **UI Progress**: Added real-time progress bars for server-side model downloads.\r\n\r\n - 2025-12-23\r\n\r\n### Added\r\n- **Orchestrator Model:** New `orchestrator.py` tool to programmatically interact with the MLC Bridge from Python.\r\n- **Health Endpoint:** Added `/health` to `webgpu_bridge.py` for extension connectivity checks.\r\n- **Audit Whitelist:** Added `/audit/server-logs` to auth whitelist in Bridge to fix Log Viewer 401 errors.\r\n\r\n### Changed\r\n- **Bridge Port:** Moved standard bridge port from `8000` to `8080` to avoid conflicts with `http.server`.\r\n- **Launch Scripts:** Updated `start-sovereign-console.bat` and `launch-chromium-d3d12.bat` to respect new port `8080` and correct paths.\r\n- **Root Dreamer:**\r\n  - Robustified `init()` to handle non-Error objects during crash.\r\n  - Improved JSON parsing to strip markdown code blocks before parsing.\r\n  - Added strict engine readiness check to `dreamLoop` to prevent race conditions.\r\n- **Root Console:**\r\n  - Added \"High Performance (Small)\" models (Qwen 2.5 1.5B, TinyLlama) to the dropdown.\r\n  - Updated JS mapper to handle new small model paths.\r\n  - Fixed crash in `executeR1Loop` where `genErr.message` could be undefined.\r\n\r\n### Fixed\r\n- **WebGPU Crash:** Mitigated `DXGI_ERROR_DEVICE_REMOVED` by advising single-tab usage and providing smaller model options for constrained profiles.\r\n- **Extension Connection:** Fixed CORS and Port mismatch preventing the Chrome Extension from connecting to the Bridge.\r\n",
    "source": "tools\\CHANGELOG.md"
  },
  {
    "id": "tools\\chat.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Anchor Console</title>\r\n    <style>\r\n        * {\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        body {\r\n            background: #0f0f11;\r\n            color: #ccc;\r\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\r\n            display: flex;\r\n            min-height: 100vh;\r\n            margin: 0;\r\n            padding-top: 8px; /* Add small top padding to prevent cutoff */\r\n            padding-bottom: 8px; /* Add small bottom padding */\r\n        }\r\n\r\n        /* --- Resizable Split Layout --- */\r\n        #container {\r\n            display: flex;\r\n            width: 100%;\r\n            min-height: calc(100vh - 16px); /* Account for body padding */\r\n            height: auto;\r\n            overflow: hidden;\r\n            margin: 8px; /* Add margin to prevent cutoff */\r\n            border-radius: 8px; /* Add slight rounding to match margin */\r\n        }\r\n\r\n        #sidebar {\r\n            width: 320px;\r\n            /* Default width */\r\n            min-width: 200px;\r\n            max-width: 80%;\r\n            background: #151517;\r\n            color: #d4d4d4;\r\n            display: flex;\r\n            flex-direction: column;\r\n            border-right: 1px solid #333;\r\n            transition: width 0.1s ease;\r\n        }\r\n\r\n        /* Resize Handle */\r\n        #resizer {\r\n            width: 5px;\r\n            cursor: col-resize;\r\n            background: #333;\r\n            transition: background 0.2s;\r\n            z-index: 10;\r\n        }\r\n\r\n        #resizer:hover,\r\n        #resizer.resizing {\r\n            background: #00ff88;\r\n        }\r\n\r\n        #main {\r\n            flex: 1;\r\n            display: flex;\r\n            flex-direction: column;\r\n            background: #1e1e1e;\r\n            position: relative;\r\n            min-width: 0;\r\n        }\r\n\r\n        /* --- Collapsible Details --- */\r\n        details {\r\n            margin-bottom: 10px;\r\n            border-bottom: 1px solid #333;\r\n        }\r\n\r\n        summary {\r\n            padding: 10px;\r\n            cursor: pointer;\r\n            font-weight: bold;\r\n            user-select: none;\r\n            background: #252526;\r\n            list-style: none;\r\n        }\r\n\r\n        summary::-webkit-details-marker {\r\n            display: none;\r\n        }\r\n\r\n        summary::after {\r\n            content: 'â–¼';\r\n            float: right;\r\n            font-size: 0.8em;\r\n            transition: transform 0.2s;\r\n        }\r\n\r\n        details[open] summary::after {\r\n            transform: rotate(180deg);\r\n        }\r\n\r\n        .panel-content {\r\n            padding: 10px;\r\n            display: flex;\r\n            flex-direction: column;\r\n            gap: 8px;\r\n        }\r\n\r\n        /* --- Chat Box --- */\r\n        #chat-box {\r\n            flex: 1;\r\n            overflow-y: auto;\r\n            margin-bottom: 20px;\r\n            padding-right: 10px;\r\n        }\r\n\r\n        #chat-box::-webkit-scrollbar {\r\n            width: 8px;\r\n        }\r\n\r\n        #chat-box::-webkit-scrollbar-track {\r\n            background: #333;\r\n        }\r\n\r\n        #chat-box::-webkit-scrollbar-thumb {\r\n            background: #555;\r\n            border-radius: 4px;\r\n        }\r\n\r\n        .msg {\r\n            padding: 12px;\r\n            margin: 8px 0;\r\n            border-radius: 6px;\r\n            background: #333;\r\n            max-width: 85%;\r\n            word-wrap: break-word;\r\n        }\r\n\r\n        .user {\r\n            background: #005f3b;\r\n            /* Anchor Green */\r\n            color: white;\r\n            align-self: flex-end;\r\n            margin-left: auto;\r\n        }\r\n\r\n        .assistant {\r\n            background: #2d2d2d;\r\n            border-left: 3px solid #00ff88;\r\n        }\r\n\r\n        .msg details {\r\n            margin-top: 10px;\r\n            font-size: 0.85rem;\r\n            opacity: 0.7;\r\n            border: none;\r\n        }\r\n\r\n        .msg pre {\r\n            background: #151515;\r\n            padding: 8px;\r\n            border-radius: 4px;\r\n            overflow-x: auto;\r\n            font-size: 0.8rem;\r\n        }\r\n\r\n        h3 {\r\n            margin: 0 0 10px 0;\r\n        }\r\n\r\n        #status-text {\r\n            font-size: 0.9rem;\r\n            color: #888;\r\n            margin-bottom: 8px;\r\n        }\r\n\r\n        #progress-bar {\r\n            height: 4px;\r\n            background: #333;\r\n            border-radius: 2px;\r\n            overflow: hidden;\r\n            margin: 8px 0;\r\n        }\r\n\r\n        #progress {\r\n            height: 100%;\r\n            background: #00ff88;\r\n            width: 0%;\r\n            transition: width 0.2s;\r\n        }\r\n\r\n        #status-log {\r\n            flex: 1;\r\n            overflow-y: auto;\r\n            font-family: 'Consolas', monospace;\r\n            font-size: 0.75rem;\r\n            margin-top: 10px;\r\n            padding: 8px;\r\n            background: #0f0f0f;\r\n            border-radius: 4px;\r\n            border: 1px solid #333;\r\n        }\r\n\r\n        #status-log div {\r\n            margin: 2px 0;\r\n            padding: 2px 0;\r\n        }\r\n\r\n        .info {\r\n            color: #888;\r\n        }\r\n\r\n        .warn {\r\n            color: #ffc107;\r\n        }\r\n\r\n        .error {\r\n            color: #ff4444;\r\n        }\r\n\r\n        .success {\r\n            color: #00ff88;\r\n        }\r\n\r\n        #input-area {\r\n            display: flex;\r\n            gap: 10px;\r\n        }\r\n\r\n        textarea {\r\n            flex: 1;\r\n            height: 60px;\r\n            background: #3c3c3c;\r\n            border: 1px solid #555;\r\n            color: white;\r\n            padding: 10px;\r\n            border-radius: 4px;\r\n            resize: vertical;\r\n            font-family: 'Segoe UI', sans-serif;\r\n        }\r\n\r\n        textarea:focus {\r\n            outline: none;\r\n            border-color: #00ff88;\r\n        }\r\n\r\n        button {\r\n            padding: 8px 20px;\r\n            background: #2d2d2d;\r\n            color: white;\r\n            border: 1px solid #444;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            font-weight: bold;\r\n            align-self: flex-end;\r\n            transition: all 0.2s;\r\n        }\r\n\r\n        button:hover {\r\n            border-color: #00ff88;\r\n            color: #00ff88;\r\n        }\r\n\r\n        button:disabled {\r\n            background: #222;\r\n            border-color: #333;\r\n            color: #555;\r\n            cursor: not-allowed;\r\n        }\r\n\r\n        .streaming {\r\n            border-right: 2px solid #00ff88;\r\n            animation: blink 0.7s infinite;\r\n        }\r\n\r\n        @keyframes blink {\r\n            50% {\r\n                border-color: transparent;\r\n            }\r\n        }\r\n\r\n        /* --- Drag & Drop --- */\r\n        #input-area.drag-active {\r\n            border: 2px dashed #00ff88;\r\n            background: #1a1a1a;\r\n        }\r\n\r\n        #image-preview-container {\r\n            display: none;\r\n            /* Hidden by default */\r\n            margin-bottom: 5px;\r\n            padding: 8px;\r\n            background: #252526;\r\n            border-radius: 6px;\r\n            border: 1px solid #444;\r\n            position: relative;\r\n            width: fit-content;\r\n        }\r\n\r\n        #image-preview-container .image-preview {\r\n            display: flex;\r\n            flex-direction: column;\r\n            align-items: center;\r\n            gap: 5px;\r\n        }\r\n\r\n        #image-preview-container img {\r\n            max-height: 100px;\r\n            max-width: 200px;\r\n            border-radius: 4px;\r\n            display: block;\r\n            object-fit: contain;\r\n        }\r\n\r\n        #image-preview-container div {\r\n            font-size: small;\r\n            margin-top: 5px;\r\n        }\r\n\r\n        #image-preview-container button {\r\n            margin-top: 5px;\r\n            padding: 4px 8px;\r\n            background: #a43131;\r\n            color: white;\r\n            border: none;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            font-size: 12px;\r\n        }\r\n\r\n        #input-area.drag-active {\r\n            border: 2px dashed #00ff88;\r\n            background: #1a1a1a;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <div id=\"container\">\r\n        <div id=\"sidebar\">\r\n            <div\r\n                style=\"padding: 10px; font-weight: bold; font-size: 16px; border-bottom: 1px solid #444; color: #00ff88;\">\r\n                âš¡ ANCHOR CONSOLE\r\n            </div>\r\n            <small id=\"status-text\" style=\"padding: 0 10px;\">Initializing...</small>\r\n            <div id=\"progress-bar\" style=\"margin: 8px 10px 15px 10px;\">\r\n                <div id=\"progress\"></div>\r\n            </div>\r\n\r\n            <!-- COLLAPSIBLE: Model Selection -->\r\n            <details open>\r\n                <summary>Model Selection</summary>\r\n                <div class=\"panel-content\">\r\n                    <label for=\"hw-profile\"\r\n                        style=\"display:block; font-size: 11px; color: #ccc; margin-bottom: 4px;\">Hardware\r\n                        Profile:</label>\r\n                    <select id=\"hw-profile\" class=\"form-control\"\r\n                        style=\"width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555; margin-bottom: 10px;\">\r\n                        <option value=\"lite\">ðŸ”‹ Lite (Mobile / Snapdragon) - 2k Context</option>\r\n                        <option value=\"mid\" selected>ðŸ’» Mid (8GB VRAM) - 4k Context</option>\r\n                        <option value=\"high\">ðŸš€ High (16GB VRAM) - 16k Context</option>\r\n                        <option value=\"ultra\">âš¡ Ultra (24GB+ VRAM) - 32k Context</option>\r\n                    </select>\r\n\r\n                    <div style=\"margin-bottom: 10px; text-align: right;\">\r\n                        <button class=\"btn btn-secondary\" onclick=\"clearModelCache()\"\r\n                            style=\"padding: 4px 8px; font-size: 0.8rem; background: #a43131; border: none;\">ðŸ—‘ï¸ Clear\r\n                            Cache</button>\r\n                    </div>\r\n\r\n                    <select id=\"model-select\" disabled\r\n                        style=\"width: 100%; padding: 5px; background: #333; color: white; border: 1px solid #555;\">\r\n                        <option value=\"\" disabled>-- Select a Model --</option>\r\n\r\n                        <optgroup label=\"âœ¨ SOTA (Latest)\">\r\n                            <option value=\"mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\">DeepSeek R1 (7B Distill)\r\n                                [Verified]</option>\r\n                            <option value=\"mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\">DeepSeek R1 (8B Llama\r\n                                Distill) [Verified]</option>\r\n                            <option value=\"mlc-ai/Qwen3-4B-q4f16_1-MLC\">Qwen 3 4B (Base) [Verified]</option>\r\n                            <option value=\"mlc-ai/Qwen3-8B-q4f16_1-MLC\">Qwen 3 8B (Base) [Verified]</option>\r\n                            <option value=\"mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC\" selected>Qwen 2.5 7B (Instruct)\r\n                                [Verified]</option>\r\n                            <option value=\"mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC\">Phi 3.5 Mini (3.8B)</option>\r\n                        </optgroup>\r\n\r\n                        <optgroup label=\"ðŸ‘ï¸ Vision Models\">\r\n                            <option value=\"mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC\">Phi 3.5 Vision (4.2B)\r\n                                [Multimodal]</option>\r\n                            <option value=\"mlc-ai/Llama-3.2-11B-Vision-Instruct-q4f16_1-MLC\">Llama 3.2 11B Vision\r\n                                [Multimodal]</option>\r\n                            <option value=\"mlc-ai/Llama-3.2-90B-Vision-Instruct-q3f16_1-MLC\">Llama 3.2 90B Vision\r\n                                [Multimodal]</option>\r\n                        </optgroup>\r\n\r\n                        <optgroup label=\"ðŸ’» Code Specialists\">\r\n                            <option value=\"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\">Qwen 2.5 Coder 1.5B [Neural\r\n                                Terminal]</option>\r\n                            <option value=\"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\">Qwen 2.5 Coder 7B [Advanced]\r\n                            </option>\r\n                        </optgroup>\r\n\r\n                        <optgroup label=\"ðŸŒŸ Large Models\">\r\n                            <option value=\"mlc-ai/Llama-3-70B-Instruct-q3f16_1-MLC\">Llama 3 70B (Instruct)</option>\r\n                            <option value=\"mlc-ai/Llama-3.1-70B-Instruct-q3f16_1-MLC\">Llama 3.1 70B (Instruct)</option>\r\n                        </optgroup>\r\n\r\n                        <optgroup label=\"ðŸš€ High Performance (Small)\">\r\n                            <option value=\"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\">Qwen 2.5 1.5B (Instruct) [Lite\r\n                                Choice]</option>\r\n                            <option value=\"mlc-ai/SmolLM2-1.7B-Instruct-q4f16_1-MLC\">SmolLM2 1.7B (Instruct)</option>\r\n                            <option value=\"mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC\">Llama 3.2 1B (Instruct)</option>\r\n                            <option value=\"mlc-ai/Qwen3-0.6B-q4f16_1-MLC\">Qwen 3 0.6B (Base) [Micro]</option>\r\n                        </optgroup>\r\n\r\n                        <optgroup label=\"ðŸ§ª Experimental / Other\">\r\n                            <option value=\"mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC\">Llama 3.2 3B</option>\r\n                            <option value=\"mlc-ai/Llama-3.1-8B-Instruct-q4f16_1-MLC\">Llama 3.1 8B</option>\r\n                            <option value=\"mlc-ai/gemma-2-2b-it-q4f16_1-MLC\">Gemma 2 2B</option>\r\n                            <option value=\"mlc-ai/gemma-2-9b-it-q4f16_1-MLC\">Gemma 2 9B</option>\r\n                            <option value=\"mlc-ai/gemma-3-2b-it-q4f16_1-MLC\">Gemma 3 2B</option>\r\n                            <option value=\"mlc-ai/TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC\">TinyLlama 1.1B</option>\r\n                            <option value=\"mlc-ai/Qwen2-7B-Instruct-q4f16_1-MLC\">Qwen 2 7B</option>\r\n                            <option value=\"mlc-ai/Mistral-7B-Instruct-v0.3-q4f16_1-MLC\">Mistral 7B v0.3</option>\r\n                            <option value=\"mlc-ai/Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC\">Hermes 2 Pro Llama 3 8B</option>\r\n                            <option value=\"mlc-ai/Hermes-3-Llama-3.1-8B-q4f16_1-MLC\">Hermes 3 Llama 3.1 8B</option>\r\n                        </optgroup>\r\n\r\n                        <option value=\"custom\">-- Custom --</option>\r\n                    </select>\r\n                    <input id=\"custom-model-input\" type=\"text\" placeholder=\"Custom model ID\"\r\n                        style=\"width: 100%; padding: 5px; font-size: 11px;\" disabled />\r\n                    <button id=\"load-model-btn\" disabled\r\n                        style=\"width: 100%; padding: 6px; margin-top:5px; font-size: 11px;\">Load Model</button>\r\n                    <div style=\"font-size: 9px; color: #888; margin-top: 5px;\">\r\n                        * Models marked with * may require checking availability in MLC-AI repository\r\n                    </div>\r\n                </div>\r\n            </details>\r\n\r\n            <!-- COLLAPSIBLE: Controls -->\r\n            <details>\r\n                <summary>System Controls</summary>\r\n                <div class=\"panel-content\">\r\n                    <div\r\n                        style=\"background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px;\">\r\n                        <input type=\"checkbox\" id=\"enable-bridge-toggle\" onchange=\"toggleBridge(this.checked)\">\r\n                        <label for=\"enable-bridge-toggle\" style=\"font-size: 11px; cursor: pointer;\">Enable Bridge Connection</label>\r\n                        <div id=\"bridge-status\" style=\"font-size: 10px; color: #666; margin-left: 20px;\">Disconnected\r\n                        </div>\r\n                    </div>\r\n                    <div\r\n                        style=\"background: #222; padding: 4px; margin-bottom: 4px; border: 1px solid #444; border-radius: 4px; display: flex; align-items: center; gap: 8px;\">\r\n                        <input type=\"checkbox\" id=\"autosave-toggle\" checked onchange=\"toggleAutoSave(this.checked)\">\r\n                        <label for=\"autosave-toggle\" style=\"font-size: 11px; cursor: pointer;\">ðŸ’¾ Auto-Save\r\n                            Memories</label>\r\n                    </div>\r\n                    <button id=\"clear-cache-btn\"\r\n                        style=\"width: 100%; padding: 6px; background: #a43131; border:none; margin-top:5px; font-size: 11px;\">âš ï¸\r\n                        Delete Model Cache</button>\r\n                    <button id=\"debug-gpu-btn\"\r\n                        style=\"width: 100%; padding: 6px; background: #555; border:none; margin-top: 5px; font-size: 11px;\">â“\r\n                        Debug GPU</button>\r\n                    <button id=\"force-unlock-btn\"\r\n                        style=\"width: 100%; padding: 6px; background: #da3633; border:none; margin-top: 5px; font-size: 11px; color: white;\">ðŸ”“\r\n                        Force Unlock GPU</button>\r\n                </div>\r\n            </details>\r\n\r\n            <!-- COLLAPSIBLE: Logs -->\r\n            <details open style=\"flex: 1; display: flex; flex-direction: column;\">\r\n                <summary>System Logs</summary>\r\n                <div class=\"panel-content\" style=\"flex: 1; display: flex; flex-direction: column; overflow: hidden;\">\r\n                    <button id=\"copy-logs-btn\" style=\"width: 100%; padding: 4px; background: #333; font-size: 10px;\">ðŸ“‹\r\n                        Copy Logs</button>\r\n                    <div id=\"status-log\"></div>\r\n                </div>\r\n            </details>\r\n        </div>\r\n\r\n        <div id=\"resizer\"></div>\r\n\r\n        <div id=\"main\">\r\n            <!-- Split view for Chat and Context -->\r\n            <div style=\"flex: 1; display: flex; height: 100%; overflow: hidden;\">\r\n                <!-- Chat Column -->\r\n                <div style=\"flex: 1; display: flex; flex-direction: column; border-right: 1px solid #333;\">\r\n                    <div style=\"padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;\">\r\n                        ðŸ’¬ Chat Stream</div>\r\n                    <div id=\"chat-box\"></div>\r\n\r\n                    <!-- Input Area with Image Upload Button -->\r\n                    <div id=\"input-container\"\r\n                        style=\"border-top: 1px solid #333; padding: 10px; display: flex; flex-direction: column;\">\r\n                        <div id=\"image-preview-container\"></div>\r\n                        <div id=\"input-area\"\r\n                            style=\"display: flex; gap: 10px; padding: 5px; border: 2px dashed transparent; border-radius: 6px; transition: all 0.2s;\">\r\n                            <button id=\"image-upload-btn\" type=\"button\"\r\n                                style=\"padding: 8px; background: #2d2d2d; color: #888; border: 1px solid #444; border-radius: 4px; cursor: pointer; font-size: 16px;\"\r\n                                title=\"Upload Image\">ðŸ“Ž</button>\r\n                            <textarea id=\"input\" disabled placeholder=\"Type your message...\"></textarea>\r\n                            <button id=\"send-btn\" disabled>Send</button>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n\r\n                <!-- Context Column -->\r\n                <div style=\"width: 40%; display: flex; flex-direction: column; background: #151515;\">\r\n                    <div style=\"padding: 10px; background: #252526; font-weight: bold; border-bottom: 1px solid #333;\">\r\n                        ðŸ§  Root Memory</div>\r\n                    <div id=\"context-box\"\r\n                        style=\"flex: 1; overflow-y: auto; padding: 10px; font-family: monospace; font-size: 12px; white-space: pre-wrap; color: #aaa;\">\r\n                        <div style=\"text-align: center; margin-top: 50px; color: #555;\">(Active retrievals will appear\r\n                            here)</div>\r\n                    </div>\r\n                </div>\r\n            </div>\r\n        </div>\r\n    </div>\r\n\r\n    <script type=\"module\">\r\n        // --- CACHE OVERRIDE: Prevent Cache API usage that causes \"Tracking Prevention\" errors ---\r\n        // This is critical for WebLLM to work in environments with strict cache policies\r\n        if ('caches' in window) {\r\n            try {\r\n                // Override the Cache API to prevent WebLLM from using it\r\n                const originalAdd = Cache.prototype.add;\r\n                const originalAddAll = Cache.prototype.addAll;\r\n                const originalPut = Cache.prototype.put;\r\n\r\n                Cache.prototype.add = function(request) {\r\n                    console.warn(\"Cache.add blocked by Root Coda security override\");\r\n                    return Promise.resolve();\r\n                };\r\n\r\n                Cache.prototype.addAll = function(requests) {\r\n                    console.warn(\"Cache.addAll blocked by Root Coda security override\");\r\n                    return Promise.resolve();\r\n                };\r\n\r\n                Cache.prototype.put = function(request, response) {\r\n                    console.warn(\"Cache.put blocked by Root Coda security override\");\r\n                    return Promise.resolve();\r\n                };\r\n\r\n                console.log(\"ðŸ›¡ï¸ Cache API overridden to prevent tracking prevention errors\");\r\n            } catch (e) {\r\n                console.warn(\"Could not override Cache API:\", e);\r\n            }\r\n        }\r\n\r\n        // --- IMPORTS ---\r\n        import { AnchorLogger, createStore, getWebGPUConfig, initCozo, GPUController } from './modules/anchor.js';\r\n        import { MemoryAuditor } from './modules/agents.js';\r\n\r\n        import { CozoDb } from './cozo_lib_wasm.js';\r\n        import { loadAllFromIndexedDb, writeToIndexedDb } from './indexeddb.js';\r\n        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';\r\n        import { CreateWebWorkerMLCEngine } from \"https://esm.run/@mlc-ai/web-llm\";\r\n        import { marked } from \"https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js\";\r\n        import { VisionController } from './modules/vision.js';\r\n\r\n        env.allowLocalModels = false;\r\n\r\n        // --- ROOT KERNEL SETUP ---\r\n        const logger = new AnchorLogger('Root-Console');\r\n        const auditor = new MemoryAuditor();\r\n\r\n        const { state, subscribe } = createStore({\r\n            status: \"Initializing...\",\r\n            progress: 0,\r\n            activeModel: null,\r\n            autoSave: true\r\n        });\r\n\r\n        // --- UTILS: The Archivist ---\r\n        async function saveTurn(role, content) {\r\n            if (!state.autoSave || !window.db) return;\r\n\r\n            // 1. Prepare Candidate\r\n            const ts = Date.now();\r\n            const id = 'msg_' + ts + '_' + role;\r\n            const candidate = { id, timestamp: ts, role, content, source: 'root_console' };\r\n\r\n            // 2. THE AUDIT\r\n            const audit = auditor.audit(candidate);\r\n            if (!audit.passed) {\r\n                ui.log(`ðŸ›‘ Memory Rejected: ${audit.reason}`, \"error\");\r\n                return; // Stop. Do not write to DB.\r\n            }\r\n\r\n            try {\r\n                // 3. Commit (Only if passed)\r\n                const query = \":put memory { id: $id, timestamp: $ts, role: $role, content: $content, source: 'root_console', embedding: null }\";\r\n                await window.db.run(query, JSON.stringify({ id, ts, role, content }));\r\n                ui.log(`ðŸ’¾ Memory Saved (${role})`, \"debug\");\r\n            } catch (e) {\r\n                ui.log(`Save Failed: ${e.message}`, \"error\");\r\n            }\r\n        }\r\n\r\n        // Expose toggle handler globally\r\n        window.toggleAutoSave = (checked) => {\r\n            state.autoSave = checked;\r\n            ui.log(`Auto-Save: ${checked ? 'ON' : 'OFF'}`, 'info');\r\n        };\r\n\r\n        // Bridge legacy UI logging to Sovereign Logger\r\n        const ui = {\r\n            log: (msg, type = 'info') => {\r\n                // Bridge to Kernel Logger\r\n                if (type === 'debug') type = 'info';\r\n                if (logger[type]) logger[type](msg); else logger.info(msg);\r\n\r\n                // Update on-screen log\r\n                const el = document.getElementById('status-log');\r\n                if (el) {\r\n                    const div = document.createElement('div');\r\n                    div.className = type;\r\n                    div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;\r\n                    div.style.fontSize = '11px';\r\n                    div.style.padding = '2px';\r\n                    div.style.borderBottom = '1px solid #222';\r\n                    el.insertBefore(div, el.firstChild);\r\n                    if (el.children.length > 100) el.removeChild(el.lastChild);\r\n                }\r\n            },\r\n            updateProgress: (pct, text) => {\r\n                const bar = document.getElementById('progress');\r\n                if (bar) bar.style.width = (pct * 100) + \"%\";\r\n                if (text) {\r\n                    const el = document.getElementById('status-text');\r\n                    if (el) el.innerText = text;\r\n                }\r\n            },\r\n            append: (role, text, meta = null) => {\r\n                const box = document.getElementById('chat-box');\r\n                const div = document.createElement('div');\r\n                div.className = `msg ${role}`;\r\n                div.innerHTML = text ? marked.parse(text) : \"\";\r\n\r\n                if (meta && meta.trace && meta.trace.length > 0) {\r\n                    const details = document.createElement('details');\r\n                    const summary = document.createElement('summary');\r\n                    summary.textContent = 'ðŸ“‹ Reasoning Trace';\r\n                    const pre = document.createElement('pre');\r\n                    pre.textContent = JSON.stringify(meta.trace, null, 2);\r\n                    details.appendChild(summary);\r\n                    details.appendChild(pre);\r\n                    div.appendChild(details);\r\n                }\r\n\r\n                box.appendChild(div);\r\n                box.scrollTop = box.scrollHeight;\r\n\r\n                return {\r\n                    div,\r\n                    update: (newText, isMarkdown = true) => {\r\n                        div.innerHTML = isMarkdown ? marked.parse(newText) : newText;\r\n                        box.scrollTop = box.scrollHeight;\r\n                    },\r\n                    appendText: (chunk) => {\r\n                        if (div.innerText.endsWith(\"...\")) div.innerText = div.innerText.slice(0, -3);\r\n                        div.innerText += chunk;\r\n                        box.scrollTop = box.scrollHeight;\r\n                    }\r\n                };\r\n            },\r\n            appendContext: (title, details) => {\r\n                const box = document.getElementById('context-box');\r\n                const div = document.createElement('div');\r\n                div.style.borderBottom = '1px solid #333';\r\n                div.style.marginBottom = '10px';\r\n                div.style.paddingBottom = '10px';\r\n\r\n                const h4 = document.createElement('div');\r\n                h4.style.fontWeight = 'bold';\r\n                h4.style.color = '#00ff88';\r\n                h4.style.marginBottom = '5px';\r\n                h4.textContent = `[${new Date().toLocaleTimeString()}] ${title}`;\r\n\r\n                const p = document.createElement('div');\r\n                p.style.whiteSpace = 'pre-wrap';\r\n                p.textContent = details;\r\n\r\n                div.appendChild(h4);\r\n                div.appendChild(p);\r\n                box.appendChild(div);\r\n                box.scrollTop = box.scrollHeight;\r\n            }\r\n        };\r\n\r\n        // --- GLOBAL STATE ---\r\n        let db;\r\n        let embedder;\r\n        let engine;\r\n        let contextManager;\r\n        let selectedModelId = null;\r\n\r\n        // Custom App Config for Qwen-Coder and other new models not in the library default\r\n        const appConfig = {\r\n            model_list: [\r\n                {\r\n                    \"model\": \"https://huggingface.co/mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n                    \"model_id\": \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n                    \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2.5-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                    \"overrides\": { \"context_window_size\": 4096 }\r\n                },\r\n                {\r\n                    \"model\": \"https://huggingface.co/mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n                    \"model_id\": \"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n                    \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                    \"overrides\": { \"context_window_size\": 4096 }\r\n                }\r\n            ],\r\n            useIndexedDBCache: true\r\n        };\r\n\r\n        // ... (Keep existing VisionController)\r\n\r\n        // VisionController will handle all image-related functionality\r\n\r\n        // --- UTILS: Response Pattern Matcher ---\r\n        class ResponsePattern {\r\n            static match(response) {\r\n                let data = response;\r\n                if (typeof response === 'string') {\r\n                    try {\r\n                        const clean = response.replace(/```json/g, '').replace(/```/g, '').trim();\r\n                        if (clean.startsWith('{')) data = JSON.parse(clean);\r\n                    } catch (e) { }\r\n                }\r\n                if (data?.rows && Array.isArray(data.rows)) return { type: 'DB_RESULT', rows: data.rows, count: data.rows.length };\r\n                if (data?.error || (typeof response === 'string' && response.toLowerCase().includes('error:'))) return { type: 'ERROR', error: data?.error || response };\r\n                if (data?.ok === false && data?.message) return { type: 'ERROR', error: data.message };\r\n                return { type: 'RAW_TEXT', text: typeof response === 'string' ? response : JSON.stringify(response) };\r\n            }\r\n        }\r\n\r\n        // --- LOGIC: Context Manager (SFS) ---\r\n        class ContextManager {\r\n            constructor(engine, db) {\r\n                this.engine = engine;\r\n                this.db = db;\r\n                this.maxIterations = 3;\r\n            }\r\n\r\n            // NEW: Pure retrieval logic (returns raw data objects)\r\n            async findRelevantMemories(userText) {\r\n                // Initialize combined results map for deduplication\r\n                const combined = new Map();\r\n\r\n                // 1. GENERATE QUERY VECTOR (Semantic)\r\n                let vectorResults = [];\r\n                if (embedder) {\r\n                    try {\r\n                        const output = await embedder(userText, { pooling: 'mean', normalize: true });\r\n                        const queryVec = Array.from(output.data);\r\n                        // Query: Find top 10 nearest neighbors\r\n                        const vecQuery = `\r\n                            ?[id, content, dist, timestamp] := *memory{id: id, content: content, embedding: embedding, timestamp: timestamp},\r\n                            !is_null(embedding),\r\n                            dist = vec_l2(embedding, $vec)\r\n                            :sort dist\r\n                            :limit 10\r\n                        `;\r\n                        const res = await this.db.run(vecQuery, JSON.stringify({ vec: queryVec }));\r\n                        const parsed = ResponsePattern.match(res);\r\n                        if (parsed.rows) {\r\n                            parsed.rows.forEach(r => {\r\n                                const item = { id: r[0], content: r[1], dist: r[2], ts: r[3], source: 'semantic' };\r\n                                combined.set(item.content, item); // Add semantic results first\r\n                            });\r\n                        }\r\n                    } catch (e) { console.warn(\"Vector Search failed\", e); }\r\n                }\r\n\r\n                // 2. BM25 SEARCH (Lexical) - REPLACES REGEX SEARCH\r\n                // This replaces the manual \"Regex\" loop with CozoDB's native FTS using BM25\r\n                try {\r\n                    // Cozo FTS Syntax: ~index_name { yield_fields | query: $q }\r\n                    const ftsQuery = `\r\n                        ?[id, content, score, timestamp] := ~memory:content_fts{\r\n                            id, content, timestamp |\r\n                            query: $q,\r\n                            k: 20  # Get top 20 matches by BM25 score\r\n                        }\r\n                        :order -score\r\n                    `;\r\n\r\n                    // Pass the raw user text. BM25 handles stop words and importance automatically.\r\n                    const res = await this.db.run(ftsQuery, JSON.stringify({ q: userText }));\r\n\r\n                    if (res.rows) {\r\n                         res.rows.forEach(r => {\r\n                            // Use BM25 score for ranking\r\n                            const item = {\r\n                                id: r[0],\r\n                                content: r[1],\r\n                                score: r[2],\r\n                                ts: r[3],\r\n                                source: 'BM25'\r\n                            };\r\n\r\n                            // Deduplicate: If Vector found it, maybe boost score?\r\n                            if (!combined.has(item.content)) {\r\n                                combined.set(item.content, item);\r\n                            }\r\n                        });\r\n                    }\r\n                } catch (e) {\r\n                    // Fallback if index missing\r\n                    console.warn(\"BM25 Search failed (Index missing?):\", e);\r\n\r\n                    // Fallback to original regex approach if BM25 index doesn't exist\r\n                    const rawWords = userText.match(/[a-zA-Z0-9_\\-]+/g) || [];\r\n                    const stopWords = new Set(['the', 'and', 'is', 'in', 'at', 'of', 'on', 'for', 'to', 'it', 'this', 'that', 'what', 'who', 'how', 'why', 'when', 'where', 'tell', 'me', 'about', 'could', 'would']);\r\n\r\n                    // Score words: Capitalized or Numbers get higher priority\r\n                    const scoredWords = rawWords.map(w => {\r\n                        let score = 0;\r\n                        const clean = w.replace(/^[-_]+|[-_]+$/g, '');\r\n                        if (clean.length <= 3 && !/^\\d+$/.test(clean)) return null; // Skip short words unless numbers\r\n                        if (stopWords.has(clean.toLowerCase())) return null;\r\n\r\n                        // Priority Scoring\r\n                        if (/^[A-Z]/.test(clean)) score += 2; // Capitalized (Proper Noun)\r\n                        if (/^\\d+$/.test(clean)) score += 3;  // Numbers (Dates/Years)\r\n\r\n                        return { word: clean, score };\r\n                    }).filter(w => w);\r\n\r\n                    // Sort by score (descending) and take top 5\r\n                    scoredWords.sort((a, b) => b.score - a.score);\r\n                    const keywords = scoredWords.slice(0, 5).map(w => w.word);\r\n\r\n                    if (keywords.length > 0) {\r\n                        // Process top 5 keywords for better accuracy\r\n                        for (const keyword of keywords) {\r\n                            const kwQuery = `?[id, content, timestamp] := *memory{id: id, content: content, timestamp: timestamp}, regex_matches(content, '(?i)${keyword}') :sort -timestamp :limit 5`;\r\n                            try {\r\n                                const res = await this.db.run(kwQuery, \"{}\");\r\n                                const parsed = ResponsePattern.match(res);\r\n                                if (parsed.rows) {\r\n                                    parsed.rows.forEach(r => {\r\n                                        const item = { id: r[0], content: r[1], dist: 0, ts: r[2], source: 'lexical' };\r\n                                        if (!combined.has(item.content)) {\r\n                                            combined.set(item.content, item); // Add only if not already present (deduplicate)\r\n                                        }\r\n                                    });\r\n                                }\r\n                            } catch (e) { console.warn(`Keyword search failed for '${keyword}':`, e); }\r\n                        }\r\n                    }\r\n                }\r\n\r\n                // Return top 15 unique items\r\n                return Array.from(combined.values()).slice(0, 15);\r\n            }\r\n\r\n            // EXISTING: Formats the data for the R1 Reasoning Loop\r\n            async retrieveInitialContext(userText) {\r\n                const topItems = await this.findRelevantMemories(userText);\r\n                if (topItems.length === 0) return \"\";\r\n\r\n                const paths = [];\r\n                const clues = topItems.map((item, index) => {\r\n                    const id = index + 1;\r\n                    let title = \"doc_\" + id;\r\n                    // Simple title extraction\r\n                    if (item.content.length > 0) {\r\n                        const safeTitle = item.content.substring(0, 30).replace(/[^a-zA-Z0-9 ]/g, '').trim().replace(/\\s+/g, '_').toLowerCase();\r\n                        if (safeTitle.length > 3) title = safeTitle;\r\n                    }\r\n                    const path = `/knowledge/${item.source}/${title}`;\r\n                    paths.push(path);\r\n\r\n                    let snippet = item.content.substring(0, 300).replace(/\\n/g, ' ');\r\n                    if (item.content.length > 300) snippet += \"...\";\r\n\r\n                    return `[CLUE #${id}] [Path: ${path}] [Source: ${item.source || 'unknown'}]\\nSnippet: \"${snippet}\"`;\r\n                });\r\n\r\n                ui.log(`âœ… Hybrid Retrieval: ${topItems.length} active clues.`, \"success\");\r\n                const treeMap = `[CURRENT REALITY MAP]\\n` + paths.map(p => `- ${p}`).join('\\n');\r\n                return `${treeMap}\\n\\n[CONTEXT CLUES]\\n${clues.join('\\n\\n')}`;\r\n            }\r\n\r\n            // ... (Keep buildVirtualPrompt and executeR1Loop exactly as they were in combined_text.txt source 1976-1996)\r\n            buildVirtualPrompt(systemPrompt, retrievedMemories, chatHistory, userText) {\r\n                const now = new Date().toLocaleString('en-US', { timeZoneName: 'short' });\r\n                const physicsInstruction = `\r\n### COGNITIVE PROTOCOL:\r\n1. **TRUST THE CLUES:** You are operating in a \"Sovereign\" environment. The [CONTEXT CLUES] provided are the Ground Truth.\r\n2. **MINIMIZE ENTROPY:** If a Clue contains the answer, OUTPUT IT DIRECTLY. Do not generate \"filler\".\r\n3. **PATH OF LEAST ACTION:** The most efficient answer is the correct one.\r\n`;\r\n                return `### SYSTEM TIME\\n${now}\\n\\n### BACKGROUND INFORMATION\\n${retrievedMemories ? retrievedMemories : \"No specific context available.\"} \\n\\n### CHAT HISTORY\\n${chatHistory.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\\n')}\\n\\n### INSTRUCTION\\n${systemPrompt}\\n${physicsInstruction}\\nBased on the Background Information above, answer the following question.\\n\\n### USER QUESTION\\n${userText}`;\r\n            }\r\n\r\n            async executeR1Loop(userText, history) {\r\n                let iteration = 0;\r\n                let currentContext = await this.retrieveInitialContext(userText);\r\n                if (currentContext) ui.appendContext(\"Reflex Retrieval\", currentContext);\r\n                while (iteration < this.maxIterations) {\r\n                    iteration++;\r\n                    const fullPrompt = this.buildVirtualPrompt(\r\n                        \"You are Root Coda, a Sovereign Agent.\\nPROTOCOL: Trust clues. Answer directly. No refusals.\",\r\n                        currentContext,\r\n                        history.slice(-5),\r\n                        userText\r\n                    );\r\n                    ui.log(`ðŸ§  Reasoning (Iter ${iteration})...`, \"info\");\r\n                    let reply = \"\";\r\n                    try {\r\n                        const response = await this.engine.chat.completions.create({\r\n                            messages: [{ role: \"user\", content: fullPrompt }],\r\n                            temperature: 0.1,\r\n                            max_tokens: 100\r\n                        });\r\n                        reply = response.choices[0].message.content.trim();\r\n                    } catch (genErr) {\r\n                        const errMsg = genErr?.message || String(genErr);\r\n                        if (errMsg.includes(\"disposed\")) return { context: currentContext, finalAnswer: \"âš ï¸ System Crash: GPU Driver lost.\" };\r\n                        throw genErr;\r\n                    }\r\n\r\n                    if (!reply) reply = \"\";\r\n\r\n                    if (reply.includes(\"NEED_CONTEXT:\")) {\r\n                        const searchTerm = reply.split(\"NEED_CONTEXT:\")[1].trim();\r\n                        ui.log(`ðŸ¤– Requested search: \"${searchTerm}\"`, \"warn\");\r\n                        const extraData = await this.retrieveInitialContext(searchTerm);\r\n                        if (extraData) {\r\n                            currentContext += `\\n--- Additional (${searchTerm}) ---\\n${extraData}`;\r\n                            ui.appendContext(`Requested: ${searchTerm}`, extraData);\r\n                        }\r\n                        continue;\r\n                    }\r\n                    return { context: currentContext, finalAnswer: reply };\r\n                }\r\n                return { context: currentContext, finalAnswer: null };\r\n            }\r\n        }\r\n\r\n        // --- HANDLERS ---\r\n        async function handleSend() {\r\n            const input = document.getElementById('input');\r\n            const text = input.value.trim();\r\n\r\n            // Get image from VisionController\r\n            const imageBase64 = vision ? vision.getImage() : null;\r\n\r\n            // Allow sending if there is text OR an image\r\n            if ((!text && !imageBase64) || !engine) return;\r\n\r\n            input.value = \"\";\r\n            input.disabled = true;\r\n            document.getElementById('send-btn').disabled = true;\r\n\r\n            // Display Logic\r\n            if (imageBase64) {\r\n                ui.append(\"user\", `![Uploaded Image](${imageBase64})\\n\\n${text}`);\r\n            } else {\r\n                ui.append(\"user\", text);\r\n            }\r\n\r\n            if (state.autoSave) await saveTurn(\"user\", text + (imageBase64 ? \" [Image Attached]\" : \"\"));\r\n\r\n            try {\r\n                // Construct Message Payload\r\n                let messages = [];\r\n                let context = \"\";\r\n\r\n                // Wrap GPU-heavy ops in lock\r\n                await GPUController.withLock(\"Root-Console-Chat\", async () => {\r\n                    // If Image: Bypass R1 Loop (Vision models typically don't do R1 reasoning yet or complex context mixing)\r\n                    // We use a direct shot for now.\r\n                    if (imageBase64) {\r\n                        messages = [\r\n                            { role: \"system\", content: \"You are a helpful assistant. Analyze the user's image and text.\" },\r\n                            {\r\n                                role: \"user\",\r\n                                content: [\r\n                                    { type: \"text\", text: text || \"What is in this image?\" },\r\n                                    { type: \"image_url\", image_url: { url: imageBase64 } }\r\n                                ]\r\n                            }\r\n                        ];\r\n                        ui.log(\"ðŸ‘ï¸ Processing Visual Data...\", \"warn\");\r\n                    } else {\r\n                        // Standard Text Path (R1 Loop)\r\n                        const r1Result = await contextManager.executeR1Loop(text, []);\r\n                        context = r1Result.context;\r\n                        const finalAnswer = r1Result.finalAnswer;\r\n\r\n                        if (finalAnswer && finalAnswer.includes(\"System Crash\")) return ui.log(\"ðŸ›‘ Execution halted (GPU Crash).\", \"error\");\r\n\r\n                        ui.log(\"ðŸ§ª Synthesizing...\", \"warn\");\r\n                        const sysPrompt = `You are a helpful assistant with access to retrieved context. Use it to answer the user's question.\\n\\nCONTEXT:\\n${context || \"(No relevant context)\"}`;\r\n                        messages = [{ role: \"system\", content: sysPrompt }, { role: \"user\", content: text }];\r\n                    }\r\n\r\n                    const msgHandle = ui.append(\"assistant\", \"\");\r\n\r\n                    const stream = await engine.chat.completions.create({\r\n                        messages: messages,\r\n                        max_tokens: 1024,\r\n                        temperature: 0.7,\r\n                        stream: true\r\n                    });\r\n\r\n                    let fullAnswer = \"\";\r\n                    for await (const chunk of stream) {\r\n                        const delta = chunk.choices[0]?.delta?.content || \"\";\r\n                        fullAnswer += delta;\r\n                        msgHandle.update(fullAnswer);\r\n                    }\r\n\r\n                    // Cleanup - Clear the image from VisionController\r\n                    if (imageBase64 && vision) {\r\n                        vision.clear();\r\n                    }\r\n\r\n                    // --- NEW CODE START: Thought Separation ---\r\n                    let finalContent = fullAnswer;\r\n                    let thoughtContent = null;\r\n\r\n                    // Regex to capture DeepSeek/Reasoning  blocks<think> blocks<think> blocks\r\n                    const thinkMatch = fullAnswer.match(/<think>([\\s\\S]*?)<\\/think>/);\r\n\r\n                    if (thinkMatch) {\r\n                        thoughtContent = thinkMatch[1].trim();\r\n                        // Remove the thought block from the \"assistant\" memory to keep it clean\r\n                        finalContent = fullAnswer.replace(/<think>[\\s\\S]*?<\\/think>/, \"\").trim();\r\n                    }\r\n\r\n                    if (state.autoSave) {\r\n                        // 1. Save the Thought (Path)\r\n                        if (thoughtContent) {\r\n                            await saveTurn(\"thought\", thoughtContent);\r\n                        }\r\n                        // 2. Save the Answer (Result)\r\n                        // We explicitly save the cleaned content as the assistant's official reply\r\n                        await saveTurn(\"assistant\", finalContent);\r\n                    }\r\n                    ui.log(\"âœ… Response generated.\", \"success\");\r\n                }); // End Lock\r\n\r\n            } catch (e) {\r\n                const msg = e.message || String(e) || \"Unknown Error\";\r\n                ui.log(`Error: ${msg}`, \"error\");\r\n                ui.append(\"assistant\", `**Error:** ${msg}`);\r\n            } finally {\r\n                input.disabled = false;\r\n                document.getElementById('send-btn').disabled = false;\r\n                input.focus();\r\n            }\r\n        }\r\n\r\n        // ARCHIVED MODEL LOADING FUNCTION - KEPT FOR FUTURE REFERENCE\r\n        // This function was attempting to use local model files with bridge downloads\r\n        // which was causing hangs and issues. See new online-only implementation below.\r\n        async function loadModel_archived() {\r\n            const select = document.getElementById('model-select');\r\n            const customInput = document.getElementById('custom-model-input');\r\n            const modelInput = select.value === 'custom' ? customInput.value : select.value;\r\n            if (!modelInput) return alert(\"Please select a model.\");\r\n\r\n            selectedModelId = modelInput;\r\n            document.getElementById('load-model-btn').disabled = true;\r\n\r\n            try {\r\n                // 0. THE BLOCKER (Model Load Lock) - Serialize model loading\r\n                ui.log(\"â³ Queueing for Model Load (Sequential)...\", \"info\");\r\n\r\n                await GPUController.withModelLoadLock(\"Root-Console-Init\", async () => {\r\n                    ui.log(`Initializing Engine (${selectedModelId})...`, \"info\");\r\n\r\n                    // --- KERNEL: Hardware Config ---\r\n                    const hwProfile = document.getElementById('hw-profile').value;\r\n                    const gpuConfig = await getWebGPUConfig(hwProfile);\r\n\r\n                    if (gpuConfig.isConstrained) {\r\n                        ui.log(`âš ï¸ Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, \"warn\");\r\n                    } else {\r\n                        ui.log(`âœ… GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, \"success\");\r\n                    }\r\n\r\n                    // --- Config Generation ---\r\n                    // (Simplified Logic for cleaner file)\r\n                    const libBase = \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/\"; // Fixed URL\r\n                    let modelLib = null;\r\n                    const lowerId = selectedModelId.toLowerCase();\r\n                    let qTag = \"q4f16_1\"; // Default\r\n\r\n                    // Mapper\r\n                    // SOTA / New\r\n                    if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('deepseek-r1') && lowerId.includes('8b') && lowerId.includes('llama')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Vision Models\r\n                    else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n\r\n                    // Large Models (70B)\r\n                    else if (lowerId.includes('llama-3-70b')) modelLib = libBase + `Llama-3-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.1-70b')) modelLib = libBase + `Llama-3.1-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Vision Models\r\n                    else if (lowerId.includes('llama-3.2-11b-vision')) modelLib = libBase + `Llama-3.2-11B-Vision-Instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.2-90b-vision')) modelLib = libBase + `Llama-3.2-90B-Vision-Instruct-q3f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n\r\n                    // Other Popular Models\r\n                    else if (lowerId.includes('llama-3.1-8b')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('gemma-2-9b')) modelLib = libBase + `gemma-2-9b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('gemma-3-2b')) modelLib = libBase + `gemma-3-2b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('mistral-7b')) modelLib = libBase + `Mistral-7B-Instruct-v0.3-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('hermes-2-pro')) modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('hermes-3')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Small / Efficient\r\n                    else if (lowerId.includes('qwen3-0.6b')) modelLib = libBase + `Qwen3-0.6B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.2-1b')) modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;\r\n\r\n                    // Fallbacks\r\n                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B\r\n\r\n\r\n\r\n                    // 1. Clear previous engine if exists\r\n                    if (engine) {\r\n                        ui.log(\"Unloading previous engine...\", \"info\");\r\n                        await engine.unload();\r\n                        engine = null;\r\n                        ui.log(\"Engine Unloaded\", \"success\");\r\n                    }\r\n\r\n\r\n\r\n                    // Custom App Config for Qwen-Coder and other new models not in the library default\r\n                    const appConfig = {\r\n                        // Disable all caching to prevent \"Tracking Prevention\" issues\r\n                        useIndexedDBCache: false,\r\n                        cacheDir: null,\r\n                        // Additional cache-busting settings\r\n                        \"mlc.cache\": false,\r\n                        \"cache.enabled\": false,\r\n\r\n                        model_list: [\r\n                            {\r\n                                \"model\": window.location.origin + \"/models/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC/resolve/main/\",\r\n                                \"model_id\": \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n                                \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                                \"vram_required_MB\": 2000,\r\n                                \"low_resource_required\": true,\r\n                                \"buffer_size_required_bytes\": gpuConfig.maxBufferSize,\r\n                                \"overrides\": {\r\n                                    \"context_window_size\": gpuConfig.isConstrained ? 2048 : 4096\r\n                                },\r\n                                \"tokenizer_files\": [\r\n                                    \"tokenizer_config.json\",\r\n                                    \"vocab.json\",\r\n                                    \"merges.txt\",\r\n                                    \"tokenizer.json\"\r\n                                ]\r\n                            },\r\n                            {\r\n                                \"model\": window.location.origin + \"/models/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC/resolve/main/\",\r\n                                \"model_id\": \"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n                                \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                                \"vram_required_MB\": 2000,\r\n                                \"low_resource_required\": true,\r\n                                \"buffer_size_required_bytes\": gpuConfig.maxBufferSize,\r\n                                \"overrides\": {\r\n                                    \"context_window_size\": gpuConfig.isConstrained ? 2048 : 4096\r\n                                },\r\n                                \"tokenizer_files\": [\r\n                                    \"tokenizer_config.json\",\r\n                                    \"vocab.json\",\r\n                                    \"merges.txt\",\r\n                                    \"tokenizer.json\"\r\n                                ]\r\n                            },\r\n                            {\r\n                                \"model\": window.location.origin + \"/models/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC/resolve/main/\",\r\n                                \"model_id\": \"mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\r\n                                \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                                \"vram_required_MB\": 2000,\r\n                                \"low_resource_required\": true,\r\n                                \"buffer_size_required_bytes\": gpuConfig.maxBufferSize,\r\n                                \"overrides\": {\r\n                                    \"context_window_size\": gpuConfig.isConstrained ? 2048 : 4096\r\n                                }\r\n                            },\r\n                            {\r\n                                \"model\": window.location.origin + \"/models/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC/resolve/main/\",\r\n                                \"model_id\": \"mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\r\n                                \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                                \"vram_required_MB\": 2000,\r\n                                \"low_resource_required\": true,\r\n                                \"buffer_size_required_bytes\": gpuConfig.maxBufferSize,\r\n                                \"overrides\": {\r\n                                    \"context_window_size\": gpuConfig.isConstrained ? 2048 : 4096\r\n                                }\r\n                            },\r\n                            {\r\n                                \"model\": window.location.origin + \"/models/Qwen2.5-7B-Instruct-q4f16_1-MLC/resolve/main/\",\r\n                                \"model_id\": \"mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC\",\r\n                                \"model_lib\": \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                                \"vram_required_MB\": 2000,\r\n                                \"low_resource_required\": true,\r\n                                \"buffer_size_required_bytes\": gpuConfig.maxBufferSize,\r\n                                \"overrides\": {\r\n                                    \"context_window_size\": gpuConfig.isConstrained ? 2048 : 4096\r\n                                }\r\n                            }\r\n                        ]\r\n                    };\r\n\r\n                    // 2.5: CHECK AND DOWNLOAD MODEL LOCALLY IF NEEDED\r\n                    const strippedId = selectedModelId.split('/').pop(); // \"mlc-ai/foo\" -> \"foo\"\r\n                    // Sanitize the path to ensure it's safe for URL construction\r\n                    const safeStrippedId = strippedId.replace(/[^a-zA-Z0-9._-]/g, '_');\r\n                    const localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;\r\n                    const check = await fetch(localModelUrl, { method: 'HEAD' });\r\n\r\n                    if (!check.ok) {\r\n                        ui.log(`â¬‡ï¸ Model missing locally. Initiating On-Demand Pull...`, \"warn\");\r\n                        const pullRes = await fetch('/v1/models/pull', { // Use relative path to current server\r\n                            method: 'POST',\r\n                            headers: { 'Content-Type': 'application/json' },\r\n                            body: JSON.stringify({\r\n                                model_id: selectedModelId,\r\n                                url: `https://huggingface.co/${selectedModelId}`\r\n                            })\r\n                        });\r\n\r\n                        if (pullRes.ok) {\r\n                            ui.log(\"â³ Downloading Model (Server-Side)...\", \"info\");\r\n                            // Poll for completion\r\n                            while (true) {\r\n                                await new Promise(r => setTimeout(r, 2000));\r\n                                const statusRes = await fetch(`/v1/models/pull/status?id=${selectedModelId}`); // Use relative path to current server\r\n                                const status = await statusRes.json();\r\n                                if (status.status === \"done\") {\r\n                                    ui.log(\"âœ… Download Complete!\", \"success\");\r\n                                    break;\r\n                                } else if (status.status === \"error\") {\r\n                                    throw new Error(`Download Failed: ${status.error}`);\r\n                                } else {\r\n                                    ui.updateProgress(0.1, `Downloading: ${status.progress} (${status.file})`);\r\n                                }\r\n                            }\r\n                        } else {\r\n                            // NEW: Handle Pull Request Failure (e.g. 404/500)\r\n                            const errText = await pullRes.text();\r\n                            throw new Error(`Bridge refused download (${pullRes.status}): ${errText}. Is the Bridge updated?`);\r\n                        }\r\n                    }\r\n\r\n                    // 3. Initialize Engine with Custom Config\r\n                    ui.log(`Initializing Engine (${selectedModelId})...`, 'info');\r\n\r\n                    // Add the selected model to the config if it's not already in the appConfig model list\r\n                    // This ensures that any selected model is properly configured with cache disabled\r\n                    const modelExists = appConfig.model_list.some(model => model.model_id === selectedModelId);\r\n                    if (!modelExists) {\r\n                        // Add the selected model to the configuration with cache disabled\r\n                        // Use full URL format to match the existing appConfig structure with resolve/main pattern\r\n                        const strippedModelId = selectedModelId.split('/').pop(); // Extract just the model name part\r\n                        // Ensure the model path is properly formatted (no special characters that could break URL parsing)\r\n                        const safeModelPath = strippedModelId.replace(/[^a-zA-Z0-9._-]/g, '_'); // Sanitize path\r\n                        appConfig.model_list.push({\r\n                            model: window.location.origin + `/models/${safeModelPath}/resolve/main/`,\r\n                            model_id: selectedModelId,\r\n                            model_lib: modelLib,\r\n                            vram_required_MB: 2000,\r\n                            low_resource_required: true,\r\n                            buffer_size_required_bytes: gpuConfig.maxBufferSize,\r\n                            overrides: {\r\n                                context_window_size: gpuConfig.isConstrained ? 2048 : 4096\r\n                            }\r\n                        });\r\n                    }\r\n\r\n                    // Note: CreateWebWorkerMLCEngine merges this with default prebuiltAppConfig\r\n                    // Additional configuration to ensure no caching occurs\r\n                    try {\r\n                        engine = await CreateWebWorkerMLCEngine(\r\n                            new Worker('./modules/llm-worker.js', { type: 'module' }),\r\n                            selectedModelId,\r\n                            {\r\n                                initProgressCallback: (report) => {\r\n                                    ui.updateProgress(report.progress, report.text);\r\n                                    if (report.progress === 1) ui.log(\"âœ… Engine Ready\", \"success\");\r\n                                },\r\n                                appConfig: appConfig, // Pass our custom config here!\r\n                                // Additional cache-busting parameters\r\n                                logLevel: \"INFO\",\r\n                                // Force-disable cache mechanisms at the engine level\r\n                                useIndexedDBCache: false,\r\n                                // Additional parameters to ensure no caching\r\n                                \"mlc.cache\": false,\r\n                                \"cache.enabled\": false\r\n                            }\r\n                        );\r\n                    } catch (cacheError) {\r\n                        // If cache-related error occurs, try fallback approach\r\n                        if (cacheError.message && (cacheError.message.includes(\"Cache\") || cacheError.message.includes(\"cache\"))) {\r\n                            ui.log(\"ðŸ”„ Cache blocked, trying fallback approach...\", \"warn\");\r\n\r\n                            // Try with minimal configuration to bypass cache issues\r\n                            engine = await CreateWebWorkerMLCEngine(\r\n                                new Worker('./modules/llm-worker.js', { type: 'module' }),\r\n                                selectedModelId,\r\n                                {\r\n                                    initProgressCallback: (report) => {\r\n                                        ui.updateProgress(report.progress, report.text);\r\n                                        if (report.progress === 1) ui.log(\"âœ… Engine Ready\", \"success\");\r\n                                    },\r\n                                    // Minimal config to avoid cache operations\r\n                                    appConfig: {\r\n                                        useIndexedDBCache: false,\r\n                                        model_list: appConfig.model_list\r\n                                    },\r\n                                    logLevel: \"INFO\"\r\n                                }\r\n                            );\r\n                        } else {\r\n                            // If it's not a cache error, re-throw\r\n                            throw cacheError;\r\n                        }\r\n                    }\r\n                }); // Uses the default 5-minute timeout for model loading\r\n\r\n                ui.log(\"ðŸŽ‰ Root Console Online\", \"success\");\r\n                await initializeContextManager();\r\n                document.getElementById('input').disabled = false;\r\n                document.getElementById('send-btn').disabled = false;\r\n                document.getElementById('input').focus();\r\n\r\n            } catch (e) {\r\n                const errorMsg = e.message || String(e);\r\n                ui.log(`Load Failed: ${errorMsg}`, \"error\");\r\n\r\n                // Specific handling for cache-related errors\r\n                if (errorMsg && (errorMsg.includes(\"Cache\") || errorMsg.includes(\"cache\"))) {\r\n                    ui.log(`ðŸš¨ CACHE ERROR: The browser is blocking cache access.`, \"error\");\r\n                    ui.log(`ðŸ’¡ SOLUTION: Try using an Incognito/Private window, or check browser security settings.`, \"warn\");\r\n                }\r\n\r\n                // Provide suggestion for common model name issues\r\n                if (errorMsg && errorMsg.includes(\"Network response was not ok\")) {\r\n                    ui.log(`ðŸ’¡ Hint: Model may not exist or be temporarily unavailable. Try another model.`, \"warn\");\r\n                }\r\n                if (errorMsg && errorMsg.includes(\"QuotaExceededError\")) {\r\n                    ui.log(`ðŸ’¡ STORAGE FULL: Click 'ðŸ—‘ï¸ Clear Cache' to reset your browser storage.`, \"error\");\r\n                    // Auto-suggest cleanup\r\n                    if (confirm(\"Storage Quota Exceeded. Would you like to run the Deep Cleanup tool now?\")) {\r\n                        clearModelCache();\r\n                    }\r\n                }\r\n                document.getElementById('load-model-btn').disabled = false;\r\n            }\r\n        }\r\n\r\n        // NEW MODEL LOADING FUNCTION - ONLINE ONLY\r\n        // Based on the working anchor-mic.html implementation\r\n        async function loadModel() {\r\n            const select = document.getElementById('model-select');\r\n            const customInput = document.getElementById('custom-model-input');\r\n            const modelInput = select.value === 'custom' ? customInput.value : select.value;\r\n            if (!modelInput) return alert(\"Please select a model.\");\r\n\r\n            selectedModelId = modelInput;\r\n            document.getElementById('load-model-btn').disabled = true;\r\n\r\n            try {\r\n                // 0. THE BLOCKER (Model Load Lock) - Serialize model loading\r\n                ui.log(\"â³ Queueing for Model Load (Sequential)...\", \"info\");\r\n\r\n                await GPUController.withModelLoadLock(\"Root-Console-Init\", async () => {\r\n                    ui.log(`Initializing Engine (${selectedModelId})...`, \"info\");\r\n\r\n                    // --- KERNEL: Hardware Config ---\r\n                    const hwProfile = document.getElementById('hw-profile').value;\r\n                    const gpuConfig = await getWebGPUConfig(hwProfile);\r\n\r\n                    if (gpuConfig.isConstrained) {\r\n                        ui.log(`âš ï¸ Clamping WebGPU buffer to ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB`, \"warn\");\r\n                    } else {\r\n                        ui.log(`âœ… GPU Configured: ${Math.round(gpuConfig.maxBufferSize / 1024 / 1024)}MB Buffer`, \"success\");\r\n                    }\r\n\r\n                    // --- Config Generation ---\r\n                    // (Simplified Logic for cleaner file)\r\n                    const libBase = \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/\"; // Fixed URL\r\n                    let modelLib = null;\r\n                    const lowerId = selectedModelId.toLowerCase();\r\n                    let qTag = \"q4f16_1\"; // Default\r\n\r\n                    // Mapper\r\n                    // SOTA / New\r\n                    if (lowerId.includes('deepseek-r1') && lowerId.includes('7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('deepseek-r1') && lowerId.includes('8b') && lowerId.includes('llama')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen3-4b')) modelLib = libBase + `Qwen3-4B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen3-8b')) modelLib = libBase + `Qwen3-8B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2.5-7b')) modelLib = libBase + `Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('phi-3.5-mini')) modelLib = libBase + `Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Vision Models\r\n                    else if (lowerId.includes('phi-3.5-vision')) modelLib = libBase + `Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n\r\n                    // Large Models (70B)\r\n                    else if (lowerId.includes('llama-3-70b')) modelLib = libBase + `Llama-3-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.1-70b')) modelLib = libBase + `Llama-3.1-70B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Vision Models\r\n                    else if (lowerId.includes('llama-3.2-11b-vision')) modelLib = libBase + `Llama-3.2-11B-Vision-Instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.2-90b-vision')) modelLib = libBase + `Llama-3.2-90B-Vision-Instruct-q3f16_1-ctx4k_cs2k-webgpu.wasm`;\r\n\r\n                    // Other Popular Models\r\n                    else if (lowerId.includes('llama-3.1-8b')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('gemma-2-9b')) modelLib = libBase + `gemma-2-9b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('gemma-3-2b')) modelLib = libBase + `gemma-3-2b-it-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2-7b')) modelLib = libBase + `Qwen2-7B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('mistral-7b')) modelLib = libBase + `Mistral-7B-Instruct-v0.3-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('hermes-2-pro')) modelLib = libBase + `Llama-3-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('hermes-3')) modelLib = libBase + `Llama-3_1-8B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    // Small / Efficient\r\n                    else if (lowerId.includes('qwen3-0.6b')) modelLib = libBase + `Qwen3-0.6B-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('llama-3.2-1b')) modelLib = libBase + `Llama-3.2-1B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('smollm2-1.7b')) modelLib = libBase + `SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('smollm2-360m')) modelLib = libBase + `SmolLM2-360M-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm`;\r\n                    else if (lowerId.includes('tinyllama-1.1b')) modelLib = libBase + `TinyLlama-1.1B-Chat-v1.0-q4f16_1-ctx2k_cs1k-webgpu.wasm`;\r\n\r\n                    // Fallbacks\r\n                    else if (lowerId.includes('qwen2.5-1.5b')) modelLib = libBase + `Qwen2-1.5B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`;\r\n\r\n                    if (!modelLib) modelLib = libBase + `Qwen2.5-3B-Instruct-${qTag}-ctx4k_cs1k-webgpu.wasm`; // Safe Fallback 3B\r\n\r\n                    // 1. Clear previous engine if exists\r\n                    if (engine) {\r\n                        ui.log(\"Unloading previous engine...\", \"info\");\r\n                        await engine.unload();\r\n                        engine = null;\r\n                        ui.log(\"Engine Unloaded\", \"success\");\r\n                    }\r\n\r\n                    // Create appConfig with online model URL format (like anchor-mic.html)\r\n                    const appConfig = {\r\n                        model_list: [{\r\n                            model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n                            model_id: selectedModelId,\r\n                            model_lib: modelLib,\r\n                            vram_required_MB: 2000,\r\n                            low_resource_required: true,\r\n                            buffer_size_required_bytes: gpuConfig.maxBufferSize,\r\n                            overrides: {\r\n                                context_window_size: gpuConfig.isConstrained ? 2048 : 4096\r\n                            }\r\n                        }],\r\n                        useIndexedDBCache: false, // Disable caching to prevent issues\r\n                    };\r\n\r\n                    // Initialize Engine with the new online-only approach\r\n                    ui.log(`Initializing Engine (${selectedModelId})...`, 'info');\r\n\r\n                    try {\r\n                        engine = await CreateWebWorkerMLCEngine(\r\n                            new Worker('./modules/llm-worker.js', { type: 'module' }),\r\n                            selectedModelId,\r\n                            {\r\n                                initProgressCallback: (report) => {\r\n                                    ui.updateProgress(report.progress, report.text);\r\n                                    if (report.progress === 1) ui.log(\"âœ… Engine Ready\", \"success\");\r\n                                },\r\n                                appConfig: appConfig,\r\n                                logLevel: \"INFO\",\r\n                                useIndexedDBCache: false, // Force disable cache\r\n                            }\r\n                        );\r\n                    } catch (error) {\r\n                        ui.log(`Engine initialization failed: ${error.message}`, \"error\");\r\n                        throw error;\r\n                    }\r\n                }); // Uses the default 5-minute timeout for model loading\r\n\r\n                ui.log(\"ðŸŽ‰ Root Console Online\", \"success\");\r\n                await initializeContextManager();\r\n                document.getElementById('input').disabled = false;\r\n                document.getElementById('send-btn').disabled = false;\r\n                document.getElementById('input').focus();\r\n\r\n            } catch (e) {\r\n                const errorMsg = e.message || String(e);\r\n                ui.log(`Load Failed: ${errorMsg}`, \"error\");\r\n\r\n                // Specific handling for cache-related errors\r\n                if (errorMsg && (errorMsg.includes(\"Cache\") || errorMsg.includes(\"cache\"))) {\r\n                    ui.log(`ðŸš¨ CACHE ERROR: The browser is blocking cache access.`, \"error\");\r\n                    ui.log(`ðŸ’¡ SOLUTION: Try using an Incognito/Private window, or check browser security settings.`, \"warn\");\r\n                }\r\n\r\n                // Provide suggestion for common model name issues\r\n                if (errorMsg && errorMsg.includes(\"Network response was not ok\")) {\r\n                    ui.log(`ðŸ’¡ Hint: Model may not exist or be temporarily unavailable. Try another model.`, \"warn\");\r\n                }\r\n                if (errorMsg && errorMsg.includes(\"QuotaExceededError\")) {\r\n                    ui.log(`ðŸ’¡ STORAGE FULL: Click 'ðŸ—‘ï¸ Clear Cache' to reset your browser storage.`, \"error\");\r\n                    // Auto-suggest cleanup\r\n                    if (confirm(\"Storage Quota Exceeded. Would you like to run the Deep Cleanup tool now?\")) {\r\n                        clearModelCache();\r\n                    }\r\n                }\r\n                document.getElementById('load-model-btn').disabled = false;\r\n            }\r\n        }\r\n\r\n        async function clearModelCache() {\r\n            if (!confirm(\"âš ï¸ STORAGE RESET REQUIRED\\n\\nYour browser's storage is full (QuotaExceededError). We need to perform a full cleanup.\\n\\nThis will:\\n1. Delete ALL downloaded models.\\n2. Clear the browser cache.\\n3. Unregister cached service workers.\\n\\nClick OK to proceed.\")) return;\r\n\r\n            ui.log(\"ðŸ§¹ Starting Deep Cleanup...\", \"warn\");\r\n\r\n            try {\r\n                // 1. Clear IndexedDB\r\n                const dbs = await window.indexedDB.databases();\r\n                for (const db of dbs) {\r\n                    ui.log(`Deleting DB: ${db.name}...`, \"info\");\r\n                    window.indexedDB.deleteDatabase(db.name);\r\n                }\r\n\r\n                // 2. Clear Cache API (Crucial for large file downloads)\r\n                if ('caches' in window) {\r\n                    const cacheKeys = await caches.keys();\r\n                    for (const key of cacheKeys) {\r\n                        ui.log(`Deleting Cache: ${key}...`, \"info\");\r\n                        await caches.delete(key);\r\n                    }\r\n                }\r\n\r\n                // 3. Unregister Service Workers\r\n                if ('serviceWorker' in navigator) {\r\n                    const registrations = await navigator.serviceWorker.getRegistrations();\r\n                    for (const registration of registrations) {\r\n                        ui.log(`Unregistering SW: ${registration.scope}...`, \"info\");\r\n                        await registration.unregister();\r\n                    }\r\n                }\r\n\r\n                ui.log(\"âœ… Cleanup Complete. Reloading...\", \"success\");\r\n                alert(\"Storage cleared successfully. The page will now reload.\");\r\n                window.location.reload();\r\n            } catch (e) {\r\n                ui.log(`âŒ Cleanup Failed: ${e.message}`, \"error\");\r\n                alert(\"Automatic cleanup failed. Please manually clear your browser's 'Site Data' in settings.\");\r\n            }\r\n        }\r\n\r\n        let vision = null; // Global VisionController instance\r\n\r\n        async function init() {\r\n            try {\r\n                ui.log(\"ðŸš€ Root Kernel Starting...\", \"info\");\r\n\r\n                // 0. STORAGE DIAGNOSTICS\r\n                if (navigator.storage && navigator.storage.persist) {\r\n                    const isPersisted = await navigator.storage.persist();\r\n                    ui.log(`ðŸ’¾ Storage Persistence: ${isPersisted ? \"GRANTED\" : \"DENIED\"}`, isPersisted ? \"success\" : \"warn\");\r\n                }\r\n                if (navigator.storage && navigator.storage.estimate) {\r\n                    const quota = await navigator.storage.estimate();\r\n                    const used = (quota.usage / 1024 / 1024).toFixed(2);\r\n                    const total = (quota.quota / 1024 / 1024).toFixed(2);\r\n                    ui.log(`ðŸ“Š Storage Quota: ${used}MB / ${total}MB`, \"info\");\r\n                }\r\n\r\n                // 1. CozoDB\r\n                await initCozo('./cozo_lib_wasm_bg.wasm');\r\n                // Recovery Logic\r\n                try {\r\n                    const [keys] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });\r\n                    if (keys.length > 0) {\r\n                        db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });\r\n                        window.db = db;\r\n                        ui.log(\"âœ… Root Graph Connected (Persistent)\", \"success\");\r\n                    } else {\r\n                        db = CozoDb.new();\r\n                        window.db = db;\r\n                        ui.log(\"âœ… Root Graph Created (Memory)\", \"info\");\r\n                    }\r\n                } catch (e) {\r\n                    db = CozoDb.new();\r\n                    window.db = db;\r\n                    ui.log(\"âš ï¸ Fallback to Memory Graph\", \"warn\");\r\n                }\r\n\r\n                // 2. Embedder (Optional)\r\n                ui.updateProgress(0.3, \"Loading Embedder...\");\r\n                const embedderPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', { device: 'wasm' });\r\n                try {\r\n                    embedder = await Promise.race([embedderPromise, new Promise((_, r) => setTimeout(() => r(new Error(\"Timeout\")), 10000))]);\r\n                    ui.log(\"âœ… Neural Embedder Ready\", \"success\");\r\n                } catch (e) { ui.log(\"âš ï¸ Embedder Skipped (Timeout)\", \"warn\"); }\r\n\r\n                // 3. Ensure FTS Index for BM25 Search\r\n                try {\r\n                    // Create FTS index for BM25 search on content field if it doesn't exist\r\n                    await db.run(\"::fts create memory:content_fts { extractor: content, tokenizer: Simple, filters: [Lowercase, AlphaNumOnly, Stemmer('english')] }\", \"{}\");\r\n                    ui.log(\"âœ… FTS Index Created for BM25 Search\", \"success\");\r\n                } catch (e) {\r\n                    // Index might already exist, which is fine\r\n                    if (!e.message.includes('already exists') && !e.message.includes('conflicts')) {\r\n                        ui.log(\"âš ï¸ FTS Index creation failed (may already exist): \" + e.message, \"warn\");\r\n                    } else {\r\n                        ui.log(\"âœ… FTS Index already exists\", \"success\");\r\n                    }\r\n                }\r\n\r\n            } catch (e) {\r\n                ui.log(`Init Error: ${e.message}`, \"error\");\r\n            } finally {\r\n                // Initialize VisionController\r\n                vision = new VisionController();\r\n                vision.setup('input-area', 'image-preview-container', 'input');\r\n\r\n                // Ensure controls are unlocked even if init fails (partial functionality)\r\n                ui.updateProgress(1.0, \"Ready\");\r\n\r\n                // Fix: Copy Logs Button\r\n                const copyBtn = document.getElementById('copy-logs-btn');\r\n                if (copyBtn) {\r\n                    copyBtn.onclick = () => {\r\n                        const logs = document.getElementById('status-log').innerText;\r\n                        navigator.clipboard.writeText(logs).then(() => ui.log(\"ðŸ“‹ Logs copied to clipboard\", \"success\"));\r\n                    };\r\n                }\r\n\r\n                document.getElementById('model-select').disabled = false;\r\n                document.getElementById('load-model-btn').disabled = false;\r\n                document.getElementById('load-model-btn').addEventListener('click', loadModel);\r\n\r\n                // Input Handlers (idempotent)\r\n                const sendBtn = document.getElementById('send-btn');\r\n                const input = document.getElementById('input');\r\n\r\n                if (sendBtn && input) {\r\n                    sendBtn.onclick = handleSend; // Use property to avoid duplicates\r\n                    input.onkeydown = (e) => {\r\n                        if (e.key === 'Enter' && !e.shiftKey && !sendBtn.disabled) {\r\n                            e.preventDefault();\r\n                            handleSend();\r\n                        }\r\n                    };\r\n                }\r\n\r\n                // --- GHOST PROTOCOL (Headless Auto-Start) ---\r\n                const urlParams = new URLSearchParams(window.location.search);\r\n                if (urlParams.get('headless') === 'true') {\r\n                    ui.log(\"ðŸ‘» Ghost Mode Detected: Initiating Auto-Sequence...\", \"warn\");\r\n\r\n                    // 1. Auto-Connect Bridge\r\n                    const bridgeToggle = document.getElementById('enable-bridge-toggle');\r\n                    if (bridgeToggle && !bridgeToggle.checked) {\r\n                        bridgeToggle.checked = true;\r\n                        toggleBridge(true);\r\n                        ui.log(\"ðŸ‘» Bridge Connection: ENABLED\", \"success\");\r\n                    }\r\n\r\n                    // 2. Auto-Load Model (Qwen-Coder 1.5B for speed/utility)\r\n                    // We use a small delay to ensure CozoDB is ready\r\n                    setTimeout(() => {\r\n                        const targetModel = \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\";\r\n                        const selector = document.getElementById('model-select');\r\n\r\n                        if (selector) {\r\n                            selector.value = targetModel;\r\n                            // If value didn't stick (because model list is dynamic), try setting custom\r\n                            if (selector.value !== targetModel) {\r\n                                selector.value = 'custom';\r\n                                document.getElementById('custom-model-input').value = targetModel;\r\n                            }\r\n\r\n                            ui.log(`ðŸ‘» Ghost Auto-Load: ${targetModel}`, \"info\");\r\n                            loadModel(); // Trigger the existing load logic\r\n                        }\r\n                    }, 2000);\r\n                }\r\n            }\r\n        }\r\n\r\n        // Initialize context manager after all dependencies are loaded\r\n        async function initializeContextManager() {\r\n            if (engine && db) {\r\n                contextManager = new ContextManager(engine, db);\r\n                ui.log(\"ðŸ§  Context Manager Ready\", \"success\");\r\n                return true;\r\n            }\r\n            return false;\r\n        }\r\n\r\n        // --- BRIDGE LOGIC ---\r\n        let bridgeWs = null;\r\n\r\n        // GPU Status Check and Force Unlock Handler\r\n        async function checkGPUStatus() {\r\n            try {\r\n                const res = await fetch(\"/v1/gpu/status\", {\r\n                    method: \"GET\",\r\n                    headers: { \"Authorization\": \"Bearer sovereign-secret\" }\r\n                });\r\n\r\n                if (res.ok) {\r\n                    const status = await res.json();\r\n                    ui.log(`GPU Status: ${status.locked ? `LOCKED by ${status.owner}` : 'FREE'}. Queue: ${status.queue_depth} items.`, \"info\");\r\n                    if (status.queued && status.queued.length > 0) {\r\n                        ui.log(`Queued: ${status.queued.join(', ')}`, \"info\");\r\n                    }\r\n\r\n                    // Check model loading status\r\n                    const modelLoadStatus = GPUController.getModelLoadStatus();\r\n                    if (modelLoadStatus.hasPendingLoad || modelLoadStatus.queueSize > 0) {\r\n                        ui.log(`Model Load Status: Queue: ${modelLoadStatus.queueSize}, Active: ${modelLoadStatus.activeLoaders.join(', ')}`, \"info\");\r\n                    }\r\n\r\n                    return status;\r\n                } else {\r\n                    ui.log(`Status check failed: ${res.status}`, \"warn\");\r\n                    return null;\r\n                }\r\n            } catch (e) {\r\n                ui.log(`Status check error: ${e.message}`, \"warn\");\r\n                return null;\r\n            }\r\n        }\r\n\r\n        document.getElementById('debug-gpu-btn').addEventListener('click', async () => {\r\n            await checkGPUStatus();\r\n        });\r\n\r\n        document.getElementById('force-unlock-btn').addEventListener('click', async () => {\r\n            if (!confirm(\"âš ï¸ Force Unlock GPU?\\nOnly do this if the system is stuck waiting for a lock.\")) return;\r\n\r\n            try {\r\n                ui.log(\"Sending Force Unlock signal...\", \"warn\");\r\n\r\n                // First try the standard reset\r\n                let res = await fetch(\"/v1/gpu/reset\", {\r\n                    method: \"POST\",\r\n                    headers: { \"Authorization\": \"Bearer sovereign-secret\" }\r\n                });\r\n\r\n                if (res.ok) {\r\n                    ui.log(\"ðŸ”“ GPU Lock Force Released (Standard).\", \"success\");\r\n                } else {\r\n                    ui.log(`Standard unlock failed: ${res.status}. Trying emergency release...`, \"warn\");\r\n\r\n                    // If standard unlock failed, try the emergency endpoint\r\n                    res = await fetch(\"/v1/gpu/force-release-all\", {\r\n                        method: \"POST\",\r\n                        headers: { \"Authorization\": \"Bearer sovereign-secret\" }\r\n                    });\r\n\r\n                    if (res.ok) {\r\n                        ui.log(\"ðŸ”“ GPU Locks Force Released (Emergency).\", \"success\");\r\n                    } else {\r\n                        ui.log(`Emergency unlock failed: ${res.status}`, \"error\");\r\n                    }\r\n                }\r\n            } catch (e) {\r\n                ui.log(`Unlock Error: ${e.message}`, \"error\");\r\n            }\r\n        });\r\n\r\n        window.toggleBridge = function (enabled) {\r\n            if (enabled) {\r\n                // Check if database is available (minimum requirement for bridge)\r\n                if (!db) {\r\n                    alert(\"Database not initialized. Please wait for CozoDB to load.\");\r\n                    document.getElementById('enable-bridge-toggle').checked = false;\r\n                    return;\r\n                }\r\n\r\n                // Use the same host and port as the current page for WebSocket connection\r\n                const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';\r\n                const wsHost = window.location.host; // This will be 'localhost:8000' when served from the bridge\r\n                bridgeWs = new WebSocket(`${wsProtocol}//${wsHost}/ws/chat`);\r\n                bridgeWs.onopen = () => { document.getElementById('bridge-status').innerText = \"ðŸŸ¢ Connected\"; ui.log(\"Bridge Online\", \"success\"); };\r\n                bridgeWs.onmessage = async (e) => {\r\n                    const msg = JSON.parse(e.data);\r\n\r\n                    // 1. STANDARD CHAT REQUEST (e.g. from VS Code) - requires LLM engine\r\n                    if (msg.type === 'chat') {\r\n                        // Check if engine is available for chat requests\r\n                        if (!engine) {\r\n                            const errorResponse = {\r\n                                id: msg.id,\r\n                                error: \"LLM engine not loaded. Bridge only supports memory queries without loaded model.\"\r\n                            };\r\n                            bridgeWs.send(JSON.stringify(errorResponse));\r\n                            ui.log(`âŒ Chat request failed: LLM engine not loaded`, \"error\");\r\n                            return;\r\n                        }\r\n\r\n                        ui.log(`Bridge Chat: ${msg.id}`, \"info\");\r\n                        try {\r\n                            const completion = await engine.chat.completions.create({\r\n                                messages: msg.data.messages,\r\n                                stream: true\r\n                            });\r\n                            for await (const chunk of completion) {\r\n                                bridgeWs.send(JSON.stringify({ id: msg.id, chunk }));\r\n                            }\r\n                            bridgeWs.send(JSON.stringify({ id: msg.id, done: true }));\r\n                        } catch (err) {\r\n                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));\r\n                        }\r\n                    }\r\n\r\n                    // 2. MEMORY SEARCH REQUEST (From Chrome Extension) - works without LLM engine\r\n                    else if (msg.type === 'memory_query') {\r\n                        const queryText = msg.data.query;\r\n                        ui.log(`ðŸ”Ž Bridge Memory Search: \"${queryText}\"`, \"info\");\r\n\r\n                        // Initialize context manager if not ready (only for memory queries)\r\n                        if (!contextManager) {\r\n                            await initializeContextManager();\r\n                        }\r\n\r\n                        // Check if context manager is now available\r\n                        if (!contextManager) {\r\n                            const errorResponse = {\r\n                                id: msg.id,\r\n                                error: \"Context manager not ready. Database may not be fully initialized.\"\r\n                            };\r\n                            bridgeWs.send(JSON.stringify(errorResponse));\r\n                            ui.log(`âŒ Context manager not ready for query: ${queryText}`, \"error\");\r\n                            return;\r\n                        }\r\n\r\n                        try {\r\n                            // Use the new decoupled method\r\n                            const results = await contextManager.findRelevantMemories(queryText);\r\n\r\n                            // Send pure JSON back to the bridge\r\n                            bridgeWs.send(JSON.stringify({\r\n                                id: msg.id,\r\n                                result: results\r\n                            }));\r\n                            ui.log(`âœ… Returned ${results.length} memories`, \"success\");\r\n                        } catch (err) {\r\n                            bridgeWs.send(JSON.stringify({ id: msg.id, error: err.message }));\r\n                            ui.log(`âŒ Search Failed: ${err.message}`, \"error\");\r\n                        }\r\n                    }\r\n                };\r\n            } else if (bridgeWs) { bridgeWs.close(); bridgeWs = null; document.getElementById('bridge-status').innerText = \"Disconnected\"; }\r\n        };\r\n\r\n        window.addEventListener('load', init);\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\chat.html"
  },
  {
    "id": "tools\\code_tools.py",
    "timestamp": 1766310900,
    "role": "file",
    "content": "\"\"\"Top-level shim to re-export `anchor.tools.code_tools` for test imports that expect `tools.code_tools`.\r\n\"\"\"\r\ntry:\r\n    from anchor.tools.code_tools import *  # noqa: F401, F403\r\nexcept Exception:\r\n    # Minimal fallback implementations\r\n    def code_search(root: str, query: str, **kwargs):\r\n        return {\"root\": root, \"query\": query, \"count\": 0, \"results\": []}\r\n\r\n    def code_grep(root: str, query: str, **kwargs):\r\n        return {\"root\": root, \"query\": query, \"files\": 0, \"total_matches\": 0, \"results\": []}\r\n\r\n__all__ = [\"code_search\", \"code_grep\"]\r\n",
    "source": "tools\\code_tools.py"
  },
  {
    "id": "tools\\context.html",
    "timestamp": 1767224809,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Anchor Context UI</title>\n    <style>\n        :root { \n            --bg: #0f172a; \n            --panel: #1e293b; \n            --text: #e2e8f0; \n            --accent: #38bdf8; \n            --success: #4ade80;\n        }\n        body { \n            background: var(--bg); \n            color: var(--text); \n            font-family: -apple-system, system-ui, sans-serif; \n            margin: 0; \n            height: 100vh; \n            display: flex; \n            flex-direction: column; \n            padding: 20px; \n            box-sizing: border-box;\n        }\n\n        /* SEARCH BAR */\n        .search-container {\n            display: flex;\n            gap: 10px;\n            margin-bottom: 20px;\n        }\n        input[type=\"text\"] {\n            flex: 1;\n            background: #000;\n            border: 1px solid #334155;\n            color: #fff;\n            padding: 15px;\n            border-radius: 8px;\n            font-size: 16px;\n            outline: none;\n        }\n        input[type=\"text\"]:focus { border-color: var(--accent); }\n        \n        button.primary {\n            background: var(--accent);\n            color: #000;\n            border: none;\n            padding: 0 25px;\n            border-radius: 8px;\n            font-weight: bold;\n            font-size: 16px;\n            cursor: pointer;\n            transition: opacity 0.2s;\n        }\n        button.primary:hover { opacity: 0.9; }\n\n        /* CONTEXT DISPLAY AREA */\n        .context-panel {\n            flex: 1;\n            background: var(--panel);\n            border-radius: 12px;\n            border: 1px solid #334155;\n            display: flex;\n            flex-direction: column;\n            overflow: hidden; /* Contains the scrollable area */\n            position: relative;\n        }\n\n        .panel-header {\n            padding: 15px;\n            background: #00000050;\n            border-bottom: 1px solid #334155;\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n        }\n        .panel-title { font-size: 14px; color: #94a3b8; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; }\n\n        /* SCROLLABLE TEXT FIELD */\n        textarea#context-display {\n            flex: 1;\n            background: transparent;\n            border: none;\n            color: #a5f3fc; /* Cyan tint for data */\n            font-family: 'Consolas', 'Monaco', monospace;\n            font-size: 14px;\n            line-height: 1.6;\n            padding: 20px;\n            resize: none;\n            outline: none;\n        }\n        /* Custom Scrollbar */\n        textarea::-webkit-scrollbar { width: 10px; }\n        textarea::-webkit-scrollbar-track { background: #0f172a; }\n        textarea::-webkit-scrollbar-thumb { background: #334155; border-radius: 5px; }\n        textarea::-webkit-scrollbar-thumb:hover { background: var(--accent); }\n\n        /* COPY BUTTON overlay */\n        .copy-btn {\n            background: #334155;\n            color: #fff;\n            border: 1px solid #475569;\n            padding: 8px 16px;\n            border-radius: 6px;\n            cursor: pointer;\n            font-size: 13px;\n            display: flex;\n            align-items: center;\n            gap: 6px;\n            transition: all 0.2s;\n        }\n        .copy-btn:hover { background: #475569; }\n        .copy-btn.copied { background: var(--success); color: #000; border-color: var(--success); }\n\n    </style>\n</head>\n<body>\n\n    <div class=\"search-container\">\n        <input type=\"text\" id=\"queryInput\" placeholder=\"Enter topic to retrieve context (e.g., 'Project Specs')...\" onkeydown=\"if(event.key==='Enter') fetchContext()\">\n        <button class=\"primary\" onclick=\"fetchContext()\">Fetch Context</button>\n    </div>\n\n    <div class=\"context-panel\">\n        <div class=\"panel-header\">\n            <span class=\"panel-title\">Active Context Memory</span>\n            <button class=\"copy-btn\" onclick=\"copyContext()\" id=\"copyBtn\">\n                ðŸ“‹ Copy to Clipboard\n            </button>\n        </div>\n        \n        <textarea id=\"context-display\" readonly placeholder=\"Context will load here...\"></textarea>\n    </div>\n\n<script>\n    const display = document.getElementById('context-display');\n    const copyBtn = document.getElementById('copyBtn');\n\n    async function fetchContext() {\n        const query = document.getElementById('queryInput').value.trim();\n        if(!query) return;\n\n        display.value = \"ðŸ” Searching Graph...\";\n        display.style.opacity = \"0.5\";\n\n        try {\n            const res = await fetch('/v1/memory/search', {\n                method: 'POST',\n                headers: {'Content-Type': 'application/json'},\n                body: JSON.stringify({ query })\n            });\n            const data = await res.json();\n            \n            display.style.opacity = \"1\";\n            if(data.context) {\n                display.value = data.context;\n            } else {\n                display.value = \"No relevant context found in memory graph.\";\n            }\n        } catch(e) {\n            display.value = \"Error connecting to Anchor Bridge: \" + e;\n        }\n    }\n\n    function copyContext() {\n        if(!display.value) return;\n        \n        display.select();\n        document.execCommand('copy');\n        \n        // Visual Feedback\n        const originalText = copyBtn.innerHTML;\n        copyBtn.innerHTML = \"âœ… Copied!\";\n        copyBtn.classList.add('copied');\n        \n        setTimeout(() => {\n            copyBtn.innerHTML = originalText;\n            copyBtn.classList.remove('copied');\n        }, 2000);\n    }\n</script>\n\n</body>\n</html>",
    "source": "tools\\context.html"
  },
  {
    "id": "tools\\cozo_lib_wasm.js",
    "timestamp": 1766451022,
    "role": "file",
    "content": "import { loadAllFromIndexedDb, flushPendingWrites, writeToIndexedDb, setWriteCounter } from './indexeddb.js';\r\n\r\nlet wasm;\r\n\r\nconst heap = new Array(128).fill(undefined);\r\n\r\nheap.push(undefined, null, true, false);\r\n\r\nfunction getObject(idx) { return heap[idx]; }\r\n\r\nlet heap_next = heap.length;\r\n\r\nfunction dropObject(idx) {\r\n    if (idx < 132) return;\r\n    heap[idx] = heap_next;\r\n    heap_next = idx;\r\n}\r\n\r\nfunction takeObject(idx) {\r\n    const ret = getObject(idx);\r\n    dropObject(idx);\r\n    return ret;\r\n}\r\n\r\nconst cachedTextDecoder = (typeof TextDecoder !== 'undefined' ? new TextDecoder('utf-8', { ignoreBOM: true, fatal: true }) : { decode: () => { throw Error('TextDecoder not available') } } );\r\n\r\nif (typeof TextDecoder !== 'undefined') { cachedTextDecoder.decode(); };\r\n\r\nlet cachedUint8Memory0 = null;\r\n\r\nfunction getUint8Memory0() {\r\n    if (cachedUint8Memory0 === null || cachedUint8Memory0.byteLength === 0) {\r\n        cachedUint8Memory0 = new Uint8Array(wasm.memory.buffer);\r\n    }\r\n    return cachedUint8Memory0;\r\n}\r\n\r\nfunction getStringFromWasm0(ptr, len) {\r\n    ptr = ptr >>> 0;\r\n    return cachedTextDecoder.decode(getUint8Memory0().subarray(ptr, ptr + len));\r\n}\r\n\r\nfunction addHeapObject(obj) {\r\n    if (heap_next === heap.length) heap.push(heap.length + 1);\r\n    const idx = heap_next;\r\n    heap_next = heap[idx];\r\n\r\n    heap[idx] = obj;\r\n    return idx;\r\n}\r\n\r\nfunction makeMutClosure(arg0, arg1, dtor, f) {\r\n    const state = { a: arg0, b: arg1, cnt: 1, dtor };\r\n    const real = (...args) => {\r\n        // First up with a closure we increment the internal reference\r\n        // count. This ensures that the Rust closure environment won't\r\n        // be deallocated while we're invoking it.\r\n        state.cnt++;\r\n        const a = state.a;\r\n        state.a = 0;\r\n        try {\r\n            return f(a, state.b, ...args);\r\n        } finally {\r\n            if (--state.cnt === 0) {\r\n                wasm.__wbindgen_export_0.get(state.dtor)(a, state.b);\r\n\r\n            } else {\r\n                state.a = a;\r\n            }\r\n        }\r\n    };\r\n    real.original = state;\r\n\r\n    return real;\r\n}\r\nfunction __wbg_adapter_22(arg0, arg1, arg2) {\r\n    wasm.wasm_bindgen__convert__closures__invoke1_mut__hd17e34166836fd48(arg0, arg1, addHeapObject(arg2));\r\n}\r\n\r\nlet WASM_VECTOR_LEN = 0;\r\n\r\nconst cachedTextEncoder = (typeof TextEncoder !== 'undefined' ? new TextEncoder('utf-8') : { encode: () => { throw Error('TextEncoder not available') } } );\r\n\r\nconst encodeString = (typeof cachedTextEncoder.encodeInto === 'function'\r\n    ? function (arg, view) {\r\n    return cachedTextEncoder.encodeInto(arg, view);\r\n}\r\n    : function (arg, view) {\r\n    const buf = cachedTextEncoder.encode(arg);\r\n    view.set(buf);\r\n    return {\r\n        read: arg.length,\r\n        written: buf.length\r\n    };\r\n});\r\n\r\nfunction passStringToWasm0(arg, malloc, realloc) {\r\n    if (typeof arg !== 'string') arg = arg ? String(arg) : \"\";\r\n    if (realloc === undefined) {\r\n        const buf = cachedTextEncoder.encode(arg);\r\n        const ptr = malloc(buf.length, 1) >>> 0;\r\n        getUint8Memory0().subarray(ptr, ptr + buf.length).set(buf);\r\n        WASM_VECTOR_LEN = buf.length;\r\n        return ptr;\r\n    }\r\n\r\n    let len = arg.length;\r\n    let ptr = malloc(len, 1) >>> 0;\r\n\r\n    const mem = getUint8Memory0();\r\n\r\n    let offset = 0;\r\n\r\n    for (; offset < len; offset++) {\r\n        const code = arg.charCodeAt(offset);\r\n        if (code > 0x7F) break;\r\n        mem[ptr + offset] = code;\r\n    }\r\n\r\n    if (offset !== len) {\r\n        if (offset !== 0) {\r\n            arg = arg.slice(offset);\r\n        }\r\n        ptr = realloc(ptr, len, len = offset + arg.length * 3, 1) >>> 0;\r\n        const view = getUint8Memory0().subarray(ptr + offset, ptr + len);\r\n        const ret = encodeString(arg, view);\r\n\r\n        offset += ret.written;\r\n    }\r\n\r\n    WASM_VECTOR_LEN = offset;\r\n    return ptr;\r\n}\r\n\r\nlet cachedInt32Memory0 = null;\r\n\r\nfunction getInt32Memory0() {\r\n    if (cachedInt32Memory0 === null || cachedInt32Memory0.byteLength === 0) {\r\n        cachedInt32Memory0 = new Int32Array(wasm.memory.buffer);\r\n    }\r\n    return cachedInt32Memory0;\r\n}\r\n\r\nfunction handleError(f, args) {\r\n    try {\r\n        return f.apply(this, args);\r\n    } catch (e) {\r\n        wasm.__wbindgen_exn_store(addHeapObject(e));\r\n    }\r\n}\r\nfunction __wbg_adapter_76(arg0, arg1, arg2, arg3) {\r\n    wasm.wasm_bindgen__convert__closures__invoke2_mut__h62fdc46dd4e23c5d(arg0, arg1, addHeapObject(arg2), addHeapObject(arg3));\r\n}\r\n\r\n/**\r\n*/\r\nexport class CozoDb {\r\n\r\n    static __wrap(ptr) {\r\n        ptr = ptr >>> 0;\r\n        const obj = Object.create(CozoDb.prototype);\r\n        obj.__wbg_ptr = ptr;\r\n\r\n        return obj;\r\n    }\r\n\r\n    __destroy_into_raw() {\r\n        const ptr = this.__wbg_ptr;\r\n        this.__wbg_ptr = 0;\r\n\r\n        return ptr;\r\n    }\r\n\r\n    free() {\r\n        const ptr = this.__destroy_into_raw();\r\n        wasm.__wbg_cozodb_free(ptr);\r\n    }\r\n    /**\r\n    * @returns {CozoDb}\r\n    */\r\n    static new() {\r\n        const ret = wasm.cozodb_new();\r\n        return CozoDb.__wrap(ret);\r\n    }\r\n    /**\r\n    * Create CozoDb from IndexedDB\r\n    * @param {string} db_name\r\n    * @param {string} store_name\r\n    * @param {any} on_write_callback\r\n    * @returns {Promise<CozoDb>}\r\n    */\r\n    static new_from_indexed_db(db_name, store_name, on_write_callback) {\r\n        const ptr0 = passStringToWasm0(db_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n        const len0 = WASM_VECTOR_LEN;\r\n        const ptr1 = passStringToWasm0(store_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n        const len1 = WASM_VECTOR_LEN;\r\n        const ret = wasm.cozodb_new_from_indexed_db(ptr0, len0, ptr1, len1, addHeapObject(on_write_callback));\r\n        return takeObject(ret);\r\n    }\r\n    /**\r\n    * @param {string} script\r\n    * @param {string} params\r\n    * @param {boolean} immutable\r\n    * @returns {Promise<string>}\r\n    */\r\n    run(script, params, immutable) {\r\n        if (typeof params === 'undefined' || params === null) params = \"{}\";\r\n        const ptr0 = passStringToWasm0(script, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n        const len0 = WASM_VECTOR_LEN;\r\n        const ptr1 = passStringToWasm0(params, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n        const len1 = WASM_VECTOR_LEN;\r\n        const ret = wasm.cozodb_run(this.__wbg_ptr, ptr0, len0, ptr1, len1, immutable);\r\n        return takeObject(ret);\r\n    }\r\n    /**\r\n    * @param {string} data\r\n    * @returns {string}\r\n    */\r\n    export_relations(data) {\r\n        let deferred2_0;\r\n        let deferred2_1;\r\n        try {\r\n            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);\r\n            const ptr0 = passStringToWasm0(data, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n            const len0 = WASM_VECTOR_LEN;\r\n            wasm.cozodb_export_relations(retptr, this.__wbg_ptr, ptr0, len0);\r\n            var r0 = getInt32Memory0()[retptr / 4 + 0];\r\n            var r1 = getInt32Memory0()[retptr / 4 + 1];\r\n            deferred2_0 = r0;\r\n            deferred2_1 = r1;\r\n            return getStringFromWasm0(r0, r1);\r\n        } finally {\r\n            wasm.__wbindgen_add_to_stack_pointer(16);\r\n            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\r\n        }\r\n    }\r\n    /**\r\n    * @param {string} data\r\n    * @returns {string}\r\n    */\r\n    import_relations(data) {\r\n        // Normalize common JSON payload shapes so the WASM importer receives\r\n        // a canonical `{\"relations\": [{ name, headers, rows }]}` shape.\r\n        let normalized = null;\r\n        try {\r\n            const parsed = JSON.parse(data);\r\n            // Top-level 'memory' -> convert\r\n            if (parsed && parsed.memory) {\r\n                const mem = parsed.memory;\r\n                const headers = mem.headers || (mem.named_rows && mem.named_rows.headers) || null;\r\n                const rows = mem.rows || (mem.named_rows && mem.named_rows.rows) || [];\r\n                normalized = { relations: [{ name: 'memory', headers: headers || [], rows }] };\r\n            }\r\n            // Already a relations wrapper\r\n            else if (parsed && parsed.relations && Array.isArray(parsed.relations)) {\r\n                const rels = parsed.relations.map(r => {\r\n                    const headers = r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'];\r\n                    const rawRows = r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [];\r\n                    const rows = rawRows.map(row => {\r\n                        // Normalize object rows into arrays using headers order\r\n                        if (row && typeof row === 'object' && !Array.isArray(row)) {\r\n                            return headers.map(h => (row[h] === undefined ? null : row[h]));\r\n                        }\r\n                        // Ensure array rows have proper length and default embedding\r\n                        const arr = Array.isArray(row) ? row.slice(0, headers.length) : [];\r\n                        while (arr.length < headers.length) arr.push(null);\r\n                        const embIdx = headers.indexOf('embedding');\r\n                        if (embIdx >= 0 && (arr[embIdx] === null || arr[embIdx] === undefined)) arr[embIdx] = [];\r\n                        return arr;\r\n                    });\r\n                    // Provide multiple shapes to satisfy various importer variants\r\n                    return {\r\n                        name: r.name || 'memory',\r\n                        headers,\r\n                        rows,\r\n                        named_rows: { headers, rows },\r\n                        NamedRows: { headers, rows }\r\n                    };\r\n                });\r\n                normalized = { relations: rels };\r\n            }\r\n            // Else, could be raw array of records -> convert to rows\r\n            else if (Array.isArray(parsed)) {\r\n                const rows = parsed.map(rec => {\r\n                    const ts = rec.timestamp ? (isNaN(Number(rec.timestamp)) ? new Date(rec.timestamp).getTime() : Number(rec.timestamp)) : Date.now();\r\n                    return [\r\n                        `${ts}-${Math.random().toString(36).substr(2,9)}`,\r\n                        ts,\r\n                        rec.role || rec.type || 'unknown',\r\n                        (rec.content || rec.response_content || rec.message || '').substring(0, 20000),\r\n                        rec.source || 'combined_memory.json',\r\n                        null\r\n                    ];\r\n                });\r\n                normalized = { relations: [{ name: 'memory', headers: ['id','timestamp','role','content','source','embedding'], rows }] };\r\n            }\r\n        } catch (e) {\r\n            // Not valid JSON or normalization failed; fall back to raw string\r\n        }\r\n\r\n        const payload = normalized ? JSON.stringify(normalized) : data;\r\n\r\n        let deferred2_0;\r\n        let deferred2_1;\r\n        try {\r\n            // Debug: show the exact payload being passed into WASM importer (trimmed)\r\n            try {\r\n                console.log('CozoDb.import_relations - payload preview (trimmed):', payload.slice(0, 2000));\r\n                const parsedPreview = JSON.parse(payload);\r\n                console.log('CozoDb.import_relations - parsed relations preview:', parsedPreview.relations && parsedPreview.relations[0] ? Object.keys(parsedPreview.relations[0]) : null);\r\n            } catch (pe) {\r\n                console.warn('CozoDb.import_relations - payload not parseable as JSON preview.');\r\n            }\r\n            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);\r\n            const ptr0 = passStringToWasm0(payload, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n            const len0 = WASM_VECTOR_LEN;\r\n            try {\r\n                wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptr0, len0);\r\n            } catch (e) {\r\n                console.error('CozoDb.import_relations wasm call failed:', e, 'payload_preview:', payload.slice(0,2000));\r\n                throw e;\r\n            }\r\n            var r0 = getInt32Memory0()[retptr / 4 + 0];\r\n            var r1 = getInt32Memory0()[retptr / 4 + 1];\r\n            deferred2_0 = r0;\r\n            deferred2_1 = r1;\r\n            const resultStr = getStringFromWasm0(r0, r1);\r\n\r\n            // If WASM returned a NamedRows header error, try a fallback shaped payload where relation includes NamedRows explicitly.\r\n            try {\r\n                const parsedResult = JSON.parse(resultStr);\r\n                if (parsedResult && parsedResult.ok === false && typeof parsedResult.message === 'string' && parsedResult.message.includes('NamedRows requires')) {\r\n                    try {\r\n                        const parsedPayload = JSON.parse(payload);\r\n                        if (parsedPayload && parsedPayload.relations && Array.isArray(parsedPayload.relations)) {\r\n                            const fallback = { relations: parsedPayload.relations.map(r => ({ name: r.name || 'memory', NamedRows: { headers: r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'], rows: r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [] } })) };\r\n                            const fallbackStr = JSON.stringify(fallback);\r\n                            console.log('CozoDb.import_relations - retrying with fallback NamedRows payload (trimmed):', fallbackStr.slice(0,2000));\r\n                            const ptrF = passStringToWasm0(fallbackStr, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n                            const lenF = WASM_VECTOR_LEN;\r\n                            wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptrF, lenF);\r\n                            var r0b = getInt32Memory0()[retptr / 4 + 0];\r\n                            var r1b = getInt32Memory0()[retptr / 4 + 1];\r\n                            // free previous\r\n                            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\r\n                            deferred2_0 = r0b;\r\n                            deferred2_1 = r1b;\r\n                            return getStringFromWasm0(r0b, r1b);\r\n                        }\r\n                    } catch (retryErr) {\r\n                        console.warn('CozoDb.import_relations - fallback retry failed to build payload or call wasm:', retryErr);\r\n                    }\r\n                }\r\n            } catch (e) {\r\n                // ignore parse errors\r\n            }\r\n\r\n            return resultStr;\r\n        } finally {\r\n            wasm.__wbindgen_add_to_stack_pointer(16);\r\n            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\r\n        }\r\n    }\r\n}\r\n\r\nasync function __wbg_load(module, imports) {\r\n    if (typeof Response === 'function' && module instanceof Response) {\r\n        if (typeof WebAssembly.instantiateStreaming === 'function') {\r\n            try {\r\n                return await WebAssembly.instantiateStreaming(module, imports);\r\n\r\n            } catch (e) {\r\n                if (module.headers.get('Content-Type') != 'application/wasm') {\r\n                    console.warn(\"`WebAssembly.instantiateStreaming` failed because your server does not serve wasm with `application/wasm` MIME type. Falling back to `WebAssembly.instantiate` which is slower. Original error:\\n\", e);\r\n\r\n                } else {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n\r\n        const bytes = await module.arrayBuffer();\r\n        return await WebAssembly.instantiate(bytes, imports);\r\n\r\n    } else {\r\n        const instance = await WebAssembly.instantiate(module, imports);\r\n\r\n        if (instance instanceof WebAssembly.Instance) {\r\n            return { instance, module };\r\n\r\n        } else {\r\n            return instance;\r\n        }\r\n    }\r\n}\r\n\r\nfunction __wbg_get_imports() {\r\n    const imports = {};\r\n    imports.wbg = {};\r\n    imports.wbg.__wbindgen_object_drop_ref = function(arg0) {\r\n        takeObject(arg0);\r\n    };\r\n    imports.wbg.__wbg_log_b78d654f19d681e0 = function(arg0, arg1) {\r\n        console.log(getStringFromWasm0(arg0, arg1));\r\n    };\r\n    imports.wbg.__wbg_loadAllFromIndexedDb_05f8df9a19c8d344 = function(arg0, arg1, arg2, arg3, arg4) {\r\n        const ret = loadAllFromIndexedDb(getStringFromWasm0(arg0, arg1), getStringFromWasm0(arg2, arg3), getObject(arg4));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_flushPendingWrites_dd4341a0dafcf428 = function() {\r\n        const ret = flushPendingWrites();\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_string_new = function(arg0, arg1) {\r\n        const ret = getStringFromWasm0(arg0, arg1);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_cozodb_new = function(arg0) {\r\n        const ret = CozoDb.__wrap(arg0);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_cb_drop = function(arg0) {\r\n        const obj = takeObject(arg0).original;\r\n        if (obj.cnt-- == 1) {\r\n            obj.a = 0;\r\n            return true;\r\n        }\r\n        const ret = false;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_new_abda76e883ba8a5f = function() {\r\n        const ret = new Error();\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_stack_658279fe44541cf6 = function(arg0, arg1) {\r\n        const ret = getObject(arg1).stack;\r\n        const ptr1 = passStringToWasm0(ret, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\r\n        const len1 = WASM_VECTOR_LEN;\r\n        getInt32Memory0()[arg0 / 4 + 1] = len1;\r\n        getInt32Memory0()[arg0 / 4 + 0] = ptr1;\r\n    };\r\n    imports.wbg.__wbg_error_f851667af71bcfc6 = function(arg0, arg1) {\r\n        let deferred0_0;\r\n        let deferred0_1;\r\n        try {\r\n            deferred0_0 = arg0;\r\n            deferred0_1 = arg1;\r\n            console.error(getStringFromWasm0(arg0, arg1));\r\n        } finally {\r\n            wasm.__wbindgen_free(deferred0_0, deferred0_1, 1);\r\n        }\r\n    };\r\n    imports.wbg.__wbg_setWriteCounter_295838a9805b3542 = function(arg0) {\r\n        setWriteCounter(arg0 >>> 0);\r\n    };\r\n    imports.wbg.__wbg_writeToIndexedDb_ec8ba47108ce4d3a = function(arg0, arg1) {\r\n        const ret = writeToIndexedDb(getObject(arg0), getObject(arg1));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_crypto_c48a774b022d20ac = function(arg0) {\r\n        const ret = getObject(arg0).crypto;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_is_object = function(arg0) {\r\n        const val = getObject(arg0);\r\n        const ret = typeof(val) === 'object' && val !== null;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_process_298734cf255a885d = function(arg0) {\r\n        const ret = getObject(arg0).process;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_versions_e2e78e134e3e5d01 = function(arg0) {\r\n        const ret = getObject(arg0).versions;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_node_1cd7a5d853dbea79 = function(arg0) {\r\n        const ret = getObject(arg0).node;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_is_string = function(arg0) {\r\n        const ret = typeof(getObject(arg0)) === 'string';\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_msCrypto_bcb970640f50a1e8 = function(arg0) {\r\n        const ret = getObject(arg0).msCrypto;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_require_8f08ceecec0f4fee = function() { return handleError(function () {\r\n        const ret = module.require;\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbindgen_is_function = function(arg0) {\r\n        const ret = typeof(getObject(arg0)) === 'function';\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_randomFillSync_dc1e9a60c158336d = function() { return handleError(function (arg0, arg1) {\r\n        getObject(arg0).randomFillSync(takeObject(arg1));\r\n    }, arguments) };\r\n    imports.wbg.__wbg_getRandomValues_37fa2ca9e4e07fab = function() { return handleError(function (arg0, arg1) {\r\n        getObject(arg0).getRandomValues(getObject(arg1));\r\n    }, arguments) };\r\n    imports.wbg.__wbg_get_44be0491f933a435 = function(arg0, arg1) {\r\n        const ret = getObject(arg0)[arg1 >>> 0];\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_length_fff51ee6522a1a18 = function(arg0) {\r\n        const ret = getObject(arg0).length;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_newnoargs_581967eacc0e2604 = function(arg0, arg1) {\r\n        const ret = new Function(getStringFromWasm0(arg0, arg1));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_call_cb65541d95d71282 = function() { return handleError(function (arg0, arg1) {\r\n        const ret = getObject(arg0).call(getObject(arg1));\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbindgen_object_clone_ref = function(arg0) {\r\n        const ret = getObject(arg0);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_self_1ff1d729e9aae938 = function() { return handleError(function () {\r\n        const ret = self.self;\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbg_window_5f4faef6c12b79ec = function() { return handleError(function () {\r\n        const ret = window.window;\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbg_globalThis_1d39714405582d3c = function() { return handleError(function () {\r\n        const ret = globalThis.globalThis;\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbg_global_651f05c6a0944d1c = function() { return handleError(function () {\r\n        const ret = global.global;\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbindgen_is_undefined = function(arg0) {\r\n        const ret = getObject(arg0) === undefined;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_isArray_4c24b343cb13cfb1 = function(arg0) {\r\n        const ret = Array.isArray(getObject(arg0));\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_call_01734de55d61e11d = function() { return handleError(function (arg0, arg1, arg2) {\r\n        const ret = getObject(arg0).call(getObject(arg1), getObject(arg2));\r\n        return addHeapObject(ret);\r\n    }, arguments) };\r\n    imports.wbg.__wbg_now_9c5990bda04c7e53 = function() {\r\n        const ret = Date.now();\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_new_43f1b47c28813cbd = function(arg0, arg1) {\r\n        try {\r\n            var state0 = {a: arg0, b: arg1};\r\n            var cb0 = (arg0, arg1) => {\r\n                const a = state0.a;\r\n                state0.a = 0;\r\n                try {\r\n                    return __wbg_adapter_76(a, state0.b, arg0, arg1);\r\n                } finally {\r\n                    state0.a = a;\r\n                }\r\n            };\r\n            const ret = new Promise(cb0);\r\n            return addHeapObject(ret);\r\n        } finally {\r\n            state0.a = state0.b = 0;\r\n        }\r\n    };\r\n    imports.wbg.__wbg_resolve_53698b95aaf7fcf8 = function(arg0) {\r\n        const ret = Promise.resolve(getObject(arg0));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_then_f7e06ee3c11698eb = function(arg0, arg1) {\r\n        const ret = getObject(arg0).then(getObject(arg1));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_then_b2267541e2a73865 = function(arg0, arg1, arg2) {\r\n        const ret = getObject(arg0).then(getObject(arg1), getObject(arg2));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_buffer_085ec1f694018c4f = function(arg0) {\r\n        const ret = getObject(arg0).buffer;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_newwithbyteoffsetandlength_6da8e527659b86aa = function(arg0, arg1, arg2) {\r\n        const ret = new Uint8Array(getObject(arg0), arg1 >>> 0, arg2 >>> 0);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_new_8125e318e6245eed = function(arg0) {\r\n        const ret = new Uint8Array(getObject(arg0));\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_set_5cf90238115182c3 = function(arg0, arg1, arg2) {\r\n        getObject(arg0).set(getObject(arg1), arg2 >>> 0);\r\n    };\r\n    imports.wbg.__wbg_length_72e2208bbc0efc61 = function(arg0) {\r\n        const ret = getObject(arg0).length;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_instanceof_Uint8Array_d8d9cb2b8e8ac1d4 = function(arg0) {\r\n        let result;\r\n        try {\r\n            result = getObject(arg0) instanceof Uint8Array;\r\n        } catch {\r\n            result = false;\r\n        }\r\n        const ret = result;\r\n        return ret;\r\n    };\r\n    imports.wbg.__wbg_newwithlength_e5d69174d6984cd7 = function(arg0) {\r\n        const ret = new Uint8Array(arg0 >>> 0);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbg_subarray_13db269f57aa838d = function(arg0, arg1, arg2) {\r\n        const ret = getObject(arg0).subarray(arg1 >>> 0, arg2 >>> 0);\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_throw = function(arg0, arg1) {\r\n        throw new Error(getStringFromWasm0(arg0, arg1));\r\n    };\r\n    imports.wbg.__wbindgen_memory = function() {\r\n        const ret = wasm.memory;\r\n        return addHeapObject(ret);\r\n    };\r\n    imports.wbg.__wbindgen_closure_wrapper186 = function(arg0, arg1, arg2) {\r\n        const ret = makeMutClosure(arg0, arg1, 87, __wbg_adapter_22);\r\n        return addHeapObject(ret);\r\n    };\r\n\r\n    return imports;\r\n}\r\n\r\nfunction __wbg_init_memory(imports, maybe_memory) {\r\n\r\n}\r\n\r\nfunction __wbg_finalize_init(instance, module) {\r\n    wasm = instance.exports;\r\n    __wbg_init.__wbindgen_wasm_module = module;\r\n    cachedInt32Memory0 = null;\r\n    cachedUint8Memory0 = null;\r\n\r\n\r\n    return wasm;\r\n}\r\n\r\nfunction initSync(module) {\r\n    if (wasm !== undefined) return wasm;\r\n\r\n    const imports = __wbg_get_imports();\r\n\r\n    __wbg_init_memory(imports);\r\n\r\n    if (!(module instanceof WebAssembly.Module)) {\r\n        module = new WebAssembly.Module(module);\r\n    }\r\n\r\n    const instance = new WebAssembly.Instance(module, imports);\r\n\r\n    return __wbg_finalize_init(instance, module);\r\n}\r\n\r\nasync function __wbg_init(input) {\r\n    if (wasm !== undefined) return wasm;\r\n\r\n    if (typeof input === 'undefined') {\r\n        input = new URL('cyb_cozo_lib_wasm_bg.wasm', import.meta.url);\r\n    }\r\n    const imports = __wbg_get_imports();\r\n\r\n    if (typeof input === 'string' || (typeof Request === 'function' && input instanceof Request) || (typeof URL === 'function' && input instanceof URL)) {\r\n        input = fetch(input);\r\n    }\r\n\r\n    __wbg_init_memory(imports);\r\n\r\n    const { instance, module } = await __wbg_load(await input, imports);\r\n\r\n    return __wbg_finalize_init(instance, module);\r\n}\r\n\r\nexport { initSync }\r\nexport default __wbg_init;\r\n",
    "source": "tools\\cozo_lib_wasm.js"
  },
  {
    "id": "tools\\db_builder.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Root Memory Builder</title>\r\n    <style>\r\n        :root {\r\n            --bg: #1e1e1e;\r\n            --text: #e0e0e0;\r\n            --accent: #00ff88;\r\n            --danger: #ff4444;\r\n            --warn: #ffc107;\r\n            --surface: #252526;\r\n        }\r\n\r\n        body {\r\n            font-family: 'Segoe UI', sans-serif;\r\n            background: var(--bg);\r\n            color: var(--text);\r\n            padding: 20px;\r\n            max-width: 800px;\r\n            margin: 0 auto;\r\n        }\r\n\r\n        h1 {\r\n            border-bottom: 1px solid #333;\r\n            padding-bottom: 10px;\r\n            color: var(--accent);\r\n            font-weight: 300;\r\n            letter-spacing: 1px;\r\n        }\r\n\r\n        .drop-zone {\r\n            border: 2px dashed #444;\r\n            padding: 40px;\r\n            text-align: center;\r\n            margin: 20px 0;\r\n            border-radius: 8px;\r\n            transition: all 0.3s;\r\n            cursor: pointer;\r\n            background: #111;\r\n        }\r\n\r\n        .drop-zone:hover {\r\n            border-color: var(--accent);\r\n            background: var(--surface);\r\n        }\r\n\r\n        .log-area {\r\n            background: #111;\r\n            border: 1px solid #333;\r\n            padding: 10px;\r\n            height: 300px;\r\n            overflow-y: auto;\r\n            font-family: 'Consolas', monospace;\r\n            font-size: 0.85rem;\r\n            color: #aaa;\r\n            border-radius: 4px;\r\n        }\r\n\r\n        button {\r\n            background: #333;\r\n            color: white;\r\n            border: 1px solid #555;\r\n            padding: 10px 20px;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            font-size: 0.9rem;\r\n            margin-right: 10px;\r\n            transition: all 0.2s;\r\n        }\r\n\r\n        button:hover {\r\n            border-color: var(--accent);\r\n            color: var(--accent);\r\n        }\r\n\r\n        button:disabled {\r\n            background: #222;\r\n            color: #555;\r\n            border-color: #333;\r\n            cursor: not-allowed;\r\n        }\r\n\r\n        button.primary {\r\n            background: var(--accent);\r\n            color: #000;\r\n            border: none;\r\n        }\r\n\r\n        button.primary:hover {\r\n            background: #00cc6a;\r\n        }\r\n\r\n        button.danger {\r\n            background: var(--danger);\r\n            border-color: var(--danger);\r\n        }\r\n\r\n        button.danger:hover {\r\n            background: #cc0000;\r\n        }\r\n\r\n        .stat-box {\r\n            display: flex;\r\n            gap: 20px;\r\n            margin-bottom: 20px;\r\n        }\r\n\r\n        .stat {\r\n            background: var(--surface);\r\n            padding: 15px;\r\n            border-radius: 6px;\r\n            flex: 1;\r\n            text-align: center;\r\n            border: 1px solid #333;\r\n        }\r\n\r\n        .stat-val {\r\n            font-size: 1.5rem;\r\n            font-weight: bold;\r\n            display: block;\r\n            margin-bottom: 5px;\r\n        }\r\n\r\n        .stat-label {\r\n            font-size: 0.8rem;\r\n            color: #888;\r\n            text-transform: uppercase;\r\n        }\r\n\r\n        textarea {\r\n            width: 100%;\r\n            height: 60px;\r\n            background: #111;\r\n            color: #e0e0e0;\r\n            border: 1px solid #555;\r\n            padding: 10px;\r\n            border-radius: 4px;\r\n            resize: vertical;\r\n            margin-bottom: 10px;\r\n            font-family: inherit;\r\n        }\r\n\r\n        textarea:focus {\r\n            outline: none;\r\n            border-color: var(--accent);\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n\r\n    <h1>ðŸŒ± Root Memory Builder</h1>\r\n    <p>Ingest session logs into your local Root Graph.</p>\r\n\r\n    <div class=\"stat-box\">\r\n        <div class=\"stat\">\r\n            <span id=\"db-status\" class=\"stat-val\" style=\"color: #ff6b6b\">Offline</span>\r\n            <span class=\"stat-label\">Status</span>\r\n        </div>\r\n        <div class=\"stat\">\r\n            <span id=\"mem-count\" class=\"stat-val\">0</span>\r\n            <span class=\"stat-label\">Memories</span>\r\n        </div>\r\n        <div class=\"stat\">\r\n            <span id=\"vec-status\" class=\"stat-val\">Loading...</span>\r\n            <span class=\"stat-label\">Embedder</span>\r\n        </div>\r\n    </div>\r\n\r\n    <div class=\"drop-zone\" id=\"drop-zone\">\r\n        Drag & Drop <code>combined_memory.json</code>, logs, or text files here<br>\r\n        <small style=\"color: #666\">JSON, MD, TXT, PY, JS, HTML</small>\r\n    </div>\r\n    <input type=\"file\" id=\"file-input\" multiple style=\"display:none\">\r\n\r\n    <!-- Quick Add Section -->\r\n    <div style=\"margin: 20px 0; padding: 20px; border: 1px solid #333; background: var(--surface); border-radius: 8px;\">\r\n        <h3 style=\"margin-top: 0; font-weight: 300;\">ðŸ“ Quick Add</h3>\r\n        <textarea id=\"quick-mem-content\" placeholder=\"Type a memory here (e.g. 'Project X password is...')\"></textarea>\r\n        <div style=\"text-align: right;\">\r\n            <button id=\"quick-add-btn\" class=\"primary\">Add to Graph</button>\r\n        </div>\r\n    </div>\r\n\r\n    <div style=\"margin-bottom: 10px; display: flex; gap: 10px; flex-wrap: wrap;\">\r\n        <button id=\"auto-import-btn\" disabled title=\"Import from ./cozo_import_memory.json\">Auto-Import</button>\r\n        <button id=\"export-db-btn\" disabled>Export JSON</button>\r\n        <button id=\"query-btn\" disabled>Test Query</button>\r\n        <div style=\"flex-grow: 1\"></div>\r\n        <button id=\"reset-btn\" class=\"danger\" disabled>Nuke Database</button>\r\n    </div>\r\n\r\n    <div class=\"log-area\" id=\"logs\"></div>\r\n\r\n    <script type=\"module\">\r\n        import { AnchorLogger, initCozo, createStore } from './modules/anchor.js';\r\n        import { loadAllFromIndexedDb, closeDatabase, clearIndexedDbStore, flushPendingWrites } from './indexeddb.js';\r\n        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';\r\n\r\n        env.allowLocalModels = false;\r\n\r\n        // --- 0. KERNEL SETUP ---\r\n        const logger = new AnchorLogger('Root-Builder');\r\n\r\n        // Wrap logger to output to DOM as well\r\n        const originalInfo = logger.info.bind(logger);\r\n        logger.info = (msg) => { originalInfo(msg); appendLog(msg, 'info'); };\r\n\r\n        const originalWarn = logger.warn.bind(logger);\r\n        logger.warn = (msg) => { originalWarn(msg); appendLog(msg, 'warn'); };\r\n\r\n        const originalError = logger.error.bind(logger);\r\n        logger.error = (msg) => { originalError(msg); appendLog(msg, 'error'); };\r\n\r\n        const originalSuccess = logger.success.bind(logger);\r\n        logger.success = (msg) => { originalSuccess(msg); appendLog(msg, 'success'); };\r\n\r\n        function appendLog(msg, type) {\r\n            const logs = document.getElementById('logs');\r\n            const line = document.createElement('div');\r\n            const time = new Date().toLocaleTimeString();\r\n            line.textContent = `[${time}] ${msg}`;\r\n            if (type === 'error') line.style.color = '#ff6b6b';\r\n            if (type === 'success') line.style.color = '#00ff88';\r\n            if (type === 'warn') line.style.color = '#ffc107';\r\n            logs.appendChild(line);\r\n            logs.scrollTop = logs.scrollHeight;\r\n        }\r\n\r\n        const { state, subscribe } = createStore({\r\n            dbStatus: 'Offline',\r\n            dbColor: '#ff6b6b',\r\n            memCount: 0,\r\n            embedderStatus: 'Loading...',\r\n            embedderColor: '#ffc107',\r\n            isReady: false\r\n        });\r\n\r\n        // UI Bindings\r\n        subscribe((prop, val) => {\r\n            if (prop === 'dbStatus') {\r\n                const el = document.getElementById('db-status');\r\n                el.innerText = val;\r\n                el.style.color = state.dbColor;\r\n            }\r\n            if (prop === 'memCount') document.getElementById('mem-count').innerText = val;\r\n            if (prop === 'embedderStatus') {\r\n                const el = document.getElementById('vec-status');\r\n                el.innerText = val;\r\n                el.style.color = state.embedderColor;\r\n            }\r\n            if (prop === 'isReady' && val === true) {\r\n                document.getElementById('auto-import-btn').disabled = false;\r\n                document.getElementById('export-db-btn').disabled = false;\r\n                document.getElementById('reset-btn').disabled = false;\r\n                document.getElementById('query-btn').disabled = false;\r\n            }\r\n        });\r\n\r\n        let db;\r\n        let embedder;\r\n        let CozoDbClass;\r\n\r\n        // --- 1. INITIALIZATION ---\r\n        async function init() {\r\n            try {\r\n                logger.info(\"Initializing Root Kernel...\");\r\n\r\n                // Load Cozo WASM via Kernel\r\n                CozoDbClass = await initCozo('./cozo_lib_wasm_bg.wasm');\r\n\r\n                // Probe IndexedDB (Recovery Logic)\r\n                try {\r\n                    const [keys, items] = await loadAllFromIndexedDb('coda_memory', 'cozo_store', () => { });\r\n                    logger.info(`Storage Probe: Found ${keys.length} items in persistence layer.`);\r\n                    closeDatabase(); // Important: Release lock\r\n\r\n                    // Try Persistent Load\r\n                    try {\r\n                        const dbPromise = CozoDbClass.new_from_indexed_db('coda_memory', 'cozo_store', () => { });\r\n                        // Timeout protection\r\n                        const timeoutPromise = new Promise((_, reject) => setTimeout(() => reject(new Error('DB Load Timeout')), 5000));\r\n                        db = await Promise.race([dbPromise, timeoutPromise]);\r\n\r\n                        state.dbStatus = \"Active (Persistent)\";\r\n                        state.dbColor = \"#00ff88\";\r\n                        logger.success(\"Root Graph Online (Persistent).\");\r\n                    } catch (e) {\r\n                        logger.warn(`Persistence load failed: ${e.message}. Clearing corruption...`);\r\n                        await clearIndexedDbStore('coda_memory', 'cozo_store');\r\n                        // Fallback to In-Memory\r\n                        db = CozoDbClass.new();\r\n                        state.dbStatus = \"Active (Memory-Only)\";\r\n                        state.dbColor = \"#ffc107\";\r\n                        logger.warn(\"Fallback to In-Memory DB. Data will not be saved.\");\r\n                    }\r\n\r\n                } catch (e) {\r\n                    logger.warn(\"Storage probe failed. Creating fresh in-memory DB.\");\r\n                    db = CozoDbClass.new();\r\n                    state.dbStatus = \"Active (Memory-Only)\";\r\n                    state.dbColor = \"#ffc107\";\r\n                }\r\n\r\n                // Global Exposure for Debugging\r\n                window.db = db;\r\n                window.runQuery = safeRun;\r\n\r\n                // Load Embedder\r\n                pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2').then(pipe => {\r\n                    embedder = pipe;\r\n                    state.embedderStatus = \"Ready\";\r\n                    state.embedderColor = \"#00ff88\";\r\n                    logger.success(\"Neural Embedder Loaded.\");\r\n                }).catch(e => {\r\n                    state.embedderStatus = \"Error\";\r\n                    state.embedderColor = \"#ff4444\";\r\n                    logger.error(`Embedder failed: ${e.message}`);\r\n                });\r\n\r\n                // Schema Sync\r\n                await ensureSchema();\r\n                await updateStats();\r\n                state.isReady = true;\r\n\r\n            } catch (e) {\r\n                logger.error(`CRITICAL FAILURE: ${e.message}`);\r\n            }\r\n        }\r\n\r\n        async function safeRun(query, params = \"{}\") {\r\n            if (!db) throw new Error(\"DB offline\");\r\n            const res = await db.run(query, params);\r\n            let parsed = res;\r\n            if (typeof res === 'string') {\r\n                try { parsed = JSON.parse(res); } catch (e) { }\r\n            }\r\n            if (parsed && parsed.ok === false) throw new Error(parsed.message);\r\n            return parsed;\r\n        }\r\n\r\n        async function ensureSchema() {\r\n            try {\r\n                // 1. Check if table exists\r\n                await safeRun(\"?[id] := *memory{id} :limit 1\", \"{}\");\r\n\r\n                // 2. Migration: Try to add new columns if they don't exist\r\n                // (CozoDB ignores 'add' if column exists or throws specific error, we catch to be safe)\r\n                try { await safeRun(\"::alter memory add mime_type: String?\", \"{}\"); } catch (e) { }\r\n                try { await safeRun(\"::alter memory add blob_ref: String?\", \"{}\"); } catch (e) { }\r\n\r\n            } catch (e) {\r\n                logger.warn(\"Schema missing. Creating Root Graph schema...\");\r\n                await safeRun(`\r\n                    :create memory {\r\n                        id: String =>\r\n                        timestamp: Int,\r\n                        role: String,\r\n                        content: String,\r\n                        source: String,\r\n                        embedding: <F32; 384>,\r\n                        mime_type: String?,\r\n                        blob_ref: String?\r\n                    }\r\n                `, \"{}\");\r\n\r\n                // Create FTS index for BM25 search on content field\r\n                try {\r\n                    await safeRun(\"::fts create memory:content_fts { extractor: content, tokenizer: Simple, filters: [Lowercase, AlphaNumOnly, Stemmer('english')] }\", \"{}\");\r\n                    logger.success(\"FTS Index Created for BM25 Search.\");\r\n                } catch (ftsErr) {\r\n                    // Index might already exist, which is fine\r\n                    if (!ftsErr.message.includes('already exists') && !ftsErr.message.includes('conflicts')) {\r\n                        logger.warn(\"FTS index creation failed (may already exist): \" + ftsErr.message);\r\n                    }\r\n                }\r\n\r\n                logger.success(\"Schema Created.\");\r\n            }\r\n        }\r\n\r\n        async function updateStats() {\r\n            try {\r\n                const res = await safeRun(`?[count(id)] := *memory{id}`, \"{}\");\r\n                if (res.rows && res.rows.length) state.memCount = res.rows[0][0];\r\n            } catch (e) { }\r\n        }\r\n\r\n        // --- 2. INGESTION LOGIC ---\r\n        const SOV_BATCH_SIZE = 50;\r\n\r\n        async function handleFiles(files) {\r\n            if (!embedder || !db) return logger.error(\"System not ready.\");\r\n            for (const file of files) {\r\n                logger.info(`Reading ${file.name} (${file.type || 'unknown'})...`);\r\n                try {\r\n                    let records = [];\r\n\r\n                    // Binary Handling (Images/Audio)\r\n                    if (file.type.startsWith('image/') || file.type.startsWith('audio/')) {\r\n                        records = [{\r\n                            role: 'user',\r\n                            source: file.name,\r\n                            timestamp: file.lastModified,\r\n                            content: `[BINARY_REF] ${file.name}`, // Placeholder for text search\r\n                            mime_type: file.type,\r\n                            blob_ref: file.name\r\n                        }];\r\n                    }\r\n                    // Text Handling\r\n                    else {\r\n                        const text = await file.text();\r\n\r\n                        if (file.name.endsWith('.json')) {\r\n                            const json = JSON.parse(text);\r\n                            if (Array.isArray(json)) records = json;\r\n                            else if (json.conversations) records = json.conversations;\r\n                            else if (json.relations) {\r\n                                // Extract rows logic simplified\r\n                                const mem = json.relations.find(r => r.name === 'memory') || json.relations[0];\r\n                                if (mem) {\r\n                                    const hdr = mem.headers || ['id', 'timestamp', 'role', 'content', 'source', 'embedding'];\r\n                                    records = (mem.rows || []).map(r => {\r\n                                        const obj = {};\r\n                                        hdr.forEach((h, i) => obj[h] = r[i]);\r\n                                        return obj;\r\n                                    });\r\n                                }\r\n                            }\r\n                        } else {\r\n                            // Text file\r\n                            records = [{\r\n                                role: 'system',\r\n                                source: file.name,\r\n                                timestamp: file.lastModified,\r\n                                content: text\r\n                            }];\r\n                        }\r\n                    }\r\n\r\n                    await processRecords(records, file.name);\r\n                } catch (e) {\r\n                    logger.error(`Failed ${file.name}: ${e.message}`);\r\n                }\r\n            }\r\n            await updateStats();\r\n        }\r\n\r\n        async function processRecords(records, sourceName) {\r\n            let batch = [];\r\n            for (const rec of records) {\r\n                let content = rec.content || rec.message || \"\";\r\n                if (!content) continue;\r\n                if (content.length > 20000) content = content.substring(0, 20000) + \"...[TRUNCATED]\";\r\n\r\n                let embedding = rec.embedding || [];\r\n                if (!embedding.length) {\r\n                    try {\r\n                        const out = await embedder(content, { pooling: 'mean', normalize: true });\r\n                        embedding = Array.from(out.data);\r\n                    } catch (e) {\r\n                        logger.warn(`Embedding failed for item in ${sourceName}, using zero vector.`);\r\n                        embedding = new Array(384).fill(0.0);\r\n                    }\r\n                }\r\n\r\n                // Hardened Timestamp Logic\r\n                let ts = rec.timestamp ? new Date(rec.timestamp).getTime() : Date.now();\r\n                if (isNaN(ts)) ts = Date.now();\r\n\r\n                const id = `${ts}-${Math.random().toString(36).substr(2, 9)}`;\r\n                const mime = rec.mime_type || null;\r\n                const ref = rec.blob_ref || null;\r\n\r\n                batch.push([id, ts, rec.role || 'unknown', content, rec.source || sourceName, embedding, mime, ref]);\r\n\r\n                if (batch.length >= SOV_BATCH_SIZE) {\r\n                    await insertBatch(batch);\r\n                    batch = [];\r\n                }\r\n            }\r\n            if (batch.length) await insertBatch(batch);\r\n            try { await flushPendingWrites(); } catch (e) { }\r\n            logger.success(`Imported ${records.length} items from ${sourceName}`);\r\n        }\r\n\r\n        async function insertBatch(rows) {\r\n            // Strict Schema Mapping: Variables must match Schema column names for :put\r\n            const q = `\r\n                ?[id, timestamp, role, content, source, embedding, mime_type, blob_ref] <- $data\r\n                :put memory { id, timestamp, role, content, source, embedding, mime_type, blob_ref }\r\n            `;\r\n            \r\n            try {\r\n                await safeRun(q, JSON.stringify({ data: rows }));\r\n            } catch (e) {\r\n                logger.error(`Batch Insert Failed: ${e.message}`);\r\n                \r\n                // Fallback for Legacy Schema (6 columns)\r\n                // We must bind all 8 items from the input array, but only use the first 6\r\n                if (e.message.includes(\"column\") || e.message.includes(\"arity\")) {\r\n                    logger.warn(\"Retrying with legacy schema (ignoring binary fields)...\");\r\n                    const legacyQ = `\r\n                        ?[id, timestamp, role, content, source, embedding, _mime, _ref] <- $data\r\n                        :put memory { id, timestamp, role, content, source, embedding }\r\n                    `;\r\n                    await safeRun(legacyQ, JSON.stringify({ data: rows }));\r\n                } else {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n\r\n        // --- 3. EVENT LISTENERS ---\r\n        document.getElementById('drop-zone').addEventListener('click', () => document.getElementById('file-input').click());\r\n        document.getElementById('file-input').addEventListener('change', (e) => handleFiles(e.target.files));\r\n\r\n        document.getElementById('drop-zone').addEventListener('dragover', (e) => { e.preventDefault(); e.target.style.borderColor = '#00ff88'; });\r\n        document.getElementById('drop-zone').addEventListener('drop', (e) => { e.preventDefault(); e.target.style.borderColor = '#444'; handleFiles(e.dataTransfer.files); });\r\n\r\n        document.getElementById('quick-add-btn').addEventListener('click', async () => {\r\n            const val = document.getElementById('quick-mem-content').value.trim();\r\n            if (!val) return;\r\n            await processRecords([{ role: 'manual', content: val, source: 'user_input' }], 'QuickAdd');\r\n            document.getElementById('quick-mem-content').value = \"\";\r\n            await updateStats();\r\n        });\r\n\r\n        document.getElementById('reset-btn').addEventListener('click', async () => {\r\n            if (confirm(\"NUKE DATABASE? This destroys all data.\")) {\r\n                await clearIndexedDbStore('coda_memory', 'cozo_store');\r\n                location.reload();\r\n            }\r\n        });\r\n\r\n        document.getElementById('query-btn').addEventListener('click', async () => {\r\n            try {\r\n                const res = await safeRun(\"?[ts, content] := *memory{timestamp: ts, content} :sort -ts :limit 5\");\r\n                if (res.rows) logger.info(\"Recent:\\n\" + res.rows.map(r => `${new Date(r[0]).toLocaleTimeString()}: ${r[1].substring(0, 50)}...`).join('\\n'));\r\n                else logger.warn(\"No rows returned.\");\r\n            } catch (e) { logger.error(e.message); }\r\n        });\r\n\r\n        document.getElementById('auto-import-btn').addEventListener('click', async () => {\r\n            try {\r\n                const res = await fetch('./cozo_import_memory.json');\r\n                const json = await res.json();\r\n                let recs = Array.isArray(json) ? json : (json.conversations || []);\r\n                if (recs.length) await processRecords(recs, 'auto-import');\r\n                else logger.warn(\"No records found in auto-import file.\");\r\n            } catch (e) { logger.error(e.message); }\r\n        });\r\n\r\n        document.getElementById('export-db-btn').addEventListener('click', async () => {\r\n            try {\r\n                logger.info(\"Exporting Root Memory...\");\r\n                // Export the 'memory' relation including vectors\r\n                const jsonStr = await db.export_relations(JSON.stringify({relations: [\"memory\"]}));\r\n                \r\n                // Create Blob and Download\r\n                const blob = new Blob([jsonStr], {type: \"application/json\"});\r\n                const url = URL.createObjectURL(blob);\r\n                const a = document.createElement('a');\r\n                a.href = url;\r\n                a.download = `root_coda_memory_dump_${Date.now()}.json`;\r\n                document.body.appendChild(a);\r\n                a.click();\r\n                document.body.removeChild(a);\r\n                URL.revokeObjectURL(url);\r\n                \r\n                logger.success(\"Export Complete. You can now drag this file into another Root Coda instance.\");\r\n            } catch (e) {\r\n                logger.error(\"Export Failed: \" + e.message);\r\n            }\r\n        });\r\n\r\n        init();\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\db_builder.html"
  },
  {
    "id": "tools\\decode_cozo_blob.py",
    "timestamp": 1766310901,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"Quick decoder for CozoDB blob files to try to recover JSON/relations.\r\n\r\nUsage: python tools\\decode_cozo_blob.py C:\\path\\to\\cozo_blob_0.bin C:\\path\\to\\cozo_blob_1.bin\r\n\r\nIt attempts: UTF-8 decode, zlib.inflate, gzip, zstd (if installed), base64 decode, and searches for JSON-like markers.\r\n\"\"\"\r\nimport sys\r\nimport json\r\nimport zlib\r\nimport gzip\r\nimport base64\r\nimport bz2\r\nimport lzma\r\nfrom pathlib import Path\r\n\r\n# Optional compressors\r\ntry:\r\n    import zstandard as zstd\r\nexcept Exception:\r\n    zstd = None\r\n\r\ntry:\r\n    import brotli\r\nexcept Exception:\r\n    brotli = None\r\n\r\ntry:\r\n    import lz4.frame as lz4frame\r\nexcept Exception:\r\n    lz4frame = None\r\n\r\n\r\ndef hexdump(b, length=64):\r\n    return ' '.join(f\"{x:02x}\" for x in b[:length])\r\n\r\n\r\ndef try_utf8(b):\r\n    try:\r\n        s = b.decode('utf-8', errors='replace')\r\n        return s\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_zlib(b):\r\n    try:\r\n        return zlib.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_gzip(b):\r\n    try:\r\n        return gzip.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_base64(b):\r\n    try:\r\n        s = b.decode('ascii', errors='ignore').strip()\r\n        s2 = ''.join(s.split())\r\n        dec = base64.b64decode(s2)\r\n        return dec\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_bz2(b):\r\n    try:\r\n        return bz2.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_lzma(b):\r\n    try:\r\n        return lzma.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_brotli(b):\r\n    if not brotli:\r\n        return None\r\n    try:\r\n        return brotli.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_lz4(b):\r\n    if not lz4frame:\r\n        return None\r\n    try:\r\n        return lz4frame.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_zstd(b):\r\n    if not zstd:\r\n        return None\r\n    try:\r\n        dctx = zstd.ZstdDecompressor()\r\n        return dctx.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef find_json_in_bytes(b):\r\n    try:\r\n        txt = b.decode('utf-8', errors='ignore')\r\n    except Exception:\r\n        txt = None\r\n    if not txt:\r\n        return None\r\n    idx = txt.find('{')\r\n    if idx >= 0:\r\n        # Try progressive parse\r\n        for end in range(idx+100, min(len(txt), idx+20000), 100):\r\n            try:\r\n                candidate = txt[idx:end]\r\n                parsed = json.loads(candidate)\r\n                return parsed\r\n            except Exception:\r\n                continue\r\n    if 'relations' in txt or 'memory' in txt or 'NamedRows' in txt:\r\n        return txt\r\n    return None\r\n\r\n\r\ndef analyze(path: Path):\r\n    print(f\"\\n=== Analyzing: {path} ===\")\r\n    b = path.read_bytes()\r\n    print(f\"Size: {len(b)} bytes\")\r\n    print(\"Hex (first 64 bytes):\", hexdump(b, 64))\r\n\r\n    s = try_utf8(b)\r\n    if s and ('{' in s or 'relations' in s or 'memory' in s or 'NamedRows' in s):\r\n        print(\"\\n-- UTF-8 text looks promising (preview):\\n\")\r\n        print(s[:2000])\r\n        return\r\n\r\n    z = try_zlib(b)\r\n    if z:\r\n        print(\"\\n-- zlib decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = z.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(z[:200]))\r\n        return\r\n\r\n    g = try_gzip(b)\r\n    if g:\r\n        print(\"\\n-- gzip decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = g.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(g[:200]))\r\n        return\r\n\r\n    zd = try_zstd(b)\r\n    if zd:\r\n        print(\"\\n-- zstd decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = zd.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(zd[:200]))\r\n        return\r\n\r\n    bz = try_bz2(b)\r\n    if bz:\r\n        print(\"\\n-- bz2 decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = bz.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(bz[:200]))\r\n        return\r\n\r\n    lz = try_lzma(b)\r\n    if lz:\r\n        print(\"\\n-- lzma decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = lz.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(lz[:200]))\r\n        return\r\n\r\n    br = try_brotli(b)\r\n    if br:\r\n        print(\"\\n-- brotli decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = br.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(br[:200]))\r\n        return\r\n\r\n    l4 = try_lz4(b)\r\n    if l4:\r\n        print(\"\\n-- lz4 decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = l4.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(l4[:200]))\r\n        return\r\n\r\n    bb = try_base64(b)\r\n    if bb:\r\n        print(\"\\n-- base64 decoded (preview):\\n\")\r\n        try:\r\n            txt = bb.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(bb[:200]))\r\n        return\r\n\r\n    found = find_json_in_bytes(b)\r\n    if found:\r\n        print(\"\\n-- Found JSON-like content:\\n\")\r\n        if isinstance(found, str):\r\n            print(found[:2000])\r\n        else:\r\n            print(json.dumps(found, indent=2)[:4000])\r\n        return\r\n\r\n    out = path.with_suffix(path.suffix + '.raw')\r\n    out.write_bytes(b)\r\n    print(f\"\\nNo decode succeeded. Wrote raw blob to {out}\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    if len(sys.argv) < 2:\r\n        print('Usage: python tools\\\\decode_cozo_blob.py C:\\\\path\\\\to\\\\blob1 [blob2 ...]')\r\n        sys.exit(1)\r\n    for p in sys.argv[1:]:\r\n        analyze(Path(p))",
    "source": "tools\\decode_cozo_blob.py"
  },
  {
    "id": "tools\\eyes.py",
    "timestamp": 1766310901,
    "role": "file",
    "content": "import argparse\r\nimport requests\r\nimport sys\r\nimport os\r\n\r\ndef ingest(content, source_type=\"text\", adapter=\"eyes-cli\"):\r\n    url = \"http://localhost:8000/archivist/ingest\"\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"X-API-Key\": \"ece-secret-key\" \r\n    }\r\n    payload = {\r\n        \"content\": content,\r\n        \"type\": source_type,\r\n        \"adapter\": adapter\r\n    }\r\n    \r\n    print(f\"Sending to {url}...\")\r\n    try:\r\n        response = requests.post(url, json=payload, headers=headers)\r\n        response.raise_for_status()\r\n        print(f\"âœ… Success: {response.json()}\")\r\n    except requests.exceptions.RequestException as e:\r\n        print(f\"âŒ Error: {e}\")\r\n        if hasattr(e, 'response') and e.response is not None:\r\n            print(f\"Details: {e.response.text}\")\r\n        sys.exit(1)\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"Sovereign Eyes - Ingest content into ECE Memory\")\r\n    parser.add_argument(\"input\", help=\"File path or text content to ingest\")\r\n    parser.add_argument(\"--type\", default=\"text\", help=\"Source type (text, web_page, etc.)\")\r\n    parser.add_argument(\"--adapter\", default=\"eyes-cli\", help=\"Adapter name\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    content = args.input\r\n    \r\n    # Check if input is a file\r\n    if os.path.exists(args.input):\r\n        try:\r\n            with open(args.input, 'r', encoding='utf-8') as f:\r\n                content = f.read()\r\n            print(f\"ðŸ“– Read content from file: {args.input}\")\r\n        except Exception as e:\r\n            print(f\"âš ï¸ Could not read file '{args.input}', treating as raw text.\")\r\n            \r\n    ingest(content, args.type, args.adapter)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "source": "tools\\eyes.py"
  },
  {
    "id": "tools\\index.html",
    "timestamp": 1767231012,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Anchor Core | Launcher</title>\n    <style>\n        :root {\n            --bg-color: #1a1a1a;\n            --card-bg: #2d2d2d;\n            --accent: #0078d4;\n            --text: #ffffff;\n            --secondary-text: #aaaaaa;\n        }\n\n        body {\n            font-family: 'Segoe UI', system-ui, sans-serif;\n            background-color: var(--bg-color);\n            color: var(--text);\n            margin-top: 45vh;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            justify-content: center;\n            height: 100vh;\n        }\n\n        h1 {\n            font-weight: 300;\n            margin-bottom: 40px;\n            letter-spacing: 2px;\n        }\n\n        .grid {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            width: 100%;\n            max-width: 900px;\n            padding: 20px;\n        }\n\n        .card {\n            background-color: var(--card-bg);\n            border-radius: 12px;\n            padding: 30px;\n            text-decoration: none;\n            color: var(--text);\n            transition: transform 0.2s, box-shadow 0.2s;\n            border: 1px solid #333;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            text-align: center;\n        }\n\n        .card:hover {\n            transform: translateY(-5px);\n            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.4);\n            border-color: var(--accent);\n        }\n\n        .icon {\n            font-size: 48px;\n            margin-bottom: 20px;\n        }\n\n        .title {\n            font-size: 1.25rem;\n            font-weight: 600;\n            margin-bottom: 10px;\n        }\n\n        .desc {\n            font-size: 0.9rem;\n            color: var(--secondary-text);\n        }\n\n        .status {\n            margin-top: 15px;\n            font-size: 0.8rem;\n            padding: 4px 8px;\n            border-radius: 4px;\n            background: #333;\n            color: #888;\n        }\n\n        .card:hover .status {\n            background: rgba(0, 120, 212, 0.2);\n            color: var(--accent);\n        }\n    </style>\n</head>\n\n<body>\n\n    <h1>ANCHOR <span style=\"color:var(--accent); font-weight:600;\">CORE</span></h1>\n\n    <div class=\"grid\">\n        <a href=\"chat.html\" class=\"card\">\n            <div class=\"icon\">ðŸ’¬</div>\n            <div class=\"title\">Anchor Chat</div>\n            <div class=\"desc\">The core WebGPU interface. Run LLMs locally with memory integration.</div>\n            <div class=\"status\">Active</div>\n        </a>\n\n        <a href=\"log-viewer.html\" class=\"card\">\n            <div class=\"icon\">ðŸ“Š</div>\n            <div class=\"title\">Log Viewer</div>\n            <div class=\"desc\">Monitor engine performance, VRAM usage, and debug events.</div>\n            <div class=\"status\">Utility</div>\n        </a>\n\n        <a href=\"db_builder.html\" class=\"card\">\n            <div class=\"icon\">ðŸ§ </div>\n            <div class=\"title\">DB Builder</div>\n            <div class=\"desc\">Manage and build the CozoDB memory store.</div>\n            <div class=\"status\">Admin</div>\n        </a>\n\n        <a href=\"anchor-mic.html\" class=\"card\">\n            <div class=\"icon\">ðŸŽ™ï¸</div>\n            <div class=\"title\">Root Mic</div>\n            <div class=\"desc\">Dictate directly to the engine using Whisper.</div>\n            <div class=\"status\">Input</div>\n        </a>\n\n        <a href=\"memory-builder.html\" class=\"card\">\n            <div class=\"icon\">ðŸ§ </div>\n            <div class=\"title\">Memory Builder</div>\n            <div class=\"desc\">Background memory consolidation and association.</div>\n            <div class=\"status\">Subconscious</div>\n        </a>\n\n        <a href=\"terminal.html\" class=\"card\">\n            <div class=\"icon\">ðŸ’»</div>\n            <div class=\"title\">Terminal</div>\n            <div class=\"desc\">Direct shell access. Command the host via the Neural Shell Protocol.</div>\n            <div class=\"status\">Active</div>\n        </a>\n\n        <div class=\"card\" onclick=\"spawnAnchorShell()\">\n            <div class=\"icon\">âš“</div>\n            <div class=\"title\">Anchor Shell</div>\n            <div class=\"desc\">Launch the native PowerShell client connected to the Ghost Engine.</div>\n            <div class=\"status\">Offline</div>\n        </div>\n\n        <a href=\"context.html\" class=\"card\">\n            <div class=\"icon\">ðŸ“‹</div>\n            <div class=\"title\">Context UI</div>\n            <div class=\"desc\">Simplified context retrieval with scrollable display and one-click copy.</div>\n            <div class=\"status\">Active</div>\n        </a>\n\n        <a href=\"mobile-chat.html\" class=\"card\">\n            <div class=\"icon\">ðŸ“±</div>\n            <div class=\"title\">Mobile Chat</div>\n            <div class=\"desc\">Lightweight, mobile-friendly UI for on-the-go access.</div>\n            <div class=\"status\">Active</div>\n        </a>\n    </div>\n\n    <div style=\"margin-top: 50px; color: #555; font-size: 0.8rem;\">\n        Running on Localhost â€¢ WebGPU Enabled\n    </div>\n\n    <script>\n    async function spawnAnchorShell() {\n        try {\n            const res = await fetch('http://localhost:8000/v1/system/spawn_shell', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json', 'Authorization': 'Bearer sovereign-secret' },\n                body: JSON.stringify({})\n            });\n            if (res.ok) {\n                console.log(\"Anchor Shell Spawned\");\n                alert(\"Anchor terminal launched in new PowerShell window!\");\n            } else {\n                alert(\"Failed to spawn. Is Bridge running?\");\n            }\n        } catch (e) {\n            alert(\"Error: \" + e.message);\n        }\n    }\n    </script>\n\n</body>\n\n</html>",
    "source": "tools\\index.html"
  },
  {
    "id": "tools\\indexeddb.js",
    "timestamp": 1766310901,
    "role": "file",
    "content": "let db = null;\r\nlet cozoDbStore = null;\r\nlet writeCounter = 0;\r\nlet writeCallback = null;\r\n\r\nlet cmdFlag = false;\r\n\r\nexport function setWriteCounter(count) {\r\n  writeCounter = count;\r\n}\r\n\r\nfunction storeRequestToPromise(req) {\r\n  return new Promise((resolve, reject) => {\r\n    req.onsuccess = () => resolve(req.result);\r\n    req.onerror = (e) => reject(e.error);\r\n  });\r\n}\r\n\r\nasync function openDatabase(dbName, storeName) {\r\n  cozoDbStore = storeName;\r\n\r\n  return new Promise((resolve, reject) => {\r\n    const request = indexedDB.open(dbName, 1);\r\n    request.onupgradeneeded = function (event) {\r\n      const db = event.target.result;\r\n      if (!db.objectStoreNames.contains(storeName)) {\r\n        db.createObjectStore(storeName);\r\n      }\r\n    };\r\n\r\n    request.onsuccess = function (event) {\r\n      db = event.target.result;\r\n      resolve(db);\r\n    };\r\n    request.onerror = function (event) {\r\n      reject(event.error);\r\n    };\r\n  });\r\n}\r\n\r\nasync function readStore() {\r\n  return new Promise((resolve, reject) => {\r\n    const transaction = db.transaction(cozoDbStore, \"readonly\");\r\n    const store = transaction.objectStore(cozoDbStore);\r\n\r\n    const itemsPromise = storeRequestToPromise(store.getAll());\r\n    const keysPromise = storeRequestToPromise(store.getAllKeys());\r\n\r\n    Promise.all([keysPromise, itemsPromise])\r\n      .then((results) => {\r\n        const keys = results[0].map((item) => new Uint8Array(item));\r\n        const items = results[1];\r\n        resolve([keys, items]);\r\n      })\r\n      .catch(reject);\r\n  });\r\n}\r\n\r\nexport async function flushPendingWrites(timeoutDuration = 60000) {\r\n  let timeout = null;\r\n\r\n  // allow only one command runnig at a time\r\n  if (cmdFlag) {\r\n    await new Promise((resolve, reject) => {\r\n      const interval = setInterval(() => {\r\n        if (!cmdFlag) {\r\n          clearInterval(interval);\r\n          resolve();\r\n        }\r\n      }, 10);\r\n    });\r\n  }\r\n\r\n  cmdFlag = true;\r\n\r\n  const waitPromise = new Promise((resolve, reject) => {\r\n    const interval = setInterval(() => {\r\n      if (writeCounter <= 0) {\r\n        if (timeout) {\r\n          clearTimeout(timeout);\r\n        }\r\n        clearInterval(interval);\r\n        resolve();\r\n      }\r\n    }, 10);\r\n  });\r\n\r\n  const timeoutPromise = new Promise((_, reject) => {\r\n    timeout = setTimeout(() => {\r\n      reject(new Error(\"waitForPendingWrites timed out!\"));\r\n    }, timeoutDuration);\r\n  });\r\n\r\n  // wait until all pending writes are done\r\n  return Promise.race([waitPromise, timeoutPromise]).finally(() => {\r\n    cmdFlag = false;\r\n  });\r\n}\r\n\r\nexport async function loadAllFromIndexedDb(dbName, storeName, onWriteCallback) {\r\n  writeCallback = onWriteCallback;\r\n  await openDatabase(dbName, storeName);\r\n  return await readStore();\r\n}\r\n\r\nexport async function writeToIndexedDb(key, value) {\r\n  return new Promise((resolve, reject) => {\r\n    const transaction = db.transaction(cozoDbStore, \"readwrite\");\r\n    const store = transaction.objectStore(cozoDbStore);\r\n    const request = value ? store.put(value, key) : store.delete(key);\r\n    return storeRequestToPromise(request)\r\n      .then(resolve)\r\n      .catch(reject)\r\n      .finally(() => {\r\n        writeCounter--;\r\n        writeCallback && writeCallback(writeCounter);\r\n      });\r\n  });\r\n}\r\n\r\nexport function closeDatabase() {\r\n  if (db) {\r\n    db.close();\r\n    db = null;\r\n  }\r\n}\r\n\r\nexport async function clearIndexedDbStore(dbName, storeName) {\r\n  // Ensure we open a connection first if one isn't open\r\n  if (!db) {\r\n    await openDatabase(dbName, storeName);\r\n  }\r\n  return new Promise((resolve, reject) => {\r\n    const transaction = db.transaction(storeName, \"readwrite\");\r\n    const store = transaction.objectStore(storeName);\r\n    const request = store.clear();\r\n\r\n    request.onsuccess = () => resolve();\r\n    request.onerror = (e) => reject(e.target.error);\r\n  });\r\n}\r\n",
    "source": "tools\\indexeddb.js"
  },
  {
    "id": "tools\\log-viewer.html",
    "timestamp": 1767238126,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Anchor Core System Logs</title>\n    <style>\n        body {\n            background-color: #000;\n            color: #0f0;\n            font-family: 'Consolas', 'Monaco', monospace;\n            margin: 0;\n            display: flex;\n            flex-direction: column;\n            height: 100vh;\n            overflow: hidden;\n        }\n\n        #toolbar {\n            background-color: #111;\n            padding: 10px;\n            border-bottom: 1px solid #333;\n            display: flex;\n            gap: 10px;\n            align-items: center;\n        }\n\n        button {\n            background: #333;\n            color: #fff;\n            border: 1px solid #444;\n            cursor: pointer;\n            padding: 5px 10px;\n        }\n\n        button:hover {\n            background: #444;\n        }\n\n        #main-container {\n            display: flex;\n            flex: 1;\n            overflow: hidden;\n        }\n\n        .panel {\n            flex: 1;\n            display: flex;\n            flex-direction: column;\n            min-width: 0;\n        }\n\n        .panel-header {\n            background: #1a1a1a;\n            padding: 5px 10px;\n            font-weight: bold;\n            border-bottom: 1px solid #333;\n            display: flex;\n            justify-content: space-between;\n        }\n\n        .log-container {\n            flex: 1;\n            overflow-y: auto;\n            padding: 10px;\n            white-space: pre-wrap;\n            font-size: 12px;\n        }\n\n        .log-line {\n            padding: 2px 0;\n            border-bottom: 1px solid #111;\n            word-break: break-all;\n        }\n\n        .log-line:hover {\n            background-color: #111;\n        }\n\n        .info {\n            color: #88ccff;\n        }\n\n        .warning {\n            color: #ffcc00;\n        }\n\n        .error {\n            color: #ff4444;\n            font-weight: bold;\n        }\n\n        .debug {\n            color: #888;\n        }\n\n        .success {\n            color: #00ff00;\n        }\n    </style>\n</head>\n\n<body>\n    <div id=\"toolbar\">\n        <span style=\"color: #88ccff; font-weight: bold;\">Anchor Core Log Stream</span>\n        <span id=\"connection-status\" style=\"color: #888; margin-left: 10px;\">â— Listening...</span>\n        <div style=\"flex: 1;\"></div>\n        <button id=\"copy-all-btn\" title=\"Copy all logs to clipboard\">ðŸ“‹ Copy All</button>\n        <button id=\"retry-logs-btn\" title=\"Retry fetching backend logs\">Retry Backend Logs</button>\n        <button id=\"clear-btn\">Clear All</button>\n    </div>\n\n    <div id=\"main-container\">\n        <!-- Single Combined Log Panel -->\n        <div class=\"panel\">\n            <div class=\"panel-header\">\n                <span>All System Logs</span>\n            </div>\n            <div id=\"combined-logs\" class=\"log-container\"></div>\n        </div>\n    </div>\n\n    <script>\n        const combinedContainer = document.getElementById('combined-logs');\n        const clearBtn = document.getElementById('clear-btn');\n        const copyBtn = document.getElementById('copy-all-btn');\n        const statusIndicator = document.getElementById('connection-status');\n\n        copyBtn.onclick = () => {\n            const allText = combinedContainer.innerText;\n\n            navigator.clipboard.writeText(allText).then(() => {\n                const original = copyBtn.innerHTML;\n                copyBtn.innerHTML = \"âœ… Copied!\";\n                setTimeout(() => copyBtn.innerHTML = original, 2000);\n            }).catch(e => alert(\"Copy failed: \" + e));\n        };\n\n        const logChannel = new BroadcastChannel('sovereign-logs');\n        const codaChannel = new BroadcastChannel('coda_logs');\n        // Simple de-duplication for server polling\n        const serverLogSet = new Set();\n\n        function markActive() {\n            statusIndicator.style.color = '#0f0';\n            statusIndicator.innerText = 'â— Active';\n            setTimeout(() => {\n                statusIndicator.style.color = '#888';\n                statusIndicator.innerText = 'â— Listening...';\n            }, 2000);\n        }\n\n        logChannel.onmessage = (event) => {\n            markActive();\n            const data = event.data;\n\n            if (data.source === 'system') {\n                appendLog(combinedContainer, `[${data.time}] [${data.type}] ${data.msg}`, data.type);\n            } else if (data.source === 'chat') {\n                appendLog(combinedContainer, `[${data.time}] ${data.role}: ${data.text}`, 'info');\n            }\n        };\n\n        // Mission Control channel (coda_logs) - short JSON messages\n        codaChannel.onmessage = (event) => {\n            markActive();\n            const data = event.data;\n            try {\n                // Known sources: 'Sovereign-Console' | 'Sovereign-Chat' | 'Sovereign-DB' | 'Sovereign-Embed'\n                if (data.source === 'Sovereign-Console') {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [SOVEREIGN] ${data.message}`, data.type || 'info');\n                } else if (data.source === 'Sovereign-Chat') {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');\n                } else if (data.source === 'Sovereign-DB') {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [DB] ${data.message}`, data.type || 'info');\n                } else if (data.source === 'Sovereign-Embed' || data.source === 'WebGPU-Embed') {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [EMBED] ${data.message}`, 'debug');\n                } else if (data.source === 'WebGPU-Chat') {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');\n                } else {\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${JSON.stringify(data)}`, 'debug');\n                }\n            } catch (e) {\n                appendLog(combinedContainer, `[coda_logs parsing error] ${e.message}`, 'error');\n            }\n        };\n\n        // Poll backend server logs every 2s with graceful 404 handling and retry\n        let pollIntervalId = null;\n        let serverLogsErrorShown = false;\n\n        async function pollServerLogs() {\n            try {\n                // Use the new logs endpoint\n                const resp = await fetch('http://localhost:8000/logs/recent');\n\n                if (resp.status === 404) {\n                    if (!serverLogsErrorShown) {\n                        appendLog(combinedContainer, `[ECE-Core] Logs endpoint returned 404. Is the backend running?`, 'warning');\n                        statusIndicator.style.color = '#ffcc00';\n                        statusIndicator.innerText = 'â— Backend logs unavailable';\n                        serverLogsErrorShown = true;\n                    }\n                    // Stop polling to avoid tight 404 loops\n                    if (pollIntervalId) {\n                        clearInterval(pollIntervalId);\n                        pollIntervalId = null;\n                    }\n                    return;\n                }\n\n                if (!resp.ok) {\n                    appendLog(combinedContainer, `[ECE-Core] Poll error: HTTP ${resp.status}`, 'error');\n                    return;\n                }\n\n                const payload = await resp.json();\n                if (payload && Array.isArray(payload.logs)) {\n                    for (const logEntry of payload.logs) {\n                        const line = `[${new Date(logEntry.timestamp).toLocaleTimeString()}] [${logEntry.source}] ${logEntry.message}`;\n                        if (!serverLogSet.has(line)) {\n                            serverLogSet.add(line);\n                            appendLog(combinedContainer, `[ECE-Core] ${line}`, logEntry.type || 'info');\n                        }\n                    }\n                    // Prevent unbounded growth\n                    if (serverLogSet.size > 500) {\n                        serverLogSet.clear();\n                    }\n                }\n            } catch (e) {\n                if (!serverLogsErrorShown) {\n                    appendLog(combinedContainer, `[ECE-Core] Poll error: ${e.message}`, 'error');\n                    statusIndicator.style.color = '#ff4444';\n                    statusIndicator.innerText = 'â— Poll error';\n                    serverLogsErrorShown = true;\n                }\n            }\n        }\n\n        function startPolling() {\n            if (pollIntervalId) return;\n            serverLogsErrorShown = false;\n            statusIndicator.style.color = '#0f0';\n            statusIndicator.innerText = 'â— Active';\n            pollIntervalId = setInterval(pollServerLogs, 2000);\n            pollServerLogs();\n        }\n\n        function stopPolling() {\n            if (pollIntervalId) {\n                clearInterval(pollIntervalId);\n                pollIntervalId = null;\n            }\n        }\n\n        // Start polling initially\n        startPolling();\n\n        // Retry button\n        document.getElementById('retry-logs-btn').addEventListener('click', () => {\n            appendLog(combinedContainer, '[ECE-Core] Manual retry requested', 'info');\n            statusIndicator.style.color = '#88ccff';\n            statusIndicator.innerText = 'â— Retrying...';\n            startPolling();\n        });\n\n        function appendLog(container, text, type) {\n            const div = document.createElement('div');\n            div.className = 'log-line';\n            div.innerHTML = colorize(text, type);\n            container.appendChild(div);\n            container.scrollTop = container.scrollHeight;\n        }\n\n        function colorize(line, type) {\n            if (type === 'error' || line.includes('ERROR') || line.includes('âŒ')) return `<span class=\"error\">${line}</span>`;\n            if (type === 'warn' || line.includes('WARNING')) return `<span class=\"warning\">${line}</span>`;\n            if (type === 'success' || line.includes('SUCCESS') || line.includes('âœ…')) return `<span class=\"success\">${line}</span>`;\n            if (type === 'info' || line.includes('INFO')) return `<span class=\"info\">${line}</span>`;\n            return `<span class=\"debug\">${line}</span>`;\n        }\n\n        clearBtn.onclick = () => {\n            combinedContainer.innerHTML = '';\n        };\n    </script>\n</body>\n\n</html>",
    "source": "tools\\log-viewer.html"
  },
  {
    "id": "tools\\low_resource_config.py",
    "timestamp": 1767229749,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nConfiguration for Anchor Core optimized for low-resource devices (phones, small laptops)\n\"\"\"\n\nimport os\nimport json\n\nclass LowResourceConfig:\n    \"\"\"Configuration optimized for low-resource devices\"\"\"\n    \n    def __init__(self):\n        # Conservative GPU settings for low VRAM devices\n        self.gpu_config = {\n            \"max_buffer_size\": 64 * 1024 * 1024,  # 64MB instead of 256MB\n            \"max_concurrent_operations\": 1,\n            \"use_compressed_weights\": True,\n            \"power_preference\": \"low-power\",\n            \"force_fallback_adapter\": False,\n            \"max_active_webgl_contexts\": 1,\n            \"max_webgl_contexts_per_group\": 1\n        }\n        \n        # Conservative model settings\n        self.model_config = {\n            \"default_model\": \"Phi-3.5-mini-instruct-q4f16_1-MLC\",  # Smallest recommended\n            \"max_model_size_gb\": 0.5,  # Maximum model size for low-resource devices\n            \"use_cpu_fallback\": True,  # Allow CPU fallback if GPU fails\n            \"context_window\": 2048,  # Reduced context window\n            \"batch_size\": 1,  # Minimal batch size\n            \"quantization\": \"q4f16_1\"  # Most compressed format\n        }\n        \n        # Memory management settings\n        self.memory_config = {\n            \"max_cache_size_mb\": 128,  # Reduced cache size\n            \"enable_disk_cache\": False,  # Disable disk cache to save space\n            \"gc_frequency\": 10,  # More frequent garbage collection\n            \"preload_models\": False  # Don't preload models\n        }\n        \n        # Network and performance settings\n        self.performance_config = {\n            \"timeout_seconds\": 120,  # Longer timeouts for slower devices\n            \"max_retries\": 3,\n            \"concurrent_requests\": 1,  # Single-threaded for low-resource\n            \"enable_compression\": True\n        }\n\n    def get_config(self):\n        \"\"\"Return the complete low-resource configuration\"\"\"\n        return {\n            \"gpu\": self.gpu_config,\n            \"model\": self.model_config,\n            \"memory\": self.memory_config,\n            \"performance\": self.performance_config\n        }\n\n# Singleton instance\nconfig = LowResourceConfig()\n\ndef get_low_resource_config():\n    \"\"\"Get the low-resource configuration\"\"\"\n    return config.get_config()\n\nif __name__ == \"__main__\":\n    # Print configuration for debugging\n    print(json.dumps(get_low_resource_config(), indent=2))",
    "source": "tools\\low_resource_config.py"
  },
  {
    "id": "tools\\memory-builder.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Root Dreamer (Subconscious)</title>\r\n    <link rel=\"icon\"\r\n        href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ™</text></svg>\">\r\n    <style>\r\n        :root {\r\n            --bg-color: #0d1117;\r\n            --text-color: #c9d1d9;\r\n            --border-color: #30363d;\r\n            --accent-color: #58a6ff;\r\n            --success-color: #238636;\r\n            --danger-color: #da3633;\r\n            --thought-color: #d29922;\r\n            --raw-color: #8b949e;\r\n        }\r\n\r\n        body {\r\n            background-color: var(--bg-color);\r\n            color: var(--text-color);\r\n            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif;\r\n            margin: 0;\r\n            padding: 20px;\r\n            display: flex;\r\n            flex-direction: column;\r\n            height: 100vh;\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        header {\r\n            display: flex;\r\n            justify-content: space-between;\r\n            align-items: center;\r\n            border-bottom: 1px solid var(--border-color);\r\n            padding-bottom: 10px;\r\n            margin-bottom: 20px;\r\n        }\r\n\r\n        h1 {\r\n            margin: 0;\r\n            font-size: 1.5rem;\r\n        }\r\n\r\n        .status-panel {\r\n            display: grid;\r\n            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\r\n            gap: 10px;\r\n            margin-bottom: 20px;\r\n        }\r\n\r\n        .card {\r\n            background: #161b22;\r\n            border: 1px solid var(--border-color);\r\n            border-radius: 6px;\r\n            padding: 15px;\r\n        }\r\n\r\n        .stat-value {\r\n            font-size: 2rem;\r\n            font-weight: bold;\r\n            color: var(--accent-color);\r\n        }\r\n\r\n        .stat-label {\r\n            font-size: 0.9rem;\r\n            color: #8b949e;\r\n        }\r\n\r\n        .controls {\r\n            display: flex;\r\n            gap: 10px;\r\n        }\r\n\r\n        button {\r\n            background-color: #21262d;\r\n            color: var(--text-color);\r\n            border: 1px solid var(--border-color);\r\n            padding: 8px 16px;\r\n            border-radius: 6px;\r\n            cursor: pointer;\r\n            font-size: 0.9rem;\r\n            transition: background-color 0.2s;\r\n        }\r\n\r\n        button:hover {\r\n            background-color: #30363d;\r\n        }\r\n\r\n        button.active {\r\n            background-color: var(--success-color);\r\n            color: white;\r\n            border-color: var(--success-color);\r\n        }\r\n\r\n        button.stop {\r\n            background-color: var(--danger-color);\r\n            border-color: var(--danger-color);\r\n        }\r\n\r\n        /* Tabs */\r\n        .tabs {\r\n            display: flex;\r\n            border-bottom: 1px solid var(--border-color);\r\n            margin-bottom: 0;\r\n        }\r\n\r\n        .tab {\r\n            padding: 10px 20px;\r\n            cursor: pointer;\r\n            border-bottom: 2px solid transparent;\r\n            color: #8b949e;\r\n        }\r\n\r\n        .tab:hover {\r\n            color: var(--text-color);\r\n        }\r\n\r\n        .tab.active {\r\n            color: var(--text-color);\r\n            border-bottom-color: var(--accent-color);\r\n        }\r\n\r\n        /* Logs */\r\n        .log-container {\r\n            flex-grow: 1;\r\n            background: #000;\r\n            border: 1px solid var(--border-color);\r\n            border-top: none;\r\n            border-bottom-left-radius: 6px;\r\n            border-bottom-right-radius: 6px;\r\n            padding: 10px;\r\n            font-family: 'Courier New', Courier, monospace;\r\n            font-size: 0.85rem;\r\n            overflow-y: auto;\r\n            white-space: pre-wrap;\r\n            display: none;\r\n            /* Hidden by default */\r\n        }\r\n\r\n        .log-container.active {\r\n            display: block;\r\n        }\r\n\r\n        .log-entry {\r\n            margin-bottom: 5px;\r\n            border-bottom: 1px solid #222;\r\n            padding-bottom: 2px;\r\n        }\r\n\r\n        .log-info {\r\n            color: #58a6ff;\r\n        }\r\n\r\n        .log-success {\r\n            color: #238636;\r\n        }\r\n\r\n        .log-warn {\r\n            color: #d29922;\r\n        }\r\n\r\n        .log-error {\r\n            color: #f85149;\r\n        }\r\n\r\n        .log-thought {\r\n            color: var(--thought-color);\r\n            font-style: italic;\r\n            border-left: 2px solid var(--thought-color);\r\n            padding-left: 10px;\r\n            margin: 5px 0;\r\n        }\r\n\r\n        .log-raw {\r\n            color: var(--raw-color);\r\n            font-size: 0.8rem;\r\n            opacity: 0.8;\r\n        }\r\n\r\n        .log-json {\r\n            color: var(--success-color);\r\n            font-weight: bold;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <header>\r\n        <h1>Root Dreamer (Subconscious)</h1>\r\n        <div class=\"controls\">\r\n            <button id=\"btn-reset\" class=\"stop\" style=\"margin-right: 10px;\">Reset Engine</button>\r\n            <button id=\"btn-toggle\">Wake Up</button>\r\n        </div>\r\n    </header>\r\n\r\n    <div class=\"status-panel\">\r\n        <div class=\"card\">\r\n            <div class=\"stat-value\" id=\"stat-processed\">0</div>\r\n            <div class=\"stat-label\">Memories Processed</div>\r\n        </div>\r\n        <div class=\"card\">\r\n            <div class=\"stat-value\" id=\"stat-synapses\">0</div>\r\n            <div class=\"stat-label\">Synapses Formed</div>\r\n        </div>\r\n        <div class=\"card\">\r\n            <div class=\"stat-value\" id=\"stat-model\">Loading...</div>\r\n            <div class=\"stat-label\">Model Status</div>\r\n        </div>\r\n    </div>\r\n\r\n    <div class=\"tabs\">\r\n        <div class=\"tab active\" data-target=\"log-main\">Stream</div>\r\n        <div class=\"tab\" data-target=\"log-thoughts\">Thoughts (Internal Monologue)</div>\r\n        <div class=\"tab\" data-target=\"log-synapses\">Synapses (Connections)</div>\r\n        <div class=\"tab\" data-target=\"log-raw\">Raw Protocol</div>\r\n    </div>\r\n\r\n    <div id=\"log-main\" class=\"log-container active\"></div>\r\n    <div id=\"log-thoughts\" class=\"log-container\"></div>\r\n    <div id=\"log-synapses\" class=\"log-container\"></div>\r\n    <div id=\"log-raw\" class=\"log-container\"></div>\r\n\r\n    <script type=\"module\">\r\n        import { AnchorLogger, createStore, getWebGPUConfig, initCozo, GPUController } from './modules/anchor.js';\r\n        import { CreateWebWorkerMLCEngine } from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n        const logger = new AnchorLogger('Dreamer');\r\n        const MODEL_ID = \"Qwen2.5-1.5B-Instruct-q4f32_1-MLC\";\r\n\r\n        // State\r\n        const store = createStore({\r\n            processedCount: 0,\r\n            synapseCount: 0,\r\n            isDreaming: false,\r\n            modelReady: false,\r\n            isReloading: false\r\n        });\r\n\r\n        // DOM Elements\r\n        const elProcessed = document.getElementById('stat-processed');\r\n        const elSynapses = document.getElementById('stat-synapses');\r\n        const elModel = document.getElementById('stat-model');\r\n        const elToggle = document.getElementById('btn-toggle');\r\n        const elReset = document.getElementById('btn-reset');\r\n\r\n        const logs = {\r\n            main: document.getElementById('log-main'),\r\n            thoughts: document.getElementById('log-thoughts'),\r\n            synapses: document.getElementById('log-synapses'),\r\n            raw: document.getElementById('log-raw')\r\n        };\r\n\r\n        // Tab Switching\r\n        document.querySelectorAll('.tab').forEach(t => {\r\n            t.addEventListener('click', () => {\r\n                document.querySelectorAll('.tab').forEach(x => x.classList.remove('active'));\r\n                document.querySelectorAll('.log-container').forEach(x => x.classList.remove('active'));\r\n                t.classList.add('active');\r\n                document.getElementById(t.dataset.target).classList.add('active');\r\n            });\r\n        });\r\n\r\n        // Logging Helper\r\n        function appendLog(targetId, msg, type = 'info') {\r\n            const container = logs[targetId];\r\n            if (!container) return;\r\n            const div = document.createElement('div');\r\n            if (type === 'thought') div.className = 'log-thought';\r\n            else if (type === 'json') div.className = 'log-json';\r\n            else if (type === 'raw') div.className = 'log-raw';\r\n            else div.className = `log-entry log-${type}`;\r\n            const ts = `[${new Date().toLocaleTimeString()}] `;\r\n            div.textContent = (type === 'thought' || type === 'json' || type === 'raw') ? msg : ts + msg;\r\n            container.appendChild(div);\r\n            container.scrollTop = container.scrollHeight;\r\n        }\r\n\r\n        // State Subscription\r\n        store.subscribe((prop, val) => {\r\n            if (prop === 'processedCount') elProcessed.textContent = val;\r\n            if (prop === 'synapseCount') elSynapses.textContent = val;\r\n            if (prop === 'modelReady') elModel.textContent = val ? \"Ready\" : \"Loading...\";\r\n            if (prop === 'isDreaming') {\r\n                elToggle.textContent = val ? \"Sleep\" : \"Wake Up\";\r\n                elToggle.className = val ? \"active\" : \"\";\r\n            }\r\n        });\r\n\r\n        // --- NEW: Consciousness Semaphore Listener ---\r\n        const stateChannel = new BroadcastChannel('sovereign-state');\r\n        stateChannel.onmessage = (e) => {\r\n            if (e.data.type === 'STATE_CHANGE') {\r\n                const state = e.data.state;\r\n                if (state === 'COGNITION' || state === 'LISTENING') {\r\n                    if (store.state.isDreaming) {\r\n                        appendLog('main', `[Semaphore] Pausing for ${state}...`, 'warn');\r\n                        store.state.isDreaming = false; // Auto-pause\r\n                    }\r\n                } else if (state === 'IDLE') {\r\n                    // Optional: Auto-resume? For now, let's keep manual control for safety.\r\n                    // appendLog('main', `[Semaphore] System Idle.`, 'info');\r\n                }\r\n            }\r\n        };\r\n\r\n        let db;\r\n        async function runQuery(query, params = \"{}\", immutable = false) {\r\n            if (!db) throw new Error(\"Database not initialized\");\r\n            const res = await db.run(query, params, immutable);\r\n            let parsed = res;\r\n            if (typeof res === 'string') {\r\n                try { parsed = JSON.parse(res); } catch (e) { logger.warn(\"Failed to parse Cozo response\"); }\r\n            }\r\n            if (parsed && parsed.ok === false) throw new Error(parsed.message);\r\n            return parsed;\r\n        }\r\n\r\n        async function pruneMemories() {\r\n            const cutoff = Date.now() - (7 * 24 * 60 * 60 * 1000); // 7 Days\r\n            appendLog('main', \"Running Hygiene Check (Pruning)...\", 'info');\r\n            \r\n            // Find old, non-user, vector-less memories\r\n            const findQuery = `?[id] := *memory{id, timestamp, role, embedding}, timestamp < ${cutoff}, role != 'user', is_null(embedding)`;\r\n            const res = await runQuery(findQuery);\r\n            \r\n            if (res.rows && res.rows.length > 0) {\r\n                const ids = res.rows.map(r => r[0]);\r\n                appendLog('main', `Pruning ${ids.length} stale memories...`, 'warn');\r\n                // Batch delete\r\n                await runQuery(`?[id] <- $ids :delete memory {id}`, JSON.stringify({ ids }));\r\n            } else {\r\n                appendLog('main', \"Hygiene Pass: No stale memories found.\", 'success');\r\n            }\r\n        }\r\n\r\n        async function loadModel() {\r\n            if (store.state.isReloading) return; // Prevent double-loading\r\n            store.state.isReloading = true;\r\n            store.state.modelReady = false;\r\n            \r\n            appendLog('main', `Checking Hardware Compatibility...`);\r\n            \r\n            try {\r\n                // Pre-check hardware capabilities\r\n                const gpuConfig = await getWebGPUConfig('lite');\r\n                if (gpuConfig.isConstrained) {\r\n                     const limitMB = Math.round(gpuConfig.maxBufferSize / 1024 / 1024);\r\n                     appendLog('main', `âš ï¸ Hardware Constrained (Buffer Limit: ${limitMB}MB).`, 'error');\r\n                     appendLog('main', `âŒ Dreamer disabled to prevent System Crash.`, 'error');\r\n                     elModel.textContent = \"Disabled (Low VRAM)\";\r\n                     elToggle.disabled = true;\r\n                     store.state.isReloading = false;\r\n                     return; \r\n                }\r\n\r\n                appendLog('main', `Loading Model: ${MODEL_ID}...`);\r\n\r\n                await GPUController.withModelLoadLock(\"Dreamer-Init\", async () => {\r\n                    // Create a new worker for the Dreamer to avoid conflicts with other components\r\n                    const worker = new Worker('./modules/llm-worker.js', { type: 'module' });\r\n\r\n                    const config = {\r\n                        model_list: [{\r\n                            model: \"https://huggingface.co/mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC/resolve/main/\",\r\n                            model_id: \"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\",\r\n                            model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                            vram_required_MB: 2000,\r\n                            low_resource_required: true\r\n                        }]\r\n                    };\r\n\r\n                    // Safe Dispose\r\n                    if (window.dreamerEngine) {\r\n                        try {\r\n                            appendLog('main', \"Disposing old engine...\", 'raw');\r\n                            await window.dreamerEngine.dispose();\r\n                        } catch (disposeErr) {\r\n                            console.warn(\"Dispose error (safe to ignore):\", disposeErr);\r\n                        }\r\n                        window.dreamerEngine = null;\r\n                        window.engine = null;\r\n                    }\r\n\r\n                    // Use a dedicated engine instance\r\n                    window.dreamerEngine = await CreateWebWorkerMLCEngine(worker, \"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\", {\r\n                        appConfig: config,\r\n                        initProgressCallback: (report) => {\r\n                            elModel.textContent = report.text;\r\n                        }\r\n                    });\r\n\r\n                    window.engine = window.dreamerEngine;\r\n                });\r\n                \r\n                appendLog('main', \"Model Loaded. Ready to dream.\", 'success');\r\n                store.state.modelReady = true;\r\n                store.state.isReloading = false;\r\n                \r\n                // Auto-resume if we were dreaming\r\n                if (store.state.isDreaming) {\r\n                    appendLog('main', \"Resuming Dream Loop...\", 'info');\r\n                    dreamLoop();\r\n                }\r\n\r\n            } catch (error) {\r\n                appendLog('main', `Model Load Error: ${error.message}`, 'error');\r\n                store.state.isReloading = false; // Release lock\r\n\r\n                // Retry logic\r\n                setTimeout(() => {\r\n                    appendLog('main', \"Retrying model load in 5 seconds...\", 'info');\r\n                    loadModel();\r\n                }, 5000);\r\n            }\r\n        }\r\n\r\n        async function init() {\r\n            try {\r\n                appendLog('main', \"Initializing CozoDB...\");\r\n                const CozoDb = await initCozo();\r\n                try {\r\n                    db = await CozoDb.new_from_indexed_db('coda_memory', 'cozo_store', () => { });\r\n                    appendLog('main', \"Connected to Persistent Brain.\", 'success');\r\n                } catch (e) {\r\n                    db = CozoDb.new();\r\n                    appendLog('main', \"Connected to Transient Brain.\", 'warn');\r\n                }\r\n\r\n                // Tables - Idempotent Creation\r\n                try {\r\n                    await runQuery(\":create memory { id: String => timestamp: Int, role: String, content: String, source: String, embedding: <F32; 384>? }\");\r\n                } catch (e) {\r\n                    if (!e.message.includes('conflicts')) throw e;\r\n                }\r\n\r\n                // Create FTS index for BM25 search on content field\r\n                try {\r\n                    await runQuery(\"::fts create memory:content_fts { extractor: content, tokenizer: Simple, filters: [Lowercase, AlphaNumOnly, Stemmer('english')] }\");\r\n                } catch (e) {\r\n                    // Index might already exist, which is fine\r\n                    if (!e.message.includes('already exists') && !e.message.includes('conflicts')) {\r\n                        console.warn(\"FTS index creation failed (may already exist):\", e.message);\r\n                    }\r\n                }\r\n\r\n                try {\r\n                    await runQuery(\":create relationships { source_id: String, target_id: String, type: String => weight: Float }\");\r\n                } catch (e) {\r\n                    if (!e.message.includes('conflicts')) throw e;\r\n                }\r\n\r\n                await loadModel();\r\n            } catch (e) {\r\n                const msg = e.message || String(e);\r\n                appendLog('main', `Initialization Error: ${msg}`, 'error');\r\n            }\r\n        }\r\n\r\n        let isLoopRunning = false;\r\n        let dreamCycles = 0; // Hygiene Counter\r\n\r\n        async function dreamLoop() {\r\n            // STRICT GUARDS\r\n            if (isLoopRunning || !store.state.isDreaming || store.state.isReloading) return;\r\n            if (!store.state.modelReady || !window.dreamerEngine) {\r\n                // Do not wait/retry here. The loadModel success handler will restart us.\r\n                return; \r\n            }\r\n\r\n            isLoopRunning = true;\r\n            try {\r\n                // HYGIENE CHECK (Every 10 cycles)\r\n                dreamCycles++;\r\n                if (dreamCycles % 10 === 0) {\r\n                    await pruneMemories();\r\n                }\r\n\r\n                // Find memories that don't have relationships\r\n                const query = `?[id, content] := *memory{id: id, content: content}, not *relationships{source_id: id} :limit 1`;\r\n                const result = await runQuery(query);\r\n\r\n                if (!result.rows || result.rows.length === 0) {\r\n                    appendLog('main', \"No orphans found. Resting...\", 'info');\r\n                    await new Promise(r => setTimeout(r, 5000));\r\n                    isLoopRunning = false;\r\n                    if (store.state.isDreaming) dreamLoop();\r\n                    return;\r\n                }\r\n\r\n                const [id, content] = result.rows[0];\r\n                appendLog('main', `Dreaming on memory: ${id.substring(0, 15)}...`);\r\n                const safeContent = content.length > 3000 ? content.substring(0, 3000) + \"...\" : content;\r\n\r\n                const prompt = `Analyze this memory. Output Step 1: Thoughts (monologue) and Step 2: JSON array of concepts. Use <thought> and <json> tags. Memory: ${safeContent}`;\r\n\r\n                await GPUController.withLock(\"Dreamer-Think\", async () => {\r\n                    try {\r\n                        const response = await window.dreamerEngine.chat.completions.create({\r\n                            messages: [{ role: \"user\", content: prompt }],\r\n                            temperature: 0.3,\r\n                            max_tokens: 1024\r\n                        });\r\n\r\n                        const rawReply = response.choices[0].message.content;\r\n                        const thoughtMatch = rawReply.match(/<thought>([\\s\\S]*?)<\\/thought>/);\r\n                        const jsonMatch = rawReply.match(/<json>([\\s\\S]*?)<\\/json>/);\r\n\r\n                        if (thoughtMatch) appendLog('thoughts', `[${id}] ${thoughtMatch[1].trim()}`, 'thought');\r\n\r\n                        let relationships = [];\r\n                        try {\r\n                            let jsonStr = jsonMatch ? jsonMatch[1] : rawReply;\r\n                            jsonStr = jsonStr.replace(/```json/g, '').replace(/```/g, '').trim();\r\n                            const start = jsonStr.indexOf('[');\r\n                            const end = jsonStr.lastIndexOf(']');\r\n                            if (start !== -1 && end !== -1) {\r\n                                relationships = JSON.parse(jsonStr.substring(start, end + 1));\r\n                                appendLog('synapses', `[${id}] Synthesized ${relationships.length} links.`, 'json');\r\n                            }\r\n                        } catch (pe) {\r\n                            relationships = [{ target: \"failed_parse\", type: \"system_flag\", weight: 0.0 }];\r\n                        }\r\n\r\n                        const rows = relationships.map(r => [String(id), String(r.target || \"unknown\"), String(r.type || 'relates_to'), parseFloat(r.weight || 0.5)]);\r\n                        if (rows.length > 0) {\r\n                            await runQuery(`?[src, tgt, type, w] <- $rows :put relationships {source_id: src, target_id: tgt, type: type, weight: w}`, JSON.stringify({ rows }));\r\n                            store.state.processedCount++;\r\n                            store.state.synapseCount += rows.length;\r\n                        }\r\n                    } catch (inferenceError) {\r\n                        if (inferenceError.message && (inferenceError.message.includes(\"disposed\") || inferenceError.message.includes(\"Instance reference no longer exists\"))) {\r\n                            throw new Error(\"ENGINE_DISPOSED\"); // Re-throw to outer catch\r\n                        }\r\n                        throw inferenceError; \r\n                    }\r\n                });\r\n                \r\n                await new Promise(r => setTimeout(r, 2000));\r\n                \r\n                // Normal continuation\r\n                isLoopRunning = false;\r\n                if (store.state.isDreaming) dreamLoop();\r\n\r\n            } catch (err) {\r\n                isLoopRunning = false;\r\n                const msg = err.message || String(err);\r\n                \r\n                if (msg.includes(\"ENGINE_DISPOSED\") || msg.includes(\"disposed\") || msg.includes(\"Model not loaded\")) {\r\n                    appendLog('main', \"âš ï¸ Engine Lost. Triggering Emergency Reload...\", 'warn');\r\n                    store.state.modelReady = false;\r\n                    loadModel(); // This initiates the reload sequence\r\n                    // Do NOT call dreamLoop() here. loadModel success will do it.\r\n                    return;\r\n                }\r\n                \r\n                appendLog('main', `Dream Error: ${msg}`, 'error`);\r\n                // For non-fatal errors, wait and retry\r\n                await new Promise(r => setTimeout(r, 5000));\r\n                if (store.state.isDreaming) dreamLoop();\r\n            } \r\n        }\r\n\r\n        elToggle.addEventListener('click', () => {\r\n            store.state.isDreaming = !store.state.isDreaming;\r\n            if (store.state.isDreaming) dreamLoop();\r\n        });\r\n\r\n        elReset.addEventListener('click', async () => {\r\n            store.state.isDreaming = false;\r\n            store.state.modelReady = false;\r\n            appendLog('main', \"ðŸ›‘ Resetting Engine...\", 'warn');\r\n            await loadModel();\r\n        });\r\n\r\n        init();\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\memory-builder.html"
  },
  {
    "id": "tools\\mobile-chat.html",
    "timestamp": 1766310901,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\">\r\n    <title>Sovereign Coda Mobile</title>\r\n    <style>\r\n        :root {\r\n            --bg: #0f1115;\r\n            --surface: #1a1d23;\r\n            --primary: #3b82f6;\r\n            --text: #e2e8f0;\r\n            --text-dim: #94a3b8;\r\n        }\r\n\r\n        body {\r\n            margin: 0;\r\n            background: var(--bg);\r\n            color: var(--text);\r\n            font-family: -apple-system, system-ui, sans-serif;\r\n            display: flex;\r\n            flex-direction: column;\r\n            height: 100vh;\r\n            overflow: hidden;\r\n        }\r\n\r\n        /* Login Modal */\r\n        #auth-overlay {\r\n            position: fixed;\r\n            top: 0;\r\n            left: 0;\r\n            right: 0;\r\n            bottom: 0;\r\n            background: var(--bg);\r\n            display: flex;\r\n            align-items: center;\r\n            justify-content: center;\r\n            z-index: 1000;\r\n        }\r\n\r\n        .card {\r\n            background: var(--surface);\r\n            padding: 2rem;\r\n            border-radius: 12px;\r\n            width: 90%;\r\n            max-width: 320px;\r\n            text-align: center;\r\n            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);\r\n        }\r\n\r\n        input {\r\n            width: 100%;\r\n            padding: 12px;\r\n            margin: 10px 0;\r\n            border-radius: 8px;\r\n            border: 1px solid #333;\r\n            background: #000;\r\n            color: white;\r\n            font-size: 16px;\r\n            box-sizing: border-box;\r\n            /* Fix padding expanding width */\r\n        }\r\n\r\n        button {\r\n            width: 100%;\r\n            padding: 12px;\r\n            border-radius: 8px;\r\n            border: none;\r\n            background: var(--primary);\r\n            color: white;\r\n            font-weight: bold;\r\n            font-size: 16px;\r\n            cursor: pointer;\r\n        }\r\n\r\n        button:active {\r\n            opacity: 0.8;\r\n            transform: scale(0.98);\r\n        }\r\n\r\n        /* Chat UI */\r\n        header {\r\n            padding: 15px;\r\n            background: var(--surface);\r\n            border-bottom: 1px solid #333;\r\n            display: flex;\r\n            justify-content: space-between;\r\n            align-items: center;\r\n        }\r\n\r\n        #chat-box {\r\n            flex: 1;\r\n            overflow-y: auto;\r\n            padding: 15px;\r\n            display: flex;\r\n            flex-direction: column;\r\n            gap: 12px;\r\n        }\r\n\r\n        .msg {\r\n            max-width: 85%;\r\n            padding: 10px 14px;\r\n            border-radius: 16px;\r\n            line-height: 1.4;\r\n            word-wrap: break-word;\r\n        }\r\n\r\n        .user {\r\n            align-self: flex-end;\r\n            background: var(--primary);\r\n            color: white;\r\n            border-bottom-right-radius: 4px;\r\n        }\r\n\r\n        .assistant {\r\n            align-self: flex-start;\r\n            background: var(--surface);\r\n            color: var(--text);\r\n            border-bottom-left-radius: 4px;\r\n        }\r\n\r\n        .system {\r\n            align-self: center;\r\n            font-size: 0.8rem;\r\n            color: var(--text-dim);\r\n            background: transparent;\r\n        }\r\n\r\n        #input-area {\r\n            padding: 10px;\r\n            background: var(--surface);\r\n            display: flex;\r\n            gap: 10px;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n\r\n    <!-- Auth Modal -->\r\n    <div id=\"auth-overlay\">\r\n        <div class=\"card\">\r\n            <h2>ðŸ”’ Secure Access</h2>\r\n            <p style=\"color:var(--text-dim); margin-bottom:20px\">Enter the bridge token from your PC console.</p>\r\n            <input type=\"password\" id=\"token-input\" placeholder=\"Paste Token Here\">\r\n            <button onclick=\"saveToken()\">Connect</button>\r\n        </div>\r\n    </div>\r\n\r\n    <!-- Main Chat -->\r\n    <header>\r\n        <span style=\"font-weight:bold\">Sovereign Coda</span>\r\n        <button onclick=\"logout()\" style=\"width:auto; padding:5px 10px; font-size:12px; background:#333\">Logout</button>\r\n    </header>\r\n\r\n    <div id=\"chat-box\"></div>\r\n\r\n    <div id=\"input-area\">\r\n        <input type=\"text\" id=\"msg-input\" placeholder=\"Message...\" onkeypress=\"handleKey(event)\" autocomplete=\"off\">\r\n        <button onclick=\"send()\" style=\"width:60px\">âž¤</button>\r\n    </div>\r\n\r\n    <script>\r\n        let TOKEN = localStorage.getItem('bridge_token');\r\n        const DOM = {\r\n            auth: document.getElementById('auth-overlay'),\r\n            input: document.getElementById('msg-input'),\r\n            box: document.getElementById('chat-box')\r\n        };\r\n\r\n        if (TOKEN) {\r\n            DOM.auth.style.display = 'none';\r\n        }\r\n\r\n        function saveToken() {\r\n            const val = document.getElementById('token-input').value.trim();\r\n            if (val) {\r\n                localStorage.setItem('bridge_token', val);\r\n                TOKEN = val;\r\n                DOM.auth.style.display = 'none';\r\n            }\r\n        }\r\n\r\n        function logout() {\r\n            localStorage.removeItem('bridge_token');\r\n            location.reload();\r\n        }\r\n\r\n        function append(role, text) {\r\n            const div = document.createElement('div');\r\n            div.className = `msg ${role}`;\r\n            div.innerText = text;\r\n            DOM.box.appendChild(div);\r\n            DOM.box.scrollTop = DOM.box.scrollHeight;\r\n            return div;\r\n        }\r\n\r\n        function handleKey(e) {\r\n            if (e.key === 'Enter') send();\r\n        }\r\n\r\n        async function send() {\r\n            const text = DOM.input.value.trim();\r\n            if (!text) return;\r\n\r\n            DOM.input.value = '';\r\n            append('user', text);\r\n\r\n            // Create pending placeholder\r\n            const loadingDiv = append('assistant', '...');\r\n\r\n            try {\r\n                const res = await fetch('/v1/chat/completions', {\r\n                    method: 'POST',\r\n                    headers: {\r\n                        'Content-Type': 'application/json',\r\n                        'Authorization': `Bearer ${TOKEN}`\r\n                    },\r\n                    body: JSON.stringify({\r\n                        model: \"mobile-chat\", // Bridge ignores this usually, or we can fetch models\r\n                        messages: [{ role: \"user\", content: text }],\r\n                        stream: false // Simple non-streaming for mobile stability first\r\n                    })\r\n                });\r\n\r\n                if (res.status === 401) {\r\n                    loadingDiv.innerText = \"âŒ Unauthorized. Check Token.\";\r\n                    logout(); // Force re-login\r\n                    return;\r\n                }\r\n\r\n                if (!res.ok) {\r\n                    throw new Error(`Server Error: ${res.status}`);\r\n                }\r\n\r\n                const data = await res.json();\r\n                const reply = data.choices?.[0]?.message?.content || \"No response.\";\r\n                loadingDiv.innerText = reply;\r\n\r\n            } catch (e) {\r\n                loadingDiv.innerText = `Error: ${e.message}`;\r\n                loadingDiv.style.color = 'red';\r\n            }\r\n        }\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\mobile-chat.html"
  },
  {
    "id": "tools\\orchestrator.py",
    "timestamp": 1767195827,
    "role": "file",
    "content": "import os\r\nimport requests\r\nimport json\r\nimport logging\r\n\r\n# Config - Defaults match start-bridge.bat\r\nBRIDGE_PORT = os.getenv(\"BRIDGE_PORT\", \"8000\")\r\nBRIDGE_HOST = os.getenv(\"BRIDGE_HOST\", \"localhost\")\r\nBRIDGE_URL = f\"http://{BRIDGE_HOST}:{BRIDGE_PORT}\"\r\nMLC_INFERENCE_ENDPOINT = f\"{BRIDGE_URL}/v1/chat/completions\"\r\nBRIDGE_TOKEN = os.getenv(\"BRIDGE_TOKEN\", \"sovereign-secret\")\r\n\r\nlogger = logging.getLogger(\"Orchestrator\")\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nclass MLCConnectionError(Exception):\r\n    \"\"\"Raised when connection to the MLC Bridge fails.\"\"\"\r\n    pass\r\n\r\nclass Orchestrator:\r\n    def __init__(self, endpoint=MLC_INFERENCE_ENDPOINT, token=BRIDGE_TOKEN):\r\n        self.endpoint = endpoint\r\n        self.token = token\r\n        self.active_model = None\r\n        self.headers = {\r\n            \"Authorization\": f\"Bearer {self.token}\",\r\n            \"Content-Type\": \"application/json\"\r\n        }\r\n\r\n    def load_mlc_model(self, model_name):\r\n        \"\"\"\r\n        Configures the orchestrator for a specific model.\r\n        Verifies connectivity to the Bridge and logs available models.\r\n        \"\"\"\r\n        logger.info(f\"Preparing Orchestrator for model: {model_name}\")\r\n        \r\n        try:\r\n            # Health check / Model list\r\n            r = requests.get(f\"{BRIDGE_URL}/v1/models\", headers=self.headers, timeout=2.0)\r\n            r.raise_for_status()\r\n            models_data = r.json().get(\"data\", [])\r\n            available_ids = [m['id'] for m in models_data]\r\n            logger.info(f\"Bridge connected. Active Workers: {available_ids}\")\r\n            \r\n            if not available_ids:\r\n                logger.warning(\"No WebGPU workers connected to Bridge. Open model-server-chat.html\")\r\n            \r\n        except requests.exceptions.RequestException as e:\r\n            raise MLCConnectionError(f\"Failed to connect to Wave Bridge at {BRIDGE_URL}. Ensure start-bridge.bat is running. Error: {e}\")\r\n\r\n        self.active_model = model_name\r\n        return True\r\n\r\n    def invoke_mlc_inference(self, prompt, system_prompt=\"You are a helpful AI orchestrator.\"):\r\n        \"\"\"\r\n        Sends a prompt to the MLC engine via the Bridge.\r\n        \"\"\"\r\n        if not self.active_model:\r\n            raise ValueError(\"No model loaded. Call load_mlc_model() first.\")\r\n\r\n        payload = {\r\n            \"model\": self.active_model,\r\n            \"messages\": [\r\n                {\"role\": \"system\", \"content\": system_prompt},\r\n                {\"role\": \"user\", \"content\": prompt}\r\n            ],\r\n            \"stream\": False\r\n        }\r\n\r\n        try:\r\n            logger.info(f\"Sending inference request to {self.endpoint}...\")\r\n            response = requests.post(\r\n                self.endpoint, \r\n                json=payload, \r\n                headers=self.headers,\r\n                timeout=60 # Inference can be slow\r\n            )\r\n            response.raise_for_status()\r\n            \r\n            result = self._parse_response(response.json())\r\n            return result\r\n\r\n        except requests.exceptions.RequestException as e:\r\n            logger.error(f\"Inference request failed: {e}\")\r\n            raise MLCConnectionError(f\"Inference failed: {e}\")\r\n\r\n    def _parse_response(self, mlc_response):\r\n        \"\"\"\r\n        Extracts the content from the OpenAI-compatible JSON response.\r\n        \"\"\"\r\n        try:\r\n            # Check for bridge-reported errors\r\n            if \"error\" in mlc_response:\r\n                raise Exception(mlc_response[\"error\"])\r\n\r\n            choices = mlc_response.get(\"choices\", [])\r\n            if not choices:\r\n                logger.warning(\"Empty choices in response\")\r\n                return \"\"\r\n            \r\n            content = choices[0].get(\"message\", {}).get(\"content\", \"\")\r\n            return content\r\n        except Exception as e:\r\n            logger.error(f\"Error parsing MLC response: {e}\")\r\n            return f\"[Error parsing output: {e}]\"\r\n\r\n# CLI usage\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n    parser = argparse.ArgumentParser(description=\"Orchestrator CLI\")\r\n    parser.add_argument(\"--prompt\", type=str, help=\"Prompt to send\")\r\n    parser.add_argument(\"--model\", type=str, default=\"webgpu-chat\", help=\"Model ID\")\r\n    args = parser.parse_args()\r\n\r\n    if args.prompt:\r\n        orc = Orchestrator()\r\n        try:\r\n            orc.load_mlc_model(args.model)\r\n            print(f\"\\nResponse:\\n{orc.invoke_mlc_inference(args.prompt)}\")\r\n        except Exception as e:\r\n            print(f\"Error: {e}\")\r\n    else:\r\n        print(\"Usage: python orchestrator.py --prompt 'Hello'\")\r\n",
    "source": "tools\\orchestrator.py"
  },
  {
    "id": "tools\\prepare_cozo_import.py",
    "timestamp": 1766310901,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nPrepare a CozoDB import payload from an existing combined_memory.json file.\r\nUsage:\r\n  python tools/prepare_cozo_import.py [input_path] [output_path]\r\nIf input_path is omitted the script will search likely locations.\r\n\"\"\"\r\nimport json\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n# Defaults\r\nPOSSIBLE_INPUTS = [\r\n    Path(\"combined_memory.json\")\r\n]\r\nDEFAULT_OUTPUT = Path(\"cozo_import_memory.json\")\r\n\r\ndef find_input(path_arg=None):\r\n    if path_arg:\r\n        p = Path(path_arg)\r\n        if p.exists():\r\n            return p\r\n        else:\r\n            print(f\"âŒ Specified input not found: {p}\")\r\n            return None\r\n    for p in POSSIBLE_INPUTS:\r\n        if p.exists():\r\n            return p\r\n    # fallback: search workspace for first combined_memory.json\r\n    for p in Path('.').rglob('combined_memory.json'):\r\n        return p\r\n    return None\r\n\r\n\r\ndef normalize_record(rec):\r\n    # Ensure the fields Cozo expects. Return None to skip invalid records.\r\n    uid = rec.get(\"id\") or rec.get(\"uid\") or rec.get(\"uuid\") or None\r\n    if not uid:\r\n        # try deterministic id from source+timestamp\r\n        src = rec.get(\"source\") or rec.get(\"file\") or \"\"\r\n        ts = rec.get(\"timestamp\") or rec.get(\"created_at\") or 0\r\n        uid = f\"auto:{abs(hash(src + str(ts)))}\"\r\n    try:\r\n        uid = str(uid)\r\n    except Exception:\r\n        uid = str(uid)\r\n\r\n    try:\r\n        ts = int(rec.get(\"timestamp\", rec.get(\"created_at\", 0)) or 0)\r\n    except Exception:\r\n        try:\r\n            ts = int(float(rec.get(\"timestamp\", 0)))\r\n        except Exception:\r\n            ts = 0\r\n\r\n    role = str(rec.get(\"role\", \"system\"))\r\n    content = rec.get(\"content\", \"\")\r\n    if not isinstance(content, str):\r\n        try:\r\n            content = json.dumps(content, ensure_ascii=False)\r\n        except Exception:\r\n            content = str(content)\r\n    # cap content size to be safe\r\n    MAX_CONTENT = 200_000\r\n    if len(content) > MAX_CONTENT:\r\n        content = content[:MAX_CONTENT]\r\n\r\n    source = rec.get(\"source\", rec.get(\"file\", \"historical_import\"))\r\n    try:\r\n        source = str(source)\r\n    except Exception:\r\n        source = \"historical_import\"\r\n\r\n    embedding = rec.get(\"embedding\", None)\r\n    # Keep embedding as-is if present and looks like a list of numbers\r\n    if isinstance(embedding, list):\r\n        # Optionally validate length later; leave as-is\r\n        pass\r\n    else:\r\n        embedding = None\r\n\r\n    return [uid, ts, role, content, source, embedding]\r\n\r\n\r\ndef main():\r\n    input_arg = sys.argv[1] if len(sys.argv) > 1 else None\r\n    output_arg = sys.argv[2] if len(sys.argv) > 2 else None\r\n\r\n    inp = find_input(input_arg)\r\n    if not inp:\r\n        print(\"âŒ Could not find a combined_memory.json file. Provide the path as the first argument.\")\r\n        return 1\r\n\r\n    out = Path(output_arg) if output_arg else DEFAULT_OUTPUT\r\n\r\n    print(f\"Reading {inp}...\")\r\n    try:\r\n        raw = json.loads(inp.read_text(encoding='utf-8'))\r\n    except json.JSONDecodeError as e:\r\n        print(f\"âŒ JSON decode error: {e}\")\r\n        return 1\r\n\r\n    if not isinstance(raw, list):\r\n        print(\"â— Warning: input root is not a list. Attempting to find list under 'records' or 'data'.\")\r\n        if isinstance(raw, dict):\r\n            if 'records' in raw and isinstance(raw['records'], list):\r\n                raw = raw['records']\r\n            elif 'data' in raw and isinstance(raw['data'], list):\r\n                raw = raw['data']\r\n            else:\r\n                print(\"âŒ Could not find the expected list of records in input JSON.\")\r\n                return 1\r\n\r\n    print(f\"Found {len(raw)} records. Normalizing and formatting...\")\r\n\r\n    rows = []\r\n    for rec in raw:\r\n        nr = normalize_record(rec)\r\n        if nr is None:\r\n            continue\r\n        rows.append(nr)\r\n\r\n    payload = {\r\n        \"relations\": [\r\n            {\r\n                \"name\": \"memory\",\r\n                \"headers\": [\"id\", \"timestamp\", \"role\", \"content\", \"source\", \"embedding\"],\r\n                \"rows\": rows,\r\n            }\r\n        ]\r\n    }\r\n\r\n    print(f\"Writing {len(rows)} rows to {out}...\")\r\n    out.write_text(json.dumps(payload, ensure_ascii=False, separators=(',', ':')), encoding='utf-8')\r\n    print(\"âœ… Done. You can now drag 'cozo_import_memory.json' into the Builder or use the console import helper.\")\r\n    return 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    raise SystemExit(main())\r\n",
    "source": "tools\\prepare_cozo_import.py"
  },
  {
    "id": "tools\\README.md",
    "timestamp": 1767225090,
    "role": "file",
    "content": "# Tools Directory\n\nContains the browser-native HTML/JS applications (Console, Builder, Mic) and the Python WebGPU bridge for the Sovereign Context Engine.\n\n## Core Components\n\n- `webgpu_bridge.py` - The unified bridge serving UI, API, and orchestrating system components\n- `anchor.py` - Native terminal client for the Anchor system\n- `orchestrator.py` - Component orchestration logic\n\n## Vision and AI Components\n\n- `vision_engine.py` - Python-powered Vision Language Model (VLM) integration for image analysis\n- `modules/` - JavaScript modules for the browser-based kernel\n\n## UI Components\n\n- `sidecar.html` - Browser-based control center with dual tabs for context retrieval and vision ingestion\n- `context.html` - Manual context retrieval UI with scrollable display and one-click copy functionality\n- `chat.html` - Main chat interface for interacting with WebGPU-powered LLMs\n- `terminal.html` - Web-based terminal interface\n- `anchor-mic.html` - Audio input interface\n- `memory-builder.html` - Memory management interface\n- `db_builder.html` - Database builder interface\n\n## Test Files\n\n- `test_model_loading.py` - Model loading functionality tests\n- `test_model_availability.py` - Model availability verification tests\n- `test_orchestrator.py` - Orchestrator component tests\n\n## Usage\n\n### Browser-Based Control Center\n1. Start the Anchor Core with `start-anchor.bat`\n2. Access the sidecar at `http://localhost:8000/sidecar`\n3. Access the context UI at `http://localhost:8000/context`\n\n### Vision Integration\n1. Ensure Ollama is running with a VLM model (e.g., `ollama pull llava`)\n2. Upload images via the Vision tab in the sidecar\n3. Images will be processed and stored in the memory graph\n",
    "source": "tools\\README.md"
  },
  {
    "id": "tools\\sidecar.html",
    "timestamp": 1767230099,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Anchor Sidecar</title>\n    <style>\n        :root { --bg: #0f172a; --panel: #1e293b; --text: #e2e8f0; --accent: #38bdf8; }\n        body { background: var(--bg); color: var(--text); font-family: sans-serif; margin: 0; display: flex; height: 100vh; }\n        .sidebar { width: 60px; background: #000; display: flex; flex-direction: column; align-items: center; padding-top: 20px; }\n        .tab-btn { width: 40px; height: 40px; margin-bottom: 10px; border-radius: 8px; border: none; background: #334155; color: #fff; cursor: pointer; font-size: 20px; }\n        .tab-btn.active { background: var(--accent); color: #000; }\n        .main { flex: 1; padding: 20px; overflow-y: auto; display: none; }\n        .main.active { display: block; }\n        .card { background: var(--panel); padding: 20px; border-radius: 12px; margin-bottom: 20px; border: 1px solid #334155; }\n        textarea { width: 100%; background: #000; color: #0f0; border: 1px solid #334155; padding: 10px; font-family: monospace; border-radius: 6px; box-sizing: border-box; }\n        button.action { background: var(--accent); color: #000; border: none; padding: 10px 20px; border-radius: 6px; font-weight: bold; cursor: pointer; margin-top: 10px; }\n    </style>\n</head>\n<body>\n\n<div class=\"sidebar\">\n    <button class=\"tab-btn active\">ðŸ”</button>\n</div>\n\n<div id=\"tab-search\" class=\"main active\">\n    <div class=\"card\">\n        <h2>Anchor Retrieval</h2>\n        <p>Query your local memory graph.</p>\n        <textarea id=\"queryInput\" rows=\"3\" placeholder=\"Topic to search...\"></textarea>\n        <button class=\"action\" onclick=\"runSearch()\">Fetch Context</button>\n    </div>\n\n    <div class=\"card\">\n        <h2>Ground Truth</h2>\n        <textarea id=\"resultOutput\" rows=\"15\" readonly placeholder=\"Context will appear here...\"></textarea>\n        <button class=\"action\" onclick=\"copyResult()\">Copy to Clipboard</button>\n    </div>\n</div>\n\n<script>\n    async function runSearch() {\n        const query = document.getElementById('queryInput').value;\n        const out = document.getElementById('resultOutput');\n        out.value = \"Searching Graph...\";\n        \n        try {\n            const res = await fetch('/v1/memory/search', {\n                method: 'POST',\n                headers: {'Content-Type': 'application/json'},\n                body: JSON.stringify({ query })\n            });\n            const data = await res.json();\n            if(data.context) out.value = data.context;\n            else out.value = \"Error: \" + JSON.stringify(data);\n        } catch(e) {\n            out.value = \"Connection Error: \" + e;\n        }\n    }\n\n    function copyResult() {\n        const copyText = document.getElementById(\"resultOutput\");\n        copyText.select();\n        document.execCommand(\"copy\");\n    }\n</script>\n</body>\n</html>",
    "source": "tools\\sidecar.html"
  },
  {
    "id": "tools\\sidecar_full.html",
    "timestamp": 1767225506,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Anchor Sidecar</title>\n    <style>\n        :root { --bg: #0f172a; --panel: #1e293b; --text: #e2e8f0; --accent: #38bdf8; }\n        body { background: var(--bg); color: var(--text); font-family: sans-serif; margin: 0; display: flex; height: 100vh; }\n        \n        .sidebar { width: 60px; background: #000; display: flex; flex-direction: column; align-items: center; padding-top: 20px; }\n        .tab-btn { width: 40px; height: 40px; margin-bottom: 10px; border-radius: 8px; border: none; background: #334155; color: #fff; cursor: pointer; font-size: 20px; }\n        .tab-btn.active { background: var(--accent); color: #000; }\n\n        .main { flex: 1; padding: 20px; overflow-y: auto; display: none; }\n        .main.active { display: block; }\n\n        .card { background: var(--panel); padding: 20px; border-radius: 12px; margin-bottom: 20px; border: 1px solid #334155; }\n        h2 { margin-top: 0; color: var(--accent); }\n        \n        textarea { width: 100%; background: #000; color: #0f0; border: 1px solid #334155; padding: 10px; font-family: monospace; border-radius: 6px; box-sizing: border-box; }\n        button.action { background: var(--accent); color: #000; border: none; padding: 10px 20px; border-radius: 6px; font-weight: bold; cursor: pointer; margin-top: 10px; }\n        button.action:hover { opacity: 0.9; }\n\n        #drop-zone { border: 2px dashed #475569; padding: 40px; text-align: center; border-radius: 12px; cursor: pointer; transition: 0.2s; }\n        #drop-zone.dragover { border-color: var(--accent); background: #1e293b; }\n        \n        .log-entry { margin: 5px 0; font-family: monospace; font-size: 12px; opacity: 0.8; }\n        .success { color: #4ade80; }\n        .error { color: #f87171; }\n    </style>\n</head>\n<body>\n\n<div class=\"sidebar\">\n    <button class=\"tab-btn active\" onclick=\"switchTab('search')\">ðŸ”</button>\n    <button class=\"tab-btn\" onclick=\"switchTab('vision')\">ðŸ‘ï¸</button>\n</div>\n\n<div id=\"tab-search\" class=\"main active\">\n    <div class=\"card\">\n        <h2>Anchor Retrieval</h2>\n        <p>Query your graph to generate a Reality Map for external chats.</p>\n        <textarea id=\"queryInput\" rows=\"3\" placeholder=\"What information do you need context for?\"></textarea>\n        <button class=\"action\" onclick=\"runSearch()\">Fetch Context</button>\n    </div>\n\n    <div class=\"card\">\n        <h2>Result</h2>\n        <textarea id=\"resultOutput\" rows=\"15\" readonly placeholder=\"Context will appear here...\"></textarea>\n        <button class=\"action\" onclick=\"copyResult()\">Copy to Clipboard</button>\n    </div>\n</div>\n\n<div id=\"tab-vision\" class=\"main\">\n    <div class=\"card\">\n        <h2>Vision Ingestion</h2>\n        <p>Drop an image here to process it with the background VLM and add it to the Graph.</p>\n        <div id=\"drop-zone\">\n            <span style=\"font-size: 40px\">ðŸ“¸</span><br>\n            Drag & Drop Image or Click to Upload\n            <input type=\"file\" id=\"fileInput\" hidden accept=\"image/*\">\n        </div>\n    </div>\n    \n    <div class=\"card\">\n        <h2>Processing Logs</h2>\n        <div id=\"vision-logs\"></div>\n    </div>\n</div>\n\n<script>\n    function switchTab(tab) {\n        document.querySelectorAll('.main').forEach(el => el.classList.remove('active'));\n        document.querySelectorAll('.tab-btn').forEach(el => el.classList.remove('active'));\n        document.getElementById('tab-' + tab).classList.add('active');\n        event.target.classList.add('active');\n    }\n\n    // --- SEARCH LOGIC ---\n    async function runSearch() {\n        const query = document.getElementById('queryInput').value;\n        const out = document.getElementById('resultOutput');\n        out.value = \"Searching Graph...\";\n        \n        try {\n            const res = await fetch('/v1/memory/search', {\n                method: 'POST',\n                headers: {'Content-Type': 'application/json'},\n                body: JSON.stringify({ query })\n            });\n            const data = await res.json();\n            if(data.context) out.value = data.context;\n            else out.value = \"Error: \" + JSON.stringify(data);\n        } catch(e) {\n            out.value = \"Connection Error: \" + e;\n        }\n    }\n\n    function copyResult() {\n        const copyText = document.getElementById(\"resultOutput\");\n        copyText.select();\n        document.execCommand(\"copy\");\n        alert(\"Copied to clipboard!\");\n    }\n\n    // --- VISION LOGIC ---\n    const dropZone = document.getElementById('drop-zone');\n    const fileInput = document.getElementById('fileInput');\n\n    dropZone.onclick = () => fileInput.click();\n    fileInput.onchange = (e) => handleUpload(e.target.files[0]);\n    \n    dropZone.ondragover = (e) => { e.preventDefault(); dropZone.classList.add('dragover'); };\n    dropZone.ondragleave = () => dropZone.classList.remove('dragover');\n    dropZone.ondrop = (e) => {\n        e.preventDefault();\n        dropZone.classList.remove('dragover');\n        if(e.dataTransfer.files[0]) handleUpload(e.dataTransfer.files[0]);\n    };\n\n    async function handleUpload(file) {\n        if(!file) return;\n        log(`ðŸ“¤ Uploading ${file.name}...`, 'normal');\n        \n        const formData = new FormData();\n        formData.append('image', file);\n\n        try {\n            const res = await fetch('/v1/vision/ingest', { method: 'POST', body: formData });\n            const data = await res.json();\n            if(data.status === 'success') {\n                log(`âœ… Analyzed: \"${data.description.substring(0, 50)}...\"`, 'success');\n                log(`ðŸ’¾ Stored in Memory Graph`, 'success');\n            } else {\n                log(`âŒ Error: ${data.message}`, 'error');\n            }\n        } catch(e) {\n            log(`âŒ Network Error: ${e}`, 'error');\n        }\n    }\n\n    function log(msg, type) {\n        const div = document.createElement('div');\n        div.className = 'log-entry ' + type;\n        div.innerText = `[${new Date().toLocaleTimeString()}] ${msg}`;\n        document.getElementById('vision-logs').prepend(div);\n    }\n</script>\n</body>\n</html>",
    "source": "tools\\sidecar_full.html"
  },
  {
    "id": "tools\\start-bridge.bat",
    "timestamp": 1767195827,
    "role": "file",
    "content": "@echo off\r\necho Starting WebGPU Bridge...\r\necho This bridge allows external tools (like Wave Terminal) to talk to the browser.\r\necho.\r\nset BRIDGE_PORT=8000\r\nset BRIDGE_TOKEN=sovereign-secret\r\npython webgpu_bridge.py\r\npause",
    "source": "tools\\start-bridge.bat"
  },
  {
    "id": "tools\\terminal.html",
    "timestamp": 1767207248,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Sovereign Neural Shell</title>\n    <style>\n        :root {\n            --bg: #0c0c0c;\n            --term: #00ff00;\n            --dim: #003300;\n            --warn: #ffcc00;\n            --err: #ff4444;\n        }\n\n        body {\n            background: var(--bg);\n            color: var(--term);\n            font-family: 'Consolas', 'Monaco', monospace;\n            margin: 0;\n            padding: 20px;\n            height: 100vh;\n            box-sizing: border-box;\n            display: flex;\n            flex-direction: column;\n            overflow: hidden;\n        }\n\n        #terminal-output {\n            flex: 1;\n            overflow-y: auto;\n            white-space: pre-wrap;\n            margin-bottom: 20px;\n            border: 1px solid var(--dim);\n            padding: 10px;\n            border-radius: 4px;\n            box-shadow: 0 0 10px var(--dim) inset;\n        }\n\n        .line {\n            margin-bottom: 5px;\n        }\n\n        .user-cmd {\n            color: #fff;\n            font-weight: bold;\n        }\n\n        .system-out {\n            color: #ccc;\n        }\n\n        .error {\n            color: var(--err);\n        }\n\n        .info {\n            color: var(--warn);\n        }\n\n        #input-area {\n            display: flex;\n            gap: 10px;\n            align-items: center;\n            background: #111;\n            padding: 10px;\n            border-radius: 4px;\n            border: 1px solid #333;\n        }\n\n        #cmd-input {\n            flex: 1;\n            background: transparent;\n            border: none;\n            color: #fff;\n            font-family: inherit;\n            font-size: 1.1em;\n            outline: none;\n        }\n\n        #status-light {\n            width: 10px;\n            height: 10px;\n            border-radius: 50%;\n            background: #333;\n            transition: background 0.3s;\n        }\n\n        #status-light.on {\n            background: var(--term);\n            box-shadow: 0 0 10px var(--term);\n        }\n\n        #status-light.busy {\n            background: var(--warn);\n        }\n    </style>\n</head>\n\n<body>\n    <div id=\"terminal-output\">\n        <div class=\"line info\">Welcome to the Neural Shell v1.0</div>\n        <div class=\"line info\">Bridge: http://localhost:8000 | Protocol: /v1/shell/exec</div>\n        <div class=\"line\">--------------------------------------------------</div>\n    </div>\n\n    <div id=\"input-area\">\n        <div id=\"status-light\" title=\"Bridge Status\"></div>\n        <span style=\"color: var(--term); margin-right: 5px;\">$</span>\n        <input type=\"text\" id=\"cmd-input\" placeholder=\"Enter shell command (e.g. 'dir', 'whoami')...\" autofocus>\n    </div>\n\n    <script>\n        const DOM = {\n            out: document.getElementById('terminal-output'),\n            in: document.getElementById('cmd-input'),\n            status: document.getElementById('status-light')\n        };\n\n        const BRIDGE_URL = \"http://localhost:8000\";\n        const TOKEN = \"sovereign-secret\"; // Hardcoded for local dev\n\n        function log(msg, type = 'system-out') {\n            const div = document.createElement('div');\n            div.className = `line ${type}`;\n            div.textContent = msg;\n            DOM.out.appendChild(div);\n            DOM.out.scrollTop = DOM.out.scrollHeight;\n        }\n\n        async function checkPulse() {\n            try {\n                const res = await fetch(`${BRIDGE_URL}/health`);\n                if (res.ok) DOM.status.classList.add('on');\n                else DOM.status.classList.remove('on');\n            } catch { DOM.status.classList.remove('on'); }\n        }\n\n        async function execute(cmd) {\n            DOM.status.classList.add('busy');\n            log(`$ ${cmd}`, 'user-cmd');\n\n            try {\n                const res = await fetch(`${BRIDGE_URL}/v1/shell/exec`, {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json',\n                        'Authorization': `Bearer ${TOKEN}`\n                    },\n                    body: JSON.stringify({ cmd })\n                });\n\n                const data = await res.json();\n\n                if (data.stdout) log(data.stdout, 'system-out');\n                if (data.stderr) log(data.stderr, 'error');\n                if (data.error) log(`Bridge Error: ${data.error}`, 'error');\n                if (data.code !== 0 && !data.error && !data.stderr) log(`Process exited with code ${data.code}`, 'info');\n\n            } catch (e) {\n                log(`Network Error: ${e.message}`, 'error');\n            } finally {\n                DOM.status.classList.remove('busy');\n            }\n        }\n\n        DOM.in.addEventListener('keydown', async (e) => {\n            if (e.key === 'Enter') {\n                const raw = DOM.in.value.trim();\n                if (!raw) return;\n\n                // ðŸ§  Neural Mode: Intercept '?'\n                if (raw.startsWith('?')) {\n                    const query = raw.substring(1).trim();\n                    if (query) {\n                        log(`ðŸ§  Thinking: \"${query}\"...`, 'info');\n                        DOM.in.value = ''; // Clear input during thinking\n                        DOM.in.disabled = true;\n\n                        const cmd = await askBrain(query);\n\n                        DOM.in.disabled = false;\n                        DOM.in.focus();\n\n                        if (cmd) {\n                            // Typer effect for cool factor\n                            await typeEffect(cmd);\n                        }\n                    }\n                    return;\n                }\n\n                // Standard Execution\n                execute(raw);\n                DOM.in.value = '';\n            }\n        });\n\n        // ðŸ§  The \"Brain\" Connection\n        async function askBrain(query) {\n            DOM.status.classList.add('busy');\n            try {\n                // Use the new shell/exec endpoint which handles NL->Command conversion\n                const res = await fetch(`${BRIDGE_URL}/v1/shell/exec`, {\n                    method: 'POST',\n                    headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${TOKEN}` },\n                    body: JSON.stringify({\n                        prompt: query  // Send natural language as prompt\n                    })\n                });\n\n                if (!res.ok) {\n                    const errText = await res.text();\n                    throw new Error(`${res.status} ${res.statusText} - ${errText || \"Check Bridge/Console connection\"}`);\n                }\n\n                const data = await res.json();\n                if (data.error) throw new Error(data.error);\n\n                // The endpoint returns {command, explanation}\n                return data.command || data;\n\n            } catch (e) {\n                log(`Brain Error: ${e.message}`, 'error');\n                log(`(Ensure chat.html is open with Ghost Engine loaded)`, 'info');\n                return null;\n            } finally {\n                DOM.status.classList.remove('busy');\n            }\n        }\n\n        async function typeEffect(text) {\n            DOM.in.value = '';\n            for (let i = 0; i < text.length; i++) {\n                DOM.in.value += text[i];\n                await new Promise(r => setTimeout(r, 10)); // 10ms delay per char\n            }\n        }\n\n        // Init\n        setInterval(checkPulse, 2000);\n        checkPulse();\n        DOM.in.focus();\n    </script>\n</body>\n\n</html>",
    "source": "tools\\terminal.html"
  },
  {
    "id": "tools\\verify_hf_models.py",
    "timestamp": 1767198085,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nHugging Face Model URL Verification Script for Anchor Core\n\nThis script tests if model files are available on Hugging Face before attempting \nto download or use them locally. This helps ensure models are online before \nrunning local availability tests.\n\"\"\"\n\nimport requests\nimport sys\nimport time\nfrom urllib.parse import urljoin\nimport json\nfrom pathlib import Path\n\n\nclass HuggingFaceModelVerifier:\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        })\n\n    def test_hf_model_availability(self, model_id, expected_files=None):\n        \"\"\"Test if a model is available on Hugging Face by checking required files\"\"\"\n        print(f\"Testing Hugging Face model availability: {model_id}\")\n        \n        if expected_files is None:\n            # Default MLC-LLM required files\n            expected_files = [\n                \"ndarray-cache.json\",\n                \"tokenizer.json\", \n                \"mlc-chat-config.json\",\n                \"tokenizer_config.json\",\n                \"params_shard_0.safetensors\"  # Common sharded parameter file\n            ]\n        \n        model_result = {\n            \"model_id\": model_id,\n            \"available\": True,\n            \"files\": {},\n            \"huggingface_url\": f\"https://huggingface.co/{model_id}\"\n        }\n        \n        missing_files = []\n        \n        for file_name in expected_files:\n            hf_url = f\"https://huggingface.co/{model_id}/resolve/main/{file_name}\"\n            \n            try:\n                response = self.session.head(hf_url, timeout=15)\n                \n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": response.status_code,\n                    \"exists\": response.status_code == 200,\n                    \"checked_at\": time.time()\n                }\n                \n                model_result[\"files\"][file_name] = file_status\n                \n                if response.status_code == 200:\n                    print(f\"  OK {file_name}\")\n                elif response.status_code == 404:\n                    print(f\"  MISSING {file_name} - NOT FOUND on Hugging Face\")\n                    missing_files.append(file_name)\n                    model_result[\"available\"] = False\n                else:\n                    print(f\"  WARNING {file_name} - Status {response.status_code}\")\n                    model_result[\"available\"] = False\n                    \n            except requests.exceptions.RequestException as e:\n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": \"ERROR\",\n                    \"exists\": False,\n                    \"error\": str(e),\n                    \"checked_at\": time.time()\n                }\n                model_result[\"files\"][file_name] = file_status\n                print(f\"  ERROR {file_name} - {e}\")\n                model_result[\"available\"] = False\n        \n        if missing_files:\n            print(f\"  INFO Missing files on Hugging Face: {len(missing_files)} required files not found\")\n        else:\n            print(f\"  SUCCESS Model {model_id} is fully available on Hugging Face!\")\n        \n        return model_result\n\n    def get_model_info(self, model_id):\n        \"\"\"Get model information from Hugging Face API\"\"\"\n        info_url = f\"https://huggingface.co/api/models/{model_id}\"\n        \n        try:\n            response = self.session.get(info_url, timeout=10)\n            if response.status_code == 200:\n                return response.json()\n            else:\n                print(f\"  WARNING Could not fetch model info for {model_id} - Status: {response.status_code}\")\n                return None\n        except Exception as e:\n            print(f\"  ERROR Could not fetch model info for {model_id} - {e}\")\n            return None\n\n    def test_model_files_list(self, model_id):\n        \"\"\"Get list of files in the model repository\"\"\"\n        files_url = f\"https://huggingface.co/api/models/{model_id}/tree/main\"\n        \n        try:\n            response = self.session.get(files_url, timeout=10)\n            if response.status_code == 200:\n                files_data = response.json()\n                file_names = [f[\"path\"] for f in files_data if f[\"type\"] == \"file\"]\n                return file_names\n            else:\n                print(f\"  WARNING Could not fetch file list for {model_id} - Status: {response.status_code}\")\n                return []\n        except Exception as e:\n            print(f\"  ERROR Could not fetch file list for {model_id} - {e}\")\n            return []\n\n    def run_hf_verification_tests(self, model_list):\n        \"\"\"Run Hugging Face availability tests for a list of models\"\"\"\n        print(\"Running Hugging Face Model Availability Tests\")\n        print(\"=\" * 60)\n        \n        available_models = []\n        unavailable_models = []\n        \n        for model_id in model_list:\n            print(f\"\\nTesting: {model_id}\")\n            \n            # First, try to get model info\n            model_info = self.get_model_info(model_id)\n            if model_info:\n                print(f\"  INFO Model name: {model_info.get('modelId', 'Unknown')}\")\n                print(f\"  INFO Downloads: {model_info.get('downloads', 'Unknown')}\")\n                print(f\"  INFO Likes: {model_info.get('likes', 'Unknown')}\")\n            \n            # Check specific files we need for MLC-LLM\n            result = self.test_hf_model_availability(model_id)\n            \n            if result[\"available\"]:\n                available_models.append(model_id)\n                print(f\"  RESULT: AVAILABLE on Hugging Face\")\n            else:\n                unavailable_models.append(model_id)\n                print(f\"  RESULT: NOT AVAILABLE on Hugging Face\")\n                \n                # Show what files are actually available as fallback\n                available_files = self.test_model_files_list(model_id)\n                if available_files:\n                    mlc_files = [f for f in available_files if any(req in f for req in ['config', 'tokenizer', 'params', 'ndarray'])]\n                    if mlc_files:\n                        print(f\"  INFO Some MLC-LLM files available: {mlc_files[:5]}...\")  # Show first 5\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"HUGGING FACE VERIFICATION SUMMARY:\")\n        print(f\"  Available Models: {len(available_models)}\")\n        for model in available_models:\n            print(f\"    OK {model}\")\n        \n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\n        for model in unavailable_models:\n            print(f\"    MISSING {model}\")\n        \n        print(\"\\nRECOMMENDATION:\")\n        if unavailable_models:\n            print(\"  - Avoid using models that are not available on Hugging Face\")\n            print(\"  - Consider alternative model IDs for unavailable models\")\n        else:\n            print(\"  - All models are available on Hugging Face for download!\")\n        \n        return available_models, unavailable_models\n\n\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Verify Hugging Face Model Availability for Anchor Core\")\n    parser.add_argument(\"--models\", nargs=\"+\",\n                       help=\"Specific models to test (default: predefined list)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for verification report (JSON format)\")\n    \n    args = parser.parse_args()\n    \n    verifier = HuggingFaceModelVerifier()\n    \n    # Default model list if none provided\n    default_models = [\n        \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\", \n        \"mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC\",\n        \"mlc-ai/gemma-2-9b-it-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC\"\n    ]\n    \n    models_to_test = args.models if args.models else default_models\n    \n    available, unavailable = verifier.run_hf_verification_tests(models_to_test)\n    \n    if args.output:\n        # Generate a simple report\n        report = {\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"tested_models\": models_to_test,\n            \"available_models\": available,\n            \"unavailable_models\": unavailable,\n            \"summary\": {\n                \"total\": len(models_to_test),\n                \"available\": len(available),\n                \"unavailable\": len(unavailable)\n            }\n        }\n        with open(args.output, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"\\nTest report saved to: {args.output}\")\n    \n    # Exit with error code if no models are available\n    if not available:\n        print(\"\\nERROR: No models are available on Hugging Face!\")\n        sys.exit(1)\n    else:\n        print(f\"\\nSUCCESS: {len(available)} model(s) are available on Hugging Face!\")\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\verify_hf_models.py"
  },
  {
    "id": "tools\\verify_local_models.py",
    "timestamp": 1767198160,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nLocal Model File Verification Script for Anchor Core\n\nThis script tests if required model files exist locally in the models directory\nbefore attempting to load them into the MLC-LLM engine. This is separate from \nthe Hugging Face verification to ensure we can distinguish between online \navailability and local file presence.\n\"\"\"\n\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nimport json\n\n\nclass LocalModelVerifier:\n    def __init__(self, models_dir=\"models\"):\n        self.models_dir = Path(models_dir)\n        self.results = {}\n\n    def test_local_model_availability(self, model_name):\n        \"\"\"Test if a model is available locally by checking required files\"\"\"\n        print(f\"Testing local model availability: {model_name}\")\n        \n        # Define the required model files for MLC-LLM\n        required_files = [\n            \"ndarray-cache.json\",\n            \"tokenizer.json\",\n            \"mlc-chat-config.json\", \n            \"tokenizer_config.json\"\n        ]\n        \n        # Also check for parameter files (sharded format is common)\n        param_patterns = [\"params_shard_*.bin\", \"params_shard_*.safetensors\", \"params.json\"]\n        \n        model_path = self.models_dir / model_name\n        if not model_path.exists():\n            print(f\"  MISSING Model directory does not exist: {model_path}\")\n            return {\n                \"model_name\": model_name,\n                \"available\": False,\n                \"directory_exists\": False,\n                \"files\": {},\n                \"reason\": \"Model directory does not exist\"\n            }\n        \n        model_result = {\n            \"model_name\": model_name,\n            \"available\": True,\n            \"directory_exists\": True,\n            \"files\": {},\n            \"reason\": \"All required files present\"\n        }\n        \n        missing_files = []\n        \n        # Check standard files\n        for file_name in required_files:\n            file_path = model_path / file_name\n            file_exists = file_path.exists()\n            \n            file_status = {\n                \"path\": str(file_path),\n                \"exists\": file_exists,\n                \"checked_at\": time.time()\n            }\n            \n            model_result[\"files\"][file_name] = file_status\n            \n            if file_exists:\n                print(f\"  OK {file_name}\")\n            else:\n                print(f\"  MISSING {file_name}\")\n                missing_files.append(file_name)\n                model_result[\"available\"] = False\n                model_result[\"reason\"] = f\"Missing required file: {file_name}\"\n        \n        # Check for parameter files (at least one pattern should match)\n        param_found = False\n        for pattern in param_patterns:\n            param_files = list(model_path.glob(pattern))\n            if param_files:\n                param_found = True\n                for param_file in param_files[:3]:  # Show first 3 matches\n                    print(f\"  OK {param_file.name} (parameter file)\")\n                break\n        \n        if not param_found:\n            print(f\"  WARNING No parameter files found (looked for: {', '.join(param_patterns)})\")\n            # Don't mark as unavailable just for missing params, as they might be loaded differently\n        \n        if missing_files:\n            print(f\"  INFO Missing files locally: {len(missing_files)} required files not found\")\n        else:\n            print(f\"  SUCCESS Model {model_name} is fully available locally!\")\n        \n        self.results[model_name] = model_result\n        return model_result\n\n    def scan_all_local_models(self):\n        \"\"\"Scan the models directory for all available models\"\"\"\n        if not self.models_dir.exists():\n            print(f\"Models directory does not exist: {self.models_dir}\")\n            return []\n        \n        model_dirs = [d for d in self.models_dir.iterdir() if d.is_dir()]\n        return [d.name for d in model_dirs]\n\n    def run_local_verification_tests(self, model_list):\n        \"\"\"Run local availability tests for a list of models\"\"\"\n        print(\"Running Local Model Availability Tests\")\n        print(\"=\" * 60)\n        \n        available_models = []\n        unavailable_models = []\n        \n        for model_name in model_list:\n            print(f\"\\nTesting: {model_name}\")\n            result = self.test_local_model_availability(model_name)\n            \n            if result[\"available\"]:\n                available_models.append(model_name)\n                print(f\"  RESULT: AVAILABLE locally\")\n            else:\n                unavailable_models.append(model_name)\n                print(f\"  RESULT: NOT AVAILABLE locally - {result['reason']}\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"LOCAL VERIFICATION SUMMARY:\")\n        print(f\"  Available Models: {len(available_models)}\")\n        for model in available_models:\n            print(f\"    OK {model}\")\n        \n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\n        for model in unavailable_models:\n            result = self.results.get(model, {})\n            reason = result.get(\"reason\", \"Unknown reason\")\n            print(f\"    MISSING {model} - {reason}\")\n        \n        print(\"\\nRECOMMENDATION:\")\n        if unavailable_models:\n            print(\"  - Download required models using the /v1/models/pull endpoint\")\n            print(\"  - Or ensure models are pre-loaded in the models/ directory\")\n        else:\n            print(\"  - All models are available locally!\")\n        \n        return available_models, unavailable_models\n\n\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Verify Local Model File Availability for Anchor Core\")\n    parser.add_argument(\"--models\", nargs=\"+\",\n                       help=\"Specific models to test (default: scan models directory)\")\n    parser.add_argument(\"--models-dir\", default=\"models\",\n                       help=\"Models directory path (default: models)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for verification report (JSON format)\")\n    \n    args = parser.parse_args()\n    \n    verifier = LocalModelVerifier(models_dir=args.models_dir)\n    \n    if args.models:\n        # Test specific models\n        models_to_test = args.models\n    else:\n        # Scan models directory for available models\n        print(f\"Scanning models directory: {args.models_dir}\")\n        models_to_test = verifier.scan_all_local_models()\n        if not models_to_test:\n            print(\"No models found in directory, using default test models...\")\n            models_to_test = [\n                \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n                \"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\",\n                \"Phi-3.5-mini-instruct-q4f16_1-MLC\"\n            ]\n    \n    available, unavailable = verifier.run_local_verification_tests(models_to_test)\n    \n    if args.output:\n        report = {\n            \"models_dir\": args.models_dir,\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"tested_models\": models_to_test,\n            \"results\": verifier.results,\n            \"summary\": {\n                \"total\": len(models_to_test),\n                \"available\": len(available),\n                \"unavailable\": len(unavailable)\n            }\n        }\n        with open(args.output, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"\\nTest report saved to: {args.output}\")\n    \n    # Exit with error code if no models are available locally\n    if not available and args.models:  # Only error if specific models were requested\n        print(\"\\nERROR: No requested models are available locally!\")\n        sys.exit(1)\n    else:\n        print(f\"\\nSUCCESS: {len(available)} model(s) are available locally!\")\n        if not args.models:\n            print(f\"Found {len(models_to_test)} models in the models directory.\")\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\verify_local_models.py"
  },
  {
    "id": "tools\\verify_model_complete.py",
    "timestamp": 1767198894,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nCombined Model Verification Script for Anchor Core\n\nThis script first verifies that models are available online on Hugging Face,\nthen checks if they are available locally, providing a complete verification\npipeline before attempting to use models in the system.\n\"\"\"\n\nimport requests\nimport sys\nimport time\nfrom urllib.parse import urljoin\nimport json\nfrom pathlib import Path\n\n\nclass CombinedModelVerifier:\n    def __init__(self, models_dir=\"models\", base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\n        self.models_dir = Path(models_dir)\n        self.base_url = base_url\n        self.token = token\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n        self.session = requests.Session()\n        self.session.headers.update({\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        })\n        self.results = {}\n\n    def test_hf_model_availability(self, model_id, expected_files=None):\n        \"\"\"Test if a model is available on Hugging Face by checking required files\"\"\"\n        print(f\"  Testing Hugging Face availability: {model_id}\")\n        \n        if expected_files is None:\n            # Default MLC-LLM required files\n            expected_files = [\n                \"ndarray-cache.json\",\n                \"tokenizer.json\", \n                \"mlc-chat-config.json\",\n                \"tokenizer_config.json\"\n            ]\n        \n        model_result = {\n            \"huggingface\": {\n                \"available\": True,\n                \"files\": {},\n                \"huggingface_url\": f\"https://huggingface.co/{model_id}\"\n            }\n        }\n        \n        missing_files = []\n        \n        for file_name in expected_files:\n            hf_url = f\"https://huggingface.co/{model_id}/resolve/main/{file_name}\"\n            \n            try:\n                response = self.session.head(hf_url, timeout=15)\n                \n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": response.status_code,\n                    \"exists\": response.status_code == 200,\n                    \"checked_at\": time.time()\n                }\n                \n                model_result[\"huggingface\"][\"files\"][file_name] = file_status\n                \n                if response.status_code in [200, 302, 307]:  # 302/307 are redirects, meaning file exists\n                    print(f\"    OK {file_name}\")\n                elif response.status_code == 404:\n                    print(f\"    MISSING {file_name} - NOT FOUND on Hugging Face\")\n                    missing_files.append(file_name)\n                    model_result[\"huggingface\"][\"available\"] = False\n                else:\n                    print(f\"    WARNING {file_name} - Status {response.status_code}\")\n                    model_result[\"huggingface\"][\"available\"] = False\n                    \n            except requests.exceptions.RequestException as e:\n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": \"ERROR\",\n                    \"exists\": False,\n                    \"error\": str(e),\n                    \"checked_at\": time.time()\n                }\n                model_result[\"huggingface\"][\"files\"][file_name] = file_status\n                print(f\"    ERROR {file_name} - {e}\")\n                model_result[\"huggingface\"][\"available\"] = False\n        \n        if missing_files:\n            print(f\"    INFO Missing files on Hugging Face: {len(missing_files)} required files not found\")\n        else:\n            print(f\"    SUCCESS Available on Hugging Face!\")\n        \n        return model_result[\"huggingface\"]\n\n    def test_local_model_availability(self, model_name):\n        \"\"\"Test if a model is available locally by checking required files\"\"\"\n        print(f\"  Testing local availability: {model_name}\")\n        \n        # Define the required model files for MLC-LLM\n        required_files = [\n            \"ndarray-cache.json\",\n            \"tokenizer.json\",\n            \"mlc-chat-config.json\", \n            \"tokenizer_config.json\"\n        ]\n        \n        model_path = self.models_dir / model_name\n        if not model_path.exists():\n            print(f\"    MISSING Model directory does not exist: {model_path}\")\n            return {\n                \"available\": False,\n                \"directory_exists\": False,\n                \"files\": {},\n                \"reason\": \"Model directory does not exist\"\n            }\n        \n        model_result = {\n            \"available\": True,\n            \"directory_exists\": True,\n            \"files\": {},\n            \"reason\": \"All required files present\"\n        }\n        \n        missing_files = []\n        \n        # Check standard files\n        for file_name in required_files:\n            file_path = model_path / file_name\n            file_exists = file_path.exists()\n            \n            file_status = {\n                \"path\": str(file_path),\n                \"exists\": file_exists,\n                \"checked_at\": time.time()\n            }\n            \n            model_result[\"files\"][file_name] = file_status\n            \n            if file_exists:\n                print(f\"    OK {file_name}\")\n            else:\n                print(f\"    MISSING {file_name}\")\n                missing_files.append(file_name)\n                model_result[\"available\"] = False\n                model_result[\"reason\"] = f\"Missing required file: {file_name}\"\n        \n        if missing_files:\n            print(f\"    INFO Missing files locally: {len(missing_files)} required files not found\")\n        else:\n            print(f\"    SUCCESS Available locally!\")\n        \n        return model_result\n\n    def test_bridge_redirect_availability(self, model_name):\n        \"\"\"Test if the bridge redirect endpoint can serve the model files\"\"\"\n        print(f\"  Testing bridge redirect availability: {model_name}\")\n        \n        # Define the required model files for MLC-LLM using the resolve/main pattern\n        required_files = [\n            f\"models/{model_name}/resolve/main/ndarray-cache.json\",\n            f\"models/{model_name}/resolve/main/tokenizer.json\",\n            f\"models/{model_name}/resolve/main/mlc-chat-config.json\",\n            f\"models/{model_name}/resolve/main/tokenizer_config.json\"\n        ]\n        \n        model_result = {\n            \"available\": True,\n            \"files\": {},\n            \"download_required\": False\n        }\n        \n        missing_files = []\n        \n        for file_path in required_files:\n            try:\n                url = urljoin(self.base_url, file_path)\n                response = self.session.head(url, timeout=10)  # Short timeout for availability check\n\n                file_status = {\n                    \"url\": url,\n                    \"status_code\": response.status_code,\n                    \"exists\": response.status_code == 200,\n                    \"checked_at\": time.time()\n                }\n\n                model_result[\"files\"][file_path] = file_status\n\n                if response.status_code == 200:\n                    print(f\"    OK {file_path}\")\n                elif response.status_code == 404:\n                    print(f\"    MISSING {file_path} - NOT FOUND via bridge\")\n                    missing_files.append(file_path)\n                    model_result[\"available\"] = False\n                    model_result[\"download_required\"] = True\n                else:\n                    print(f\"    WARNING {file_path} - Status {response.status_code}\")\n                    model_result[\"available\"] = False\n\n            except requests.exceptions.RequestException as e:\n                file_status = {\n                    \"url\": urljoin(self.base_url, file_path),\n                    \"status_code\": \"ERROR\",\n                    \"exists\": False,\n                    \"error\": str(e),\n                    \"checked_at\": time.time()\n                }\n                model_result[\"files\"][file_path] = file_status\n                print(f\"    ERROR {file_path} - {e}\")\n                model_result[\"available\"] = False\n\n        if missing_files:\n            print(f\"    INFO Missing files via bridge: {len(missing_files)} required files not found\")\n        else:\n            print(f\"    SUCCESS Available via bridge!\")\n        \n        return model_result\n\n    def verify_model_complete(self, model_id):\n        \"\"\"Complete verification: Hugging Face -> Local -> Bridge\"\"\"\n        print(f\"\\nVerifying model: {model_id}\")\n        print(\"-\" * 50)\n        \n        # Extract model name from model_id (remove mlc-ai/ prefix if present)\n        model_name = model_id.split('/')[-1] if '/' in model_id else model_id\n        \n        verification_result = {\n            \"model_id\": model_id,\n            \"model_name\": model_name,\n            \"huggingface\": None,\n            \"local\": None,\n            \"bridge\": None,\n            \"overall_status\": \"UNKNOWN\",\n            \"recommendation\": \"UNKNOWN\"\n        }\n        \n        # Step 1: Check Hugging Face availability\n        hf_result = self.test_hf_model_availability(model_id)\n        verification_result[\"huggingface\"] = hf_result\n        \n        if not hf_result[\"available\"]:\n            verification_result[\"overall_status\"] = \"UNAVAILABLE\"\n            verification_result[\"recommendation\"] = \"Model not available on Hugging Face - cannot proceed\"\n            print(f\"  ERROR: Model not available on Hugging Face\")\n            return verification_result\n        \n        # Step 2: Check local availability\n        local_result = self.test_local_model_availability(model_name)\n        verification_result[\"local\"] = local_result\n        \n        # Step 3: Check bridge redirect availability\n        bridge_result = self.test_bridge_redirect_availability(model_name)\n        verification_result[\"bridge\"] = bridge_result\n        \n        # Determine overall status\n        if local_result[\"available\"]:\n            verification_result[\"overall_status\"] = \"AVAILABLE_LOCALLY\"\n            verification_result[\"recommendation\"] = \"Ready to use - available locally\"\n        elif bridge_result[\"available\"]:\n            verification_result[\"overall_status\"] = \"AVAILABLE_VIA_BRIDGE\"\n            verification_result[\"recommendation\"] = \"Available via bridge redirect (may download from online)\"\n        else:\n            verification_result[\"overall_status\"] = \"DOWNLOAD_REQUIRED\"\n            verification_result[\"recommendation\"] = \"Not available locally, needs download via /v1/models/pull\"\n        \n        print(f\"  Overall Status: {verification_result['overall_status']}\")\n        print(f\"  Recommendation: {verification_result['recommendation']}\")\n        \n        return verification_result\n\n    def run_complete_verification_tests(self, model_list):\n        \"\"\"Run complete verification tests for a list of models\"\"\"\n        print(\"Running Complete Model Verification Tests\")\n        print(\"Phase 1: Hugging Face availability check\")\n        print(\"Phase 2: Local file availability check\") \n        print(\"Phase 3: Bridge redirect availability check\")\n        print(\"=\" * 80)\n        \n        results = []\n        available_online = []\n        available_locally = []\n        available_via_bridge = []\n        download_required = []\n        completely_unavailable = []\n        \n        for model_id in model_list:\n            result = self.verify_model_complete(model_id)\n            results.append(result)\n            \n            if result[\"overall_status\"] == \"AVAILABLE_LOCALLY\":\n                available_locally.append(model_id)\n            elif result[\"overall_status\"] == \"AVAILABLE_VIA_BRIDGE\":\n                available_via_bridge.append(model_id)\n            elif result[\"overall_status\"] == \"DOWNLOAD_REQUIRED\":\n                download_required.append(model_id)\n            elif result[\"overall_status\"] == \"UNAVAILABLE\":\n                completely_unavailable.append(model_id)\n        \n        # Final summary\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPLETE VERIFICATION SUMMARY:\")\n        \n        if available_locally:\n            print(f\"\\nâœ… AVAILABLE LOCALLY - Ready to use immediately:\")\n            for model in available_locally:\n                print(f\"    {model}\")\n        \n        if available_via_bridge:\n            print(f\"\\nðŸ”„ AVAILABLE VIA BRIDGE - Will redirect to online sources:\")\n            for model in available_via_bridge:\n                print(f\"    {model}\")\n        \n        if download_required:\n            print(f\"\\nDOWNLOAD REQUIRED - Use /v1/models/pull to download:\")\n            for model in download_required:\n                print(f\"    {model}\")\n\n        if completely_unavailable:\n            print(f\"\\nCOMPLETELY UNAVAILABLE - Not on Hugging Face:\")\n            for model in completely_unavailable:\n                print(f\"    {model}\")\n        \n        print(f\"\\nTOTALS:\")\n        print(f\"  Models tested: {len(model_list)}\")\n        print(f\"  Available locally: {len(available_locally)}\")\n        print(f\"  Available via bridge: {len(available_via_bridge)}\")\n        print(f\"  Need download: {len(download_required)}\")\n        print(f\"  Completely unavailable: {len(completely_unavailable)}\")\n        \n        return results, {\n            \"available_locally\": available_locally,\n            \"available_via_bridge\": available_via_bridge,\n            \"download_required\": download_required,\n            \"completely_unavailable\": completely_unavailable\n        }\n\n\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Complete Model Verification for Anchor Core\")\n    parser.add_argument(\"--models\", nargs=\"+\",\n                       help=\"Specific models to test (default: predefined list)\")\n    parser.add_argument(\"--models-dir\", default=\"models\",\n                       help=\"Models directory path (default: models)\")\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for verification report (JSON format)\")\n    \n    args = parser.parse_args()\n    \n    verifier = CombinedModelVerifier(\n        models_dir=args.models_dir,\n        base_url=args.url,\n        token=args.token\n    )\n    \n    # Default model list if none provided\n    default_models = [\n        \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\", \n        \"mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC\",\n        \"mlc-ai/gemma-2-9b-it-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC\"\n    ]\n    \n    models_to_test = args.models if args.models else default_models\n    \n    print(f\"Starting complete verification for {len(models_to_test)} models...\")\n    print(f\"Models directory: {args.models_dir}\")\n    print(f\"Server URL: {args.url}\")\n    \n    results, summary = verifier.run_complete_verification_tests(models_to_test)\n    \n    if args.output:\n        report = {\n            \"models_dir\": args.models_dir,\n            \"server_url\": args.url,\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"tested_models\": models_to_test,\n            \"results\": results,\n            \"summary\": summary\n        }\n        with open(args.output, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"\\nComplete verification report saved to: {args.output}\")\n    \n    # Exit with error code if no models are available at all\n    total_available = len(summary[\"available_locally\"]) + len(summary[\"available_via_bridge\"])\n    if total_available == 0 and args.models:  # Only error if specific models were requested\n        print(\"\\nERROR: No requested models are available for use!\")\n        sys.exit(1)\n    else:\n        print(f\"\\nSUCCESS: {total_available} model(s) are available for use!\")\n        if summary[\"completely_unavailable\"]:\n            print(f\"Note: {len(summary['completely_unavailable'])} models are not available on Hugging Face.\")\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\verify_model_complete.py"
  },
  {
    "id": "tools\\webgpu-server-chat.html",
    "timestamp": 1767195827,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>WebGPU Chat Server (Worker)</title>\r\n    <style>\r\n        body {\r\n            background: #001;\r\n            color: #0f0;\r\n            font-family: monospace;\r\n            padding: 20px;\r\n        }\r\n\r\n        .log {\r\n            margin-top: 10px;\r\n            color: #8c8;\r\n            font-size: 0.9em;\r\n        }\r\n\r\n        .success {\r\n            color: #0f0;\r\n        }\r\n\r\n        .error {\r\n            color: #f00;\r\n        }\r\n\r\n        #status {\r\n            font-weight: bold;\r\n            font-size: 1.2em;\r\n            margin-bottom: 20px;\r\n        }\r\n\r\n        input,\r\n        select {\r\n            background: #222;\r\n            color: #fff;\r\n            border: 1px solid #444;\r\n            padding: 5px;\r\n        }\r\n\r\n        button {\r\n            background: #060;\r\n            color: #fff;\r\n            border: none;\r\n            padding: 5px 15px;\r\n            cursor: pointer;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <div id=\"status\">ðŸ”´ Disconnected</div>\r\n    <div>\r\n        <label>Chat Model:</label>\r\n        <input type=\"text\" id=\"model-input\" value=\"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\" style=\"width: 400px;\">\r\n        <!-- Common options helper -->\r\n        <select id=\"model-helper\" onchange=\"document.getElementById('model-input').value = this.value\">\r\n            <option value=\"\">-- Presets --</option>\r\n            <option value=\"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\">Qwen2.5-1.5B (Fast)</option>\r\n            <option value=\"Qwen2.5-7B-Instruct-q4f16_1-MLC\">Qwen2.5-7B (Balanced)</option>\r\n            <option value=\"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\">DeepSeek-R1-7B</option>\r\n            <option value=\"DeepSeek-V2-Lite-Chat-q4f16_1-MLC\">DeepSeek-V2-Lite</option>\r\n            <option value=\"Llama-3.1-8B-Instruct-q4f32_1-MLC\">Llama-3.1-8B</option>\r\n        </select>\r\n    </div>\r\n    <div style=\"margin-top: 10px;\">\r\n        <label>Bridge URL:</label>\r\n        <input type=\"text\" id=\"bridge-url\" value=\"ws://localhost:8000/ws/chat\" style=\"width: 300px;\">\r\n        <button id=\"connect-btn\">Start Server</button>\r\n    </div>\r\n    <div id=\"logs\" style=\"margin-top: 20px; border-top: 1px solid #333;\"></div>\r\n\r\n    <script type=\"module\">\r\n        import { CreateMLCEngine } from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n        const statusDiv = document.getElementById('status');\r\n        const logsDiv = document.getElementById('logs');\r\n        const connectBtn = document.getElementById('connect-btn');\r\n        const modelInput = document.getElementById('model-input');\r\n\r\n        let engine = null;\r\n        let ws = null;\r\n\r\n        function log(msg, type = 'log') {\r\n            const div = document.createElement('div');\r\n            div.className = type;\r\n            div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;\r\n            logsDiv.prepend(div);\r\n        }\r\n\r\n        connectBtn.onclick = async () => {\r\n            connectBtn.disabled = true;\r\n            const model = modelInput.value;\r\n\r\n            try {\r\n                log(`Loading WebGPU Engine (${model})...`);\r\n                engine = await CreateMLCEngine(model, {\r\n                    initProgressCallback: (report) => {\r\n                        statusDiv.textContent = `ðŸŸ¡ Loading Model: ${report.text}`;\r\n                    }\r\n                });\r\n                log(\"Engine Loaded.\", \"success\");\r\n\r\n                connectWebSocket();\r\n            } catch (e) {\r\n                log(`Engine Load Failed: ${e.message}`, \"error\");\r\n                connectBtn.disabled = false;\r\n            }\r\n        };\r\n\r\n        function connectWebSocket() {\r\n            const bridgeUrl = document.getElementById('bridge-url').value;\r\n            log(`Connecting to Bridge (${bridgeUrl})...`);\r\n            ws = new WebSocket(bridgeUrl);\r\n\r\n            ws.onopen = () => {\r\n                statusDiv.textContent = \"ðŸŸ¢ Bridge Connected (Ready for Requests)\";\r\n                log(\"Bridge Connected.\", \"success\");\r\n            };\r\n\r\n            ws.onclose = () => {\r\n                statusDiv.textContent = \"ðŸ”´ Disconnected\";\r\n                log(\"Bridge Disconnected. Retrying in 3s...\", \"error\");\r\n                setTimeout(connectWebSocket, 3000);\r\n            };\r\n\r\n            ws.onmessage = async (event) => {\r\n                const msg = JSON.parse(event.data);\r\n                if (msg.type === 'chat') {\r\n                    handleChatRequest(msg.id, msg.data);\r\n                }\r\n            };\r\n        }\r\n\r\n        async function handleChatRequest(reqId, data) {\r\n            log(`Chat Request ${reqId.slice(0, 8)}...`);\r\n            try {\r\n                // OpenAI Compatibility Mapping\r\n                const messages = data.messages;\r\n                const stream = data.stream !== false; // Default to true if undefined\r\n\r\n                const completion = await engine.chat.completions.create({\r\n                    messages,\r\n                    stream: true,\r\n                    temperature: data.temperature || 0.7,\r\n                    max_tokens: data.max_tokens || 4096\r\n                });\r\n\r\n                for await (const chunk of completion) {\r\n                    // Forward chunk to bridge\r\n                    ws.send(JSON.stringify({\r\n                        id: reqId,\r\n                        chunk: chunk\r\n                    }));\r\n                }\r\n\r\n                // Signal done\r\n                ws.send(JSON.stringify({\r\n                    id: reqId,\r\n                    done: true\r\n                }));\r\n\r\n                log(`Request ${reqId.slice(0, 8)} Complete.`, \"success\");\r\n\r\n            } catch (e) {\r\n                log(`Request Failed: ${e.message}`, \"error\");\r\n                if (ws && ws.readyState === WebSocket.OPEN) {\r\n                    ws.send(JSON.stringify({ id: reqId, error: e.message }));\r\n                }\r\n            }\r\n        }\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\webgpu-server-chat.html"
  },
  {
    "id": "tools\\webgpu_bridge.py",
    "timestamp": 1767242390,
    "role": "file",
    "content": "import os\nimport sys\nimport asyncio\nimport uuid\nimport json\nimport time\nimport uvicorn\nfrom fastapi import FastAPI, WebSocket, Request\nfrom fastapi.responses import JSONResponse, FileResponse, RedirectResponse, HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom typing import Dict\n\n# --- CONFIGURATION ---\nPORT = 8000\nHOST = \"0.0.0.0\"\n\nif sys.platform == \"win32\":\n    sys.stdout.reconfigure(encoding='utf-8')\n\n# --- LOG COLLECTION ---\nimport datetime\nimport asyncio\nfrom collections import deque\nimport os\nfrom datetime import datetime as dt\n\n# Create logs directory if it doesn't exist\nlogs_dir = os.path.join(os.path.dirname(__file__), \"..\", \"logs\")\nos.makedirs(logs_dir, exist_ok=True)\n\n# Global log buffer for collecting system logs\nMAX_LOG_ENTRIES = 1000\nlog_buffer = deque(maxlen=MAX_LOG_ENTRIES)\nlog_buffer_lock = asyncio.Lock()\n\nasync def add_log_entry(source: str, log_type: str, message: str):\n    \"\"\"Add a log entry to the global log buffer and save to file\"\"\"\n    global log_buffer\n    async with log_buffer_lock:\n        log_entry = {\n            \"timestamp\": dt.now().isoformat(),\n            \"source\": source,\n            \"type\": log_type,\n            \"message\": message\n        }\n        log_buffer.append(log_entry)\n\n        # Also write to individual log file with truncation\n        try:\n            log_file_path = os.path.join(logs_dir, f\"{source.lower().replace('-', '_').replace(' ', '_')}.log\")\n\n            # Read existing content and truncate if needed\n            if os.path.exists(log_file_path):\n                with open(log_file_path, 'r', encoding='utf-8') as f:\n                    lines = f.readlines()\n\n                # Keep only last 1000 lines\n                if len(lines) >= 1000:\n                    lines = lines[-500:]  # Keep last 500 to have room for new entries\n\n                # Write back truncated content plus new entry\n                with open(log_file_path, 'w', encoding='utf-8') as f:\n                    f.writelines(lines)\n                # Append the new log entry\n                with open(log_file_path, 'a', encoding='utf-8') as f:\n                    f.write(f\"[{dt.now().strftime('%Y-%m-%d %H:%M:%S')}] [{log_type.upper()}] {message}\\n\")\n            else:\n                # File doesn't exist, create it with the new entry\n                with open(log_file_path, 'w', encoding='utf-8') as f:\n                    f.write(f\"[{dt.now().strftime('%Y-%m-%d %H:%M:%S')}] [{log_type.upper()}] {message}\\n\")\n        except Exception:\n            # If file writing fails, just continue\n            pass\n\nasync def get_recent_logs(limit: int = 100):\n    \"\"\"Get recent log entries\"\"\"\n    async with log_buffer_lock:\n        recent_logs = list(log_buffer)[-limit:]\n        return recent_logs\n\n# Initialize with startup message - defer until after event loop starts\ndef initialize_logging():\n    # This will be called when the app starts up\n    pass\n\napp = FastAPI(title=\"Anchor Core (Text-Only)\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --- STATE ---\nworkers: Dict[str, WebSocket] = {\"chat\": None}\nactive_requests: Dict[str, asyncio.Queue] = {}\n\n# --- STATIC & MODELS ---\nclass NoCacheFileResponse(FileResponse):\n    async def __call__(self, scope, receive, send):\n        async def send_wrapper(message):\n            if message['type'] == 'http.response.start':\n                headers = message.get('headers', [])\n                headers.extend([(b\"Cache-Control\", b\"no-store, no-cache, must-revalidate\")])\n                message['headers'] = headers\n            await send(message)\n        await super().__call__(scope, receive, send_wrapper)\n\nmodels_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\n\n@app.get(\"/models/{file_path:path}\")\nasync def models_redirect(file_path: str):\n    local_path = os.path.join(models_dir, file_path)\n    if os.path.exists(local_path) and os.path.isfile(local_path):\n        return NoCacheFileResponse(local_path)\n    return RedirectResponse(url=f\"https://huggingface.co/{file_path}\", status_code=302)\n\n@app.head(\"/models/{file_path:path}\")\nasync def models_redirect_head(file_path: str):\n    from starlette.responses import Response\n    local_path = os.path.join(models_dir, file_path)\n    if os.path.exists(local_path) and os.path.isfile(local_path):\n        return Response(headers={\"content-length\": str(os.path.getsize(local_path))})\n    return RedirectResponse(url=f\"https://huggingface.co/{file_path}\", status_code=302)\n\n# --- API ENDPOINTS ---\n\n@app.post(\"/v1/chat/completions\")\nasync def chat_completions(request: Request):\n    \"\"\"Proxies chat to Ghost Engine and accumulates the stream.\"\"\"\n    if not workers[\"chat\"]:\n        await add_log_entry(\"Chat-API\", \"error\", \"Ghost Engine Disconnected - The headless browser is not connected.\")\n        return JSONResponse(status_code=503, content={\"error\": \"Ghost Engine Disconnected\", \"message\": \"The headless browser is not connected.\"})\n\n    try:\n        body = await request.json()\n        req_id = str(uuid.uuid4())\n        active_requests[req_id] = asyncio.Queue()\n\n        # Forward request to Ghost Engine\n        await workers[\"chat\"].send_json({\"id\": req_id, \"type\": \"chat\", \"data\": body})\n        await add_log_entry(\"Chat-API\", \"info\", f\"Forwarded chat request {req_id} to Ghost Engine\")\n\n        # Accumulate the stream from the browser into one big response\n        full_content = \"\"\n        while True:\n            # Wait 60s max per chunk\n            msg = await asyncio.wait_for(active_requests[req_id].get(), timeout=60.0)\n\n            # Check for stream end or error\n            if isinstance(msg, dict):\n                if msg.get(\"done\"):\n                    await add_log_entry(\"Chat-API\", \"info\", f\"Chat request {req_id} completed successfully\")\n                    break\n                if msg.get(\"error\"):\n                    # Check if it's a WebGPU adapter error and provide helpful message\n                    error_msg = msg.get(\"error\", \"\")\n                    await add_log_entry(\"Chat-API\", \"error\", f\"Chat request {req_id} error: {str(error_msg)}\")\n                    if \"No WebGPU Adapter found\" in str(error_msg) or \"GPU crash\" in str(error_msg):\n                        del active_requests[req_id]\n                        return {\n                            \"id\": req_id,\n                            \"object\": \"chat.completion\",\n                            \"created\": int(time.time()),\n                            \"choices\": [{\n                                \"index\": 0,\n                                \"message\": {\n                                    \"role\": \"assistant\",\n                                    \"content\": \"âš ï¸ WebGPU Adapter not found. This device may not support WebGPU acceleration. Try using a CPU-based model or check GPU drivers. Error: \" + str(error_msg)\n                                },\n                                \"finish_reason\": \"stop\"\n                            }]\n                        }\n                    return JSONResponse(status_code=500, content=msg)\n                # Handle direct object response (non-streaming)\n                if msg.get(\"choices\"):\n                    await add_log_entry(\"Chat-API\", \"info\", f\"Received direct response for request {req_id}\")\n                    return msg\n\n            # Append chunk content\n            if isinstance(msg, dict) and \"chunk\" in msg:\n                chunk = msg[\"chunk\"]\n                # Parse OpenAI chunk format\n                if isinstance(chunk, dict):\n                    content = chunk.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n                    full_content += content\n\n        del active_requests[req_id]\n\n        # Return standard OpenAI non-streaming response\n        await add_log_entry(\"Chat-API\", \"info\", f\"Returning completed response for request {req_id}\")\n        return {\n            \"id\": req_id,\n            \"object\": \"chat.completion\",\n            \"created\": int(time.time()),\n            \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": full_content}, \"finish_reason\": \"stop\"}]\n        }\n\n    except asyncio.TimeoutError:\n        await add_log_entry(\"Chat-API\", \"error\", f\"Chat request {req_id} timed out after 60 seconds\")\n        if req_id in active_requests: del active_requests[req_id]\n        return {\n            \"id\": req_id,\n            \"object\": \"chat.completion\",\n            \"created\": int(time.time()),\n            \"choices\": [{\n                \"index\": 0,\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": \"âš ï¸ Request timed out. The model may not be loaded or WebGPU adapter is unavailable. Please check that the Ghost Engine is running and has access to GPU resources.\"\n                },\n                \"finish_reason\": \"stop\"\n            }]\n        }\n    except Exception as e:\n        await add_log_entry(\"Chat-API\", \"error\", f\"Chat request {req_id} failed with exception: {str(e)}\")\n        if req_id in active_requests: del active_requests[req_id]\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n\n@app.post(\"/v1/memory/search\")\nasync def memory_search(request: Request):\n    if not workers[\"chat\"]:\n        await add_log_entry(\"Memory-API\", \"error\", \"Memory search requested but Ghost Engine is disconnected\")\n        return JSONResponse(status_code=503, content={\"error\": \"Ghost Engine Disconnected\"})\n\n    try:\n        body = await request.json()\n        query = body.get(\"query\", \"\")\n        req_id = str(uuid.uuid4())\n        active_requests[req_id] = asyncio.Queue()\n\n        await add_log_entry(\"Memory-API\", \"info\", f\"Forwarding memory search request '{query}' (ID: {req_id}) to Ghost Engine\")\n        await workers[\"chat\"].send_json({\"type\": \"direct_search_request\", \"id\": req_id, \"query\": query})\n\n        result = await asyncio.wait_for(active_requests[req_id].get(), timeout=15.0)\n        del active_requests[req_id]\n\n        await add_log_entry(\"Memory-API\", \"info\", f\"Memory search completed successfully for query '{query}'\")\n        return {\"status\": \"success\", \"context\": result}\n    except asyncio.TimeoutError:\n        await add_log_entry(\"Memory-API\", \"error\", f\"Memory search timed out for query '{body.get('query', '')}'\")\n        if req_id in active_requests: del active_requests[req_id]\n        return JSONResponse(status_code=504, content={\"error\": \"Search request timed out\"})\n    except Exception as e:\n        await add_log_entry(\"Memory-API\", \"error\", f\"Memory search failed with error: {str(e)}\")\n        if req_id in active_requests: del active_requests[req_id]\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n\n@app.get(\"/context\", response_class=HTMLResponse)\n@app.get(\"/sidecar\", response_class=HTMLResponse)\nasync def get_ui():\n    return FileResponse(\"tools/context.html\")\n\n# --- LOGGING ENDPOINTS ---\n@app.get(\"/logs/recent\")\nasync def get_recent_logs_endpoint():\n    \"\"\"API endpoint to get recent system logs\"\"\"\n    logs = await get_recent_logs(50)\n    return {\"logs\": logs}\n\n@app.post(\"/logs/collect\")\nasync def collect_log(request: Request):\n    \"\"\"API endpoint to collect logs from various system components\"\"\"\n    try:\n        body = await request.json()\n        source = body.get(\"source\", \"unknown\")\n        log_type = body.get(\"type\", \"info\")\n        message = body.get(\"message\", \"\")\n\n        await add_log_entry(source, log_type, message)\n\n        return {\"status\": \"success\", \"message\": \"Log entry added\"}\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"nominal\", \"engine\": \"connected\" if workers[\"chat\"] else \"waiting\", \"timestamp\": datetime.datetime.now().isoformat()}\n\n# --- WEBSOCKETS ---\n@app.websocket(\"/ws/chat\")\nasync def ws_chat(websocket: WebSocket):\n    await websocket.accept()\n    workers[\"chat\"] = websocket\n    await add_log_entry(\"WebGPU-Bridge\", \"success\", \"Ghost Engine Connected\")\n    print(\"ðŸŸ¢ Ghost Engine Connected\")\n    try:\n        while True:\n            data = await websocket.receive_text()\n            msg = json.loads(data)\n\n            # Route messages to waiting requests\n            if \"id\" in msg and msg[\"id\"] in active_requests:\n                await active_requests[msg[\"id\"]].put(msg)\n\n            # Special case for memory search results\n            if msg.get(\"type\") == \"direct_search_result\":\n                rid = msg.get(\"id\")\n                if rid in active_requests:\n                    await active_requests[rid].put(msg.get(\"result\"))\n            # Handle other message types that might come from the Ghost Engine\n            elif msg.get(\"type\") == \"engine_ready\":\n                await add_log_entry(\"Ghost-Engine\", \"success\", \"Ghost Engine Ready - Model loaded and ready for requests\")\n                print(\"ðŸ”§ Ghost Engine Ready - Model loaded and ready for requests\")\n            elif msg.get(\"type\") == \"model_loading\":\n                status = msg.get('status', 'Loading...')\n                await add_log_entry(\"Ghost-Engine\", \"info\", f\"Ghost Engine Loading: {status}\")\n                print(f\"âš™ï¸ Ghost Engine Loading: {status}\")\n            elif msg.get(\"type\") == \"error\":\n                error_msg = msg.get('message', 'Unknown error')\n                await add_log_entry(\"Ghost-Engine\", \"error\", f\"Ghost Engine Error: {error_msg}\")\n                print(f\"âŒ Ghost Engine Error: {error_msg}\")\n    except Exception as e:\n        workers[\"chat\"] = None\n        error_msg = f\"Ghost Engine Disconnected: {str(e)}\"\n        await add_log_entry(\"WebGPU-Bridge\", \"error\", error_msg)\n        print(error_msg)\n        # Attempt to clear any pending requests\n        for req_id in list(active_requests.keys()):\n            try:\n                # Put an error message in the queue to unblock waiting requests\n                await active_requests[req_id].put({\"error\": \"Ghost Engine disconnected\", \"done\": True})\n            except:\n                pass\n\napp.mount(\"/\", StaticFiles(directory=\".\", html=True), name=\"ui\")\nasync def startup_event():\n    \"\"\"Initialize logging when the app starts\"\"\"\n    await add_log_entry(\"System\", \"info\", f\"Anchor Core started on port {PORT}\")\n\n# Register the startup event\napp.on_event(\"startup\")(startup_event)\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"error\")",
    "source": "tools\\webgpu_bridge.py"
  },
  {
    "id": "tools\\modules\\agents.js",
    "timestamp": 1766951602,
    "role": "file",
    "content": "import { AnchorLogger } from './anchor.js';\n\n/**\n * THE AUDITOR (Quality Assurance)\n * Enforces the \"Clean Table\" schema defined by NotebookLM.\n * Rejects memories that don't fit the \"Law\" of the Graph.\n */\nexport class MemoryAuditor {\n    constructor() {\n        this.logger = new AnchorLogger('Auditor');\n        \n        // The \"Law\" - Derived from your NotebookLM Table & Specs\n        this.schema = {\n            required: ['id', 'timestamp', 'role', 'content'],\n            roles: ['user', 'assistant', 'system', 'file', 'manual', 'thought'],\n            constraints: {\n                id_format: /^[0-9]+-[a-z0-9]+$/, // e.g. \"1735165200000-x9s8d7f\"\n                content_min_len: 1,\n                content_max_len: 20000 // Limit for WASM stability\n            }\n        };\n    }\n\n    /**\n     * Audit a proposed memory before insertion.\n     * @param {Object} memory - The candidate memory object\n     * @returns {Object} { passed: boolean, reason: string }\n     */\n    audit(memory) {\n        // 1. Schema Check (Missing Fields)\n        for (const field of this.schema.required) {\n            if (memory[field] === undefined || memory[field] === null) {\n                return this._reject(`Missing required field: ${field}`);\n            }\n        }\n\n        // 2. Type Safety\n        if (typeof memory.id !== 'string') return this._reject(\"ID must be a string\");\n        if (typeof memory.timestamp !== 'number') return this._reject(\"Timestamp must be a number\");\n        if (typeof memory.content !== 'string') return this._reject(\"Content must be a string\");\n\n        // 3. Role Enforcement\n        if (!this.schema.roles.includes(memory.role)) {\n            return this._reject(`Invalid Role: '${memory.role}'. Allowed: ${this.schema.roles.join(', ')}`);\n        }\n\n        // 4. Content Hygiene\n        if (memory.content.length < this.schema.constraints.content_min_len) {\n            return this._reject(\"Content is empty\");\n        }\n        \n        // Auto-Truncate warning (Auditor doesn't modify, just warns/rejects)\n        if (memory.content.length > this.schema.constraints.content_max_len) {\n            this.logger.warn(`Content exceeds safety limit (${memory.content.length} chars). Truncation recommended.`);\n            // We allow it but warn, assuming Builder/Console handles truncation\n        }\n\n        // 5. Logic Check (Timestamp Sanity)\n        // If timestamp is from 1970 (0) or future (> 1 day ahead), flag it\n        const now = Date.now();\n        const oneDay = 86400000;\n        if (memory.timestamp < 1000000000000) return this._reject(\"Timestamp appears to be in seconds, not ms (or invalid)\");\n        if (memory.timestamp > now + oneDay) return this._reject(\"Timestamp is in the future\");\n\n        return { passed: true, reason: \"Valid\" };\n    }\n\n    _reject(reason) {\n        this.logger.warn(`Audit Failed: ${reason}`);\n        return { passed: false, reason };\n    }\n}",
    "source": "tools\\modules\\agents.js"
  },
  {
    "id": "tools\\modules\\anchor.js",
    "timestamp": 1767195827,
    "role": "file",
    "content": "/* tools/modules/anchor.js */\r\n\r\n// Import CozoDB bindings from the parent directory\r\nimport initWasm, { CozoDb } from '../cozo_lib_wasm.js';\r\n\r\n/**\r\n * Anchor Coda Kernel (v2.0)\r\n * Standard Library for Logging, State, Hardware, and Memory.\r\n */\r\n\r\n// --- 1. THE NERVOUS SYSTEM (Unified Logging) ---\r\nexport class AnchorLogger {\r\n    constructor(sourceId) {\r\n        this.source = sourceId;\r\n        this.logChannel = new BroadcastChannel('anchor-logs');\r\n        this.codaChannel = new BroadcastChannel('coda_logs');\r\n    }\r\n\r\n    info(msg) { this._emit(msg, 'info'); }\r\n    warn(msg) { this._emit(msg, 'warn'); }\r\n    error(msg) { this._emit(msg, 'error'); }\r\n    success(msg) { this._emit(msg, 'success'); }\r\n\r\n    _emit(msg, type) {\r\n        // 1. Console Fallback\r\n        const style = type === 'error' ? 'color:red' : (type === 'success' ? 'color:green' : 'color:blue');\r\n        console.log(`%c[${this.source}] ${msg}`, style);\r\n\r\n        // 2. Broadcast to Mission Control\r\n        const timestamp = new Date().toISOString();\r\n        const timeShort = new Date().toLocaleTimeString();\r\n\r\n        try {\r\n            // New JSON Channel (for Mission Control)\r\n            this.codaChannel.postMessage({\r\n                source: this.source,\r\n                type,\r\n                message: msg,\r\n                timestamp\r\n            });\r\n            // Legacy Channel (for Log Viewer compatibility)\r\n            this.logChannel.postMessage({\r\n                source: 'system',\r\n                msg: `[${this.source}] ${msg}`,\r\n                type,\r\n                time: timeShort\r\n            });\r\n        } catch (e) {\r\n            console.warn('Logger broadcast failed', e);\r\n        }\r\n    }\r\n}\r\n\r\n// --- 2. THE STATE MANAGER (Nano Store) ---\r\n// Zero-dependency reactive state.\r\nexport function createStore(initialState) {\r\n    const listeners = new Set();\r\n    \r\n    const proxy = new Proxy(initialState, {\r\n        set(target, property, value) {\r\n            target[property] = value;\r\n            listeners.forEach(fn => fn(property, value));\r\n            return true;\r\n        }\r\n    });\r\n\r\n    return {\r\n        state: proxy,\r\n        subscribe: (fn) => listeners.add(fn),\r\n        unsubscribe: (fn) => listeners.delete(fn)\r\n    };\r\n}\r\n\r\n// --- 3. HARDWARE DETECTOR (The XPS Fix) ---\r\n// Centralized WebGPU configuration to prevent crashes on 256MB cards.\r\nexport async function getWebGPUConfig(profile = 'mid') {\r\n    if (!navigator.gpu) {\r\n        throw new Error(\"WebGPU is not supported by this browser. Ensure you are using a modern browser (Chrome 113+, Edge 113+) and it is not disabled in flags.\");\r\n    }\r\n    \r\n    // 1. Request Adapter (Progressive Fallback)\r\n    let adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' });\r\n    \r\n    if (!adapter) {\r\n        console.warn(\"[Kernel] High-performance adapter failed. Trying default...\");\r\n        adapter = await navigator.gpu.requestAdapter();\r\n    }\r\n    \r\n    if (!adapter) {\r\n        console.warn(\"[Kernel] Default adapter failed. Trying low-power fallback...\");\r\n        adapter = await navigator.gpu.requestAdapter({ powerPreference: 'low-power' });\r\n    }\r\n    \r\n    if (!adapter) {\r\n        throw new Error(\"No WebGPU Adapter found. This often happens after a GPU crash. Please RESTART YOUR BROWSER or check for driver updates.\");\r\n    }\r\n\r\n    // 2. Detect Hardware Limits\r\n    const hardwareLimit = adapter.limits.maxStorageBufferBindingSize;\r\n    let requested = 1024 * 1024 * 1024; // Default 1GB\r\n\r\n    // 3. Apply Profile Strategy\r\n    if (profile === 'lite') requested = 256 * 1024 * 1024; // 256MB\r\n    else if (profile === 'mid') requested = 1024 * 1024 * 1024; // 1GB\r\n    else if (profile === 'high') requested = 2048 * 1024 * 1024; // 2GB\r\n\r\n    // 4. The Safety Clamp\r\n    const finalLimit = Math.min(requested, hardwareLimit);\r\n    const isConstrained = finalLimit < requested;\r\n\r\n    return {\r\n        adapter,\r\n        deviceConfig: {\r\n            requiredLimits: { maxStorageBufferBindingSize: finalLimit },\r\n            requiredFeatures: adapter.features.has(\"shader-f16\") ? [\"shader-f16\"] : []\r\n        },\r\n        maxBufferSize: finalLimit,\r\n        isConstrained\r\n    };\r\n}\r\n\r\n// --- 4. MEMORY CORE (CozoDB Helper) ---\r\nexport async function initCozo(wasmUrl = '../cozo_lib_wasm_bg.wasm') {\r\n    await initWasm(wasmUrl);\r\n    return CozoDb;\r\n}\r\n\r\n// --- 5. THE BLOCKER (GPU Mutex) ---\r\nclass GPUController {\r\n    static get BRIDGE_URL() { return 'http://localhost:8000'; }\r\n\r\n    // System Consciousness States\r\n    static States = {\r\n        IDLE: 'IDLE',\r\n        DREAMING: 'DREAMING',\r\n        COGNITION: 'COGNITION',\r\n        LISTENING: 'LISTENING'\r\n    };\r\n\r\n    static currentState = 'IDLE';\r\n    static stateChannel = new BroadcastChannel('anchor-state');\r\n\r\n    // Separate locks for different operations\r\n    static modelLoadPromise = null;  // Promise to serialize model loading\r\n    static activeModelLoaders = new Set();  // Track active model loaders\r\n\r\n    static broadcastState(newState) {\r\n        if (this.currentState === newState) return;\r\n        this.currentState = newState;\r\n        this.stateChannel.postMessage({ type: 'STATE_CHANGE', state: newState });\r\n        console.log(`[GPUController] State changed to: ${newState}`);\r\n    }\r\n\r\n    // Enhanced GPU lock with retry logic and better error handling\r\n    static async acquireLock(agentId, timeout = 120000) { // Increased default timeout to 120 seconds\r\n        const startTime = Date.now();\r\n\r\n        // 1. Determine Intended State\r\n        let intendedState = this.States.IDLE;\r\n        if (agentId.includes('Dreamer')) intendedState = this.States.DREAMING;\r\n        else if (agentId.includes('Console') || agentId.includes('Chat')) intendedState = this.States.COGNITION;\r\n        else if (agentId.includes('Mic')) intendedState = this.States.LISTENING;\r\n\r\n        // 2. SEMAPHORE CHECK (Consciousness Priority)\r\n        const highPriority = [this.States.COGNITION, this.States.LISTENING];\r\n        \r\n        // If Dreamer tries to wake while Brain/Ears are active -> REJECT IMMEDIATELY\r\n        if (intendedState === this.States.DREAMING && highPriority.includes(this.currentState)) {\r\n             return { success: false, error: `Consciousness Semaphore: System is busy (${this.currentState}). Dreamer suppressed.` };\r\n        }\r\n\r\n        // If High Priority Agent (Brain/Ears) starts -> Assert Dominance\r\n        if (highPriority.includes(intendedState)) {\r\n            this.broadcastState(intendedState);\r\n        } else if (intendedState === this.States.DREAMING) {\r\n            this.broadcastState(this.States.DREAMING);\r\n        }\r\n\r\n        while (Date.now() - startTime < timeout) {\r\n            try {\r\n                // This request will HANG until the lock is available (Queue)\r\n                const controller = new AbortController();\r\n                const timeoutId = setTimeout(() => controller.abort(), timeout);\r\n\r\n                const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/lock`, {\r\n                    method: 'POST',\r\n                    headers: {\r\n                        'Content-Type': 'application/json',\r\n                        'Authorization': 'Bearer sovereign-secret'\r\n                    },\r\n                    body: JSON.stringify({ id: agentId }),\r\n                    signal: controller.signal\r\n                });\r\n\r\n                clearTimeout(timeoutId);\r\n\r\n                if (res.ok) {\r\n                    const data = await res.json();\r\n                    return { success: true, token: data.token };\r\n                }\r\n\r\n                // Handle specific error codes\r\n                if (res.status === 503) {\r\n                    const errorData = await res.json();\r\n                    return { success: false, error: errorData.msg || `Queue Timeout (${res.status})` };\r\n                }\r\n\r\n                return { success: false, error: `GPU Lock Failed (${res.status})` };\r\n            } catch (e) {\r\n                if (e.name === 'AbortError') {\r\n                    return { success: false, error: 'Lock acquisition timeout' };\r\n                }\r\n\r\n                console.warn(\"Bridge unreachable (No Lock)\", e);\r\n\r\n                // If bridge is down, try direct WebGPU access as fallback\r\n                if (e.message.includes('fetch') || e.message.includes('network')) {\r\n                    console.warn(\"Bridge offline, attempting direct WebGPU access...\");\r\n                    return { success: true, token: \"direct-webgpu-fallback\" };\r\n                }\r\n\r\n                return { success: false, error: e.message };\r\n            }\r\n\r\n            // Small delay before retry to avoid excessive polling\r\n            await new Promise(resolve => setTimeout(resolve, 1000));\r\n        }\r\n\r\n        return { success: false, error: `Lock acquisition timeout after ${timeout}ms` };\r\n    }\r\n\r\n    static async releaseLock(agentId) {\r\n        try {\r\n            await fetch(`${this.BRIDGE_URL}/v1/gpu/unlock`, {\r\n                method: 'POST',\r\n                headers: {\r\n                    'Content-Type': 'application/json',\r\n                    'Authorization': 'Bearer sovereign-secret'\r\n                },\r\n                body: JSON.stringify({ id: agentId }),\r\n                keepalive: true // Critical: Ensure request survives tab close\r\n            });\r\n        } catch (e) {\r\n            console.warn(\"Failed to release lock\", e);\r\n            // Don't throw error on release failure to avoid blocking cleanup\r\n        } finally {\r\n             // Reset state to IDLE if a high-priority agent is finishing\r\n             if (agentId.includes('Console') || agentId.includes('Chat') || agentId.includes('Mic')) {\r\n                 this.broadcastState(this.States.IDLE);\r\n             }\r\n        }\r\n    }\r\n\r\n    // Enhanced withLock with better error handling and retry logic\r\n    static async withLock(agentId, taskFn, timeout = 120000) {\r\n        const lock = await this.acquireLock(agentId, timeout);\r\n        if (!lock.success) {\r\n            console.error(`GPU lock acquisition failed for ${agentId}: ${lock.error}`);\r\n            throw new Error(`Could not acquire GPU lock: ${lock.error}`);\r\n        }\r\n\r\n        let taskResult;\r\n        let taskError;\r\n\r\n        try {\r\n            taskResult = await taskFn();\r\n        } catch (e) {\r\n            taskError = e;\r\n        } finally {\r\n            // Always try to release the lock, even if the task fails\r\n            await this.releaseLock(agentId);\r\n        }\r\n\r\n        if (taskError) {\r\n            throw taskError;\r\n        }\r\n\r\n        return taskResult;\r\n    }\r\n\r\n    // NEW: Serialize model loading to prevent multiple models loading simultaneously\r\n    static async withModelLoadLock(agentId, taskFn, timeout = 300000) { // 5-minute timeout for model loading\r\n        // Create a promise chain to serialize model loading at the application level\r\n        // This ensures only one model loading operation happens at a time across all components\r\n        const previousLoad = this.modelLoadPromise;\r\n\r\n        // Create a new promise that waits for the previous one to complete\r\n        this.modelLoadPromise = (async () => {\r\n            if (previousLoad) {\r\n                try {\r\n                    await previousLoad;\r\n                } catch (e) {\r\n                    console.warn(\"Previous model load failed, continuing:\", e);\r\n                }\r\n            }\r\n\r\n            // Mark this loader as active\r\n            this.activeModelLoaders.add(agentId);\r\n\r\n            try {\r\n                console.log(`[${agentId}] Starting sequential model loading (Queue: ${this.activeModelLoaders.size - 1} waiting)`);\r\n                // The task function itself will handle GPU lock acquisition as needed\r\n                const result = await taskFn();\r\n                console.log(`[${agentId}] Sequential model loading completed`);\r\n                return result;\r\n            } finally {\r\n                // Remove from active loaders\r\n                this.activeModelLoaders.delete(agentId);\r\n            }\r\n        })();\r\n\r\n        try {\r\n            return await this.modelLoadPromise;\r\n        } catch (error) {\r\n            // Clean up on error\r\n            this.activeModelLoaders.delete(agentId);\r\n            throw error;\r\n        }\r\n    }\r\n\r\n    // Get status of model loading queue\r\n    static getModelLoadStatus() {\r\n        return {\r\n            queueSize: this.activeModelLoaders.size,\r\n            activeLoaders: Array.from(this.activeModelLoaders),\r\n            hasPendingLoad: this.modelLoadPromise !== null,\r\n            modelLoadQueueInfo: `Model Load Queue: ${this.activeModelLoaders.size} active, ${this.modelLoadPromise ? 'loading' : 'idle'}`\r\n        };\r\n    }\r\n\r\n    // Get mind blanking status\r\n    static getMindBlankingStatus() {\r\n        const isBlanking = Date.now() < this.blankingUntil;\r\n        const remainingTime = Math.max(0, this.blankingUntil - Date.now());\r\n        return {\r\n            isBlanking,\r\n            blankingUntil: new Date(this.blankingUntil).toISOString(),\r\n            remainingMs: remainingTime,\r\n            remainingSeconds: Math.ceil(remainingTime / 1000),\r\n            lastIntensiveTask: this.lastIntensiveTask,\r\n            blankingDuration: this.BLANKING_DURATION\r\n        };\r\n    }\r\n\r\n    // New: Check GPU status to help with debugging\r\n    static async checkStatus() {\r\n        try {\r\n            const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/status`, {\r\n                method: 'GET',\r\n                headers: {\r\n                    'Authorization': 'Bearer sovereign-secret'\r\n                }\r\n            });\r\n\r\n            if (res.ok) {\r\n                const bridgeStatus = await res.json();\r\n                // Add mind blanking status to the bridge status\r\n                const blankingStatus = this.getMindBlankingStatus();\r\n                return {\r\n                    ...bridgeStatus,\r\n                    mindBlanking: blankingStatus\r\n                };\r\n            }\r\n            return { error: `Status check failed (${res.status})` };\r\n        } catch (e) {\r\n            return { error: e.message };\r\n        }\r\n    }\r\n}\r\n\r\n// Ensure explicit export if previous style failed in some browsers\r\nexport { GPUController };\r\n",
    "source": "tools\\modules\\anchor.js"
  },
  {
    "id": "tools\\modules\\gpu-hot-reloader.js",
    "timestamp": 1767195827,
    "role": "file",
    "content": "/*\r\n * GPU Management Hot Reloader\r\n * Provides hot reload capability for GPU management in the browser\r\n */\r\n\r\nclass GPUHotReloader {\r\n    constructor() {\r\n        this.gpuController = null;\r\n        this.checkInterval = 5000; // Check every 5 seconds\r\n        this.lastModified = {};\r\n        this.isReloading = false;\r\n        this.reloaderEnabled = true;\r\n    }\r\n\r\n    // Enable/disable hot reload\r\n    setEnabled(enabled) {\r\n        this.reloaderEnabled = enabled;\r\n        if (enabled) {\r\n            this.startMonitoring();\r\n        } else {\r\n            this.stopMonitoring();\r\n        }\r\n    }\r\n\r\n    // Start monitoring for changes\r\n    startMonitoring() {\r\n        if (this.monitorInterval) return;\r\n\r\n        console.log(\"ðŸ”„ GPU Hot Reloader: Starting monitoring...\");\r\n        this.monitorInterval = setInterval(() => {\r\n            this.checkForChanges();\r\n        }, this.checkInterval);\r\n    }\r\n\r\n    // Stop monitoring\r\n    stopMonitoring() {\r\n        if (this.monitorInterval) {\r\n            clearInterval(this.monitorInterval);\r\n            this.monitorInterval = null;\r\n            console.log(\"ðŸ›‘ GPU Hot Reloader: Stopped monitoring\");\r\n        }\r\n    }\r\n\r\n    // Check for changes in GPU-related files\r\n    async checkForChanges() {\r\n        if (!this.reloaderEnabled || this.isReloading) return;\r\n\r\n        try {\r\n            // Check if any GPU-related files have been modified\r\n            const gpuFiles = [\r\n                'tools/modules/anchor.js',\r\n                'tools/model-server-chat.html',\r\n                'tools/anchor-mic.html',\r\n                'tools/memory-builder.html'\r\n            ];\r\n\r\n            for (const file of gpuFiles) {\r\n                const modified = await this.checkFileModified(file);\r\n                if (modified) {\r\n                    console.log(`ðŸ”„ GPU Hot Reloader: Detected change in ${file}`);\r\n                    await this.reloadGPUManagement();\r\n                    break; // Only reload once per check cycle\r\n                }\r\n            }\r\n        } catch (error) {\r\n            console.warn('GPU Hot Reloader: Error checking for changes:', error);\r\n        }\r\n    }\r\n\r\n    // Check if a file has been modified since last check\r\n    async checkFileModified(filepath) {\r\n        try {\r\n            // Try to check file modification time via server endpoint\r\n            const response = await fetch(`http://localhost:8080/file-mod-time?path=${encodeURIComponent(filepath)}`);\r\n            if (response.ok) {\r\n                const data = await response.json();\r\n                const currentModTime = data.modTime;\r\n\r\n                if (filepath in this.lastModified) {\r\n                    if (this.lastModified[filepath] !== currentModTime) {\r\n                        this.lastModified[filepath] = currentModTime;\r\n                        return true;\r\n                    }\r\n                } else {\r\n                    this.lastModified[filepath] = currentModTime;\r\n                }\r\n            }\r\n        } catch (error) {\r\n            // Fallback: try to re-fetch and compare content\r\n            try {\r\n                const response = await fetch(filepath + '?t=' + Date.now(), { method: 'HEAD' });\r\n                const lastModified = response.headers.get('Last-Modified');\r\n\r\n                if (filepath in this.lastModified) {\r\n                    if (this.lastModified[filepath] !== lastModified) {\r\n                        this.lastModified[filepath] = lastModified;\r\n                        return true;\r\n                    }\r\n                } else {\r\n                    this.lastModified[filepath] = lastModified;\r\n                }\r\n            } catch (e) {\r\n                // If we can't check, assume no change to avoid constant reloads\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n\r\n    // Reload GPU management logic\r\n    async reloadGPUManagement() {\r\n        if (this.isReloading) return;\r\n\r\n        this.isReloading = true;\r\n        console.log(\"ðŸ”„ GPU Hot Reloader: Reloading GPU management logic...\");\r\n\r\n        try {\r\n            // Force release any current GPU locks to prevent stale state\r\n            await this.forceReleaseGPU();\r\n\r\n            // Clear any cached modules if possible\r\n            this.clearModuleCache();\r\n\r\n            // Reload the GPU controller with fresh logic\r\n            await this.reloadGPUController();\r\n\r\n            console.log(\"âœ… GPU Hot Reloader: GPU management reloaded successfully\");\r\n        } catch (error) {\r\n            console.error(\"âŒ GPU Hot Reloader: Error during reload:\", error);\r\n        } finally {\r\n            this.isReloading = false;\r\n        }\r\n    }\r\n\r\n    // Force release current GPU locks\r\n    async forceReleaseGPU() {\r\n        try {\r\n            // Call the emergency release endpoint if available\r\n            await fetch(\"http://localhost:8080/v1/gpu/force-release-all\", {\r\n                method: \"POST\",\r\n                headers: { \"Authorization\": \"Bearer sovereign-secret\" }\r\n            });\r\n            console.log(\"âœ… GPU Hot Reloader: Force released all GPU locks\");\r\n        } catch (e) {\r\n            console.warn(\"âš ï¸ GPU Hot Reloader: Could not force release GPU locks:\", e);\r\n        }\r\n    }\r\n\r\n    // Clear any cached modules (attempt to force reload)\r\n    clearModuleCache() {\r\n        // In a real implementation, this would clear module caches\r\n        // For now, we'll just log\r\n        console.log(\"ðŸ§¹ GPU Hot Reloader: Clearing module cache (not implemented in browser)\");\r\n    }\r\n\r\n    // Reload GPU controller with fresh logic\r\n    async reloadGPUController() {\r\n        // Since we can't truly reload modules in the browser,\r\n        // we'll trigger a soft reload of GPU state\r\n        if (window.GPUController) {\r\n            // If there's a way to refresh the GPU controller state, do it here\r\n            console.log(\"ðŸ”„ GPU Hot Reloader: Refreshing GPU controller state\");\r\n        }\r\n    }\r\n\r\n    // Manual trigger for hot reload\r\n    async triggerReload() {\r\n        console.log(\"ðŸ”„ GPU Hot Reloader: Manual reload triggered\");\r\n        await this.reloadGPUManagement();\r\n    }\r\n}\r\n\r\n// Global instance\r\nwindow.GPU_HOT_RELOADER = new GPUHotReloader();\r\n\r\n// Auto-start if in development mode\r\nif (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {\r\n    window.GPU_HOT_RELOADER.startMonitoring();\r\n    console.log(\"ðŸ”„ GPU Hot Reloader: Auto-started in development mode\");\r\n}\r\n\r\n// Expose for manual control\r\nwindow.triggerGPUHotReload = () => window.GPU_HOT_RELOADER.triggerReload();\r\nwindow.setGPUHotReloadEnabled = (enabled) => window.GPU_HOT_RELOADER.setEnabled(enabled);\r\n\r\nconsole.log(\"ðŸ”„ GPU Hot Reloader: Initialized and ready\");",
    "source": "tools\\modules\\gpu-hot-reloader.js"
  },
  {
    "id": "tools\\modules\\llm-engine-manager.js",
    "timestamp": 1766610426,
    "role": "file",
    "content": "/*\n * LLM Engine Manager\n * Provides lifecycle management for LLM engine instances with hibernation capabilities\n */\n\nclass LLMEngineManager {\n    constructor() {\n        this.engines = new Map(); // Map of componentId -> engine instance\n        this.engineWorkers = new Map(); // Map of componentId -> worker instance\n        this.engineConfigs = new Map(); // Map of componentId -> config\n        this.engineStates = new Map(); // Map of componentId -> state (loaded, hibernated, etc.)\n    }\n\n    // Create a new engine instance for a specific component\n    async createEngine(componentId, modelConfig, webllmModule, options = {}) {\n        try {\n            // Store the config for potential hibernation/resume\n            this.engineConfigs.set(componentId, modelConfig);\n\n            // Create a dedicated worker for this component\n            const worker = new Worker('./modules/llm-worker.js', { type: 'module' });\n            this.engineWorkers.set(componentId, worker);\n\n            // Create the engine using the provided webllm module\n            const engine = await this.createWebWorkerMLCEngine(worker, modelConfig.model_id, webllmModule, {\n                appConfig: { model_list: [modelConfig] },\n                ...options\n            });\n\n            // Store the engine\n            this.engines.set(componentId, engine);\n            this.engineStates.set(componentId, 'loaded');\n\n            return engine;\n        } catch (error) {\n            console.error(`Failed to create engine for ${componentId}:`, error);\n            throw error;\n        }\n    }\n\n    // Get an existing engine instance\n    getEngine(componentId) {\n        return this.engines.get(componentId);\n    }\n\n    // Check if an engine is loaded\n    isLoaded(componentId) {\n        return this.engineStates.get(componentId) === 'loaded';\n    }\n\n    // Hibernate an engine (save its state and dispose resources)\n    async hibernate(componentId) {\n        const engine = this.engines.get(componentId);\n        if (!engine) return false;\n\n        try {\n            // In a real implementation, we would save the engine state here\n            // For now, we'll just dispose the engine and mark as hibernated\n            if (typeof engine.dispose === 'function') {\n                await engine.dispose();\n            }\n\n            this.engineStates.set(componentId, 'hibernated');\n            return true;\n        } catch (error) {\n            console.error(`Failed to hibernate engine for ${componentId}:`, error);\n            return false;\n        }\n    }\n\n    // Resume an engine from hibernation\n    async resume(componentId) {\n        if (this.engineStates.get(componentId) !== 'hibernated') {\n            // If not hibernated, try to get existing engine\n            return this.getEngine(componentId);\n        }\n\n        const config = this.engineConfigs.get(componentId);\n        if (!config) {\n            throw new Error(`No config found for hibernated engine: ${componentId}`);\n        }\n\n        // Recreate the engine\n        return await this.createEngine(componentId, config);\n    }\n\n    // Dispose an engine completely\n    async dispose(componentId) {\n        const engine = this.engines.get(componentId);\n        const worker = this.engineWorkers.get(componentId);\n\n        try {\n            if (engine && typeof engine.dispose === 'function') {\n                await engine.dispose();\n            }\n        } catch (error) {\n            console.warn(`Error disposing engine for ${componentId}:`, error);\n        }\n\n        try {\n            if (worker && typeof worker.terminate === 'function') {\n                worker.terminate();\n            }\n        } catch (error) {\n            console.warn(`Error terminating worker for ${componentId}:`, error);\n        }\n\n        // Clean up maps\n        this.engines.delete(componentId);\n        this.engineWorkers.delete(componentId);\n        this.engineConfigs.delete(componentId);\n        this.engineStates.delete(componentId);\n\n        return true;\n    }\n\n    // Get all component IDs\n    getComponentIds() {\n        return Array.from(this.engines.keys());\n    }\n\n    // Create WebWorkerMLCEngine with error handling\n    async createWebWorkerMLCEngine(worker, modelId, webllmModule, options) {\n        // Use the provided webllm module to create the engine\n        if (webllmModule && typeof webllmModule.CreateWebWorkerMLCEngine !== 'undefined') {\n            return await webllmModule.CreateWebWorkerMLCEngine(worker, modelId, options);\n        } else if (webllmModule && typeof webllmModule.CreateMLCEngine !== 'undefined') {\n            // Fallback to CreateMLCEngine if CreateWebWorkerMLCEngine is not available\n            return await webllmModule.CreateMLCEngine(modelId, options);\n        } else {\n            throw new Error('WebLLM module not provided or does not contain required engine creation functions');\n        }\n    }\n\n    // Save engine state to storage (simplified approach)\n    saveStateToStorage() {\n        const state = {\n            components: Array.from(this.engineStates.entries()),\n            configs: Array.from(this.engineConfigs.entries())\n        };\n\n        try {\n            localStorage.setItem('llm_engine_manager_state', JSON.stringify(state));\n            return true;\n        } catch (error) {\n            console.error('Failed to save engine state:', error);\n            return false;\n        }\n    }\n\n    // Load engine state from storage\n    loadStateFromStorage() {\n        try {\n            const stateStr = localStorage.getItem('llm_engine_manager_state');\n            if (!stateStr) return false;\n\n            const state = JSON.parse(stateStr);\n\n            // Restore states and configs\n            state.components.forEach(([id, status]) => {\n                this.engineStates.set(id, status);\n            });\n\n            state.configs.forEach(([id, config]) => {\n                this.engineConfigs.set(id, config);\n            });\n\n            return true;\n        } catch (error) {\n            console.error('Failed to load engine state:', error);\n            return false;\n        }\n    }\n}\n\n// Export the class for module usage\nexport { LLMEngineManager };\n\n// Also make it available globally for backward compatibility\nif (typeof window !== 'undefined') {\n    window.LLMEngineManager = LLMEngineManager;\n    console.log('ðŸ”„ LLM Engine Manager: Module loaded');\n}",
    "source": "tools\\modules\\llm-engine-manager.js"
  },
  {
    "id": "tools\\modules\\llm-worker.js",
    "timestamp": 1766916224,
    "role": "file",
    "content": "// --- CACHE OVERRIDE: Prevent Cache API usage in worker thread ---\r\n// This is critical for WebLLM to work in environments with strict cache policies\r\nif ('caches' in self) {\r\n    try {\r\n        // Override the Cache API to prevent WebLLM from using it in the worker\r\n        const originalAdd = Cache.prototype.add;\r\n        const originalAddAll = Cache.prototype.addAll;\r\n        const originalPut = Cache.prototype.put;\r\n\r\n        Cache.prototype.add = function(request) {\r\n            console.warn(\"Worker: Cache.add blocked by Root Coda security override\");\r\n            return Promise.resolve();\r\n        };\r\n\r\n        Cache.prototype.addAll = function(requests) {\r\n            console.warn(\"Worker: Cache.addAll blocked by Root Coda security override\");\r\n            return Promise.resolve();\r\n        };\r\n\r\n        Cache.prototype.put = function(request, response) {\r\n            console.warn(\"Worker: Cache.put blocked by Root Coda security override\");\r\n            return Promise.resolve();\r\n        };\r\n\r\n        console.log(\"ðŸ›¡ï¸ Worker: Cache API overridden to prevent tracking prevention errors\");\r\n    } catch (e) {\r\n        console.warn(\"Worker: Could not override Cache API:\", e);\r\n    }\r\n}\r\n\r\nimport { WebWorkerMLCEngineHandler, MLCEngine } from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n// The handler bridges messages between the Main Thread and the Engine\r\nconst engine = new MLCEngine();\r\nconst handler = new WebWorkerMLCEngineHandler(engine);\r\n\r\nself.onmessage = (msg) => {\r\n    handler.onmessage(msg);\r\n};",
    "source": "tools\\modules\\llm-worker.js"
  },
  {
    "id": "tools\\modules\\vision.js",
    "timestamp": 1766692821,
    "role": "file",
    "content": "/**\r\n * VisionController - Handles drag-and-drop, clipboard paste, and image preview logic\r\n */\r\nexport class VisionController {\r\n    constructor() {\r\n        this.imageBase64 = null;\r\n        this.dropZone = null;\r\n        this.previewContainer = null;\r\n        this.inputId = null;\r\n    }\r\n\r\n    setup(dropZoneId, previewContainerId, inputId) {\r\n        this.dropZone = document.getElementById(dropZoneId);\r\n        this.previewContainer = document.getElementById(previewContainerId);\r\n        this.inputId = inputId;\r\n\r\n        if (!this.dropZone || !this.previewContainer) {\r\n            console.error('VisionController: Required elements not found');\r\n            return;\r\n        }\r\n\r\n        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\r\n            this.dropZone.addEventListener(eventName, (e) => { e.preventDefault(); e.stopPropagation(); }, false);\r\n            document.body.addEventListener(eventName, (e) => { e.preventDefault(); e.stopPropagation(); }, false);\r\n        });\r\n\r\n        // Visual feedback\r\n        ['dragenter', 'dragover'].forEach(eventName => {\r\n            this.dropZone.addEventListener(eventName, () => this.dropZone.classList.add('drag-active'), false);\r\n        });\r\n\r\n        ['dragleave', 'drop'].forEach(eventName => {\r\n            this.dropZone.addEventListener(eventName, () => this.dropZone.classList.remove('drag-active'), false);\r\n        });\r\n\r\n        this.dropZone.addEventListener('drop', (e) => this.handleDrop(e), false);\r\n        document.addEventListener('paste', (e) => this.handlePaste(e));\r\n\r\n        // File Input Fallback\r\n        const fileInput = document.createElement('input');\r\n        fileInput.type = 'file';\r\n        fileInput.accept = 'image/*';\r\n        fileInput.style.display = 'none';\r\n        fileInput.id = 'vision-file-input';\r\n        document.body.appendChild(fileInput);\r\n\r\n        const uploadButton = document.getElementById('image-upload-btn');\r\n        if (uploadButton) {\r\n            uploadButton.addEventListener('click', (e) => {\r\n                e.preventDefault();\r\n                fileInput.click();\r\n            });\r\n        }\r\n\r\n        fileInput.addEventListener('change', (e) => {\r\n            if (e.target.files && e.target.files[0]) this.handleFile(e.target.files[0]);\r\n        });\r\n    }\r\n\r\n    handleDrop(e) {\r\n        const dt = e.dataTransfer;\r\n        const files = dt.files;\r\n        if (files.length > 0) this.handleFile(files[0]);\r\n    }\r\n\r\n    handlePaste(e) {\r\n        const items = e.clipboardData.items;\r\n        for (let i = 0; i < items.length; i++) {\r\n            if (items[i].type.indexOf('image') !== -1) {\r\n                const blob = items[i].getAsFile();\r\n                if (blob) {\r\n                    this.handleFile(blob);\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    handleFile(file) {\r\n        if (!file.type.match('image.*')) {\r\n            console.error('VisionController: Only image files are supported');\r\n            return;\r\n        }\r\n        const reader = new FileReader();\r\n        reader.onload = (e) => {\r\n            this.imageBase64 = e.target.result;\r\n            this.showPreview(this.imageBase64, file.name);\r\n        };\r\n        reader.readAsDataURL(file);\r\n    }\r\n\r\n    showPreview(base64Url, fileName) {\r\n        if (!this.previewContainer) return;\r\n        this.previewContainer.innerHTML = '';\r\n        this.previewContainer.style.display = 'block'; // Show container\r\n\r\n        const previewDiv = document.createElement('div');\r\n        previewDiv.className = 'image-preview';\r\n        \r\n        const img = document.createElement('img');\r\n        img.src = base64Url;\r\n        img.style.maxWidth = '100px';\r\n        img.style.maxHeight = '100px';\r\n        img.style.borderRadius = '4px';\r\n        \r\n        const removeBtn = document.createElement('button');\r\n        removeBtn.textContent = 'âŒ';\r\n        removeBtn.style.marginLeft = '10px';\r\n        removeBtn.onclick = () => this.clear();\r\n        \r\n        previewDiv.appendChild(img);\r\n        previewDiv.appendChild(removeBtn);\r\n        this.previewContainer.appendChild(previewDiv);\r\n    }\r\n\r\n    clear() {\r\n        this.imageBase64 = null;\r\n        if (this.previewContainer) {\r\n            this.previewContainer.innerHTML = '';\r\n            this.previewContainer.style.display = 'none';\r\n        }\r\n        const fileInput = document.getElementById('vision-file-input');\r\n        if (fileInput) fileInput.value = '';\r\n    }\r\n\r\n    getImage() {\r\n        return this.imageBase64;\r\n    }\r\n}",
    "source": "tools\\modules\\vision.js"
  },
  {
    "id": "tools\\modules\\whisper-worker.js",
    "timestamp": 1766544064,
    "role": "file",
    "content": "import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\n\n// Configure for WASM\nenv.allowLocalModels = false;\nenv.useBrowserCache = true;\n\nclass WhisperWorker {\n    constructor() {\n        this.pipe = null;\n    }\n\n    async init() {\n        if (!this.pipe) {\n            console.log(\"[WhisperWorker] Loading model...\");\n            this.pipe = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', { \n                device: 'wasm'\n            });\n            console.log(\"[WhisperWorker] Model loaded.\");\n        }\n    }\n\n    async transcribe(audioData) {\n        if (!this.pipe) await this.init();\n        \n        console.log(\"[WhisperWorker] Processing audio...\");\n        const result = await this.pipe(audioData, { \n            language: 'english',\n            chunk_length_s: 30,\n            stride_length_s: 5\n        });\n        \n        return result.text.trim();\n    }\n}\n\nconst worker = new WhisperWorker();\n\nself.onmessage = async (e) => {\n    const { type, data, id } = e.data;\n\n    try {\n        if (type === 'init') {\n            await worker.init();\n            self.postMessage({ type: 'init_done', id });\n        } \n        else if (type === 'transcribe') {\n            const text = await worker.transcribe(data);\n            self.postMessage({ type: 'transcribe_result', text, id });\n        }\n    } catch (err) {\n        self.postMessage({ type: 'error', error: err.message, id });\n    }\n};",
    "source": "tools\\modules\\whisper-worker.js"
  }
]